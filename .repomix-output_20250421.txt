This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-04-21T09:00:07.359Z

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
.cursorignore
.gitignore
app.py
config.py
curve_processing.py
data_loader.py
data_validation.py
fileoveright.md
issue_processing.py
LICENSE
metric_calculator.py
process_data.py
process_w_secs.py
process_weights.py
README.md
requirements.txt
security_processing.py
static/css/style.css
static/js/main.js
static/js/modules/charts/timeSeriesChart.js
static/js/modules/ui/chartRenderer.js
static/js/modules/ui/securityTableFilter.js
static/js/modules/ui/tableSorter.js
static/js/modules/utils/helpers.js
templates/attribution_charts.html
templates/attribution_radar.html
templates/attribution_security_page.html
templates/attribution_summary.html
templates/base.html
templates/comparison_details_page.html
templates/comparison_page.html
templates/curve_details.html
templates/curve_summary.html
templates/delete_metric_page.html
templates/duration_comparison_details_page.html
templates/duration_comparison_page.html
templates/exclusions_page.html
templates/fund_detail_page.html
templates/fund_duration_details.html
templates/get_data.html
templates/index.html
templates/issues_page.html
templates/metric_page_js.html
templates/securities_page.html
templates/security_details_page.html
templates/spread_duration_comparison_details_page.html
templates/spread_duration_comparison_page.html
templates/weight_check_page.html
utils.py
Version2.md
views/__init__.py
views/api_core.py
views/api_routes_call.py
views/api_routes_data.py
views/api_views.py
views/attribution_views.py
views/comparison_views.py
views/curve_views.py
views/duration_comparison_views.py
views/exclusion_views.py
views/fund_views.py
views/issue_views.py
views/main_views.py
views/metric_views.py
views/security_views.py
views/spread_duration_comparison_views.py
views/weight_views.py
weight_processing.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".cursorignore">
# Add directories or file patterns to ignore during indexing (e.g. foo/ or *.csv)
</file>

<file path=".gitignore">
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class
*.csv
*.xlsx

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
# Usually these files are written by a python script from a template
# before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
.hypothesis/
.pytest_cache/

# Translations
*.mo
*.pot
*.log

# Django stuff:
*.log
local_settings.py
db.sqlite3

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
.python-version

# celery beat schedule file
celerybeat-schedule

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/
</file>

<file path="app.py">
# This file defines the main entry point and structure for the Simple Data Checker Flask web application.
# It utilizes the Application Factory pattern (`create_app`) to initialize and configure the Flask app.
# Key responsibilities include:
# - Creating the Flask application instance.
# - Setting up basic configuration (like the secret key).
# - Ensuring necessary folders (like the instance folder) exist.
# - Determining and configuring the absolute data folder path using `utils.get_data_folder_path`.
# - Centralizing logging configuration (File and Console handlers).
# - Registering Blueprints (`main_bp`, `metric_bp`, `security_bp`, `fund_bp`, `exclusion_bp`, `comparison_bp`, `duration_comparison_bp`, `spread_duration_comparison_bp`, `api_bp`, `weight_bp`) from the `views`
#   directory, which contain the application\'s routes and view logic.
# - Providing a conditional block (`if __name__ == '__main__':`) to run the development server
#   when the script is executed directly.
# This modular structure using factories and blueprints makes the application more organized and scalable.
# This file contains the main Flask application factory.
from flask import Flask, render_template, Blueprint, jsonify
import os
import logging
from logging.handlers import RotatingFileHandler # Import handler
# --- Add imports for the new route ---
import subprocess
import sys # To get python executable path
import json
import threading
import time
import datetime
# --- End imports ---
# Import configurations and utilities
from config import COLOR_PALETTE # Import other needed configs
from utils import get_data_folder_path # Import the path utility
def create_app():
    """Factory function to create and configure the Flask app."""
    app = Flask(__name__, instance_relative_config=True) # instance_relative_config=True allows for instance folder config
    app.logger.info(f"Application root path: {app.root_path}")
    # Basic configuration (can be expanded later, e.g., loading from config file)
    app.config.from_mapping(
        SECRET_KEY='dev', # Default secret key for development. CHANGE for production!
        # Add other default configurations if needed
    )
    # Load configuration from config.py (e.g., COLOR_PALETTE)
    app.config.from_object('config')
    app.logger.info("Loaded configuration from config.py")
    # --- Determine and set the Data Folder Path --- 
    # Use the utility function to get the absolute data path, using the app's root path as the base
    # This ensures consistency whether the path in config.py is relative or absolute
    absolute_data_path = get_data_folder_path(app_root_path=app.root_path)
    app.config['DATA_FOLDER'] = absolute_data_path
    app.logger.info(f"Data folder path set to: {app.config['DATA_FOLDER']}")
    # --- End Data Folder Path Setup ---
    # Ensure the instance folder exists (needed for logging)
    try:
        os.makedirs(app.instance_path, exist_ok=True) # exist_ok=True prevents error if exists
        app.logger.info(f"Instance folder ensured at: {app.instance_path}")
    except OSError as e:
        app.logger.error(f"Could not create instance folder at {app.instance_path}: {e}", exc_info=True)
        # Depending on severity, might want to raise an exception or exit
    # --- Centralized Logging Configuration --- 
    # Remove Flask's default handlers
    app.logger.handlers.clear()
    app.logger.setLevel(logging.DEBUG) # Set the app logger level (DEBUG captures everything)
    # Formatter
    log_formatter = logging.Formatter(
        '%(asctime)s %(levelname)s: %(message)s [in %(pathname)s:%(lineno)d]'
    )
    # File Handler (Rotating)
    log_file_path = os.path.join(app.instance_path, 'app.log')
    max_log_size = 1024 * 1024 * 10 # 10 MB
    backup_count = 5
    try:
        file_handler = RotatingFileHandler(
            log_file_path,
            maxBytes=max_log_size,
            backupCount=backup_count
        )
        file_handler.setFormatter(log_formatter)
        file_handler.setLevel(logging.DEBUG) # Log DEBUG and higher to file
        app.logger.addHandler(file_handler)
        app.logger.info(f"File logging configured to: {log_file_path} (Level: DEBUG)")
    except Exception as e:
        app.logger.error(f"Failed to configure file logging to {log_file_path}: {e}", exc_info=True)
    # Console Handler
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(log_formatter)
    # Set console level potentially higher for less noise (e.g., INFO) or keep DEBUG for development
    console_handler.setLevel(logging.DEBUG)
    app.logger.addHandler(console_handler)
    app.logger.info("Centralized logging configured (File & Console).")
    # --- End Logging Configuration ---
    # Configure logging (consider moving to a dedicated logging setup function)
    # Note: BasicConfig should ideally be called only once. If utils.py also calls it,
    # it might conflict or be ineffective here. A more robust setup is recommended.
    # logging.basicConfig(level=logging.INFO,
    #                     format='%(asctime)s %(levelname)s %(name)s %(threadName)s : %(message)s')
    # app.logger.info("Logging configured.") # BasicConfig might be configured in utils already
    # Serve static files (for JS, CSS, etc.)
    # Note: static_url_path defaults to /static, static_folder defaults to 'static' in root
    # No need to set app.static_folder = 'static' explicitly unless changing the folder name/path
    # --- Register Blueprints ---
    from views.main_views import main_bp
    from views.metric_views import metric_bp
    from views.security_views import security_bp
    from views.fund_views import fund_bp
    from views.api_views import api_bp
    from views.exclusion_views import exclusion_bp
    from views.comparison_views import comparison_bp
    from views.weight_views import weight_bp
    # --- Import new blueprints ---
    from views.duration_comparison_views import duration_comparison_bp
    from views.spread_duration_comparison_views import spread_duration_comparison_bp
    # --- End import new blueprints ---
    from views.curve_views import curve_bp # Import the new blueprint
    from views.issue_views import issue_bp # Import the issue tracking blueprint
    from views.attribution_views import attribution_bp # Import the attribution blueprint
    app.register_blueprint(main_bp)
    app.register_blueprint(metric_bp)
    app.register_blueprint(security_bp)
    app.register_blueprint(fund_bp)
    app.register_blueprint(api_bp)
    app.register_blueprint(exclusion_bp)
    app.register_blueprint(comparison_bp)
    app.register_blueprint(weight_bp)
    # --- Register new blueprints ---
    app.register_blueprint(duration_comparison_bp)
    app.register_blueprint(spread_duration_comparison_bp)
    # --- End register new blueprints ---
    app.register_blueprint(curve_bp) # Register the new blueprint
    app.register_blueprint(issue_bp) # Register the issue tracking blueprint
    app.register_blueprint(attribution_bp) # Register the attribution blueprint
    app.logger.info("Registered Blueprints:")
    app.logger.info(f"- {main_bp.name} (prefix: {main_bp.url_prefix})")
    app.logger.info(f"- {metric_bp.name} (prefix: {metric_bp.url_prefix})")
    app.logger.info(f"- {security_bp.name} (prefix: {security_bp.url_prefix})")
    app.logger.info(f"- {fund_bp.name} (prefix: {fund_bp.url_prefix})")
    app.logger.info(f"- {api_bp.name} (prefix: {api_bp.url_prefix})")
    app.logger.info(f"- {exclusion_bp.name} (prefix: {exclusion_bp.url_prefix})")
    app.logger.info(f"- {comparison_bp.name} (prefix: {comparison_bp.url_prefix})")
    app.logger.info(f"- {weight_bp.name} (prefix: {weight_bp.url_prefix})")
    # --- Print new blueprints ---
    print(f"- {duration_comparison_bp.name} (prefix: {duration_comparison_bp.url_prefix})")
    print(f"- {spread_duration_comparison_bp.name} (prefix: {spread_duration_comparison_bp.url_prefix})")
    # --- End print new blueprints ---
    app.logger.info(f"- {curve_bp.name} (prefix: {curve_bp.url_prefix})") # Log registration
    app.logger.info(f"- {issue_bp.name} (prefix: {issue_bp.url_prefix})") # Log registration for issues
    app.logger.info(f"- {attribution_bp.name} (prefix: {attribution_bp.url_prefix})") # Log registration for attribution
    # Add a simple test route to confirm app creation (optional)
    @app.route('/hello')
    def hello():
        return 'Hello, World! App factory is working.'
    # --- Add the new cleanup route ---
    @app.route('/run-cleanup', methods=['POST'])
    def run_cleanup():
        """Endpoint to trigger the process_data.py script."""
        script_path = os.path.join(os.path.dirname(__file__), 'process_data.py')
        python_executable = sys.executable # Use the same python that runs flask
        if not os.path.exists(script_path):
            app.logger.error(f"Cleanup script not found at: {script_path}")
            return jsonify({'status': 'error', 'message': 'Cleanup script not found.'}), 500
        app.logger.info(f"Attempting to run cleanup script: {script_path}")
        try:
            # Run the script using the same Python interpreter that is running Flask
            # Capture stdout and stderr, decode as UTF-8, handle potential errors
            result = subprocess.run(
                [python_executable, script_path],
                capture_output=True,
                text=True,
                check=False, # Don't raise exception on non-zero exit code
                encoding='utf-8' # Explicitly set encoding
            )
            log_output = f"STDOUT:\n{result.stdout}\nSTDERR:\n{result.stderr}"
            if result.returncode == 0:
                app.logger.info(f"Cleanup script finished successfully. Output:\n{log_output}")
                return jsonify({'status': 'success', 'output': result.stdout or "No output", 'error': result.stderr}), 200
            else:
                app.logger.error(f"Cleanup script failed with return code {result.returncode}. Output:\n{log_output}")
                return jsonify({'status': 'error', 'message': 'Cleanup script failed.', 'output': result.stdout, 'error': result.stderr}), 500
        except Exception as e:
            app.logger.error(f"Exception occurred while running cleanup script: {e}", exc_info=True)
            return jsonify({'status': 'error', 'message': f'An exception occurred: {e}'}), 500
    # --- End new route ---
    # --- Scheduled API Calls: manual scheduler loop using threading ---
    schedules_file = os.path.join(app.instance_path, 'schedules.json')
    if not os.path.exists(schedules_file):
        with open(schedules_file, 'w') as f:
            json.dump([], f)
    # Helper functions to load/save schedules
    def load_schedules():
        with open(schedules_file, 'r') as f:
            return json.load(f)
    def save_schedules(schedules):
        with open(schedules_file, 'w') as f:
            json.dump(schedules, f)
    # Job runner function
    def run_scheduled_job(schedule):
        with app.app_context():
            payload = {
                'date_mode': schedule['date_mode'],
                'write_mode': schedule['write_mode'],
                'funds': schedule['funds']
            }
            if schedule['date_mode'] == 'quick':
                payload['days_back'] = schedule['days_back']
                payload['end_date'] = schedule['end_date']
            else:
                payload['start_date'] = schedule['start_date']
                payload['custom_end_date'] = schedule['custom_end_date']
            response = app.test_client().post('/run_api_calls', json=payload)
            app.logger.info(f"Scheduled job {schedule['id']} executed. Status: {response.status_code}")
    # Manual scheduling loop
    def schedule_loop():
        last_checked = None
        while True:
            now = datetime.datetime.now()
            current_minute = now.replace(second=0, microsecond=0)
            if current_minute != last_checked:
                last_checked = current_minute
                for sched in load_schedules():
                    sched_time = datetime.datetime.strptime(sched['schedule_time'], '%H:%M').time()
                    if sched_time.hour == now.hour and sched_time.minute == now.minute:
                        try:
                            run_scheduled_job(sched)
                        except Exception as e:
                            app.logger.error(f"Error running scheduled job {sched['id']}: {e}", exc_info=True)
            time.sleep(1)
    threading.Thread(target=schedule_loop, daemon=True).start()
    app.logger.info("Started manual schedule loop thread")
    # --- End manual scheduling ---
    return app
# --- Application Execution ---
if __name__ == '__main__':
    app = create_app() # Create the app instance using the factory
    app.run(debug=True, host='0.0.0.0') # Run in debug mode for development, accessible on network
</file>

<file path="config.py">
# This file defines configuration variables for the Simple Data Checker application.
# It centralizes settings like file paths and visual parameters (e.g., chart colors)
# to make them easily adjustable without modifying the core application code.
"""
Configuration settings for the Flask application.
"""
# Define the primary data directory.
# This path is read by the `utils.get_data_folder_path` function during application startup.
# - If this is an absolute path (e.g., 'C:/MyApp/Data', '/var/data'), it will be used directly.
# - If this is a relative path (e.g., 'Data', '../SharedData'), it will be resolved
#   to an absolute path relative to the application's root directory (determined by Flask's app.root_path
#   or the script's location for standalone scripts).
# **Use forward slashes (/) for paths, even on Windows, for consistency.**
# If this variable is missing, empty, or the file doesn't exist, the utility function
# will fall back to using 'Data' relative to the application root.
DATA_FOLDER = 'Data'
# Define a list of distinct colors for chart lines
# Add more colors if you expect more fund columns
COLOR_PALETTE = [
    'blue', 'red', 'green', 'purple', '#FF7F50', # Coral
    '#6495ED', # CornflowerBlue
    '#DC143C', # Crimson
    '#00FFFF'  # Aqua
]
</file>

<file path="curve_processing.py">
# Purpose: Handles loading, preprocessing, and analysis of yield curve data (curves.csv).
# The `load_curve_data` function expects the absolute path to the data folder to be provided.
# Stdlib imports
import os
import re
from datetime import timedelta
# Third-party imports
import pandas as pd
import numpy as np
# Local imports
# Removed: from config import DATA_FOLDER
import logging # Import logging
# Get the logger instance. Assumes Flask app has configured logging.
logger = logging.getLogger(__name__)
# Constants for term conversion
TERM_MULTIPLIERS = {
    'D': 1,
    'W': 7,
    'M': 30,  # Approximate
    'Y': 365 # Approximate
}
def _term_to_days(term_str):
    """Converts a term string (e.g., '7D', '1M', '2Y') to an approximate number of days."""
    if not isinstance(term_str, str):
        return None # Handle non-string inputs
    term_str = term_str.upper()
    match = re.match(r"(\d+)([DWMY])", term_str)
    if match:
        num, unit = match.groups()
        multiplier = TERM_MULTIPLIERS.get(unit)
        if multiplier:
            try:
                return int(num) * multiplier
            except ValueError:
                 logger.warning(f"Could not convert number part '{num}' in term '{term_str}' to integer.")
                 return None
    try:
        # Handle simple integer strings representing days (fallback)
        return int(term_str)
    except ValueError:
        logger.warning(f"Could not parse term '{term_str}' to days.")
        return None # Indicate failure to parse
def load_curve_data(data_folder_path: str):
    """Loads and preprocesses the curve data from 'curves.csv' within the given folder.
    Args:
        data_folder_path (str): The absolute path to the folder containing 'curves.csv'.
                                The caller is responsible for providing the correct path,
                                typically obtained from `current_app.config['DATA_FOLDER']`.
    Returns:
        pd.DataFrame: Processed curve data indexed by [Currency, Date, Term],
                      or an empty DataFrame if loading/processing fails.
    """
    if not data_folder_path:
        logger.error("No data_folder_path provided to load_curve_data.")
        return pd.DataFrame()
    file_path = os.path.join(data_folder_path, 'curves.csv')
    logger.info(f"Attempting to load curve data from: {file_path}")
    if not os.path.exists(file_path):
        logger.error(f"Curve data file not found at {file_path}")
        return pd.DataFrame() # Return empty DataFrame
    try:
        # Load without automatic date parsing initially
        df = pd.read_csv(file_path)
        logger.info(f"Successfully loaded {len(df)} rows from {file_path}")
        # Explicitly parse the 'Date' column - robust to different formats
        if 'Date' in df.columns:
            # Specify the exact format to handle YYYY-MM-DDTHH:MM:SS
            df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%dT%H:%M:%S', errors='coerce')
            # Drop rows where date parsing failed
            original_rows_date = len(df)
            df.dropna(subset=['Date'], inplace=True)
            rows_after_date = len(df)
            if original_rows_date > rows_after_date:
                logger.warning(f"Dropped {original_rows_date - rows_after_date} rows due to unparseable 'Date'.")
        else:
            logger.error(f"Critical: 'Date' column not found in {file_path}")
            return pd.DataFrame()
    except Exception as e:
        logger.error(f"Error reading curve data CSV '{file_path}': {e}", exc_info=True)
        return pd.DataFrame()
    # Rename columns for consistency if necessary (adjust based on actual CSV)
    rename_map = {'Currency Code': 'Currency', 'Daily Value': 'Value'}
    df.rename(columns=rename_map, inplace=True)
    logger.debug(f"Renamed columns: {rename_map}")
    # Convert Term to days for sorting and plotting
    df['TermDays'] = df['Term'].apply(_term_to_days)
    logger.debug("Applied _term_to_days conversion.")
    # Convert Value to numeric first
    original_rows = len(df)
    df['Value'] = pd.to_numeric(df['Value'], errors='coerce')
    rows_after_numeric = len(df.dropna(subset=['Value']))
    if original_rows > rows_after_numeric:
         logger.warning(f"Dropped {original_rows - rows_after_numeric} rows due to non-numeric 'Value'.")
    df.dropna(subset=['Value'], inplace=True)
    # Drop rows where term conversion failed
    original_rows = len(df)
    rows_after_term = len(df.dropna(subset=['TermDays']))
    if original_rows > rows_after_term:
         logger.warning(f"Dropped {original_rows - rows_after_term} rows due to unparseable 'Term'.")
    df.dropna(subset=['TermDays'], inplace=True)
    if df.empty:
         logger.warning("DataFrame is empty after initial processing and dropping NaNs.")
         return df # Return empty if all rows were dropped
    # Set index and sort
    try:
        df.sort_values(by=['Currency', 'Date', 'TermDays'], inplace=True)
        # Use MultiIndex for efficient lookups
        df.set_index(['Currency', 'Date', 'Term'], inplace=True)
        logger.info(f"Curve data processed. Final shape: {df.shape}")
    except KeyError as e:
         logger.error(f"Missing expected column for sorting/indexing: {e}. Columns present: {df.columns.tolist()}")
         return pd.DataFrame() # Return empty on structure error
    except Exception as e:
         logger.error(f"Unexpected error during sorting/indexing: {e}", exc_info=True)
         return pd.DataFrame()
    return df
def get_latest_curve_date(df):
    """Gets the most recent date in the DataFrame's index."""
    if df.empty or 'Date' not in df.index.names:
        logger.warning("Cannot get latest date: DataFrame is empty or 'Date' is not in the index.")
        return None
    try:
        latest_date = df.index.get_level_values('Date').max()
        logger.debug(f"Latest date found: {latest_date}")
        return latest_date
    except Exception as e:
         logger.error(f"Error getting latest date from index: {e}", exc_info=True)
         return None
def check_curve_inconsistencies(df):
    """
    Checks for inconsistencies in yield curves compared to the previous day.
    Returns a dictionary summarizing potential issues for the latest date.
    """
    if df.empty:
        logger.warning("Skipping inconsistency check: Input DataFrame is empty.")
        return {}
    latest_date = get_latest_curve_date(df)
    if not latest_date:
        logger.warning("Skipping inconsistency check: Could not determine latest date.")
        return {}
    logger.info(f"Checking inconsistencies for latest date: {latest_date.strftime('%Y-%m-%d')}")
    # Find the previous available date
    try:
        all_dates = df.index.get_level_values('Date').unique()
        available_dates = sorted(all_dates, reverse=True)
        previous_date = None
        if len(available_dates) > 1:
            # Use get_loc for robust index finding
            latest_date_pos = available_dates.get_loc(latest_date)
            if latest_date_pos + 1 < len(available_dates):
                previous_date = available_dates[latest_date_pos + 1]
                logger.info(f"Previous date found for comparison: {previous_date.strftime('%Y-%m-%d')}")
            else:
                logger.info("Latest date is the only date available. Cannot compare change profile.")
        else:
            logger.info("Only one date available. Cannot compare change profile.")
    except Exception as e:
        logger.error(f"Error determining previous date: {e}", exc_info=True)
        previous_date = None
    summary = {}
    currencies = df.index.get_level_values('Currency').unique()
    logger.debug(f"Checking currencies: {currencies.tolist()}")
    for currency in currencies:
        try:
            # --- Get Latest Curve Data (Safer Filtering) ---
            logger.debug(f"Attempting to filter latest data for {currency} on {latest_date.strftime('%Y-%m-%d')}")
            latest_mask = (df.index.get_level_values('Currency') == currency) & \
                          (df.index.get_level_values('Date') == latest_date)
            latest_curve_filtered = df[latest_mask]
            if latest_curve_filtered.empty:
                logger.warning(f"No latest data found for {currency} on {latest_date.strftime('%Y-%m-%d')} using boolean mask.")
                summary.setdefault(currency, []).append("Missing latest data")
                continue
            latest_curve = latest_curve_filtered.reset_index().sort_values('TermDays')
            logger.debug(f"Successfully filtered latest data for {currency}. Shape: {latest_curve.shape}")
            # --- Basic Check 1: Monotonicity ---
            diffs = latest_curve['Value'].diff()
            large_drops = diffs[diffs < -0.5]
            if not large_drops.empty:
                terms = latest_curve.loc[large_drops.index, 'Term'].tolist()
                issue_msg = f"Potential non-monotonic drop(s) < -0.5 between terms near: {terms} on {latest_date.strftime('%Y-%m-%d')}"
                summary.setdefault(currency, []).append(issue_msg)
                logger.warning(f"{currency}: {issue_msg}")
            # --- Check 2: Compare change shape with previous day ---
            if previous_date:
                logger.debug(f"Attempting to filter previous data for {currency} on {previous_date.strftime('%Y-%m-%d')}")
                prev_mask = (df.index.get_level_values('Currency') == currency) & \
                            (df.index.get_level_values('Date') == previous_date)
                previous_curve_filtered = df[prev_mask]
                if previous_curve_filtered.empty:
                    logger.warning(f"No previous day data ({previous_date.strftime('%Y-%m-%d')}) found for {currency} using boolean mask.")
                    summary.setdefault(currency, []).append("Missing previous data for comparison")
                else:
                    previous_curve = previous_curve_filtered.reset_index().sort_values('TermDays')
                    logger.debug(f"Successfully filtered previous data for {currency}. Shape: {previous_curve.shape}")
                    merged = pd.merge(
                        latest_curve[['Term', 'TermDays', 'Value']],
                        previous_curve[['Term', 'TermDays', 'Value']],
                        on='TermDays',
                        suffixes=('_latest', '_prev'),
                        how='inner'
                    )
                    if merged.empty:
                        logger.warning(f"No common terms found between dates for {currency}.")
                    else:
                        merged['ValueChange'] = merged['Value_latest'] - merged['Value_prev']
                        merged.sort_values('TermDays', inplace=True)
                        merged['ChangeDiff'] = merged['ValueChange'].diff()
                        change_diff_std = merged['ChangeDiff'].std()
                        change_diff_mean = merged['ChangeDiff'].mean()
                        threshold_std = np.nan_to_num(change_diff_mean + 3 * change_diff_std)
                        threshold_abs = 0.2
                        final_threshold = max(abs(threshold_std), threshold_abs)
                        anomalous_jumps = merged[abs(merged['ChangeDiff'].fillna(0)) > final_threshold]
                        if not anomalous_jumps.empty:
                            anomalous_terms = anomalous_jumps['Term_latest'].tolist()
                            issue_msg = f"Anomalous change profile jump vs {previous_date.strftime('%Y-%m-%d')} near terms: {anomalous_terms}"
                            summary.setdefault(currency, []).append(issue_msg)
                            logger.warning(f"{currency}: {issue_msg}")
            if currency not in summary:
                summary[currency] = ["OK"]
                logger.info(f"{currency}: Checks passed.")
        except pd.errors.InvalidIndexError as e:
            logger.error(f"InvalidIndexError processing curve for {currency}: {e}", exc_info=True)
            summary.setdefault(currency, []).append(f"Processing error (InvalidIndexError)")
        except KeyError as e:
            logger.error(f"KeyError processing curve for {currency}: {e}", exc_info=True)
            summary.setdefault(currency, []).append(f"Processing error (KeyError)")
        except Exception as e:
            logger.error(f"Unexpected error processing curve for {currency}: {e}", exc_info=True)
            summary.setdefault(currency, []).append(f"Processing error: {type(e).__name__}")
    logger.info("Finished inconsistency checks.")
    return summary
if __name__ == '__main__':
    # Example usage when run directly:
    print("Running curve_processing.py directly...")
    logger.info("--- Starting Standalone Execution ---")
    print("Loading curve data...")
    curve_df = load_curve_data()
    if not curve_df.empty:
        print("\nCurve data loaded successfully.")
        latest_dt = get_latest_curve_date(curve_df)
        print(f"\nLatest Date found: {latest_dt.strftime('%Y-%m-%d') if latest_dt else 'N/A'}")
        print("\nChecking for inconsistencies on latest date...")
        inconsistency_summary = check_curve_inconsistencies(curve_df)
        print("\n--- Inconsistency Summary ---")
        if inconsistency_summary:
            for currency, issues in inconsistency_summary.items():
                print(f"  {currency}: {', '.join(issues)}")
        else:
            print("  No inconsistencies detected or data was insufficient for checks.")
        # Example: Get data for a specific currency and date
        if latest_dt:
            test_currency = 'USD'
            try:
                # Use .loc with pd.IndexSlice for clarity
                idx = pd.IndexSlice
                usd_latest = curve_df.loc[idx[test_currency, latest_dt, :]]
                # Reset index to get TermDays as a column for printing
                print(f"\n{test_currency} Curve on latest date ({latest_dt.strftime('%Y-%m-%d')}):")
                print(usd_latest.reset_index()[['Term', 'TermDays', 'Value']].sort_values('TermDays').to_string())
            except KeyError:
                print(f"\n{test_currency} data not found for the latest date.")
            except Exception as e:
                 print(f"\nError retrieving {test_currency} data: {e}")
    else:
        print("\nFailed to load or process curve data.")
    logger.info("--- Finished Standalone Execution ---")
</file>

<file path="data_loader.py">
# This file is responsible for loading and preprocessing data from CSV files.
# It includes functions to dynamically identify essential columns (Date, Code, Benchmark)
# based on patterns, handle potential naming variations, parse dates, standardize column names,
# set appropriate data types, and prepare the data in a pandas DataFrame format
# suitable for further analysis and processing within the application.
# It also supports loading a secondary file (e.g., prefixed with 'sp_') for comparison.
# data_loader.py
# This file is responsible for loading and preprocessing data from time-series CSV files (typically prefixed with `ts_`).
# It includes functions to dynamically identify essential columns (Date, Code, Benchmark)
# based on patterns, handle potential naming variations, parse dates (handling 'YYYY-MM-DD' and 'DD/MM/YYYY'),
# standardize column names, set appropriate data types, and prepare the data in a pandas DataFrame format
# suitable for further analysis within the application. It includes robust error handling and logging.
# It now also supports loading and processing a secondary comparison file (e.g., sp_*.csv).
import pandas as pd
import os
import logging
from typing import List, Tuple, Optional
import re # Import regex for pattern matching
from flask import current_app # Import current_app to access config
# Get the logger instance. Assumes Flask app has configured logging.
logger = logging.getLogger(__name__)
# --- Removed logging setup block --- 
# Logging is now handled centrally by the Flask app factory in app.py
# Define constants
# Removed DATA_FOLDER constant - path is now dynamically determined
# Standard internal column names after renaming
STD_DATE_COL = 'Date'
STD_CODE_COL = 'Code'
STD_BENCHMARK_COL = 'Benchmark'
def _find_column(pattern: str, columns: List[str], filename_for_logging: str, col_type: str) -> str:
    """Helper function to find a single column matching a pattern (case-insensitive)."""
    matches = [col for col in columns if re.search(pattern, col, re.IGNORECASE)]
    if len(matches) == 1:
        logger.info(f"Found {col_type} column in '{filename_for_logging}': '{matches[0]}'")
        return matches[0]
    elif len(matches) > 1:
        # Log error before raising
        logger.error(f"Multiple possible {col_type} columns found in '{filename_for_logging}' matching pattern '{pattern}': {matches}. Please ensure unique column names.")
        raise ValueError(f"Multiple possible {col_type} columns found in '{filename_for_logging}' matching pattern '{pattern}': {matches}. Please ensure unique column names.")
    else:
         # Log error before raising
        logger.error(f"No {col_type} column found in '{filename_for_logging}' matching pattern '{pattern}'. Found columns: {columns}")
        raise ValueError(f"No {col_type} column found in '{filename_for_logging}' matching pattern '{pattern}'. Found columns: {columns}")
def _create_empty_dataframe(original_fund_val_col_names: List[str], benchmark_col_present: bool) -> pd.DataFrame:
    """Creates an empty DataFrame with the expected structure."""
    final_benchmark_col_name = STD_BENCHMARK_COL if benchmark_col_present else None
    expected_cols = [STD_DATE_COL, STD_CODE_COL] + original_fund_val_col_names
    if final_benchmark_col_name:
        expected_cols.append(final_benchmark_col_name)
    # Create an empty df with the right index and columns
    empty_index = pd.MultiIndex(levels=[[], []], codes=[[], []], names=[STD_DATE_COL, STD_CODE_COL])
    value_cols = [col for col in expected_cols if col not in [STD_DATE_COL, STD_CODE_COL]]
    return pd.DataFrame(index=empty_index, columns=value_cols)
def _process_single_file(
    filepath: str,
    filename_for_logging: str
) -> Optional[Tuple[pd.DataFrame, List[str], Optional[str]]]:
    """Internal helper to load and process a single CSV file.
    Handles finding columns, parsing dates, renaming, indexing, and type conversion.
    Returns None if the file is not found or critical processing steps fail.
    Returns:
        Optional[Tuple[pd.DataFrame, List[str], Optional[str]]]:
               Processed DataFrame, list of original fund value column names,
               and the standardized benchmark column name if present, otherwise None.
               Returns None if processing fails critically.
    """
    if not os.path.exists(filepath):
        logger.warning(f"File not found, skipping: {filepath}")
        return None # Return None if file doesn't exist
    try:
        # Read only the header first
        header_df = pd.read_csv(filepath, nrows=0, encoding='utf-8', encoding_errors='replace', on_bad_lines='skip')
        original_cols = [col.strip() for col in header_df.columns.tolist()]
        logger.info(f"Processing file: '{filename_for_logging}'. Original columns: {original_cols}")
        # Dynamically find required columns
        date_pattern = r'\b(Position\s*)?Date\b'
        actual_date_col = _find_column(date_pattern, original_cols, filename_for_logging, 'Date')
        code_pattern = r'\b(Fund\s*)?Code\b' # Allow 'Fund Code' or 'Code'
        actual_code_col = _find_column(code_pattern, original_cols, filename_for_logging, 'Code')
        benchmark_col_present = False
        actual_benchmark_col = None
        try:
            benchmark_pattern = r'\b(Benchmark|Bench)\b' # Allow 'Benchmark' or 'Bench'
            actual_benchmark_col = _find_column(benchmark_pattern, original_cols, filename_for_logging, 'Benchmark')
            benchmark_col_present = True
        except ValueError:
            logger.info(f"No Benchmark column found in '{filename_for_logging}' matching pattern. Proceeding without benchmark.")
        # Identify original fund value columns
        excluded_cols_for_funds = {actual_date_col, actual_code_col}
        if benchmark_col_present and actual_benchmark_col:
            excluded_cols_for_funds.add(actual_benchmark_col)
        original_fund_val_col_names = [col for col in original_cols if col not in excluded_cols_for_funds]
        if not original_fund_val_col_names and not benchmark_col_present:
             logger.error(f"No fund value columns and no benchmark column identified in '{filename_for_logging}'. Cannot process.")
             return None # Cannot proceed
        # Read the full CSV
        df = pd.read_csv(filepath, encoding='utf-8', encoding_errors='replace', on_bad_lines='skip', dtype={actual_date_col: str})
        df.columns = df.columns.str.strip()
        # Rename columns
        rename_map = {
            actual_date_col: STD_DATE_COL,
            actual_code_col: STD_CODE_COL
        }
        if benchmark_col_present and actual_benchmark_col:
            rename_map[actual_benchmark_col] = STD_BENCHMARK_COL
        df.rename(columns=rename_map, inplace=True)
        logger.info(f"Renamed columns in '{filename_for_logging}': {list(rename_map.keys())} -> {list(rename_map.values())}")
        # Robust Date Parsing
        date_series = df[STD_DATE_COL]
        parsed_dates = pd.to_datetime(date_series, errors='coerce', dayfirst=None, yearfirst=None) # Let pandas infer
        # Check if all parsing failed
        if parsed_dates.isnull().all() and len(date_series) > 0:
             # Try again with dayfirst=True if initial inference failed
            logger.warning(f"Initial date parsing failed for {filename_for_logging}. Trying with dayfirst=True.")
            parsed_dates = pd.to_datetime(date_series, errors='coerce', dayfirst=True)
            if parsed_dates.isnull().all() and len(date_series) > 0:
                logger.error(f"Could not parse any dates in column '{STD_DATE_COL}' (original: '{actual_date_col}') in file {filename_for_logging} even with dayfirst=True.")
                return None # Cannot proceed without valid dates
        nat_count = parsed_dates.isnull().sum()
        total_count = len(parsed_dates)
        success_count = total_count - nat_count
        logger.info(f"Parsed {success_count}/{total_count} dates in {filename_for_logging}. ({nat_count} resulted in NaT).")
        if nat_count > 0:
             logger.warning(f"{nat_count} date values in '{STD_DATE_COL}' from {filename_for_logging} became NaT.")
        df[STD_DATE_COL] = parsed_dates
        original_row_count = len(df)
        df.dropna(subset=[STD_DATE_COL], inplace=True)
        rows_dropped = original_row_count - len(df)
        if rows_dropped > 0:
            logger.warning(f"Dropped {rows_dropped} rows from {filename_for_logging} due to failed date parsing.")
        # Set Index
        if df.empty:
            logger.warning(f"DataFrame became empty after dropping rows with unparseable dates in {filename_for_logging}.")
            # Return empty structure but indicate success in file processing up to this point
            empty_df = _create_empty_dataframe(original_fund_val_col_names, benchmark_col_present)
            final_bm_col = STD_BENCHMARK_COL if benchmark_col_present else None
            return empty_df, original_fund_val_col_names, final_bm_col
        df.set_index([STD_DATE_COL, STD_CODE_COL], inplace=True)
        # Convert value columns to numeric
        value_cols_to_convert = original_fund_val_col_names[:]
        if benchmark_col_present:
            value_cols_to_convert.append(STD_BENCHMARK_COL)
        valid_cols_for_conversion = [col for col in value_cols_to_convert if col in df.columns]
        if not valid_cols_for_conversion:
             logger.error(f"No valid fund or benchmark value columns found to convert in {filename_for_logging} after processing.")
             # Return partially processed DF but log error
             final_bm_col = STD_BENCHMARK_COL if benchmark_col_present else None
             return df, original_fund_val_col_names, final_bm_col # Return what we have
        df[valid_cols_for_conversion] = df[valid_cols_for_conversion].apply(pd.to_numeric, errors='coerce')
        nan_check_cols = [col for col in valid_cols_for_conversion if col in df.columns]
        if nan_check_cols and df[nan_check_cols].isnull().all().all():
            logger.warning(f"All values in value columns {nan_check_cols} became NaN after conversion in file {filename_for_logging}. Check data types.")
        final_benchmark_col_name = STD_BENCHMARK_COL if benchmark_col_present else None
        logger.info(f"Successfully processed file: '{filename_for_logging}'. Index: {df.index.names}. Columns: {df.columns.tolist()}")
        return df, original_fund_val_col_names, final_benchmark_col_name
    except FileNotFoundError:
        logger.error(f"File not found during processing: {filepath}")
        return None # Handled above, but belt-and-suspenders
    except ValueError as e:
        logger.error(f"Value error processing {filename_for_logging}: {e}")
        return None # Return None on critical errors like missing columns
    except Exception as e:
        logger.exception(f"Unexpected error processing {filename_for_logging}: {e}") # Log full traceback
        return None # Return None on unexpected errors
# Simplified return type: focus on the dataframes and metadata needed downstream
LoadResult = Tuple[
    Optional[pd.DataFrame],      # Primary DataFrame
    Optional[List[str]],         # Primary original value columns
    Optional[str],               # Primary benchmark column name (standardized)
    Optional[pd.DataFrame],      # Secondary DataFrame
    Optional[List[str]],         # Secondary original value columns
    Optional[str]                # Secondary benchmark column name (standardized)
]
def load_and_process_data(
    primary_filename: str,
    secondary_filename: Optional[str] = None,
    data_folder_path: Optional[str] = None # Renamed and made optional
) -> LoadResult:
    """Loads and processes a primary CSV file and optionally a secondary CSV file.
    Retrieves the data folder path from Flask's current_app.config['DATA_FOLDER']
    if data_folder_path is not provided. Assumes execution within a Flask request context
    when data_folder_path is None.
    Uses the internal _process_single_file helper for processing each file.
    Args:
        primary_filename (str): The name of the primary CSV file.
        secondary_filename (Optional[str]): The name of the secondary CSV file. Defaults to None.
        data_folder_path (Optional[str]): Explicit path to the folder containing the data files.
                                           If None, path is retrieved from current_app.config.
    Returns:
        LoadResult: A tuple containing the processed DataFrames and metadata for
                    primary and (optionally) secondary files. Elements corresponding
                    to a file will be None if the file doesn't exist or processing fails,
                    or if the data folder path cannot be determined.
    """
    data_folder: Optional[str] = None
    if data_folder_path is None:
        try:
            # Retrieve the absolute path configured during app initialization
            data_folder = current_app.config['DATA_FOLDER']
            logger.info(f"Using data folder from current_app.config: {data_folder}")
            if not data_folder:
                 logger.error("DATA_FOLDER in current_app.config is not set or empty.")
                 return None, None, None, None, None, None
        except RuntimeError:
            logger.error("Cannot access current_app.config. load_and_process_data must be called within a Flask request context or be provided with an explicit data_folder_path.")
            # Return None for all parts of the tuple if path cannot be determined
            return None, None, None, None, None, None
        except KeyError:
            logger.error("'DATA_FOLDER' key not found in current_app.config. Ensure it is set during app initialization.")
            return None, None, None, None, None, None
    else:
        # Use the explicitly provided path
        data_folder = data_folder_path
        logger.info(f"Using explicitly provided data_folder_path: {data_folder}")
    # Ensure data_folder is not None before proceeding (should be handled above, but belt-and-suspenders)
    if data_folder is None:
         logger.critical("Data folder path could not be determined. Aborting load.")
         return None, None, None, None, None, None
    # --- Process Primary File --- 
    primary_filepath = os.path.join(data_folder, primary_filename)
    logger.info(f"--- Starting data load for primary: {primary_filename} from {primary_filepath} ---")
    primary_result = _process_single_file(primary_filepath, primary_filename)
    df1, cols1, bench1 = (None, None, None)
    if primary_result:
        df1, cols1, bench1 = primary_result
        logger.info(f"Primary file '{primary_filename}' processed. Shape: {df1.shape if df1 is not None else 'N/A'}. Benchmark: {bench1}")
    else:
        logger.warning(f"Processing failed for primary file: {primary_filename}")
    # --- Process Secondary File (if provided) --- 
    df2, cols2, bench2 = (None, None, None)
    if secondary_filename:
        secondary_filepath = os.path.join(data_folder, secondary_filename)
        logger.info(f"--- Starting data load for secondary: {secondary_filename} from {secondary_filepath} ---")
        secondary_result = _process_single_file(secondary_filepath, secondary_filename)
        if secondary_result:
            df2, cols2, bench2 = secondary_result
            logger.info(f"Secondary file '{secondary_filename}' processed. Shape: {df2.shape if df2 is not None else 'N/A'}. Benchmark: {bench2}")
        else:
            logger.warning(f"Processing failed for secondary file: {secondary_filename}")
    return df1, cols1, bench1, df2, cols2, bench2
# --- Standalone Execution / Testing --- 
# Note: If run directly, this block cannot use current_app.config.
# It needs to determine the data folder path independently, potentially using get_data_folder_path from utils.
# Example (requires config.py and utils.py to be importable):
# if __name__ == '__main__':
#     try:
#         from utils import get_data_folder_path
#         # Determine the root path assuming data_loader.py is one level down from the project root
#         script_dir = os.path.dirname(os.path.abspath(__file__))
#         project_root = os.path.dirname(script_dir)
#         # Use the utility function to get the configured path
#         standalone_data_path = get_data_folder_path(app_root_path=project_root)
#         print(f"[Standalone] Determined data path: {standalone_data_path}")
#
#         # Example usage:
#         primary_file = 'ts_NAV_Report_Short.csv' # Replace with your actual test file
#         # secondary_file = 'sp_NAV_Report_Short.csv' # Optional secondary file
#         df1, cols1, bench1, df2, cols2, bench2 = load_and_process_data(
#             primary_filename=primary_file,
#             # secondary_filename=secondary_file,
#             data_folder_path=standalone_data_path # Pass the determined path explicitly
#         )
#
#         if df1 is not None:
#             print(f"\n--- Primary Data ({primary_file}) ---")
#             print(df1.head())
#             print(f"Original Value Columns: {cols1}")
#             print(f"Benchmark Column: {bench1}")
#         else:
#             print(f"\nFailed to load primary data ({primary_file}). Check logs.")
#
#         # if secondary_file and df2 is not None:
#         #     print(f"\n--- Secondary Data ({secondary_file}) ---")
#         #     print(df2.head())
#         #     print(f"Original Value Columns: {cols2}")
#         #     print(f"Benchmark Column: {bench2}")
#         # elif secondary_file:
#         #      print(f"\nFailed to load secondary data ({secondary_file}). Check logs.")
#
#     except ImportError:
#         print("Error: Could not import utils.get_data_folder_path. Ensure utils.py and config.py exist and are accessible.")
#     except Exception as e:
#         print(f"An error occurred during standalone execution: {e}")
</file>

<file path="data_validation.py">
'''
Placeholder module for validating data retrieved from the API.
This module will contain functions to check the structure, data types,
and potentially the content consistency of the DataFrames returned by the
Rex API before they are saved as CSV files.
'''
import pandas as pd
def validate_data(df: pd.DataFrame, filename: str):
    """
    Validates the structure and types of the DataFrame based on filename conventions.
    This is a placeholder function. Implement specific checks based on the
    expected format for different file types (e.g., 'ts_*.csv', 'sec_*.csv').
    Args:
        df (pd.DataFrame): The DataFrame returned by the API call.
        filename (str): The intended filename for the data (e.g., 'ts_Duration.csv').
    Returns:
        tuple[bool, list[str]]: A tuple containing:
            - bool: True if the data is valid, False otherwise.
            - list[str]: A list of validation error messages, empty if valid.
    """
    errors = []
    if df is None or not isinstance(df, pd.DataFrame):
        errors.append("Invalid input: DataFrame is None or not a pandas DataFrame.")
        return False, errors
    if df.empty:
        # It might be valid for some queries to return no data, but flag it for review.
        errors.append("Warning: DataFrame is empty.")
        # Decide if empty is truly invalid or just a warning.
        # For now, let's consider it potentially valid but issue a warning.
        # return False, errors # Uncomment if empty df is strictly invalid
    # Example checks based on filename conventions:
    if filename.startswith('ts_'):
        # Checks for time-series files
        required_cols = ['Date', 'Code'] # Assuming these are standard post-processing names
        if not all(col in df.columns for col in required_cols):
            errors.append(f"Missing required columns for time-series data: Expected {required_cols}, got {list(df.columns)}")
        # Check if 'Date' column is datetime type (or can be coerced)
        # try:
        #     pd.to_datetime(df['Date'])
        # except Exception as e:
        #     errors.append(f"'Date' column cannot be parsed as datetime: {e}")
        # Check if value columns (excluding Date, Code, Benchmark if exists) are numeric
        value_cols = [col for col in df.columns if col not in ['Date', 'Code', 'Benchmark']]
        for col in value_cols:
            if not pd.api.types.is_numeric_dtype(df[col]):
                 errors.append(f"Column '{col}' in time-series data is not numeric.")
    elif filename.startswith('sec_'):
        # Checks for security-level files
        # Example: Check for an ID column (e.g., 'Security ID', 'ISIN')
        # Example: Check if columns intended as dates are parseable
        # Example: Check if value columns are numeric
        pass # Add specific checks here
    elif filename == 'FundList.csv':
        # Example: Check required columns for FundList
        required_cols = ['Fund Code', 'Total Asset Value USD', 'Picked']
        if not all(col in df.columns for col in required_cols):
             errors.append(f"Missing required columns for FundList.csv: Expected {required_cols}, got {list(df.columns)}")
    # --- Add more specific validation rules as needed based on data specs --- 
    is_valid = len(errors) == 0
    return is_valid, errors
# Example Usage (can be run manually for testing):
if __name__ == '__main__':
    # Create dummy dataframes for testing validation logic
    print("Testing validation functions...")
    # Test case 1: Valid time-series data
    valid_ts_data = {
        'Date': pd.to_datetime(['2023-01-01', '2023-01-01', '2023-01-02', '2023-01-02']),
        'Code': ['FUNDA', 'FUNDB', 'FUNDA', 'FUNDB'],
        'Value': [10.1, 20.2, 10.5, 20.8],
        'Benchmark': [10.0, 20.0, 10.4, 20.7]
    }
    valid_ts_df = pd.DataFrame(valid_ts_data)
    is_valid, errors = validate_data(valid_ts_df, 'ts_ExampleMetric.csv')
    print(f"Valid TS Data Test: Valid={is_valid}, Errors={errors}")
    assert is_valid
    # Test case 2: Invalid time-series data (missing column)
    invalid_ts_data = {
        'Date': pd.to_datetime(['2023-01-01']),
        # 'Code': ['FUNDA'], # Missing Code column
        'Value': [10.1]
    }
    invalid_ts_df = pd.DataFrame(invalid_ts_data)
    is_valid, errors = validate_data(invalid_ts_df, 'ts_AnotherMetric.csv')
    print(f"Invalid TS Data Test (Missing Col): Valid={is_valid}, Errors={errors}")
    assert not is_valid
    assert "Missing required columns" in errors[0]
    # Test case 3: Invalid time-series data (non-numeric value)
    invalid_ts_data_type = {
        'Date': pd.to_datetime(['2023-01-01']),
        'Code': ['FUNDA'],
        'Value': ['abc'] # Non-numeric value
    }
    invalid_ts_df_type = pd.DataFrame(invalid_ts_data_type)
    is_valid, errors = validate_data(invalid_ts_df_type, 'ts_BadData.csv')
    print(f"Invalid TS Data Test (Bad Type): Valid={is_valid}, Errors={errors}")
    # Note: This specific check might depend on when type conversion happens.
    # If conversion happens *before* validation, this might pass if 'abc' becomes NaN.
    # The check here assumes the raw data from API might be non-numeric.
    assert not is_valid # Assuming the validation catches non-numeric directly
    assert "not numeric" in errors[0]
    # Test case 4: Empty DataFrame
    empty_df = pd.DataFrame()
    is_valid, errors = validate_data(empty_df, 'ts_EmptyData.csv')
    print(f"Empty DF Test: Valid={is_valid}, Errors={errors}")
    assert is_valid # Currently allows empty with warning
    assert "DataFrame is empty" in errors[0]
    # Test case 5: None DataFrame
    none_df = None
    is_valid, errors = validate_data(none_df, 'ts_NoneData.csv')
    print(f"None DF Test: Valid={is_valid}, Errors={errors}")
    assert not is_valid
    assert "DataFrame is None" in errors[0]
    print("Validation tests completed.")
</file>

<file path="fileoveright.md">
Great! Here’s a detailed, actionable plan for both the **UI** and **backend** to support:
- Running extra funds and extra dates
- Overwriting overlapping data (if a fund/date exists, it is replaced)
- Expanding (appending) new data otherwise
- A clear UI showing when each fund/date combination was last written to
- Retaining the “Run and Overwrite” button to start every file from scratch

---

## 1. **UI Changes (`get_data.html` and JS)**

### a. **Date Range Selection**
- Replace or supplement the “Days Back” and “End Date” fields with a **date range picker** (Start Date, End Date).
- Optionally, keep “Days Back” for quick selection, but when a custom range is picked, use that.

### b. **Overwrite/Expand Mode**
- Add a **dropdown or radio group**:
  - “Expand (append new data, overwrite overlaps)”
  - “Run and Overwrite All” (existing button, starts every file from scratch)

### c. **Status Table**
- Add a new table (or extend the existing one) to show, for each fund/date combination:
  - Fund
  - Date
  - File Name
  - Last Written (timestamp, if available)
  - Action Taken (Overwritten, Appended, Skipped, Error)
- This table is updated after each run.

### d. **Confirmation**
- If “Run and Overwrite All” is selected, show a confirmation dialog.

---

## 2. **Backend/API Changes**

### a. **API Parameters**
- Accept:
  - `start_date`, `end_date` (or a list of dates)
  - `funds` (list)
  - `mode`: `"expand"` or `"overwrite_all"`

### b. **Processing Logic**
- For **expand** mode:
  - For each fund/date:
    - If data exists for that fund/date, **overwrite** it.
    - If not, **append** as new.
- For **overwrite_all** mode:
  - For each relevant file, **delete all existing data** and write only the new data.

### c. **Tracking Last Written**
- For each fund/date/file, record the last written timestamp (could be a new column in the file, or a separate metadata file, or just in the response summary).

### d. **Result Reporting**
- Return a summary for each fund/date/file:
  - Action taken (Overwritten, Appended, Skipped, Error)
  - Last written timestamp

---

## 3. **Implementation Steps**

### **UI**
1. Add a date range picker (start/end date) to the form.
2. Add a dropdown/radio for “Expand” vs “Run and Overwrite All”.
3. Update JS to send `start_date`, `end_date`, `mode` in the API request.
4. Update the results table to show fund/date/file/action/last written.
5. Add a confirmation dialog for “Run and Overwrite All”.

### **Backend**
1. Update `/run-api-calls` and `/rerun-api-call` endpoints to accept new parameters.
2. Refactor file processing:
   - For “expand”: load file, drop rows for selected fund/date, append new data, save.
   - For “overwrite_all”: clear file, write only new data.
3. For each fund/date/file, record the action and last written timestamp in the response.
4. Return a detailed summary to the frontend.

---

## 4. **Example User Flow**

1. User selects funds: FUND1, FUND2.
2. User picks date range: 2024-05-01 to 2024-05-10.
3. User selects “Expand (append/overwrite overlaps)” and clicks “Simulate API Calls”.
4. Results table shows, for each fund/date/file, whether data was appended or overwritten, and the last written time.
5. User selects “Run and Overwrite All”, confirms, and all files are reset and rewritten from scratch.

---

## 5. **Technical Notes**

- **Overwrite on overlap:** For “expand”, always drop existing rows for the fund/date before appending new data.
- **Last written tracking:** If not storing in the data file, keep a simple JSON or CSV metadata file per data file, mapping (fund, date) to last written timestamp.
- **Atomic writes:** Use temp files and rename to avoid corruption.
- **Performance:** For large files, use pandas for efficient row filtering and appending.

---

## 6. **Next Steps**

Would you like to start with the UI or backend code?  
Or should I provide a code sketch for both in parallel (recommended for clarity)?
</file>

<file path="issue_processing.py">
# issue_processing.py
# Purpose: Handles loading, adding, and updating data issues stored in Data/data_issues.csv.
import pandas as pd
import os
from datetime import datetime
from config import DATA_FOLDER
ISSUES_FILE = os.path.join(DATA_FOLDER, 'data_issues.csv')
FUND_LIST_FILE = os.path.join(DATA_FOLDER, 'FundList.csv')
def load_issues():
    """Loads the data issues from the CSV file into a pandas DataFrame."""
    if os.path.exists(ISSUES_FILE):
        try:
            df = pd.read_csv(ISSUES_FILE,
                             parse_dates=['DateRaised', 'DateClosed', 'IssueDate'])
            # Ensure Status exists and fill missing with 'Open' for backward compatibility if needed
            if 'Status' not in df.columns:
                df['Status'] = 'Open'
            else:
                df['Status'] = df['Status'].fillna('Open')
            # Ensure required columns exist, fill with NaT/None if missing
            required_cols = {
                'IssueID': None, 'DateRaised': pd.NaT, 'RaisedBy': None,
                'FundImpacted': None, 'DataSource': None, 'IssueDate': pd.NaT,
                'Description': None, 'Status': 'Open', 'DateClosed': pd.NaT,
                'ClosedBy': None, 'ResolutionComment': None
            }
            for col, default in required_cols.items():
                if col not in df.columns:
                    df[col] = default
            # Convert relevant columns to appropriate types after loading/filling
            df['DateRaised'] = pd.to_datetime(df['DateRaised']).dt.date
            df['DateClosed'] = pd.to_datetime(df['DateClosed']).dt.date
            df['IssueDate'] = pd.to_datetime(df['IssueDate']).dt.date
            df['IssueID'] = df['IssueID'].astype(str) # Ensure IssueID is string
            return df
        except Exception as e:
            print(f"Error loading issues file: {e}")
            # Return an empty DataFrame with correct columns on error
            return pd.DataFrame(columns=list(required_cols.keys()))
    else:
        # Return an empty DataFrame with correct columns if file doesn't exist
        return pd.DataFrame(columns=list(required_cols.keys()))
def _save_issues(df):
    """Saves the DataFrame back to the CSV file."""
    try:
        # Convert date columns to string format for CSV consistency
        date_cols = ['DateRaised', 'DateClosed', 'IssueDate']
        for col in date_cols:
            if col in df.columns:
                df[col] = pd.to_datetime(df[col]).dt.strftime('%Y-%m-%d')
        df.to_csv(ISSUES_FILE, index=False)
    except Exception as e:
        print(f"Error saving issues file: {e}")
def _generate_issue_id(existing_ids):
    """Generates a unique sequential issue ID (e.g., ISSUE-001)."""
    # Check if the series is empty or if all existing IDs are null/NaN
    if existing_ids.empty or existing_ids.isnull().all():
        return "ISSUE-001"
    # Filter out potential NaN values before extracting numbers
    max_num = existing_ids.str.extract(r'ISSUE-(\d+)', expand=False).astype(int).max()
    new_num = max_num + 1
    return f"ISSUE-{new_num:03d}"
def add_issue(raised_by, fund_impacted, data_source, issue_date, description):
    """Adds a new issue to the CSV file."""
    df = load_issues()
    new_id = _generate_issue_id(df['IssueID'])
    new_issue = pd.DataFrame({
        'IssueID': [new_id],
        'DateRaised': [datetime.now().date()],
        'RaisedBy': [raised_by],
        'FundImpacted': [fund_impacted],
        'DataSource': [data_source],
        'IssueDate': [issue_date], # Date the issue relates to
        'Description': [description],
        'Status': ['Open'],
        'DateClosed': [pd.NaT],
        'ClosedBy': [None],
        'ResolutionComment': [None]
    })
    df_updated = pd.concat([df, new_issue], ignore_index=True)
    _save_issues(df_updated)
    return new_id # Return the ID of the newly added issue
def close_issue(issue_id, closed_by, resolution_comment):
    """Marks an issue as closed in the CSV file."""
    df = load_issues()
    # Ensure IssueID is string for comparison
    df['IssueID'] = df['IssueID'].astype(str)
    issue_id_str = str(issue_id) # Ensure input is also string
    if issue_id_str in df['IssueID'].values:
        idx = df[df['IssueID'] == issue_id_str].index
        df.loc[idx, 'Status'] = 'Closed'
        df.loc[idx, 'DateClosed'] = datetime.now().date()
        df.loc[idx, 'ClosedBy'] = closed_by
        df.loc[idx, 'ResolutionComment'] = resolution_comment
        _save_issues(df)
        return True
    else:
        print(f"Issue ID {issue_id_str} not found.")
        return False
def load_fund_list():
    """Loads the list of funds from FundList.csv."""
    if os.path.exists(FUND_LIST_FILE):
        try:
            # Assuming FundList.csv has a column named 'FundCode' or similar
            # Adjust the column name as necessary
            fund_df = pd.read_csv(FUND_LIST_FILE)
            if 'FundCode' in fund_df.columns: # Check for 'FundCode'
                return fund_df['FundCode'].unique().tolist()
            elif 'Code' in fund_df.columns: # Fallback check for 'Code'
                 return fund_df['Code'].unique().tolist()
            elif 'Fund Code' in fund_df.columns: # Add check for 'Fund Code' with space
                 return fund_df['Fund Code'].unique().tolist()
            else:
                 print(f"Warning: Could not find a suitable fund code column ('FundCode', 'Code', or 'Fund Code') in {FUND_LIST_FILE}")
                 return [] # Return empty if column not found
        except Exception as e:
            print(f"Error loading fund list: {e}")
            return []
    else:
        print("Warning: FundList.csv not found.")
        return []
# Ensure the issues file exists with headers if it doesn't
if not os.path.exists(ISSUES_FILE):
    print(f"Creating initial issues file: {ISSUES_FILE}")
    # Create empty DataFrame with correct columns and save it
    initial_df = pd.DataFrame(columns=[
        'IssueID', 'DateRaised', 'RaisedBy', 'FundImpacted', 'DataSource',
        'IssueDate', 'Description', 'Status', 'DateClosed', 'ClosedBy',
        'ResolutionComment'
    ])
    _save_issues(initial_df)
</file>

<file path="LICENSE">
MIT License

Copyright (c) [2025] [Robert Clark]

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</file>

<file path="metric_calculator.py">
# This file provides functions for calculating various statistical metrics from the preprocessed data.
# Key functionalities include calculating historical statistics (mean, max, min), latest values,
# period-over-period changes, and Z-scores for changes for both benchmark and fund columns.
# It operates on a pandas DataFrame indexed by Date and Fund Code, producing a summary DataFrame
# containing these metrics for each fund, often sorted by the most significant recent changes (Z-scores).
# It now supports calculating metrics for both a primary and an optional secondary DataFrame.
# metric_calculator.py
# This file contains functions for calculating metrics from the processed data.
# Updated to handle primary and optional secondary data sources.
import pandas as pd
import numpy as np
import logging
import os # Needed for logging setup
from typing import List, Dict, Any, Tuple, Optional
# --- Logging Setup ---
# Use the same log file as data_loader
LOG_FILENAME = 'data_processing_errors.log'
LOG_FORMAT = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
# Get the logger for the current module
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
# Prevent adding handlers multiple times (especially if imported by other modules)
if not logger.handlers:
    # Console Handler (INFO and above)
    ch = logging.StreamHandler()
    ch.setLevel(logging.INFO)
    ch_formatter = logging.Formatter(LOG_FORMAT)
    ch.setFormatter(ch_formatter)
    logger.addHandler(ch)
    # File Handler (WARNING and above)
    try:
        # Create log file path relative to this file's location
        log_filepath = os.path.join(os.path.dirname(__file__), '..', LOG_FILENAME)
        fh = logging.FileHandler(log_filepath, mode='a')
        fh.setLevel(logging.WARNING)
        fh_formatter = logging.Formatter(LOG_FORMAT)
        fh.setFormatter(fh_formatter)
        logger.addHandler(fh)
    except Exception as e:
        # Log to stderr if file logging setup fails
        import sys
        print(f"Error setting up file logging for metric_calculator: {e}", file=sys.stderr)
# --- End Logging Setup ---
# Configure logging (can be configured globally elsewhere if part of a larger app)
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
def _calculate_column_stats(
    col_series: pd.Series,
    col_change_series: pd.Series,
    latest_date: pd.Timestamp,
    col_name: str,
    prefix: str = "" # Optional prefix for metric names (e.g., "S&P " )
) -> Dict[str, Any]:
    """Helper function to calculate stats for a single column series.
    Calculates historical mean/max/min, latest value, latest change, and change z-score.
    Handles potential NaN values resulting from calculations or missing data gracefully.
    Args:
        col_series (pd.Series): The historical data for the column.
        col_change_series (pd.Series): The historical changes for the column.
        latest_date (pd.Timestamp): The overall latest date in the dataset.
        col_name (str): The name of the column being processed.
        prefix (str): A prefix to add to the metric names in the output dictionary.
    Returns:
        Dict[str, Any]: A dictionary containing the calculated metrics for this column.
    """
    metrics = {}
    # Calculate base historical stats for the column level
    # Pandas functions like mean, max, min typically handle NaNs by skipping them.
    metrics[f'{prefix}{col_name} Mean'] = col_series.mean()
    metrics[f'{prefix}{col_name} Max'] = col_series.max()
    metrics[f'{prefix}{col_name} Min'] = col_series.min()
    # Calculate stats for the column change
    change_mean = col_change_series.mean()
    change_std = col_change_series.std()
    # Get latest values if data exists for the latest date
    # Check if the latest_date exists in the specific series index
    if latest_date in col_series.index:
        latest_value = col_series.loc[latest_date]
        # Use .get() for change series to handle potential index mismatch (though unlikely if derived correctly)
        latest_change = col_change_series.get(latest_date, np.nan)
        metrics[f'{prefix}{col_name} Latest Value'] = latest_value
        metrics[f'{prefix}{col_name} Change'] = latest_change
        # Calculate Change Z-Score: (latest_change - change_mean) / change_std
        change_z_score = np.nan # Default to NaN
        if pd.notna(latest_change) and pd.notna(change_mean) and pd.notna(change_std) and change_std != 0:
            change_z_score = (latest_change - change_mean) / change_std
        elif change_std == 0 and pd.notna(latest_change) and pd.notna(change_mean):
             # Handle case where std dev is zero (e.g., constant series)
             if latest_change == change_mean:
                 change_z_score = 0.0 # No deviation
             else:
                 change_z_score = np.inf if latest_change > change_mean else -np.inf # Infinite deviation
             logger.debug(f"Standard deviation of change for '{prefix}{col_name}' is zero. Z-score set to {change_z_score}.")
        else:
            # Log if Z-score calculation couldn't be performed due to NaNs
            if not (pd.notna(latest_change) and pd.notna(change_mean) and pd.notna(change_std)):
                 logger.debug(f"Cannot calculate Z-score for '{prefix}{col_name}' due to NaN inputs (latest_change={latest_change}, change_mean={change_mean}, change_std={change_std})")
        metrics[f'{prefix}{col_name} Change Z-Score'] = change_z_score
    else:
        # Data for the latest date is missing for this specific column/fund
        logger.debug(f"Latest date {latest_date} not found for column '{prefix}{col_name}'. Setting latest metrics to NaN.")
        metrics[f'{prefix}{col_name} Latest Value'] = np.nan
        metrics[f'{prefix}{col_name} Change'] = np.nan
        metrics[f'{prefix}{col_name} Change Z-Score'] = np.nan
    return metrics
def _process_dataframe_metrics(
    df: pd.DataFrame,
    fund_codes: pd.Index,
    fund_cols: List[str],
    benchmark_col: Optional[str],
    latest_date: pd.Timestamp,
    metric_prefix: str = ""
) -> Tuple[List[Dict[str, Any]], Dict[str, float]]:
    """Processes a single DataFrame (primary or secondary) to calculate metrics.
    Args:
        df (pd.DataFrame): The DataFrame to process (already sorted by date index).
        fund_codes (pd.Index): Unique fund codes from the combined data.
        fund_cols (List[str]): List of original fund value column names for this df.
        benchmark_col (Optional[str]): Standardized benchmark column name for this df, if present.
        latest_date (pd.Timestamp): The latest date across combined data.
        metric_prefix (str): Prefix to add to metric names (e.g., "S&P ").
    Returns:
        Tuple[List[Dict[str, Any]], Dict[str, float]]:
            - List of metric dictionaries, one per fund.
            - Dictionary mapping fund code to its max absolute change Z-score for sorting.
    """
    if df is None or df.empty:
        logger.warning(f"Input DataFrame for prefix '{metric_prefix}' is None or empty. Returning empty results.")
        return [], {}
    # Determine which columns to actually process based on presence in df
    cols_to_process = []
    output_col_name_map = {} # Map processed col name to output name (original/std)
    if benchmark_col and benchmark_col in df.columns:
        cols_to_process.append(benchmark_col)
        output_col_name_map[benchmark_col] = benchmark_col
    elif benchmark_col:
        logger.warning(f"Specified {metric_prefix}benchmark column '{benchmark_col}' not found in DataFrame columns: {df.columns.tolist()}")
    for f_col in fund_cols:
        if f_col in df.columns:
            cols_to_process.append(f_col)
            output_col_name_map[f_col] = f_col
        else:
            logger.warning(f"Specified {metric_prefix}fund column '{f_col}' not found in DataFrame columns: {df.columns.tolist()}")
    if not cols_to_process:
        logger.error(f"No valid columns (benchmark or funds) found in the {metric_prefix}DataFrame to calculate metrics for.")
        return [], {}
    logger.info(f"Calculating {metric_prefix}metrics for columns: {cols_to_process}")
    fund_metrics_list = []
    max_abs_z_scores: Dict[str, float] = {}
    for fund_code in fund_codes:
        fund_specific_metrics: Dict[str, Any] = {'Fund Code': fund_code} # Initialize with Fund Code
        current_fund_max_abs_z: float = -1.0
        try:
            # Check if fund exists in this specific dataframe
            if fund_code not in df.index.get_level_values(1):
                logger.debug(f"Fund code '{fund_code}' not found in {metric_prefix}DataFrame. Adding empty metrics.")
                # Add NaN placeholders for all expected metrics for this source
                for col_name_proc in cols_to_process:
                    output_name = output_col_name_map[col_name_proc]
                    fund_specific_metrics[f'{metric_prefix}{output_name} Mean'] = np.nan
                    fund_specific_metrics[f'{metric_prefix}{output_name} Max'] = np.nan
                    fund_specific_metrics[f'{metric_prefix}{output_name} Min'] = np.nan
                    fund_specific_metrics[f'{metric_prefix}{output_name} Latest Value'] = np.nan
                    fund_specific_metrics[f'{metric_prefix}{output_name} Change'] = np.nan
                    fund_specific_metrics[f'{metric_prefix}{output_name} Change Z-Score'] = np.nan
                fund_metrics_list.append(fund_specific_metrics)
                max_abs_z_scores[fund_code] = np.nan # No Z-score if fund not present
                continue # Move to the next fund code
            # Extract data for the fund
            fund_data_hist = df.loc[(slice(None), fund_code), cols_to_process]
            fund_data_hist = fund_data_hist.reset_index(level=1, drop=True).sort_index()
            for col_name in cols_to_process:
                if col_name not in fund_data_hist.columns:
                    logger.warning(f"Column '{col_name}' unexpectedly not found for fund '{fund_code}' in {metric_prefix}DF. Skipping metrics.")
                    continue
                col_hist = fund_data_hist[col_name]
                col_change_hist = pd.Series(index=col_hist.index, dtype=np.float64)
                if not col_hist.dropna().empty and len(col_hist.dropna()) > 1:
                    col_change_hist = col_hist.diff()
                else:
                    logger.debug(f"Cannot calculate difference for {metric_prefix}column '{col_name}', fund '{fund_code}' due to insufficient data.")
                # Calculate stats for this specific column
                output_name = output_col_name_map[col_name]
                col_stats = _calculate_column_stats(col_hist, col_change_hist, latest_date, output_name, prefix=metric_prefix)
                fund_specific_metrics.update(col_stats)
                # Update the fund's max absolute Z-score *for this source*
                col_z_score = col_stats.get(f'{metric_prefix}{output_name} Change Z-Score', np.nan)
                compare_z_score = col_z_score
                if np.isinf(compare_z_score):
                    compare_z_score = 1e9 * np.sign(compare_z_score)
                if pd.notna(compare_z_score):
                    current_fund_max_abs_z = max(current_fund_max_abs_z, abs(compare_z_score))
            fund_metrics_list.append(fund_specific_metrics)
            max_abs_z_scores[fund_code] = current_fund_max_abs_z if current_fund_max_abs_z >= 0 else np.nan
        except Exception as e:
            logger.error(f"Error processing {metric_prefix}metrics for fund code '{fund_code}': {e}", exc_info=True)
            # Add placeholder with NaNs if error occurs mid-fund processing
            if 'Fund Code' not in fund_specific_metrics: # Ensure Fund Code is there
                fund_specific_metrics['Fund Code'] = fund_code
            # Add NaN placeholders for potentially missing metrics
            for col_name_proc in cols_to_process:
                output_name = output_col_name_map[col_name_proc]
                if f'{metric_prefix}{output_name} Mean' not in fund_specific_metrics: fund_specific_metrics[f'{metric_prefix}{output_name} Mean'] = np.nan
                if f'{metric_prefix}{output_name} Max' not in fund_specific_metrics: fund_specific_metrics[f'{metric_prefix}{output_name} Max'] = np.nan
                # ... (add for all metrics) ...
                if f'{metric_prefix}{output_name} Change Z-Score' not in fund_specific_metrics: fund_specific_metrics[f'{metric_prefix}{output_name} Change Z-Score'] = np.nan
            fund_metrics_list.append(fund_specific_metrics)
            max_abs_z_scores[fund_code] = np.nan # Mark as NaN for sorting if error occurred
    return fund_metrics_list, max_abs_z_scores
def calculate_latest_metrics(
    primary_df: Optional[pd.DataFrame],
    primary_fund_cols: Optional[List[str]],
    primary_benchmark_col: Optional[str],
    secondary_df: Optional[pd.DataFrame] = None,
    secondary_fund_cols: Optional[List[str]] = None,
    secondary_benchmark_col: Optional[str] = None,
    secondary_prefix: str = "S&P " # Prefix for secondary metrics
) -> pd.DataFrame:
    """Calculates latest metrics for primary and optional secondary data, including relative metrics.
    Merges metrics from both sources based on Fund Code.
    Sorts the final DataFrame by the maximum absolute Z-score from the *primary* source.
    Args:
        primary_df (Optional[pd.DataFrame]): Primary processed DataFrame.
        primary_fund_cols (Optional[List[str]]): List of primary fund value column names.
        primary_benchmark_col (Optional[str]): Standardized primary benchmark column name.
        secondary_df (Optional[pd.DataFrame]): Secondary processed DataFrame. Defaults to None.
        secondary_fund_cols (Optional[List[str]]): List of secondary fund value column names. Defaults to None.
        secondary_benchmark_col (Optional[str]): Standardized secondary benchmark column name. Defaults to None.
        secondary_prefix (str): Prefix for secondary metric columns. Defaults to "S&P ".
    Returns:
        pd.DataFrame: Combined metrics indexed by Fund Code, sorted by primary max abs Z-score.
                      Returns an empty DataFrame if primary data is missing or processing fails critically.
    """
    if primary_df is None or primary_df.empty or primary_fund_cols is None:
        logger.warning("Primary DataFrame or fund columns are missing. Cannot calculate metrics.")
        return pd.DataFrame()
    if primary_df.index.nlevels != 2:
        logger.error("Primary DataFrame must have a MultiIndex with 2 levels (Date, Fund Code).")
        return pd.DataFrame()
    # Combine fund codes and find the overall latest date
    all_dfs = [df for df in [primary_df, secondary_df] if df is not None and not df.empty]
    if not all_dfs:
        logger.warning("No valid DataFrames provided. Cannot calculate metrics.")
        return pd.DataFrame()
    try:
        combined_index = pd.concat(all_dfs).index
        latest_date = combined_index.get_level_values(0).max()
        fund_codes = combined_index.get_level_values(1).unique()
        # Ensure DataFrames are sorted by date index for diff calculation
        primary_df_sorted = primary_df.sort_index(level=0)
        secondary_df_sorted = secondary_df.sort_index(level=0) if secondary_df is not None else None
    except Exception as e:
        logger.error(f"Error preparing combined data for metric calculation: {e}", exc_info=True)
        return pd.DataFrame()
    # --- Calculate Base Metrics for Primary Data --- #
    primary_metrics_list, primary_max_abs_z = _process_dataframe_metrics(
        primary_df_sorted,
        fund_codes, # Use combined fund codes
        primary_fund_cols,
        primary_benchmark_col,
        latest_date,
        metric_prefix="" # No prefix for primary
    )
    primary_metrics_df = pd.DataFrame(primary_metrics_list).set_index('Fund Code') if primary_metrics_list else pd.DataFrame(index=fund_codes)
    # --- Calculate Base Metrics for Secondary Data (if present) --- #
    secondary_metrics_df = pd.DataFrame(index=fund_codes) # Initialize empty df with correct index
    if secondary_df_sorted is not None and secondary_fund_cols is not None:
        logger.info(f"Processing secondary data with prefix: '{secondary_prefix}'")
        secondary_metrics_list, _ = _process_dataframe_metrics(
            secondary_df_sorted,
            fund_codes, # Use combined fund codes
            secondary_fund_cols,
            secondary_benchmark_col,
            latest_date,
            metric_prefix=secondary_prefix
        )
        if secondary_metrics_list:
            secondary_metrics_df = pd.DataFrame(secondary_metrics_list).set_index('Fund Code')
    else:
        logger.info("No valid secondary data provided or fund columns missing, skipping secondary metrics.")
    # --- Calculate RELATIVE Metrics (Primary) ---
    if primary_benchmark_col and primary_benchmark_col in primary_df_sorted.columns and primary_fund_cols:
        # Find the first primary fund column that actually exists
        pri_fund_col_used = next((col for col in primary_fund_cols if col in primary_df_sorted.columns), None)
        if pri_fund_col_used:
            logger.info(f"Calculating primary relative metrics ({pri_fund_col_used} - {primary_benchmark_col}).")
            relative_metrics_list = []
            for fund_code in fund_codes:
                if fund_code not in primary_df_sorted.index.get_level_values(1):
                    continue # Skip if fund not in primary data
                fund_data = primary_df_sorted.loc[(slice(None), fund_code), [pri_fund_col_used, primary_benchmark_col]]
                fund_data = fund_data.reset_index(level=1, drop=True).sort_index()
                # Calculate historical relative series
                port_col_hist = fund_data[pri_fund_col_used]
                bench_col_hist = fund_data[primary_benchmark_col]
                if not port_col_hist.dropna().empty and not bench_col_hist.dropna().empty:
                    relative_hist = port_col_hist - bench_col_hist
                    relative_change_hist = pd.Series(index=relative_hist.index, dtype=np.float64)
                    if not relative_hist.dropna().empty and len(relative_hist.dropna()) > 1:
                        relative_change_hist = relative_hist.diff()
                    # Calculate stats for the relative series
                    relative_stats = _calculate_column_stats(
                        relative_hist, relative_change_hist, latest_date, "Relative", prefix=""
                    )
                    relative_stats['Fund Code'] = fund_code # Add Fund Code for merging
                    relative_metrics_list.append(relative_stats)
                else:
                     logger.debug(f"Skipping primary relative calculation for {fund_code} due to insufficient data.")
            if relative_metrics_list:
                relative_primary_df = pd.DataFrame(relative_metrics_list).set_index('Fund Code')
                # Add these relative columns to the primary metrics dataframe
                primary_metrics_df = primary_metrics_df.merge(relative_primary_df, left_index=True, right_index=True, how='left')
        else:
            logger.warning("Could not find a valid primary fund column to calculate relative metrics.")
    else:
         logger.info("Skipping primary relative metric calculation (benchmark or fund column missing).")
    # --- Calculate RELATIVE Metrics (Secondary) ---
    if secondary_df_sorted is not None and secondary_benchmark_col and secondary_benchmark_col in secondary_df_sorted.columns and secondary_fund_cols:
        # Find the first secondary fund column that actually exists
        sec_fund_col_used = next((col for col in secondary_fund_cols if col in secondary_df_sorted.columns), None)
        if sec_fund_col_used:
            logger.info(f"Calculating secondary relative metrics ({sec_fund_col_used} - {secondary_benchmark_col}) with prefix '{secondary_prefix}'.")
            relative_metrics_list_sec = []
            for fund_code in fund_codes:
                if fund_code not in secondary_df_sorted.index.get_level_values(1):
                    continue # Skip if fund not in secondary data
                fund_data_sec = secondary_df_sorted.loc[(slice(None), fund_code), [sec_fund_col_used, secondary_benchmark_col]]
                fund_data_sec = fund_data_sec.reset_index(level=1, drop=True).sort_index()
                # Calculate historical relative series
                port_col_hist_sec = fund_data_sec[sec_fund_col_used]
                bench_col_hist_sec = fund_data_sec[secondary_benchmark_col]
                if not port_col_hist_sec.dropna().empty and not bench_col_hist_sec.dropna().empty:
                    relative_hist_sec = port_col_hist_sec - bench_col_hist_sec
                    relative_change_hist_sec = pd.Series(index=relative_hist_sec.index, dtype=np.float64)
                    if not relative_hist_sec.dropna().empty and len(relative_hist_sec.dropna()) > 1:
                        relative_change_hist_sec = relative_hist_sec.diff()
                     # Calculate stats for the secondary relative series
                    relative_stats_sec = _calculate_column_stats(
                        relative_hist_sec, relative_change_hist_sec, latest_date, "Relative", prefix=secondary_prefix
                    )
                    relative_stats_sec['Fund Code'] = fund_code # Add Fund Code for merging
                    relative_metrics_list_sec.append(relative_stats_sec)
                else:
                     logger.debug(f"Skipping secondary relative calculation for {fund_code} due to insufficient data.")
            if relative_metrics_list_sec:
                relative_secondary_df = pd.DataFrame(relative_metrics_list_sec).set_index('Fund Code')
                 # Add these relative columns to the secondary metrics dataframe
                secondary_metrics_df = secondary_metrics_df.merge(relative_secondary_df, left_index=True, right_index=True, how='left')
        else:
            logger.warning("Could not find a valid secondary fund column to calculate relative metrics.")
    else:
         logger.info("Skipping secondary relative metric calculation (benchmark or fund column missing or secondary data missing).")
    # --- Combine Base and Relative Metrics --- #
    if not primary_metrics_df.empty:
        # Merge primary and secondary base+relative metrics based on Fund Code index
        combined_metrics_df = primary_metrics_df.merge(
            secondary_metrics_df, left_index=True, right_index=True, how='outer', suffixes=('', '_sec_merge_temp') # Avoid direct column clashes if secondary only had relative
        )
        # Clean up potential temporary suffixes if secondary only contributed relative cols
        combined_metrics_df.columns = [col.replace('_sec_merge_temp', '') for col in combined_metrics_df.columns]
    elif not secondary_metrics_df.empty:
         # Only secondary data was processed (unlikely given initial checks, but handle)
         logger.warning("Only secondary metrics were calculated.")
         combined_metrics_df = secondary_metrics_df
    else:
        logger.warning("No metrics could be calculated for primary or secondary data.")
        return pd.DataFrame()
    # --- Sort Results --- #
    # Add the primary max abs Z-score as a temporary column for sorting
    # Use .get() on the dictionary to handle funds potentially missing from primary results
    combined_metrics_df['_sort_z'] = combined_metrics_df.index.map(lambda fc: primary_max_abs_z.get(fc, np.nan))
    # Sort by the temporary Z-score column (descending), put NaNs last
    combined_metrics_df_sorted = combined_metrics_df.sort_values(by='_sort_z', ascending=False, na_position='last')
    # Drop the temporary sorting column
    combined_metrics_df_sorted = combined_metrics_df_sorted.drop(columns=['_sort_z'])
    logger.info(f"Successfully calculated and combined metrics. Final shape: {combined_metrics_df_sorted.shape}")
    return combined_metrics_df_sorted
</file>

<file path="process_data.py">
# This script serves as a pre-processing step for specific CSV files within the configured data directory.
# It targets files prefixed with 'pre_', reads them, and performs data aggregation and cleaning.
# The core logic involves grouping rows based on identical values across most columns, excluding 'Funds', 'Security Name', and potentially 'ISIN'.
# For rows sharing the same 'Security Name' but differing in other data points, the script attempts to find
# an 'ISIN' column and suffixes its value (e.g., -1, -2) to ensure uniqueness for downstream processing.
# The 'Funds' associated with identical data rows are aggregated into a single list-like string representation (e.g., '[FUND1,FUND2]').
# The processed data is then saved to a corresponding CSV file prefixed with 'sec_' (overwriting existing files)
# in the same data directory.
# It also processes weight files (`pre_w_*.csv`).
# The data directory is determined dynamically using `utils.get_data_folder_path`.
# process_data.py
# This script processes CSV files in the 'Data' directory that start with 'pre_'.
# It merges rows based on identical values in most columns (excluding 'Funds', 'Security Name', potentially 'ISIN').
# If multiple data versions exist for the same 'Security Name', it suffixes the 'ISIN' column value (-1, -2, etc.).
# The aggregated 'Funds' are stored as a list-like string in the output file, saved as 'sec_*.csv'.
import os
import pandas as pd
import logging
# Add datetime for date parsing and sorting
from datetime import datetime
import io
# Import the new weight processing function
from weight_processing import process_weight_file
# Import the path utility
from utils import get_data_folder_path
# Get the logger instance. Assumes logging is configured elsewhere (e.g., by Flask app or calling script).
logger = logging.getLogger(__name__)
# --- Removed logging setup block --- 
# Logging is now handled centrally by the Flask app factory in app.py or by the script runner.
# Removed DATES_FILE_PATH constant - path is now determined dynamically in main()
def read_and_sort_dates(dates_file_path):
    """
    Reads dates from a CSV file, sorts them, and returns them as a list of strings.
    Args:
        dates_file_path (str): Absolute path to the CSV file containing dates.
    Returns:
        list[str] | None: A sorted list of date strings (YYYY-MM-DD) or None if an error occurs.
    """
    if not dates_file_path:
        logger.error("No dates_file_path provided to read_and_sort_dates.")
        return None
    if not os.path.exists(dates_file_path):
        logger.error(f"Dates file not found at {dates_file_path}")
        return None
    try:
        dates_df = pd.read_csv(dates_file_path, parse_dates=[0]) # Assume date is the first column
        # Handle potential parsing errors if the column isn't purely dates
        if dates_df.iloc[:, 0].isnull().any():
             logger.warning(f"Warning: Some values in {dates_file_path} could not be parsed as dates.")
             # Attempt to drop NaT values and proceed
             dates_df = dates_df.dropna(subset=[dates_df.columns[0]])
             if dates_df.empty:
                 logger.error(f"Error: No valid dates found in {dates_file_path} after handling parsing issues.")
                 return None
        # Sort dates chronologically
        sorted_dates = dates_df.iloc[:, 0].sort_values()
        # Format dates as 'YYYY-MM-DD' strings for column headers
        date_strings = sorted_dates.dt.strftime('%Y-%m-%d').tolist()
        logger.info(f"Successfully read and sorted {len(date_strings)} dates from {dates_file_path}.")
        # --- Deduplicate the date list while preserving order --- 
        unique_date_strings = []
        seen_dates = set()
        duplicates_found = False
        for date_str in date_strings:
            if date_str not in seen_dates:
                unique_date_strings.append(date_str)
                seen_dates.add(date_str)
            else:
                duplicates_found = True
        if duplicates_found:
            logger.warning(f"Duplicate dates found in {dates_file_path}. Using unique sorted dates: {len(unique_date_strings)} unique dates.")
        # --- End Deduplication ---
        return unique_date_strings # Return the deduplicated list
    except FileNotFoundError:
        # This case should ideally be caught by the initial os.path.exists check, but included for robustness
        logger.error(f"Error: Dates file not found at {dates_file_path}")
        return None
    except pd.errors.EmptyDataError:
        logger.error(f"Error: Dates file is empty - {dates_file_path}")
        return None
    except IndexError:
        logger.error(f"Error: Dates file {dates_file_path} seems to have no columns.")
        return None
    except Exception as e:
        logger.error(f"An unexpected error occurred reading dates from {dates_file_path}: {e}", exc_info=True)
        return None
# Modify process_csv_file signature to accept date_columns
def process_csv_file(input_path, output_path, date_columns, dates_file_path):
    """
    Reads a 'pre_' CSV file, potentially replaces placeholder columns with dates,
    processes it according to the rules, and writes the result to a 'sec_' CSV file,
    overwriting if it exists.
    Args:
        input_path (str): Path to the input CSV file (e.g., 'Data/pre_sec_duration.csv').
        output_path (str): Path to the output CSV file (e.g., 'Data/sec_duration.csv').
        date_columns (list[str] | None): Sorted list of date strings for headers, or None if dates couldn't be read.
        dates_file_path (str): Path to the dates CSV file.
    """
    # If date_columns is None (due to error reading dates.csv), log and skip processing files needing date replacement.
    # We'll handle the actual replacement logic further down.
    if date_columns is None:
         logger.warning(f"Skipping {input_path} because date information is unavailable (check logs for errors reading dates.csv).")
         # A file might still be processable if its columns are *already* correct dates,
         # but the current logic requires date_columns for the check/replacement.
         # To proceed without dates.csv, the logic would need significant changes.
         return # Skip processing this file if dates aren't loaded.
    try:
        # Read the input CSV - add robustness
        # Skip bad lines, handle encoding errors
        df = pd.read_csv(input_path, on_bad_lines='skip', encoding='utf-8', encoding_errors='replace')
        logger.info(f"Processing file: {input_path}")
        # Log DataFrame info right after reading (DEBUG level)
        if logger.isEnabledFor(logging.DEBUG):
            logger.debug(f"DataFrame info after read for {input_path}:")
            buf = io.StringIO()
            df.info(verbose=True, buf=buf)
            logger.debug(buf.getvalue())
        if df.empty:
            logger.warning(f"Input file {input_path} is empty or contains only invalid lines. Skipping processing.")
            return
        # --- Column Header Replacement & Validation Logic ---
        original_cols = df.columns.tolist()
        # Define core columns, allowing for 'Fund' or 'Funds'
        fund_col_name = None
        if 'Funds' in original_cols:
            fund_col_name = 'Funds'
        elif 'Fund' in original_cols:
            fund_col_name = 'Fund'
            logger.info(f"Found 'Fund' column in {input_path}. Will rename to 'Funds' for processing.")
            # Rename the column IN PLACE for subsequent operations
            df.rename(columns={'Fund': 'Funds'}, inplace=True)
        else:
            logger.error(f"Skipping {input_path}: Missing required fund column (neither 'Funds' nor 'Fund' found). Found columns: {original_cols}")
            return
        # Now define required cols using the standardized 'Funds' name
        required_cols = ['Funds', 'Security Name']
        # Check for 'Security Name' (already checked for fund column)
        if 'Security Name' not in original_cols:
            logger.error(f"Skipping {input_path}: Missing required column 'Security Name'. Found columns: {original_cols}")
            return
        logger.debug(f"Required columns {required_cols} confirmed present (or renamed) in {input_path}.")
        # Find the index after the last required column to start searching for placeholders
        # Use the potentially renamed df.columns
        current_df_cols = df.columns.tolist()
        last_required_idx = -1
        for req_col in required_cols:
            try:
                last_required_idx = max(last_required_idx, current_df_cols.index(req_col))
            except ValueError: # Should not happen due to checks above, but safeguard
                 logger.error(f"Required column '{req_col}' unexpectedly not found after initial check/rename in {input_path}. Skipping.")
                 return
        candidate_start_index = last_required_idx + 1
        # Use current_df_cols which includes the renamed column if applicable
        candidate_cols = current_df_cols[candidate_start_index:]
        # Decide on the final columns to use for processing
        current_cols = current_df_cols # Default to current (potentially renamed) columns
        # --- Enhanced Placeholder Detection ---
        # Detect sequences like 'Col', 'Col.1', 'Col.2', ... (Pandas default)
        # AND sequences like 'Base', 'Base', 'Base', ... (Target for date replacement)
        potential_placeholder_base_patternA = None # For Base, Base.1, ...
        detected_sequence_patternA = []
        start_index_patternA = -1
        potential_placeholder_base_patternB = None # For Base, Base, Base, ...
        detected_sequence_patternB = []
        start_index_patternB = -1
        is_patternB_dominant = False # Flag if Pattern B is found and should trigger date replacement
        if not candidate_cols:
            logger.warning(f"File {input_path} has no columns after required columns {required_cols}. Cannot check for date placeholders.")
        else:
            # Check for Pattern B first: 'Base', 'Base', 'Base', ...
            first_candidate = candidate_cols[0]
            # Check if ALL candidate columns are identical to the first one
            if all(col == first_candidate for col in candidate_cols):
                potential_placeholder_base_patternB = first_candidate
                detected_sequence_patternB = candidate_cols
                start_index_patternB = 0 # Starts at the beginning of candidates
                logger.debug(f"Detected Pattern B: Repeated column name '{potential_placeholder_base_patternB}' for all {len(detected_sequence_patternB)} candidate columns.")
                # If we find Pattern B covering *all* candidates, we prioritize it for date replacement check
                is_patternB_dominant = True
            else:
                 logger.info(f"Candidate columns are not all identical (Pattern B check failed). First candidate: '{first_candidate}'. Candidates: {candidate_cols[:5]}...")
                 # If Pattern B check fails, proceed to check for Pattern A ('Base', 'Base.1', ...)
                 # Iterate through candidate columns to find the *start* of the sequence 'Base', 'Base.1', ...
                 found_sequence_A = False
                 for start_idx in range(len(candidate_cols)):
                     current_potential_base = candidate_cols[start_idx]
                     # Check if it's a potential base name (no '.' suffix)
                     if '.' not in current_potential_base:
                         logger.debug(f"Checking for Pattern A starting with '{current_potential_base}' at index {start_idx} in candidate columns.")
                         temp_sequence = [current_potential_base]
                         # Check subsequent columns for the pattern 'base.1', 'base.2', etc.
                         for i in range(1, len(candidate_cols) - start_idx):
                             expected_col = f"{current_potential_base}.{i}"
                             actual_col_index = start_idx + i
                             if candidate_cols[actual_col_index] == expected_col:
                                 temp_sequence.append(candidate_cols[actual_col_index])
                             else:
                                 logger.debug(f"Pattern A sequence broken at index {actual_col_index}. Expected '{expected_col}', found '{candidate_cols[actual_col_index]}'.")
                                 break # Stop checking for this base
                         if len(temp_sequence) > 1: # Found Base, Base.1 at minimum
                             potential_placeholder_base_patternA = current_potential_base
                             detected_sequence_patternA = temp_sequence
                             start_index_patternA = start_idx
                             logger.debug(f"Found Pattern A sequence starting with '{potential_placeholder_base_patternA}' at candidate index {start_index_patternA} with length {len(detected_sequence_patternA)}.")
                             found_sequence_A = True
                             break # Exit the outer loop for Pattern A search
                         else:
                             logger.debug(f"Only base '{current_potential_base}' found or Pattern A sequence too short. Continuing search.")
                             # Continue loop to check next candidate as potential base
                     else:
                         logger.debug(f"Column '{current_potential_base}' at index {start_idx} has '.' suffix, skipping as potential Pattern A base.")
                 if not found_sequence_A:
                     logger.info(f"No Pattern A sequence ('Base', 'Base.1', ...) found in candidate columns of {input_path}.")
        # --- Date Replacement Logic using Detected Patterns ---
        if date_columns is None:
            logger.warning(f"Date information from {dates_file_path} is unavailable. Cannot check or replace headers in {input_path}. Processing with original headers: {original_cols}")
        # --- Prioritize Pattern B for Date Replacement ---
        elif is_patternB_dominant:
             placeholder_count_B = len(detected_sequence_patternB)
             original_placeholder_start_index_B = candidate_start_index + start_index_patternB # Should be last_required_idx + 1
             logger.info(f"Processing based on detected Pattern B ('{potential_placeholder_base_patternB}' repeated {placeholder_count_B} times), starting at original index {original_placeholder_start_index_B}.")
             if len(date_columns) == placeholder_count_B:
                 logger.info(f"Replacing {placeholder_count_B} repeated '{potential_placeholder_base_patternB}' columns with dates.")
                 # Use current_cols here to respect potential prior rename ('Fund' -> 'Funds')
                 cols_before = current_cols[:original_placeholder_start_index_B]
                 cols_after = current_cols[original_placeholder_start_index_B + placeholder_count_B:]
                 new_columns = cols_before + date_columns + cols_after
                 if len(new_columns) != len(current_cols): # Compare against current_cols length
                     logger.error(f"Internal error (Pattern B): Column count mismatch after constructing new columns ({len(new_columns)} vs {len(current_cols)}). Reverting to original headers.")
                     # Revert logic might need refinement, but for now, keep current_cols as is.
                     # current_cols = original_cols # Reverting might lose the 'Fund'->'Funds' rename
                 else:
                     df.columns = new_columns
                     current_cols = new_columns
                     logger.info(f"Columns after Pattern B date replacement: {current_cols}")
             else:
                 logger.warning(f"Count mismatch for Pattern B in {input_path}: Found {placeholder_count_B} repeated '{potential_placeholder_base_patternB}' columns, but expected {len(date_columns)} dates. Skipping date replacement. Processing with original headers.")
                 # current_cols remains potentially renamed cols
        # --- Handle Pattern A or No Pattern ---
        # No Pattern B found, or it didn't match date count. Check Pattern A or if columns already match dates.
        else:
             if potential_placeholder_base_patternA:
                 # Pattern A ('Base', 'Base.1', ...) was found.
                 placeholder_count_A = len(detected_sequence_patternA)
                 original_placeholder_start_index_A = candidate_start_index + start_index_patternA
                 logger.debug(f"Detected Pattern A sequence based on '{potential_placeholder_base_patternA}' (length {placeholder_count_A}) starting at original index {original_placeholder_start_index_A}.")
                 # --- Attempt Date Replacement for Pattern A if lengths match ---
                 if len(date_columns) == placeholder_count_A:
                     logger.info(f"Replacing {placeholder_count_A} Pattern A columns ('{potential_placeholder_base_patternA}', '{potential_placeholder_base_patternA}.1', ...) with dates.")
                     # Use current_cols here to respect potential prior rename ('Fund' -> 'Funds')
                     cols_before = current_cols[:original_placeholder_start_index_A]
                     cols_after = current_cols[original_placeholder_start_index_A + placeholder_count_A:]
                     new_columns = cols_before + date_columns + cols_after
                     if len(new_columns) != len(current_cols): # Compare against current_cols length
                         logger.error(f"Internal error (Pattern A): Column count mismatch after constructing new columns ({len(new_columns)} vs {len(current_cols)}). Reverting to original headers.")
                         # Keep current_cols as is to avoid losing potential rename
                         # current_cols = original_cols
                     else:
                         df.columns = new_columns
                         current_cols = new_columns
                         logger.info(f"Columns after Pattern A date replacement using {len(date_columns)} dates for {input_path}.")
                 else:
                     # Lengths don't match, log warning and proceed with original (Pattern A) headers
                     logger.warning(f"Count mismatch for Pattern A in {input_path}: Found {placeholder_count_A} columns in sequence ('{potential_placeholder_base_patternA}', '{potential_placeholder_base_patternA}.1', ...), but expected {len(date_columns)} dates. Skipping date replacement. Processing with original headers.")
                     # current_cols remains potentially renamed cols
                 # --- End Date Replacement Logic for Pattern A ---
             elif candidate_cols == date_columns:
                 # No patterns found, but candidates already match dates
                 logger.debug(f"Columns in {input_path} (after required ones) already match the expected dates. No replacement needed.")
        # --- End Column Header Replacement Logic ---
        # Identify columns to check for identity (all except Funds and Security Name) using the CURRENT columns
        # These might be the original placeholders or the replaced dates.
        # Crucially, this now correctly includes any original static columns that were *not* replaced.
        id_cols = [col for col in current_cols if col not in required_cols]
        processed_rows = []
        # Convert 'Security Name' and 'Funds' to string first to handle potential non-string types causing issues later
        # Use 'Funds' as it has been standardized by rename operation if necessary
        df['Security Name'] = df['Security Name'].astype(str)
        df['Funds'] = df['Funds'].astype(str)
        # Group by the primary identifier 'Security Name'
        # Convert 'Security Name' to string first to handle potential non-string types causing groupby issues
        # df['Security Name'] = df['Security Name'].astype(str) # Already done above
        # Ensure 'Funds' is also string for consistent processing later
        # df['Funds'] = df['Funds'].astype(str) # Already done above
        # Use the potentially renamed DataFrame for grouping
        grouped_by_sec = df.groupby('Security Name', sort=False, dropna=False)
        for sec_name, sec_group in grouped_by_sec:
            # Within each security group, further group by all other identifying columns (which might now be dates)
            # This separates rows where the same Security Name has different associated data
            distinct_versions = []
            if id_cols: # Only subgroup if there are other identifying columns
                try:
                    # dropna=False treats NaNs in id_cols as equal for grouping
                    sub_grouped = sec_group.groupby(id_cols, dropna=False, sort=False)
                    distinct_versions = [group for _, group in sub_grouped]
                except KeyError as e:
                    logger.error(f"KeyError during sub-grouping for Security Name '{sec_name}' in {input_path}. Column: {e}. Grouping columns: {id_cols}. Skipping this security.", exc_info=True)
                    continue # Skip this security name if subgrouping fails
                except Exception as e:
                     logger.error(f"Unexpected error during sub-grouping for Security Name '{sec_name}' in {input_path}: {e}. Grouping columns: {id_cols}. Skipping this security.", exc_info=True)
                     continue
            else:
                # If only Security Name and Funds exist (after potential date column issues), treat the whole sec_group as one version
                 distinct_versions = [sec_group]
            num_versions = len(distinct_versions)
            # Iterate through each distinct version found for the current Security Name
            for i, current_version_df in enumerate(distinct_versions):
                if current_version_df.empty:
                    continue # Should not happen, but safeguard
                # Aggregate the unique 'Funds' for this specific version
                # Handle potential NaN values in Funds column before aggregation
                unique_funds = current_version_df['Funds'].dropna().unique()
                # Convert funds to string before joining
                funds_list = sorted([str(f) for f in unique_funds])
                # Take the first row of this version as the template for the output row
                # Use .iloc[0] safely as we checked current_version_df is not empty
                new_row_series = current_version_df.iloc[0].copy()
                # Assign the aggregated funds as a string formatted like a list: "[FUND1,FUND2,...]"
                new_row_series['Funds'] = f"[{','.join(funds_list)}]"
                # If there was more than one distinct version for this Security Name,
                # attempt to suffix the ISIN column to create a unique identifier.
                if num_versions > 1:
                    isin_col_name = 'ISIN' # Define the expected ISIN column name
                    if isin_col_name in new_row_series.index:
                        original_isin = new_row_series[isin_col_name]
                        # Ensure ISIN is treated as string for concatenation
                        new_isin = f"{str(original_isin)}-{i+1}"
                        new_row_series[isin_col_name] = new_isin
                        logger.debug(f"Suffixed ISIN for duplicate Security Name '{sec_name}'. Original: '{original_isin}', New: '{new_isin}'")
                    else:
                        # If ISIN column doesn't exist, log a warning. Do not modify Security Name.
                        logger.warning(f"Found {num_versions} distinct data versions for Security Name '{sec_name}' but column '{isin_col_name}' not found. Cannot apply suffix to ISIN.")
                # Append the processed row (as a dictionary) to our results list
                processed_rows.append(new_row_series.to_dict())
        if not processed_rows:
            logger.warning(f"No data rows processed for {input_path}. Output file will not be created.")
            # Changed behavior: Do not create an empty output file if no rows are processed.
            return
            # Create an empty DataFrame with original columns if no rows processed
            # output_df = pd.DataFrame(columns=original_cols)
        else:
             # Create the final DataFrame from the list of processed rows
            output_df = pd.DataFrame(processed_rows)
             # Ensure the column order reflects the potentially updated columns (current_cols)
             # Filter current_cols to only those present in output_df to avoid KeyErrors if a column was unexpectedly dropped
            final_cols = [col for col in current_cols if col in output_df.columns]
            output_df = output_df[final_cols]
        # Fill NaN values with 0 before saving
        output_df = output_df.fillna(0)
        # Log DataFrame info just before saving (DEBUG level)
        if logger.isEnabledFor(logging.DEBUG):
            logger.debug(f"Output DataFrame info before save for {output_path} (after NaN fill):")
            buf = io.StringIO()
            output_df.info(verbose=True, buf=buf)
            logger.debug(buf.getvalue())
        # Write the processed data to the new CSV file
        # The Funds column now contains comma-separated strings, which pandas will quote if necessary.
        output_df.to_csv(output_path, index=False, encoding='utf-8')
        logger.info(f"Successfully created: {output_path} with {len(output_df)} rows.")
    except FileNotFoundError:
        logger.error(f"Error: Input file not found - {input_path}")
    except pd.errors.EmptyDataError:
         logger.warning(f"Input file is empty or contains only header - {input_path}. Skipping.")
    except pd.errors.ParserError as pe:
        logger.error(f"Error parsing CSV file {input_path}: {pe}. Check file format and integrity.", exc_info=True)
    except Exception as e:
        logger.error(f"An unexpected error occurred processing {input_path}: {e}", exc_info=True)
def main():
    """Main execution function to find and process 'pre_' files and the weight file."""
    logger.info("--- Starting pre-processing script --- ")
    # Determine the root path for this script
    script_dir = os.path.dirname(os.path.abspath(__file__))
    # Use the utility to get the configured absolute data folder path
    # Pass the script's directory as the base for resolving relative paths if necessary
    input_dir = get_data_folder_path(app_root_path=script_dir)
    logger.info(f"Using data directory: {input_dir}")
    if not os.path.isdir(input_dir):
        logger.error(f"Data directory not found or is not a directory: {input_dir}. Cannot proceed.")
        return
    # Construct absolute path for dates.csv
    dates_file_path = os.path.join(input_dir, 'dates.csv')
    # Read and prepare date columns
    date_columns = read_and_sort_dates(dates_file_path)
    if date_columns is None:
        logger.warning("Could not read or process dates.csv. Files requiring date replacement might be skipped or processed incorrectly.")
        # Continue processing other files that might not need date replacement, but log the warning.
    # Find files starting with 'pre_' in the determined data directory
    processed_count = 0
    skipped_count = 0
    for filename in os.listdir(input_dir):
        # Skip non-CSV files and the specific weight files which are handled separately
        if not filename.endswith('.csv') or filename.startswith('pre_w_'):
            if filename.startswith('pre_w_'):
                logger.debug(f"Skipping {filename} in main loop, will be handled by weight processor.")
            continue # Skip this file in the main loop
        if filename.startswith('pre_') and filename.endswith('.csv'):
            input_path = os.path.join(input_dir, filename)
            # Create the output filename by replacing 'pre_' with '' (e.g., sec_duration.csv)
            output_filename = filename.replace('pre_', '', 1)
            output_path = os.path.join(input_dir, output_filename)
            logger.info(f"Found file to process: {input_path} -> {output_path} (will overwrite)")
            try:
                # Pass dates_file_path to the function
                process_csv_file(input_path, output_path, date_columns, dates_file_path)
                processed_count += 1
            except Exception as e:
                logger.error(f"Error processing file {input_path}: {e}", exc_info=True)
                skipped_count += 1
    logger.info(f"Finished processing general 'pre_' files. Processed: {processed_count}, Skipped due to errors: {skipped_count}")
    # --- Process the specific weight files using weight_processing --- 
    weight_files_to_process = {
        'pre_w_fund.csv': 'w_Funds.csv',
        'pre_w_bench.csv': 'w_Bench.csv',
        'pre_w_secs.csv': 'w_secs.csv'
    }
    for input_fname, output_fname in weight_files_to_process.items():
        weight_input_path = os.path.join(input_dir, input_fname)
        weight_output_path = os.path.join(input_dir, output_fname)
        if os.path.exists(weight_input_path):
            logger.info(f"Processing weight file: {weight_input_path} -> {weight_output_path}")
            try:
                # Pass the absolute input, output paths, and the absolute dates_path
                process_weight_file(weight_input_path, weight_output_path, dates_file_path)
            except Exception as e:
                logger.error(f"Error processing weight file {weight_input_path}: {e}", exc_info=True)
        else:
            logger.warning(f"Weight input file not found: {weight_input_path}. Skipping processing for {output_fname}.")
    logger.info("--- Pre-processing script finished --- ")
if __name__ == "__main__":
    main()
</file>

<file path="process_w_secs.py">
# process_w_secs.py
"""
This script specifically processes the pre_w_secs.csv file to create w_secs.csv
with dates from Dates.csv as the column headers for the weight values.
"""
import os
import pandas as pd
import logging
import re
# Configure basic logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)
def process_securities_file():
    """Process the pre_w_secs.csv file to replace weight columns with dates."""
    # Get paths
    current_dir = os.path.dirname(os.path.abspath(__file__))
    data_dir = os.path.join(current_dir, 'Data')
    input_path = os.path.join(data_dir, 'pre_w_secs.csv')
    output_path = os.path.join(data_dir, 'w_secs.csv')
    dates_path = os.path.join(data_dir, 'Dates.csv')
    if not os.path.exists(input_path):
        logger.error(f"Input file not found: {input_path}")
        return
    if not os.path.exists(dates_path):
        logger.error(f"Dates file not found: {dates_path}")
        return
    try:
        # Load the dates
        dates_df = pd.read_csv(dates_path)
        dates = dates_df['Date'].tolist()
        logger.info(f"Loaded {len(dates)} dates from {dates_path}")
        # Clean up dates by removing any time component if present
        cleaned_dates = []
        for date in dates:
            # Remove time component if it exists (T00:00:00 format)
            if isinstance(date, str) and 'T' in date:
                cleaned_date = date.split('T')[0]
                cleaned_dates.append(cleaned_date)
            else:
                # If date is in datetime format, convert to string in YYYY-MM-DD format
                try:
                    if pd.notnull(date):
                        date_obj = pd.to_datetime(date)
                        cleaned_dates.append(date_obj.strftime('%Y-%m-%d'))
                    else:
                        cleaned_dates.append(date)
                except:
                    cleaned_dates.append(date)
        logger.info(f"Cleaned up date formats to remove time components")
        dates = cleaned_dates
        # Reverse the dates so newest is on the right
        dates = dates[::-1]
        logger.info(f"Reversed dates order, newest date will be on the right")
        # Read the securities data with explicit encoding
        df = pd.read_csv(input_path, encoding='utf-8', encoding_errors='replace')
        logger.info(f"Read securities file with {len(df)} rows and {len(df.columns)} columns")
        # Get column names
        columns = df.columns.tolist()
        # Assuming the first 10 columns are metadata that we want to keep
        # (Typically: ISIN, Name, Fund, Type, Description, Rating, Bloomberg, Exchange, Country, Currency)
        metadata_cols = columns[:10]
        logger.info(f"Keeping metadata columns: {metadata_cols}")
        # All columns after index 9 are weight values that should be replaced with dates
        weight_cols = columns[10:]
        logger.info(f"Found {len(weight_cols)} weight columns to replace with dates")
        # Check if we have enough dates
        if len(weight_cols) > len(dates):
            logger.warning(f"Not enough dates ({len(dates)}) for all weight columns ({len(weight_cols)}). Using available dates only.")
            dates = dates[:len(weight_cols)]
        elif len(weight_cols) < len(dates):
            logger.warning(f"More dates ({len(dates)}) than weight columns ({len(weight_cols)}). Using first {len(weight_cols)} dates.")
            dates = dates[:len(weight_cols)]
        # Create new column names (metadata + dates)
        new_columns = metadata_cols + dates
        # Replace the column names
        df.columns = new_columns
        # Save the result (make sure no other program has the file open)
        if os.path.exists(output_path):
            try:
                os.remove(output_path)
                logger.info(f"Removed existing file: {output_path}")
            except PermissionError:
                logger.error(f"Cannot delete {output_path} - it may be open in another program.")
                return
        df.to_csv(output_path, index=False, encoding='utf-8')
        logger.info(f"Successfully created {output_path} with {len(df)} rows")
    except Exception as e:
        logger.error(f"Error processing securities file: {e}", exc_info=True)
if __name__ == "__main__":
    process_securities_file()
</file>

<file path="process_weights.py">
# process_weights.py
"""
This script processes weight files by replacing duplicate headers with dates from Dates.csv.
It directly processes pre_w_*.csv files to create corresponding w_*.csv files.
"""
import os
import logging
import pandas as pd
import re
from weight_processing import process_weight_file
# Configure basic logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)
def process_securities_file(input_path, output_path, dates_path):
    """
    Special processing for the securities weight file (pre_w_secs.csv).
    This file has a different structure with multiple metadata columns.
    Args:
        input_path (str): Path to the pre_w_secs.csv file
        output_path (str): Path where to save the processed w_secs.csv file
        dates_path (str): Path to the Dates.csv file with dates to use for headers
    """
    logger.info(f"Processing securities weight file: {input_path} -> {output_path}")
    try:
        # Read the securities file
        df = pd.read_csv(input_path, on_bad_lines='skip', encoding='utf-8', encoding_errors='replace')
        if df.empty:
            logger.warning(f"Securities file {input_path} is empty. Skipping.")
            return
        # Get original column names
        original_cols = df.columns.tolist()
        # Assuming securities file structure:
        # - First column is ISIN (or ID)
        # - There are several metadata columns (like security name, fund code, etc.)
        # - The remaining columns are weight values that need date headers
        # Identify metadata columns (keep first 10 columns as metadata)
        metadata_cols = original_cols[:10]  # Adjust this number if needed
        logger.info(f"Keeping these metadata columns: {metadata_cols}")
        # The rest are weight columns to replace with dates
        weight_cols = original_cols[10:]
        logger.info(f"Found {len(weight_cols)} weight columns to replace with dates")
        # Load dates
        try:
            dates_df = pd.read_csv(dates_path)
            dates = dates_df['Date'].tolist()
            logger.info(f"Loaded {len(dates)} dates from {dates_path}")
            # Clean up dates by removing any time component if present
            cleaned_dates = []
            for date in dates:
                # Remove time component if it exists (T00:00:00 format)
                if isinstance(date, str) and 'T' in date:
                    cleaned_date = date.split('T')[0]
                    cleaned_dates.append(cleaned_date)
                else:
                    # If date is in datetime format, convert to string in YYYY-MM-DD format
                    try:
                        if pd.notnull(date):
                            date_obj = pd.to_datetime(date)
                            cleaned_dates.append(date_obj.strftime('%Y-%m-%d'))
                        else:
                            cleaned_dates.append(date)
                    except:
                        cleaned_dates.append(date)
            logger.info(f"Cleaned up date formats to remove time components")
            dates = cleaned_dates
        except Exception as e:
            logger.error(f"Error loading dates from {dates_path}: {e}")
            return
        # Check if we have enough dates for all weight columns
        if len(weight_cols) > len(dates):
            logger.warning(f"Not enough dates ({len(dates)}) for all weight columns ({len(weight_cols)}). Using available dates only.")
            dates = dates[:len(weight_cols)]
        elif len(weight_cols) < len(dates):
            logger.warning(f"More dates ({len(dates)}) than weight columns ({len(weight_cols)}). Using first {len(weight_cols)} dates.")
            dates = dates[:len(weight_cols)]
        # Reverse the dates list so newest dates appear on the right
        dates = dates[::-1]
        logger.info(f"Reversed dates order, newest date will be on the right")
        # Create new column names with metadata columns and dates
        new_columns = metadata_cols + dates
        # Rename the columns
        df.columns = new_columns
        logger.info(f"Replaced {len(weight_cols)} weight columns with dates")
        # Save the processed DataFrame to the output path
        df.to_csv(output_path, index=False, encoding='utf-8')
        logger.info(f"Successfully processed and saved securities file to: {output_path}")
    except Exception as e:
        logger.error(f"Error processing securities file {input_path}: {e}", exc_info=True)
def process_weight_file_with_reversed_dates(input_path, output_path, dates_path):
    """
    A wrapper around process_weight_file that reverses the dates order.
    This ensures dates are ordered with newest on the right.
    Args:
        input_path (str): Path to the input weight file
        output_path (str): Path where to save the processed weight file
        dates_path (str): Path to the Dates.csv file
    """
    logger.info(f"Processing weight file with reversed dates: {input_path} -> {output_path}")
    try:
        # Load dates
        dates_df = pd.read_csv(dates_path)
        dates = dates_df['Date'].tolist()
        # Clean up dates by removing any time component if present
        cleaned_dates = []
        for date in dates:
            # Remove time component if it exists (T00:00:00 format)
            if isinstance(date, str) and 'T' in date:
                cleaned_date = date.split('T')[0]
                cleaned_dates.append(cleaned_date)
            else:
                # If date is in datetime format, convert to string in YYYY-MM-DD format
                try:
                    if pd.notnull(date):
                        date_obj = pd.to_datetime(date)
                        cleaned_dates.append(date_obj.strftime('%Y-%m-%d'))
                    else:
                        cleaned_dates.append(date)
                except:
                    cleaned_dates.append(date)
        logger.info(f"Cleaned up date formats to remove time components")
        dates = cleaned_dates
        # Read the input file
        df = pd.read_csv(input_path, on_bad_lines='skip', encoding='utf-8', encoding_errors='replace')
        if df.empty:
            logger.warning(f"Weight file {input_path} is empty. Skipping.")
            return
        # Get original column names
        original_cols = df.columns.tolist()
        # First column remains as is (Fund Code)
        id_col = original_cols[0]
        # All other columns become dates
        data_cols = original_cols[1:]
        if len(data_cols) > len(dates):
            logger.warning(f"Not enough dates ({len(dates)}) for all data columns ({len(data_cols)}). Using available dates only.")
            dates = dates[:len(data_cols)]
        elif len(data_cols) < len(dates):
            logger.warning(f"More dates ({len(dates)}) than data columns ({len(data_cols)}). Using first {len(data_cols)} dates.")
            dates = dates[:len(data_cols)]
        # Reverse the dates list so newest dates appear on the right side
        dates = dates[::-1]
        logger.info(f"Reversed dates order, newest date will be on the right")
        # Create new column names
        new_columns = [id_col] + dates
        # Rename the columns
        df.columns = new_columns
        # Save the processed DataFrame
        df.to_csv(output_path, index=False, encoding='utf-8')
        logger.info(f"Successfully processed and saved weight file to: {output_path}")
    except Exception as e:
        logger.error(f"Error processing weight file {input_path}: {e}", exc_info=True)
def main():
    """Process all weight files in the Data directory."""
    # Get the current directory and data directory
    current_dir = os.path.dirname(os.path.abspath(__file__))
    data_dir = os.path.join(current_dir, 'Data')
    # Path to Dates.csv
    dates_path = os.path.join(data_dir, 'Dates.csv')
    if not os.path.exists(dates_path):
        logger.error(f"Dates.csv not found at {dates_path}. Cannot proceed.")
        return
    # Process fund and bench weight files
    weight_files = [
        ('pre_w_Bench.csv', 'w_Bench.csv'),
        ('pre_w_Funds.csv', 'w_Funds.csv')
    ]
    # Process each weight file
    for input_file, output_file in weight_files:
        input_path = os.path.join(data_dir, input_file)
        output_path = os.path.join(data_dir, output_file)
        if os.path.exists(input_path):
            logger.info(f"Processing weight file: {input_path} -> {output_path}")
            # Use our custom function instead of the imported one to ensure date order
            process_weight_file_with_reversed_dates(input_path, output_path, dates_path)
        else:
            logger.warning(f"Input file not found: {input_path}")
    # Special handling for securities file
    secs_input_path = os.path.join(data_dir, 'pre_w_secs.csv')
    secs_output_path = os.path.join(data_dir, 'w_secs.csv')
    if os.path.exists(secs_input_path):
        process_securities_file(secs_input_path, secs_output_path, dates_path)
    else:
        logger.warning(f"Securities file not found: {secs_input_path}")
if __name__ == "__main__":
    main()
</file>

<file path="README.md">
**Important Note on Date Formats:**

Throughout this application, date processing logic (especially when identifying date columns in input files) should be flexible. Aim to handle common formats like `YYYY-MM-DD`, `DD/MM/YYYY`, and `YYYY-MM-DDTHH:MM:SS` where appropriate, particularly during initial data loading and column identification steps. While pre-processing steps might standardize dates to `YYYY-MM-DD`, initial parsing should be robust.

ISIN is used as the primary identifier for securities, and is stored in the `w_secs.csv` file.

---

# Simple Data Checker

This application provides a web interface to load, process, and check financial data, primarily focusing on time-series metrics and security-level data. It helps identify potential data anomalies by calculating changes and Z-scores.

## Features

*   **Time-Series Metric Analysis:** Load `ts_*.csv` files, view latest changes, Z-scores, and historical data charts for various metrics per fund.
    *   Optionally loads corresponding `sp_ts_*.csv` files for comparison data
    *   Route: `/metric/<metric_name>` (with toggle switch to show/hide comparison data)

*   **Security-Level Analysis:** Load wide-format `sec_*.csv` files, view latest changes and Z-scores across securities, and drill down into historical charts.
    *   Server-side pagination, filtering (search, dropdowns), and sorting
    *   Routes: `/security/summary` (main page), `/security/details/<metric_name>/<security_id>` (detail view)

*   **Fund-Specific Views:**
    *   General Fund Overview (`/fund/<fund_code>`): All metrics with comparison data toggle
    *   Fund Duration Details (`/fund/duration_details/<fund_code>`): Duration changes for securities

*   **Security Exclusions:** Manage exclusion list via `/exclusions` (stored in `Data/exclusions.csv`)

*   **Data Issue Tracking:** Log, view, and manage data issues via `/issues` (stored in `Data/data_issues.csv`)

*   **Weight Check:** Compare fund and benchmark weights via `/weights/check`

*   **Yield Curve Analysis:** Check curve inconsistencies via `/curve/summary` and `/curve/details/<currency>`

*   **Attribution Residuals Summary:** Analyze attribution data via `/attribution`
    *   3-way toggle (L0, L1, L2) for different detail levels
    *   Compares Production vs S&P data for both Benchmark and Portfolio cases
    *   Color-coded cells to highlight discrepancies

*   **Data Comparison:**
    *   Spread: `/comparison/summary` and `/comparison/details/<security_id>`
    *   Duration: `/duration_comparison/summary` and `/duration_comparison/details/<security_id>`
    *   Spread Duration: `/spread_duration_comparison/summary` and `/spread_duration_comparison/details/<security_id>`

*   **Data Simulation & Management:** API simulation via `/get_data`

*   **Special Character Handling:** Security IDs with special characters are URL-encoded in templates and decoded in view functions using `urllib.parse.unquote(security_id)`

*   **Attribution Residuals Chart:** Time-series visualization of attribution residuals

## File Structure Overview

```mermaid
graph TD
    A[Simple Data Checker] --> B(app.py);
    A --> C{Python Modules};
    A --> D{Views};
    A --> E{Templates};
    A --> F{Static Files};
    A --> G(Data);
    A --> H(Config/Utils);

    C --> C1(data_loader.py);
    C --> C2(metric_calculator.py);
    C --> C3(security_processing.py);
    C --> C4(process_data.py);
    C --> C5(curve_processing.py);
    C --> C6(issue_processing.py);

    D --> D1(main_views.py);
    D --> D2(metric_views.py);
    D --> D3(security_views.py);
    D --> D4(fund_views.py);
    D --> D5(exclusion_views.py);
    D --> D6(comparison_views.py);
    D --> D7(weight_views.py);
    D --> D8(api_views.py);
    D8 --> D8a(api_core.py);
    D8 --> D8b(api_routes_data.py);
    D8 --> D8c(api_routes_call.py);
    D --> D9(duration_comparison_views.py);
    D --> D10(spread_duration_comparison_views.py);
    D --> D11(curve_views.py);
    D --> D12(issue_views.py);
    D --> D13(attribution_views.py);

    E --> E1(base.html);
    E --> E2(index.html);
    E --> E3(metric_page_js.html);
    E --> E4(securities_page.html);
    E --> E5(security_details_page.html);
    E --> E6(fund_duration_details.html);
    E --> E7(exclusions_page.html);
    E --> E8(get_data.html);
    E --> E9(comparison_page.html);
    E --> E10(comparison_details_page.html);
    E --> E11(fund_detail_page.html);
    E --> E12(weight_check.html);
    E --> E13(duration_comparison_page.html);
    E --> E14(duration_comparison_details_page.html);
    E --> E15(spread_duration_comparison_page.html);
    E --> E16(spread_duration_comparison_details_page.html);
    E --> E17(curve_summary.html);
    E --> E18(curve_details.html);
    E --> E19(issues_page.html);
    E --> E20(attribution_summary.html);

    F --> F1(js);
    F1 --> F1a(main.js);
    F1 --> F1b(modules);
    F1b --> F1b1(ui);
    F1b1 --> F1b1a(chartRenderer.js);
    F1b1 --> F1b1b(securityTableFilter.js);
    F1b1 --> F1b1c(tableSorter.js);
    F1b1 --> F1b1d(toggleSwitchHandler.js);
    F1b --> F1b2(utils);
    F1b2 --> F1b2a(helpers.js);
    F1b --> F1b3(charts);
    F1b3 --> F1b3a(timeSeriesChart.js);

    G --> G1(ts_*.csv);
    G --> G2(sec_*.csv);
    G --> G3(pre_*.csv);
    G --> G4(new_*.csv);
    G --> G5(exclusions.csv);
    G --> G6(QueryMap.csv);
    G --> G7(FundList.csv);
    G --> G8(w_Funds.csv);
    G --> G9(w_Bench.csv);
    G --> G10(curves.csv);
    G --> G11(data_issues.csv);
    G --> G12(w_secs.csv);
    G --> G13(att_factors.csv);

    H --> H1(config.py);
    H --> H2(utils.py);

    B --> D;
    D --> C;
    D --> H;
    D --> E;
    E --> F;
```

## Application Components

### Data Files (`Data/`)

| File | Description |
|------|-------------|
| `ts_*.csv` | Time-series data indexed by Date and Code (Fund/Benchmark) |
| `sp_ts_*.csv` | (Optional) Secondary/comparison time-series data corresponding to `ts_*.csv` |
| `sec_*.csv` | Security-level data in wide format (dates as columns) |
| `pre_*.csv` | Input files for the `process_data.py` script |
| `new_*.csv` | Output files from the `process_data.py` script |
| `exclusions.csv` | Excluded securities list (`SecurityID`, `AddDate`, `EndDate`, `Comment`) |
| `QueryMap.csv` | Maps query IDs to filenames for API simulation |
| `FundList.csv` | Fund codes and metadata for the API simulation page |
| `Dates.csv` | Configuration data for specific use cases |
| `w_Funds.csv` | Daily fund weights (expected to be 100%) |
| `w_Bench.csv` | Daily benchmark weights (expected to be 100%) |
| `w_secs.csv` | Security weights with ISIN as primary identifier |
| `curves.csv` | Yield curve data (Date, Currency Code, Term, Daily Value) |
| `data_issues.csv` | Issue tracking log (ID, dates, users, details, resolution) |
| `att_factors.csv` | Attribution data with L0, L2 factors for Production and S&P. **Note:** The `L0 Total` column represents the returns for each security/fund/date. |

### Python Core Modules

| File | Purpose | Key Functions |
|------|---------|--------------|
| `app.py` | Application entry point using Flask factory pattern | `create_app()`, `run_cleanup()` |
| `config.py` | Configuration variables | `DATA_FOLDER`, `COLOR_PALETTE` |
| `data_loader.py` | Load and preprocess time-series data | `load_and_process_data()`, `_find_column()` |
| `metric_calculator.py` | Calculate statistical metrics | `calculate_latest_metrics()`, `_calculate_column_stats()` |
| `process_data.py` | Preprocess CSV files | `process_csv_file()`, `main()` |
| `security_processing.py` | Process security-level data | `load_and_process_security_data()`, `calculate_security_latest_metrics()` |
| `utils.py` | Utility functions | `_is_date_like()`, `parse_fund_list()` |
| `curve_processing.py` | Process yield curve data | `load_curve_data()`, `check_curve_inconsistencies()` |
| `issue_processing.py` | Manage data issues | `add_issue()`, `close_issue()`, `load_issues()` |

### View Modules (`views/`)

| Module | Purpose | Routes |
|--------|---------|--------|
| `main_views.py` | Main dashboard | `/` |
| `metric_views.py` | Time-series metric details | `/metric/<metric_name>` |
| `security_views.py` | Security-level data checks | `/security/summary`, `/security/details/<metric_name>/<security_id>` |
| `fund_views.py` | Fund-specific views | `/fund/<fund_code>`, `/fund/duration_details/<fund_code>` |
| `exclusion_views.py` | Security exclusion management | `/exclusions`, `/exclusions/remove` |
| `comparison_views.py` | Spread comparison | `/comparison/summary`, `/comparison/details/<security_id>` |
| `duration_comparison_views.py` | Duration comparison | `/duration_comparison/summary`, `/duration_comparison/details/<security_id>` |
| `spread_duration_comparison_views.py` | Spread duration comparison | `/spread_duration_comparison/summary`, `/spread_duration_comparison/details/<security_id>` |
| `api_views.py` | API simulation | `/get_data`, `/run-api-calls`, `/rerun-api-call` |
| `weight_views.py` | Weight checking | `/weights/check` |
| `curve_views.py` | Yield curve checking | `/curve/summary`, `/curve/details/<currency>` |
| `issue_views.py` | Issue tracking | `/issues`, `/issues/close` |
| `attribution_views.py` | Attribution analysis | `/attribution` |

### HTML Templates (`templates/`)

| Template | Purpose | Key Features |
|----------|---------|-------------|
| `base.html` | Main layout | Bootstrap, navbar, common structure |
| `index.html` | Dashboard | Metric links, Z-Score summary table |
| `metric_page_js.html` | Time-series detail page | Toggle switch for SP data |
| `securities_page.html` | Security summary table | Filter/search form, pagination |
| `security_details_page.html` | Security detail page | Multiple charts (Value, Price, Duration) |
| `fund_duration_details.html` | Fund duration details | Security duration changes table |
| `exclusions_page.html` | Exclusion management | Add/remove security exclusions |
| `get_data.html` | API simulation | Data status, fund selection, date inputs |
| `comparison_page.html` | Comparison summary | Filter form, sortable table |
| `comparison_details_page.html` | Detailed comparison | Side-by-side charts |
| `fund_detail_page.html` | Fund metrics overview | Multiple charts with SP data toggle |
| `weight_check.html` | Weight checking | Fund/benchmark weight comparison |
| `*_comparison_*.html` | Various comparison pages | Similar structure to comparison templates |
| `curve_summary.html` | Yield curve summary | Inconsistency check table |
| `curve_details.html` | Yield curve details | Chart.js line chart with date selector |
| `issues_page.html` | Issue tracking | Add/view/close issue forms and tables |
| `attribution_summary.html` | Attribution summary | Multiple detail levels with comparison data |

### JavaScript Files (`static/js/`)

| File | Purpose | Key Features |
|------|---------|-------------|
| `main.js` | Main JS entry point | Initializes components |
| `modules/ui/chartRenderer.js` | Render charts | Time-series, comparison charts |
| `modules/ui/securityTableFilter.js` | Handle table filtering | Dynamic filter application |
| `modules/ui/tableSorter.js` | Handle table sorting | Sort direction toggle |
| `modules/ui/toggleSwitchHandler.js` | Handle toggle switches | Show/hide comparison data |
| `modules/utils/helpers.js` | Utility functions | Common helper methods |
| `modules/charts/timeSeriesChart.js` | Time-series chart creation | Chart.js configuration |

### Attribution Data Calculation

Attribution residuals are calculated using the following formulas:
- `L0 Total = L1 Rates Total + L1 Credit Total + L1 FX Total + L1 Residual`
- `L1 Rates Total = L2 Rates Carry Daily + L2 Rates Convexity Daily + L2 Rates Curve Daily + L2 Rates Duration Daily + L2 Rates Roll Daily`
- `L1 Credit Total = L2 Credit Carry Daily + L2 Credit Convexity Daily + L2 Credit Defaulted Daily + L2 Credit Spread Change Daily`
- `L1 FX Total = L2 FX Carry Daily + L2 FX Change Daily`
- `L1 Residual = L0 Total - (L1 Rates + L1 Credit + L1 FX)`
- Perfect attribution is achieved when residual = 0
</file>

<file path="requirements.txt">
Flask
pandas
plotly
</file>

<file path="security_processing.py">
# This file handles the loading, processing, and analysis of security-level data.
# It assumes input CSV files are structured with one security per row and time series data
# spread across columns where headers represent dates (e.g., YYYY-MM-DD).
# Key functions:
# - `load_and_process_security_data`: Reads a wide-format CSV (given filename and data path),
#   identifies the security ID column, static attribute columns, and date columns.
#   It then 'melts' the data into a long format, converting date strings to datetime objects.
# - `calculate_security_latest_metrics`: Takes the processed long-format DataFrame and calculates
#   various metrics for each security's 'Value' over time, including latest value, change,
#   historical stats (mean, max, min), and change Z-score. It also preserves the static attributes.
import pandas as pd
import os
import numpy as np
import re # For checking date-like column headers
import logging
import traceback
# Note: Does not import current_app, relies on caller to pass the path.
# Get the logger instance. Assumes Flask app has configured logging.
logger = logging.getLogger(__name__)
# --- Removed logging setup block --- 
# Logging is now handled centrally by the Flask app factory in app.py
# Removed DATA_FOLDER constant - path is now passed to functions
def _is_date_like(column_name):
    """Check if a column name looks like a common date format.
    Recognizes formats like YYYY-MM-DD, YYYY/MM/DD, MM/DD/YYYY, M/D/YYYY, YYYYMMDD.
    """
    col_str = str(column_name)
    # Regex to match common date patterns
    # - YYYY[-/]MM[-/]DD
    # - MM[-/]DD[-/]YYYY (allows 1-2 digits for M, D and 2 or 4 for Y)
    # - YYYYMMDD
    pattern = r'^(\d{4}[-/]\d{1,2}[-/]\d{1,2}|\d{1,2}[-/]\d{1,2}[-/](\d{4}|\d{2})|\d{8})$'
    return bool(re.match(pattern, col_str))
def load_and_process_security_data(filename: str, data_folder_path: str):
    """Loads security data, identifies static/date columns, and melts to long format."""
    log_prefix = f"[{filename}] " # Prefix for logs from this function
    logger.info(f"{log_prefix}--- Entering load_and_process_security_data ---")
    if not data_folder_path:
        logger.error(f"{log_prefix}No data_folder_path provided.")
        return pd.DataFrame(), []
    filepath = os.path.join(data_folder_path, filename)
    logger.info(f"{log_prefix}Attempting to load security data from: {filepath}")
    try:
        # --- Read Header --- 
        logger.debug(f"{log_prefix}Reading header...")
        header_df = pd.read_csv(filepath, nrows=0, on_bad_lines='skip', encoding='utf-8', encoding_errors='replace')
        all_cols = [str(col).strip() for col in header_df.columns.tolist()]
        logger.debug(f"{log_prefix}Read header columns: {all_cols}")
        if not all_cols:
            logger.error(f"{log_prefix}CSV file appears to be empty or header is missing.")
            raise ValueError(f"CSV file '{filename}' appears to be empty or header is missing.")
        # --- Identify Essential ID Columns --- 
        essential_id_cols = []
        if 'ISIN' in all_cols:
            essential_id_cols.append('ISIN')
        if 'Security Name' in all_cols:
            essential_id_cols.append('Security Name')
        if not essential_id_cols:
             logger.warning(f"{log_prefix}Neither 'ISIN' nor 'Security Name' found. Using first column '{all_cols[0]}' as potential ID.")
             essential_id_cols.append(all_cols[0])
        logger.info(f"{log_prefix}Essential ID Columns identified: {essential_id_cols}")
        # --- Identify Static and Date Columns --- 
        static_cols = []
        date_cols = []
        for col in all_cols:
            if col in essential_id_cols:
                continue
            if _is_date_like(col):
                date_cols.append(col)
            else:
                static_cols.append(col)
        if not date_cols:
            logger.error(f"{log_prefix}No date-like columns found using flexible patterns. Cannot process.")
            raise ValueError("No date-like columns found using flexible patterns.")
        logger.info(f"{log_prefix}Identified Static Cols: {static_cols}")
        logger.debug(f"{log_prefix}Identified {len(date_cols)} Date Cols (e.g., {date_cols[:3]}...)")
        # --- Read Full Data --- 
        logger.debug(f"{log_prefix}Reading full data...")
        df_wide = pd.read_csv(filepath, encoding='utf-8', on_bad_lines='skip', encoding_errors='replace')
        df_wide.columns = df_wide.columns.map(lambda x: str(x).strip())
        logger.info(f"{log_prefix}Read full data. Shape: {df_wide.shape}")
        # --- Melt Data --- 
        id_vars_melt = [col for col in essential_id_cols if col in df_wide.columns] + \
                       [col for col in static_cols if col in df_wide.columns]
        value_vars = [col for col in date_cols if col in df_wide.columns]
        if not value_vars:
             logger.error(f"{log_prefix}Date columns identified in header not found in data frame after loading. Columns available: {df_wide.columns.tolist()}")
             raise ValueError("Date columns identified in header not found in data frame after loading.")
        if not id_vars_melt:
             logger.error(f"{log_prefix}No ID or static columns found to use as id_vars. Columns available: {df_wide.columns.tolist()}")
             raise ValueError("No ID or static columns found for melting.")
        logger.debug(f"{log_prefix}Melting data. ID vars: {id_vars_melt}, Value vars count: {len(value_vars)}")
        df_long = pd.melt(df_wide,
                          id_vars=id_vars_melt,
                          value_vars=value_vars,
                          var_name='Date_Str',
                          value_name='Value')
        logger.info(f"{log_prefix}Melted data shape: {df_long.shape}")
        # --- Process Date and Value --- 
        date_col_str = 'Date_Str'
        date_col_dt = 'Date'
        logger.debug(f"{log_prefix}Parsing dates and converting values...")
        # 1. Try DD/MM/YYYY first
        df_long[date_col_dt] = pd.to_datetime(df_long[date_col_str], format='%d/%m/%Y', errors='coerce')
        # 2. Try YYYY-MM-DD for any remaining NaTs
        # Create a mask for rows where the first attempt failed
        nat_mask = df_long[date_col_dt].isna()
        if nat_mask.any():
            logger.info(f"{log_prefix}Attempting fallback date parsing (YYYY-MM-DD) for {nat_mask.sum()} entries.")
            # Apply the second format ONLY to the NaT rows
            df_long.loc[nat_mask, date_col_dt] = pd.to_datetime(df_long.loc[nat_mask, date_col_str], format='%Y-%m-%d', errors='coerce')
        # Check again for NaTs after both attempts
        final_nat_count = df_long[date_col_dt].isna().sum()
        if final_nat_count > 0:
            logger.warning(f"{log_prefix}Could not parse {final_nat_count} date strings using specified formats (DD/MM/YYYY, YYYY-MM-DD).")
            # Example of unparsed date strings:
            unparsed_examples = df_long.loc[df_long[date_col_dt].isna(), date_col_str].unique()[:5]
            logger.warning(f"{log_prefix}Unparsed examples: {unparsed_examples}")
        # Convert Value column
        df_long['Value'] = pd.to_numeric(df_long['Value'], errors='coerce')
        # Drop rows where essential data is missing
        initial_rows = len(df_long)
        required_cols_for_dropna = ['Date', 'Value'] + [col for col in essential_id_cols if col in df_long.columns]
        df_long.dropna(subset=required_cols_for_dropna, inplace=True)
        rows_dropped = initial_rows - len(df_long)
        if rows_dropped > 0:
             logger.warning(f"{log_prefix}Dropped {rows_dropped} rows due to missing required values (Date, Value, or Essential IDs).")
        if df_long.empty:
             logger.warning(f"{log_prefix}DataFrame is empty after melting, conversion, and NaN drop.")
             return pd.DataFrame(), static_cols
        # Determine ID column name for index
        id_col_name = None
        if 'ISIN' in df_long.columns:
            id_col_name = 'ISIN'
        elif 'Security Name' in df_long.columns:
             id_col_name = 'Security Name'
        elif essential_id_cols and essential_id_cols[0] in df_long.columns: 
             id_col_name = essential_id_cols[0]
             logger.warning(f"{log_prefix}Using fallback ID '{id_col_name}' for index setting.")
        else:
             logger.error(f"{log_prefix}Cannot determine a valid ID column ({essential_id_cols}) to set index. Columns: {df_long.columns.tolist()}")
             return pd.DataFrame(), []
        logger.info(f"{log_prefix}Determined ID column for index: '{id_col_name}'")
        # Drop Date_Str column BEFORE setting index
        if 'Date_Str' in df_long.columns:
            df_long.drop(columns=['Date_Str'], inplace=True)
            logger.debug(f"{log_prefix}Dropped 'Date_Str' column.")
        # Sort before setting index
        logger.debug(f"{log_prefix}Sorting by '{id_col_name}' and 'Date'...")
        df_long = df_long.sort_values(by=[id_col_name, 'Date'])
        # --- SET THE MULTIINDEX --- 
        try:
            logger.debug(f"{log_prefix}Setting index to ['Date', '{id_col_name}']...")
            df_long.set_index(['Date', id_col_name], inplace=True)
            logger.info(f"{log_prefix}Set MultiIndex ('Date', '{id_col_name}'). Final shape: {df_long.shape}")
        except KeyError as e:
             logger.error(f"{log_prefix}Failed to set index using ['Date', '{id_col_name}']. Error: {e}. Columns: {df_long.columns.tolist()}")
             return pd.DataFrame(), []
        # Identify static columns *excluding* essential ID cols to return
        # We return the list of non-ID static attributes
        final_static_cols = [col for col in static_cols if col in df_long.columns]
        logger.info(f"{log_prefix}--- Exiting load_and_process_security_data. Returning DataFrame and static cols: {final_static_cols} ---")
        return df_long, final_static_cols # Return only non-ID static cols
    except FileNotFoundError:
        logger.error(f"Error: File not found at {filepath}")
        return pd.DataFrame(), [] # Return empty dataframe and list
    except ValueError as ve:
        logger.error(f"Error processing header or columns in {filename}: {ve}")
        return pd.DataFrame(), []
    except KeyError as ke:
        logger.error(f"Error melting DataFrame for {filename}, likely due to missing column used as id_var or value_var: {ke}")
        return pd.DataFrame(), []
    except Exception as e:
        logger.error(f"An unexpected error occurred loading/processing {filename}: {e}", exc_info=True)
        # traceback.print_exc() # Logger handles traceback now
        return pd.DataFrame(), []
def calculate_security_latest_metrics(df, static_cols):
    """Calculates latest metrics for each security based on its 'Value' column.
    Args:
        df (pd.DataFrame): Processed long-format DataFrame with MultiIndex (Date, Security ID).
                           Must contain a 'Value' column.
        static_cols (list[str]): List of static column names present in the DataFrame's columns (not index).
    Returns:
        pandas.DataFrame: DataFrame indexed by Security ID, including static columns and
                          calculated metrics (Latest Value, Change, Mean, Max, Min, Change Z-Score).
                          Returns an empty DataFrame if input is empty or processing fails.
    """
    if df is None or df.empty:
        logger.warning("Input DataFrame is None or empty. Cannot calculate security metrics.")
        return pd.DataFrame()
    if 'Value' not in df.columns:
        logger.error("Input DataFrame for security metrics calculation must contain a 'Value' column.")
        return pd.DataFrame()
    # Ensure index has two levels and get their names dynamically
    if df.index.nlevels != 2:
        logger.error("Input DataFrame for security metrics must have 2 index levels (Date, Security ID).")
        return pd.DataFrame()
    date_level_name, id_level_name = df.index.names
    try:
        latest_date = df.index.get_level_values(date_level_name).max()
        security_ids = df.index.get_level_values(id_level_name).unique()
        all_metrics_list = []
        for sec_id in security_ids:
            try:
                # Extract data for the current security ID
                # Use .loc for potentially cleaner selection and ensure sorting
                sec_data_hist = df.loc[(slice(None), sec_id), :].reset_index(level=id_level_name, drop=True).sort_index()
                if sec_data_hist.empty:
                     logger.debug(f"No data found for security '{sec_id}' after extraction. Skipping.")
                     continue
                sec_metrics = {} # Dictionary to hold metrics for this security
                # Add static columns first
                # Take the first available row's values, assuming they are constant per security
                # Need to handle potential multi-index if static_cols contains index names by mistake
                valid_static_cols = [col for col in static_cols if col in sec_data_hist.columns]
                if not sec_data_hist.empty:
                    static_data_row = sec_data_hist.iloc[0]
                    for static_col in valid_static_cols:
                        sec_metrics[static_col] = static_data_row.get(static_col, np.nan)
                else: # Should not happen due to check above, but safeguard
                    for static_col in valid_static_cols:
                         sec_metrics[static_col] = np.nan 
                # Ensure all expected static cols are present in the dict, even if missing from data
                for static_col in static_cols:
                     if static_col not in sec_metrics:
                          logger.warning(f"Static column '{static_col}' not found in data for security '{sec_id}', adding as NaN.")
                          sec_metrics[static_col] = np.nan
                # Calculate metrics for the 'Value' column
                value_hist = sec_data_hist['Value']
                # Calculate diff only if series has enough data
                value_change_hist = pd.Series(index=value_hist.index, dtype=np.float64)
                if not value_hist.dropna().empty and len(value_hist.dropna()) > 1:
                    value_change_hist = value_hist.diff()
                else:
                    logger.debug(f"Cannot calculate difference for 'Value' column, security '{sec_id}' due to insufficient data.")
                # Base historical stats (level) - handle potential all-NaN series
                sec_metrics['Mean'] = value_hist.mean() if value_hist.notna().any() else np.nan
                sec_metrics['Max'] = value_hist.max() if value_hist.notna().any() else np.nan
                sec_metrics['Min'] = value_hist.min() if value_hist.notna().any() else np.nan
                # Stats for change
                change_mean = value_change_hist.mean() if value_change_hist.notna().any() else np.nan
                change_std = value_change_hist.std() if value_change_hist.notna().any() else np.nan
                # Latest values
                # Check if latest_date exists in this security's specific history
                if latest_date in sec_data_hist.index:
                    latest_value = sec_data_hist.loc[latest_date, 'Value']
                    latest_change = value_change_hist.get(latest_date, np.nan)
                    sec_metrics['Latest Value'] = latest_value
                    sec_metrics['Change'] = latest_change
                    # Calculate Change Z-Score
                    change_z_score = np.nan
                    if pd.notna(latest_change) and pd.notna(change_mean) and pd.notna(change_std) and change_std != 0:
                        change_z_score = (latest_change - change_mean) / change_std
                    elif change_std == 0 and pd.notna(latest_change) and pd.notna(change_mean):
                         # Handle zero standard deviation
                         if latest_change == change_mean:
                              change_z_score = 0.0
                         else:
                             change_z_score = np.inf if latest_change > change_mean else -np.inf
                         logger.debug(f"Std dev of change for security '{sec_id}' is zero. Z-score set to {change_z_score}.")
                    else:
                         # Log if Z-score calculation failed due to NaNs
                        if not (pd.notna(latest_change) and pd.notna(change_mean) and pd.notna(change_std)):
                             logger.debug(f"Cannot calculate Z-score for security '{sec_id}' due to NaN inputs (latest_change={latest_change}, change_mean={change_mean}, change_std={change_std})")
                    sec_metrics['Change Z-Score'] = change_z_score
                else:
                    # Security missing the overall latest date
                    logger.debug(f"Security '{sec_id}' missing data for latest date {latest_date}. Setting latest metrics to NaN.")
                    sec_metrics['Latest Value'] = np.nan
                    sec_metrics['Change'] = np.nan
                    sec_metrics['Change Z-Score'] = np.nan
                # Add the security ID itself for setting the index later
                sec_metrics[id_level_name] = sec_id 
                all_metrics_list.append(sec_metrics)
            except Exception as inner_e:
                logger.error(f"Error calculating metrics for security '{sec_id}': {inner_e}", exc_info=True)
                # Optionally add a placeholder row with NaNs? Or just skip. Let's skip.
                continue
        if not all_metrics_list:
            logger.warning("No security metrics were successfully calculated. Returning empty DataFrame.")
            return pd.DataFrame()
        # Create DataFrame and set index
        latest_metrics_df = pd.DataFrame(all_metrics_list)
        # id_col_name = df.index.names[1] # Get the actual ID column name used
        if id_level_name in latest_metrics_df.columns:
             latest_metrics_df.set_index(id_level_name, inplace=True)
        else:
             logger.error(f"Security ID column '{id_level_name}' not found in the created metrics list for setting index. Columns: {latest_metrics_df.columns.tolist()}")
             # Fallback or error? Let's return as is for now, index might be RangeIndex.
        # Reorder columns to have static columns first, then calculated metrics
        metric_cols = ['Latest Value', 'Change', 'Mean', 'Max', 'Min', 'Change Z-Score']
        # Get static cols that are actually present in the final df columns (excluding the ID index)
        present_static_cols = [col for col in static_cols if col in latest_metrics_df.columns]
        final_col_order = present_static_cols + [m_col for m_col in metric_cols if m_col in latest_metrics_df.columns]
        try:
            latest_metrics_df = latest_metrics_df[final_col_order]
        except KeyError as ke:
            logger.error(f"Error reordering columns, likely a metric column is missing: {ke}. Columns available: {latest_metrics_df.columns.tolist()}")
            # Proceed with potentially incorrect order
        # Sorting (e.g., by Z-score) should be done in the view function where it's displayed
        logger.info(f"Successfully calculated metrics for {len(latest_metrics_df)} securities.")
        return latest_metrics_df
    except Exception as e:
        logger.error(f"An unexpected error occurred during security metric calculation: {e}", exc_info=True)
        # traceback.print_exc() # Logger handles traceback
        return pd.DataFrame()
</file>

<file path="static/css/style.css">
/* Basic styling for sortable table headers */
th.sortable {
    cursor: pointer;
    position: relative; /* Needed for absolute positioning of indicator */
}
/* Hide default indicator span content */
.sort-indicator {
    display: inline-block;
    width: 1em;
    height: 1em;
    margin-left: 5px;
    vertical-align: middle;
    content: "";
}
/* Style for ascending sort indicator */
th.sortable.sort-asc .sort-indicator::before {
    content: "\25B2"; /* Up arrow ▲ */
    font-size: 0.8em;
}
/* Style for descending sort indicator */
th.sortable.sort-desc .sort-indicator::before {
    content: "\25BC"; /* Down arrow ▼ */
    font-size: 0.8em;
}
/* Hover effect for sortable headers */
th.sortable:hover {
    background-color: #e9ecef; /* Light grey background on hover */
}
/* Ensure sticky header cells position correctly below the fixed navbar */
#securities-table thead.sticky-top th {
    position: sticky; /* Ensure stickiness is explicitly defined */
    top: 56px; /* Adjust to match fixed navbar height */
    z-index: 1019; /* Ensure it's above table body but below navbar (Bootstrap navbar is 1030) */
    background-color: var(--bs-table-bg); /* Match the header background */
}
/* Add padding to the first row to prevent content from being hidden under header */
#securities-table tbody tr:first-child {
    margin-top: 45px; /* Add vertical space to the first row */
}
/* Alternatively, add a pseudo element to create space before the first row */
#securities-table tbody::before {
    content: "";
    display: block;
    height: 45px; /* Adjust this value as needed to prevent overlap */
}
</file>

<file path="static/js/main.js">
// This file acts as the main entry point for the application's JavaScript.
// It runs after the DOM is fully loaded and performs several key initializations:
// 1. Imports necessary functions from UI modules (chart rendering, table filtering).
// 2. Checks for the presence of specific elements on the page to determine the context
//    (e.g., metric details page, securities list page, single security detail page).
// 3. If on a metric details page (`metric_page_js.html`):
//    - Finds the embedded JSON data (`<script id="chartData">`).
//    - Parses the JSON data containing historical values and calculated metrics for all funds.
//    - Calls `renderChartsAndTables` from `chartRenderer.js` to dynamically create
//      the metric tables and time-series charts for each fund code.
// 4. If on a securities list page (`securities_page.html`):
//    - Finds the main securities table (`<table id="securities-table">`).
//    - Calls `initSecurityTableFilter` from `securityTableFilter.js` to add
//      interactive filtering capabilities to the table header.
// 5. If on a single security detail page (`security_details_page.html`):
//    - Finds the chart canvas (`<canvas id="securityChart">`) and its associated JSON data (`<script id="chartJsonData">`).
//    - Parses the JSON data containing the time-series for that specific security.
//    - Calls `renderSingleSecurityChart` from `chartRenderer.js` to display the chart.
// This modular approach ensures that initialization code only runs when the corresponding HTML elements are present.
// static/js/main.js
// Purpose: Main entry point for client-side JavaScript. Initializes modules based on page content.
import { renderChartsAndTables, renderSingleSecurityChart, renderFundCharts, toggleSecondaryDataVisibility } from './modules/ui/chartRenderer.js';
import { initSecurityTableFilter } from './modules/ui/securityTableFilter.js';
import { initTableSorter } from './modules/ui/tableSorter.js';
document.addEventListener('DOMContentLoaded', () => {
    console.log("DOM fully loaded and parsed");
    // --- Shared Elements --- 
    const toggleSwitch = document.getElementById('toggleSpData'); // Find toggle switch globally
    // --- Metric Page (Multiple Charts per Metric) ---    
    const metricChartDataElement = document.getElementById('chartData');
    const metricChartsArea = document.getElementById('chartsArea');
    if (metricChartDataElement && metricChartsArea) {
        console.log("Metric page detected. Initializing charts.");
        try {
            const chartDataJson = metricChartDataElement.textContent;
            console.log("Raw JSON string from script tag:", chartDataJson);
            const fullChartData = JSON.parse(chartDataJson);
            console.log('Parsed fullChartData object:', fullChartData);
            // Metadata needed for toggle logic
            const metadata = fullChartData ? fullChartData.metadata : null; 
            console.log('Checking fullChartData.metadata:', metadata);
            console.log('Checking fullChartData.funds:', fullChartData ? fullChartData.funds : 'fullChartData is null/undefined');
            if (metadata && fullChartData.funds && Object.keys(fullChartData.funds).length > 0) {
                console.log("Conditional check passed. Calling renderChartsAndTables...");
                // Render charts and tables (this now just shows/hides the container)
                renderChartsAndTables(
                    metricChartsArea,
                    fullChartData
                );
                // Now, attach the event listener if the toggle exists and data is available
                if (toggleSwitch && metadata.secondary_data_available) {
                     console.log("[main.js] Attaching toggle listener for Metric Page.");
                    toggleSwitch.addEventListener('change', (event) => {
                        const showSecondary = event.target.checked;
                        console.log(`[main.js Metric Page Toggle] Toggle changed. Show Secondary: ${showSecondary}`);
                        toggleSecondaryDataVisibility(showSecondary); // Call imported function
                    });
                } else if (toggleSwitch) {
                     console.log("[main.js] Toggle exists, but secondary data not available for Metric Page.");
                     toggleSwitch.disabled = true;
                } else {
                    console.log("[main.js] Toggle switch not found for Metric Page.");
                }
            } else {
                console.error('Parsed metric chart data is missing expected structure or funds are empty:', fullChartData);
                metricChartsArea.innerHTML = '<div class="alert alert-danger">Error: Invalid data structure or no fund data.</div>';
            }
        } catch (e) {
            console.error('Error processing metric chart data:', e);
            metricChartsArea.innerHTML = '<div class="alert alert-danger">Error loading chart data. Check console.</div>';
        }
    }
    // --- Fund Detail Page (Multiple Charts per Fund) ---    
    const fundChartDataElement = document.getElementById('fundChartData');
    const fundChartsArea = document.getElementById('fundChartsArea');
    if (fundChartDataElement && fundChartsArea) {
        console.log("Fund detail page detected. Initializing charts.");
        try {
            const fundChartDataJson = fundChartDataElement.textContent;
            const allChartData = JSON.parse(fundChartDataJson);
            console.log('Parsed fund chart data:', JSON.parse(JSON.stringify(allChartData)));
            // Check if any SP data is available *before* rendering
            const anySpDataAvailable = allChartData.some(chartInfo => 
                chartInfo.datasets && chartInfo.datasets.some(ds => ds.isSpData === true)
            );
            if (Array.isArray(allChartData)) { // Check if it's an array (even if empty)
                // Render charts first
                 renderFundCharts(fundChartsArea, allChartData);
                // Setup toggle based on data availability
                if (toggleSwitch) {
                    if (anySpDataAvailable) {
                         console.log("[main.js] Attaching toggle listener for Fund Detail Page.");
                        toggleSwitch.disabled = false;
                        toggleSwitch.parentElement.querySelector('label').textContent = 'Show SP Comparison Data';
                        toggleSwitch.addEventListener('change', (event) => {
                            const showSecondary = event.target.checked;
                            console.log(`[main.js Fund Detail Page Toggle] Toggle changed. Show SP: ${showSecondary}`);
                            toggleSecondaryDataVisibility(showSecondary); // Call imported function
                        });
                    } else {
                        console.log("[main.js] Fund Detail Page: No SP data available, disabling toggle.");
                        toggleSwitch.disabled = true;
                        toggleSwitch.checked = false;
                        toggleSwitch.parentElement.querySelector('label').textContent = 'Show SP Comparison Data (N/A)';
                    }
                } else {
                    console.log("[main.js] Toggle switch not found for Fund Detail Page.");
                }
            } else {
                 console.error('Parsed fund chart data is not an array or is invalid:', allChartData);
                fundChartsArea.innerHTML = '<div class="alert alert-danger">Error: Invalid chart data received.</div>';
            }
        } catch (e) {
            console.error('Error processing fund chart data:', e);
            fundChartsArea.innerHTML = '<div class="alert alert-danger">Error loading fund charts. Check console.</div>';
        }
    }
    // --- Securities Summary Page (Filterable & Sortable Table) ---
    const securitiesTable = document.getElementById('securities-table');
    if (securitiesTable) {
        console.log("Securities page table detected. Initializing client-side sorter (filtering is server-side).");
        // initSecurityTableFilter('securities-table'); // REMOVED: Filtering is now server-side
        initTableSorter('securities-table'); // Keep client-side sorting for instant feedback after load
    } else {
        // console.log("Securities table not found, skipping table features initialization.");
    }
    // --- Comparison Summary Page (Filterable & Sortable Table) ---
    const comparisonTable = document.getElementById('comparison-table');
    if (comparisonTable) {
        console.log("Comparison page table detected. Initializing sorter.");
        // Note: Filters are handled server-side via form submission for this table
        initTableSorter('comparison-table'); // Enable client-side sorting
    }
    // --- Fund Duration Details Page ---
    const fundDurationTable = document.getElementById('fund-duration-table');
    if (fundDurationTable) {
        console.log("Fund duration details page table detected. Initializing sorter.");
        initTableSorter('fund-duration-table'); 
    }
    // Add any other global initializations here
});
</file>

<file path="static/js/modules/charts/timeSeriesChart.js">
// This file contains the specific logic for creating and configuring time-series line charts
// using the Chart.js library. It's designed to be reusable for generating consistent charts
// across different metrics and funds.
// static/js/modules/charts/timeSeriesChart.js
// Encapsulates Chart.js configuration and rendering for multiple time series datasets
/**
 * Creates and renders a time series chart using Chart.js.
 * @param {string} canvasId - The ID of the canvas element.
 * @param {object} chartData - Data object containing labels, multiple datasets, metrics.
 * @param {string} metricName - Name of the overall metric (e.g., Duration).
 * @param {string} fundCode - Code of the specific fund.
 * @param {number | null} maxZScore - The maximum absolute Z-score for this fund (used in title).
 * @param {boolean} isMissingLatest - Flag indicating if the latest point is missing for any spread.
 */
export function createTimeSeriesChart(canvasId, chartData, metricName, fundCode, maxZScore, isMissingLatest) {
    const ctx = document.getElementById(canvasId).getContext('2d');
    if (!ctx) {
        console.error(`[createTimeSeriesChart] Failed to get 2D context for canvas ID: ${canvasId}`);
        return; // Exit if canvas context is not available
    }
    // --- Prepare Chart Title (Adjusted for different contexts) --- 
    let chartTitle = metricName; // Default to just the metric name
    if (fundCode) { // If fundCode is provided (called from metric page)
        let titleSuffix = maxZScore !== null ? `(Max Spread Z: ${maxZScore.toFixed(2)})` : '(Z-Score N/A)';
        if (isMissingLatest) {
            titleSuffix = "(MISSING LATEST DATA)";
        }
        chartTitle = `${metricName} for ${fundCode} ${titleSuffix}`; 
    }
    // If fundCode is null (called from fund page), title remains just metricName
    console.log(`[createTimeSeriesChart] Using chart title: "${chartTitle}" for canvas ${canvasId}`);
    // --- Prepare Chart Data & Styling --- 
    const datasets = chartData.datasets.map((ds, index) => {
        const isBenchmark = ds.label.includes('Benchmark'); // Basic check, refine if needed
        const isLastDataset = index === chartData.datasets.length - 1; // Check if it's the benchmark dataset based on order from app.py
        return {
            ...ds,
            // Style points - highlight last point for non-benchmark lines
            pointRadius: (context) => {
                const isLastPoint = context.dataIndex === (ds.data.length - 1);
                // Only show large radius for last point of non-benchmark datasets
                return isLastPoint && !isLastDataset ? 6 : 0;
            },
            pointHoverRadius: (context) => {
                const isLastPoint = context.dataIndex === (ds.data.length - 1);
                return isLastPoint && !isLastDataset ? 8 : 5;
            },
            pointBackgroundColor: isLastDataset ? 'darkgrey' : ds.borderColor, // Use border color for fund points, grey for benchmark
            borderWidth: isLastDataset ? 2 : 1.5, // Slightly thicker benchmark line
        };
    });
    // --- Chart Configuration --- 
    const config = {
        type: 'line',
        data: {
            labels: chartData.labels, // Dates as strings
            datasets: datasets // Now includes multiple fund series + benchmark
        },
        options: {
            responsive: true,
            maintainAspectRatio: false, 
            plugins: {
                title: { display: true, text: chartTitle, font: { size: 16 } },
                legend: { position: 'top' },
                tooltip: { 
                    mode: 'index', 
                    intersect: false, 
                }
            },
            hover: { mode: 'nearest', intersect: true },
            scales: {
                x: {
                    type: 'time',
                    time: {
                        unit: 'day',
                        tooltipFormat: 'MMM dd, yyyy',
                        displayFormats: { day: 'MMM dd', week: 'MMM dd yyyy', month: 'MMM yyyy' }
                    },
                    title: { display: true, text: 'Date' }
                },
                y: {
                    display: true,
                    title: { display: true, text: metricName },
                    // Dynamic scaling based on *all* datasets
                    suggestedMin: Math.min(...datasets.flatMap(ds => ds.data.filter(d => d !== null && !isNaN(d)))),
                    suggestedMax: Math.max(...datasets.flatMap(ds => ds.data.filter(d => d !== null && !isNaN(d))))
                }
            }
        }
    };
    // --- Create Chart Instance (with Error Handling) --- 
    try {
        // Check if a chart instance already exists on the canvas and destroy it
        let existingChart = Chart.getChart(canvasId);
        if (existingChart) {
            console.log(`[createTimeSeriesChart] Destroying existing chart on canvas ${canvasId}`);
            existingChart.destroy();
        }
        // Attempt to create the new chart
        console.log(`[createTimeSeriesChart] Attempting to create new Chart on canvas ${canvasId}`);
        const chartInstance = new Chart(ctx, config); // Store the instance
        // Log success *after* instantiation
        console.log(`[createTimeSeriesChart] Successfully created chart for "${chartTitle}" on ${canvasId}`);
        return chartInstance; // Return the created chart instance
    } catch (error) {
        // Log any error during chart instantiation
        console.error(`[createTimeSeriesChart] Error creating chart on canvas ${canvasId} for "${chartTitle}":`, error);
        // Optionally display an error message in the canvas container
        const errorP = document.createElement('p');
        errorP.textContent = `Error rendering chart: ${error.message}`;
        errorP.className = 'text-danger';
        // Attempt to add error message to parent, replacing canvas if needed
        const canvasElement = document.getElementById(canvasId);
        if (canvasElement && canvasElement.parentElement) {
            canvasElement.parentElement.appendChild(errorP);
            canvasElement.style.display = 'none'; // Hide broken canvas
        }    
    }
}
</file>

<file path="static/js/modules/ui/chartRenderer.js">
// This file is responsible for dynamically creating and rendering the user interface elements
// related to charts and associated metric tables within the application.
// It separates the logic for generating the visual components from the main application flow.
// Updated to handle optional secondary data sources and toggle visibility.
// static/js/modules/ui/chartRenderer.js
// Handles creating DOM elements for charts and tables
import { createTimeSeriesChart } from '../charts/timeSeriesChart.js';
import { formatNumber } from '../utils/helpers.js';
// Store chart instances to manage them later (e.g., for toggling)
const chartInstances = {};
/**
 * Renders charts and metric tables into the specified container.
 * Handles primary and optional secondary (S&P) data source toggle.
 * @param {HTMLElement} container - The parent element to render into.
 * @param {object} payload - The full data payload object from Flask (contains metadata and funds data).
 */
export function renderChartsAndTables(container, payload) {
    const metadata = payload.metadata;
    const fundsData = payload.funds; // Renamed from chartsData for clarity
    const metricName = metadata.metric_name;
    const latestDate = metadata.latest_date;
    // Keep original column names from metadata for table generation
    const primaryFundColsMeta = metadata.fund_col_names;
    const primaryBenchColMeta = metadata.benchmark_col_name;
    const secondaryFundColsMeta = metadata.secondary_fund_col_names;
    const secondaryBenchColMeta = metadata.secondary_benchmark_col_name;
    const secondaryDataAvailableOverall = metadata.secondary_data_available;
    console.log("[chartRenderer] Rendering charts for metric:", metricName, "Latest Date:", latestDate);
    console.log("[chartRenderer] Metadata:", metadata);
    console.log("[chartRenderer] Fund Data Keys:", Object.keys(fundsData || {}));
    // Clear previous content and chart instances
    container.innerHTML = '';
    Object.keys(chartInstances).forEach(key => {
        try {
            chartInstances[key]?.destroy(); // Properly destroy old chart instances
        } catch (e) {
            console.warn(`Error destroying chart instance ${key}:`, e);
        }
        delete chartInstances[key];
    });
    if (!fundsData || Object.keys(fundsData).length === 0) {
        console.warn("[chartRenderer] No fund data available for metric:", metricName);
        container.innerHTML = '<p>No fund data available for this metric.</p>';
        return;
    }
    // --- Setup Toggle Switch --- 
    const toggleContainer = document.getElementById('sp-toggle-container');
    // Event listener will be attached by the caller (main.js)
    if (toggleContainer) {
        // Show toggle if *any* secondary data is potentially available based on metadata
        if (secondaryDataAvailableOverall) {
            console.log("[chartRenderer] Overall secondary data available, ensuring toggle container is visible.");
            toggleContainer.style.display = 'block'; 
        } else {
            console.log("[chartRenderer] Overall secondary data not available, ensuring toggle container is hidden.");
            toggleContainer.style.display = 'none'; 
        }
    } else {
        console.warn("[chartRenderer] Toggle switch container not found in the DOM.");
    }
    // --- Render Charts and Tables for Each Fund --- 
    for (const [fundCode, fundData] of Object.entries(fundsData)) {
        console.log(`[chartRenderer] Processing fund: ${fundCode}`);
        const charts = fundData.charts || [];
        const isMissingLatest = fundData.is_missing_latest;
        // Find max absolute Z-score from PRIMARY MAIN metrics for section highlight
        let maxAbsPrimaryZScore = 0;
        let primaryZScoreForTitle = null;
        const mainChartConfig = charts.find(c => c.chart_type === 'main');
        if (mainChartConfig && mainChartConfig.latest_metrics) {
            const mainMetrics = mainChartConfig.latest_metrics;
            const primaryColsToCheck = [];
            if (primaryBenchColMeta) primaryColsToCheck.push(primaryBenchColMeta);
            if (primaryFundColsMeta && Array.isArray(primaryFundColsMeta)) primaryColsToCheck.push(...primaryFundColsMeta);
            primaryColsToCheck.forEach(colName => {
                if (!colName) return;
                const zScoreKey = `${colName} Change Z-Score`; 
                const zScore = mainMetrics[zScoreKey];
                 if (zScore !== null && typeof zScore !== 'undefined' && !isNaN(zScore)) {
                     const absZ = Math.abs(zScore);
                     if (absZ > maxAbsPrimaryZScore) {
                         maxAbsPrimaryZScore = absZ;
                         primaryZScoreForTitle = zScore; 
                     }
                 }
            });
        }
        // Determine CSS class for the wrapper based on primary Z-score
        let zClass = '';
        if (maxAbsPrimaryZScore > 3) { zClass = 'very-high-z'; }
        else if (maxAbsPrimaryZScore > 2) { zClass = 'high-z'; }
        // Create a main wrapper for the fund
        const fundWrapper = document.createElement('div');
        fundWrapper.className = `fund-wrapper ${zClass}`; // Use a different class for the outer wrapper
        fundWrapper.id = `fund-wrapper-${fundCode}`;
        // Add Duration Details Link (if applicable) - Moved to fund level
        if (metricName === 'Duration') {
            const linkDiv = document.createElement('div');
            linkDiv.className = 'mb-2 text-end';
            const link = document.createElement('a');
            link.href = `/fund/duration_details/${fundCode}`;
            link.className = 'btn btn-info btn-sm';
            link.textContent = `View Security Duration Changes for ${fundCode} →`;
            linkDiv.appendChild(link);
            fundWrapper.appendChild(linkDiv);
        }
        // Create a row container for the charts within this fund
        const chartsRow = document.createElement('div');
        chartsRow.className = 'row'; // Bootstrap row class
        // Now loop through the charts for this fund (Relative first, then Main)
        charts.forEach(chartConfig => {
            const chartType = chartConfig.chart_type;
            const chartTitle = chartConfig.title;
            const chartLabels = chartConfig.labels;
            const chartDatasets = chartConfig.datasets;
            const chartMetrics = chartConfig.latest_metrics;
            const chartId = `${fundCode}-${chartType}`;
            console.log(`[chartRenderer] Creating elements for chart: ${chartId}`);
             // --- Create DOM Elements for Each Chart --- 
            const chartWrapper = document.createElement('div');
            // Add column class for side-by-side layout on large screens
            chartWrapper.className = `chart-container-wrapper chart-type-${chartType} col-lg-6`; 
            chartWrapper.id = `chart-wrapper-${chartId}`;
        // Create Chart Canvas
        const canvas = document.createElement('canvas');
            canvas.id = `chart-${chartId}`;
        canvas.className = 'chart-canvas';
            chartWrapper.appendChild(canvas);
            // Create Metrics Table (pass specific metrics and chart type)
        const table = createMetricsTable(
                chartMetrics,
            latestDate,
                chartType, // Pass chart type to determine columns
                metadata // Pass full metadata for context
            );
            chartWrapper.appendChild(table);
            // Append chartWrapper to the row, not the fundWrapper directly
            chartsRow.appendChild(chartWrapper); 
        // --- Render Chart --- 
        setTimeout(() => {
            const chartCanvas = document.getElementById(canvas.id);
             if (chartCanvas && chartCanvas.getContext('2d')) {
                    console.log(`[chartRenderer] Rendering chart for ${chartId}`);
                    // Prepare chart data object for the charting function
                    const chartDataForFunction = {
                        labels: chartLabels,
                        datasets: chartDatasets
                    };
                    // Pass specific Z-score ONLY if it's the main chart
                    const zScoreForChartTitle = (chartType === 'main') ? primaryZScoreForTitle : null;
                    const chart = createTimeSeriesChart(
                        canvas.id, 
                        chartDataForFunction, // Pass the structured data
                        chartTitle, // Use the title from config
                        fundCode, // Keep fund code for context if needed
                        zScoreForChartTitle, // Pass main Z-score only to main chart
                        isMissingLatest // Still relevant at fund level
                    );
                 if (chart) {
                        chartInstances[chartId] = chart; // Store chart instance with unique ID
                        console.log(`[chartRenderer] Stored chart instance for ${chartId}`);
                 } else {
                        console.error(`[chartRenderer] Failed to create chart instance for ${chartId}`);
                 }
            } else {
                console.error(`[chartRenderer] Could not get 2D context for canvas ${canvas.id}`);
                const errorP = document.createElement('p');
                errorP.textContent = 'Error rendering chart.';
                errorP.className = 'text-danger';
                if (chartCanvas && chartCanvas.parentNode) {
                    chartCanvas.parentNode.replaceChild(errorP, chartCanvas);
                    } else if (chartWrapper) {
                        chartWrapper.appendChild(errorP);
                }
            }
        }, 0); 
        }); // End loop through charts for the fund
        // Append the row containing the charts to the main fund wrapper
        fundWrapper.appendChild(chartsRow);
        container.appendChild(fundWrapper); // Add the fund's wrapper to the main container
    } // End loop through funds
    console.log("[chartRenderer] Finished processing all funds.");
}
/**
 * Updates the visibility of secondary/SP data datasets across all managed charts.
 * @param {boolean} show - Whether to show or hide the secondary/SP datasets.
 */
export function toggleSecondaryDataVisibility(show) { // Make sure this is exported if used by main.js
    console.log(`[chartRenderer] Toggling SP data visibility to: ${show}`);
    // Iterate through the centrally stored chart instances
    Object.entries(chartInstances).forEach(([chartId, chart]) => {
        if (!chart || typeof chart.destroy === 'undefined') { // Check if chart instance is valid
            console.warn(`[chartRenderer] Skipping invalid chart instance for ID: ${chartId}`);
            return;
        }
        let spDatasetToggled = false;
        try {
        chart.data.datasets.forEach((dataset, index) => {
            // Check the isSpData flag added from Python
            if (dataset.isSpData === true) {
                // Use setDatasetVisibility for better control than just 'hidden' property
                chart.setDatasetVisibility(index, show);
                    console.log(`[chartRenderer] Chart ${chart.canvas.id} (${chartId}) - Setting SP dataset ${index} ('${dataset.label}') visibility to ${show}`);
                spDatasetToggled = true;
            }
        });
        // Only update if an SP dataset was actually toggled for this chart
        if (spDatasetToggled) {
            chart.update(); // Update the chart to reflect visibility changes
                console.log(`[chartRenderer] Updated chart ${chart.canvas.id} (${chartId})`);
            }
        } catch (error) {
            console.error(`[chartRenderer] Error toggling visibility for chart ${chartId}:`, error);
             // Potentially remove the instance if it's causing persistent errors?
            // delete chartInstances[chartId]; 
        }
    });
}
/**
 * Creates the HTML table element displaying metrics for a specific chart.
 *
 * @param {object | null} metrics - Metrics object specific to this chart (relative or main).
 * @param {string} latestDate - The latest date string.
 * @param {string} chartType - 'relative' or 'main'.
 * @param {object} metadata - The overall metadata object from Flask (for column names).
 * @returns {HTMLTableElement} The created table element.
 */
function createMetricsTable(
    metrics, 
    latestDate, 
    chartType, 
    metadata 
) {
    const table = document.createElement('table');
    table.className = 'table table-sm table-bordered metrics-table';
    const thead = table.createTHead();
    const headerRow = thead.insertRow();
    const tbody = table.createTBody();
    const secondaryAvailable = metadata.secondary_data_available; // Overall flag
    const primaryFundColsMeta = metadata.fund_col_names || [];
    const primaryBenchColMeta = metadata.benchmark_col_name;
    const secondaryFundColsMeta = metadata.secondary_fund_col_names || [];
    const secondaryBenchColMeta = metadata.secondary_benchmark_col_name;
    const secondaryPrefix = "S&P ";
    if (!metrics || Object.keys(metrics).length === 0) {
        console.warn(`[createMetricsTable] Metrics object is null or empty for chart type: ${chartType}.`);
        headerRow.innerHTML = '<th>Metrics</th>'; // Simple header
        const row = tbody.insertRow();
        const cell = row.insertCell();
        cell.textContent = 'Metrics not available.';
        return table;
    }
    // --- Define Headers based on Chart Type --- 
    let headers = ['Column', `Latest Value (${latestDate})`, 'Change', 'Mean', 'Max', 'Min', 'Change Z-Score'];
    let secondaryHeaders = ['S&P Latest', 'S&P Change', 'S&P Mean', 'S&P Max', 'S&P Min', 'S&P Z-Score'];
    let showSecondaryColumns = false;
    if (chartType === 'relative') {
        // Check if any S&P Relative metrics actually exist
        showSecondaryColumns = Object.keys(metrics).some(key => key.startsWith(secondaryPrefix + 'Relative '));
    } else { // chartType === 'main'
        // Check if any regular S&P metrics exist (excluding relative)
        showSecondaryColumns = Object.keys(metrics).some(key => key.startsWith(secondaryPrefix) && !key.startsWith(secondaryPrefix + 'Relative '));
    }
    headerRow.innerHTML = `<th>${headers.join('</th><th>')}</th>` + 
                         (showSecondaryColumns ? `<th class="text-muted">${secondaryHeaders.join('</th><th class="text-muted">')}</th>` : '');
    // --- Populate Rows based on Chart Type --- 
    const addRow = (displayName, baseKey, isSecondary = false) => {
        const prefix = isSecondary ? secondaryPrefix : '';
        const fullBaseKey = prefix + baseKey;
        // Check if *any* metric exists for this base key and prefix
        const latestValKey = `${fullBaseKey} Latest Value`;
        const changeKey = `${fullBaseKey} Change`;
        const meanKey = `${fullBaseKey} Mean`;
        const maxKey = `${fullBaseKey} Max`;
        const minKey = `${fullBaseKey} Min`;
        const zScoreKey = `${fullBaseKey} Change Z-Score`;
        // Only add row if at least one relevant metric is present
        if (
            metrics.hasOwnProperty(latestValKey) || metrics.hasOwnProperty(changeKey) || 
            metrics.hasOwnProperty(meanKey) || metrics.hasOwnProperty(maxKey) || 
            metrics.hasOwnProperty(minKey) || metrics.hasOwnProperty(zScoreKey)
        ) {
            const row = tbody.insertRow();
            const zScore = metrics[zScoreKey];
            let zClass = '';
            if (zScore !== null && typeof zScore !== 'undefined' && !isNaN(zScore)) {
                const absZ = Math.abs(zScore);
                if (absZ > 3) { zClass = 'very-high-z'; }
                else if (absZ > 2) { zClass = 'high-z'; }
        }
            row.className = zClass;
            row.insertCell().textContent = displayName;
            row.insertCell().textContent = formatNumber(metrics[latestValKey]);
            row.insertCell().textContent = formatNumber(metrics[changeKey]);
            row.insertCell().textContent = formatNumber(metrics[meanKey]);
            row.insertCell().textContent = formatNumber(metrics[maxKey]);
            row.insertCell().textContent = formatNumber(metrics[minKey]);
            row.insertCell().textContent = formatNumber(metrics[zScoreKey]);
            if (showSecondaryColumns && !isSecondary) {
                // Add placeholder cells if primary row but secondary columns shown
                for (let i = 0; i < secondaryHeaders.length; i++) {
                    row.insertCell().textContent = '-';
                }
            }
        } else if (isSecondary && showSecondaryColumns) {
            // Add secondary row even if primary version doesn't exist, but only if secondary columns are shown
            const row = tbody.insertRow();
            row.insertCell().textContent = displayName;
            row.insertCell().textContent = formatNumber(metrics[latestValKey]);
            row.insertCell().textContent = formatNumber(metrics[changeKey]);
            row.insertCell().textContent = formatNumber(metrics[meanKey]);
            row.insertCell().textContent = formatNumber(metrics[maxKey]);
            row.insertCell().textContent = formatNumber(metrics[minKey]);
            row.insertCell().textContent = formatNumber(metrics[zScoreKey]);
            // Add empty primary cells
             for (let i = 0; i < headers.length -1; i++) { // -1 for the name column
                 row.insertCell(1).textContent = '-'; // Insert after name
             }
             row.className = 'text-muted'; // Mute the secondary row
        }
    };
    const addPairedRow = (displayName, baseKey) => {
        const primaryExists = Object.keys(metrics).some(k => k.startsWith(baseKey) && !k.startsWith(secondaryPrefix));
        const secondaryExists = showSecondaryColumns && Object.keys(metrics).some(k => k.startsWith(secondaryPrefix + baseKey));
        if (primaryExists || secondaryExists) {
            const row = tbody.insertRow();
            const zScoreKey = `${baseKey} Change Z-Score`;
            const zScore = metrics[zScoreKey]; // Use primary Z for highlight
            let zClass = '';
            if (zScore !== null && typeof zScore !== 'undefined' && !isNaN(zScore)) {
                const absZ = Math.abs(zScore);
                if (absZ > 3) { zClass = 'very-high-z'; }
                else if (absZ > 2) { zClass = 'high-z'; }
                }
            row.className = zClass;
            row.insertCell().textContent = displayName;
            // Primary Metrics
            row.insertCell().textContent = primaryExists ? formatNumber(metrics[`${baseKey} Latest Value`]) : '-';
            row.insertCell().textContent = primaryExists ? formatNumber(metrics[`${baseKey} Change`]) : '-';
            row.insertCell().textContent = primaryExists ? formatNumber(metrics[`${baseKey} Mean`]) : '-';
            row.insertCell().textContent = primaryExists ? formatNumber(metrics[`${baseKey} Max`]) : '-';
            row.insertCell().textContent = primaryExists ? formatNumber(metrics[`${baseKey} Min`]) : '-';
            row.insertCell().textContent = primaryExists ? formatNumber(metrics[zScoreKey]) : '-';
            // Secondary Metrics (if columns are shown)
            if (showSecondaryColumns) {
                row.insertCell().textContent = secondaryExists ? formatNumber(metrics[`${secondaryPrefix}${baseKey} Latest Value`]) : '-';
                row.insertCell().textContent = secondaryExists ? formatNumber(metrics[`${secondaryPrefix}${baseKey} Change`]) : '-';
                row.insertCell().textContent = secondaryExists ? formatNumber(metrics[`${secondaryPrefix}${baseKey} Mean`]) : '-';
                row.insertCell().textContent = secondaryExists ? formatNumber(metrics[`${secondaryPrefix}${baseKey} Max`]) : '-';
                row.insertCell().textContent = secondaryExists ? formatNumber(metrics[`${secondaryPrefix}${baseKey} Min`]) : '-';
                row.insertCell().textContent = secondaryExists ? formatNumber(metrics[`${secondaryPrefix}${baseKey} Change Z-Score`]) : '-';
            }
        }
    };
    if (chartType === 'relative') {
        // Add row specifically for 'Relative' if its metrics exist
        addPairedRow('Relative (Port - Bench)', 'Relative');
    } else { // chartType === 'main'
        // Add Benchmark row first if it exists
        if (primaryBenchColMeta) {
             addPairedRow(primaryBenchColMeta, primaryBenchColMeta);
        }
        // Add rows for Fund columns
        primaryFundColsMeta.forEach(fundCol => {
            addPairedRow(fundCol, fundCol);
        });
    }
    // Ensure tbody is not empty, add placeholder if needed
    if (tbody.rows.length === 0) {
        const row = tbody.insertRow();
        const cell = row.insertCell();
        cell.colSpan = headers.length + (showSecondaryColumns ? secondaryHeaders.length : 0);
        cell.textContent = 'No relevant metrics found for this chart.';
    }
    return table;
} 
/**
 * Renders a single time series chart for a specific security.
 * @param {string} canvasId - The ID of the canvas element.
 * @param {object} chartData - The chart data (labels, datasets) from Flask.
 * @param {string} securityId - The ID of the security.
 * @param {string} metricName - The name of the metric.
 */
export function renderSingleSecurityChart(canvasId, chartData, securityId, metricName) {
    const ctx = document.getElementById(canvasId);
    if (!ctx) {
        console.error(`Canvas element with ID '${canvasId}' not found.`);
        return;
    }
    if (!chartData || !chartData.labels || !chartData.datasets) {
        console.error('Invalid or incomplete chart data provided.');
        ctx.parentElement.innerHTML = '<p class="text-danger">Error: Invalid chart data.</p>';
        return;
    }
    try {
        new Chart(ctx, {
            type: 'line',
            data: {
                labels: chartData.labels,
                datasets: chartData.datasets
            },
            options: {
                responsive: true,
                maintainAspectRatio: false,
                plugins: {
                    title: {
                        display: true,
                        text: `${securityId} - ${metricName} Time Series`,
                        font: { size: 16 }
                    },
                    legend: {
                        position: 'top',
                    }
                },
                scales: {
                    x: {
                        title: {
                            display: true,
                            text: 'Date'
                        }
                    },
                    y: {
                        type: 'linear',
                        display: true,
                        position: 'left',
                        title: {
                            display: true,
                            text: `${metricName} Value`
                        },
                        beginAtZero: false,
                        ticks: {
                            maxTicksLimit: 8
                        }
                    },
                    y1: {
                        type: 'linear',
                        display: true,
                        position: 'right',
                        title: {
                            display: true,
                            text: 'Price'
                        },
                        grid: {
                            drawOnChartArea: false,
                        },
                        beginAtZero: false
                    }
                },
                interaction: {
                    intersect: false,
                    mode: 'index',
                },
            }
        });
        console.log(`Chart rendered for ${securityId} - ${metricName}`);
    } catch (error) {
        console.error(`Error creating chart for ${securityId} - ${metricName}:`, error);
        ctx.parentElement.innerHTML = '<p class="text-danger">Error rendering chart.</p>';
    }
} 
/**
 * Renders multiple charts onto a single page (like the Fund Detail page).
 * Stores created chart instances in the module-level 'chartInstances' object.
 * @param {HTMLElement} container - The parent element to render into.
 * @param {Array<object>} allChartData - An array of chart data objects, each with metricName, labels, datasets.
 */
export function renderFundCharts(container, allChartData) {
    console.log("[chartRenderer] Rendering charts for fund detail page.");
    console.log("[chartRenderer] Received Data:", JSON.parse(JSON.stringify(allChartData))); // Deep copy for logging
    container.innerHTML = ''; // Clear previous content
    // Clear previous chart instances for this specific rendering context
    Object.keys(chartInstances).forEach(key => delete chartInstances[key]); 
    if (!allChartData || !Array.isArray(allChartData) || allChartData.length === 0) {
        console.warn("[chartRenderer] No chart data provided for the fund page.");
        // Message should be handled by the template, but log it here.
        return;
    }
    // Iterate through each metric's chart data
    allChartData.forEach((metricData, index) => {
        if (!metricData || !metricData.metricName || !metricData.labels || !metricData.datasets) {
            console.warn(`[chartRenderer] Skipping chart at index ${index} due to missing data:`, metricData);
            return;
        }
        const metricName = metricData.metricName;
        const safeMetricName = metricName.replace(/[^a-zA-Z0-9]/g, '-') || 'metric'; // Create a CSS-safe ID part
        console.log(`[chartRenderer] Processing metric: ${metricName}`);
        // Create wrapper div for each chart (using Bootstrap columns for layout)
        const wrapper = document.createElement('div');
        // Uses the col classes defined in the template's fundChartsArea (row-cols-1 row-cols-lg-2)
        wrapper.className = `chart-container-wrapper fund-chart-item`; 
        wrapper.id = `fund-chart-wrapper-${safeMetricName}-${index}`;
        // Create Chart Canvas
        const canvas = document.createElement('canvas');
        // Ensure unique ID for each canvas
        canvas.id = `fund-chart-${safeMetricName}-${index}`; 
        canvas.className = 'chart-canvas';
        wrapper.appendChild(canvas);
        console.log(`[chartRenderer] Created canvas with id: ${canvas.id} for metric: ${metricName}`);
        // Append the wrapper to the main container
        container.appendChild(wrapper);
        console.log(`[chartRenderer] Appended wrapper for ${metricName} to container.`);
        // Render Chart using the existing time series chart function
        // Use setTimeout to ensure the canvas is in the DOM and sized
        setTimeout(() => {
            console.log(`[chartRenderer] Preparing to render chart for metric: ${metricName} in setTimeout.`);
             if (canvas.getContext('2d')) {
                 console.log(`[chartRenderer] Canvas context obtained. Calling createTimeSeriesChart with:`, {
                    canvasId: canvas.id,
                    data: JSON.parse(JSON.stringify(metricData)), // Log deep copy
                    titlePrefix: metricName, // Use metric name as the main title part
                    fundCodeOrSecurityId: null, // Not needed for title here
                    zScoreForTitle: null, // No specific Z-score for the whole page/chart
                    is_missing_latest: null // Not applicable here
                 });
                 // Create the chart AND store the instance
                 const chartInstance = createTimeSeriesChart(
                     canvas.id,         // The unique canvas ID
                     metricData,        // Data object with labels and datasets
                     metricName,        // Title prefix (e.g., "Yield")
                     null,              // fundCodeOrSecurityId (not needed for title)
                     null,              // zScoreForTitle (not applicable)
                     null               // is_missing_latest (not applicable)
                 );
                 if (chartInstance) {
                     // Store the instance in the module-level object
                     chartInstances[canvas.id] = chartInstance;
                     console.log(`[chartRenderer] Stored chart instance for ${metricName} with key ${canvas.id}`);
                 } else {
                     console.error(`[chartRenderer] Failed to get chart instance for metric: ${metricName}`);
                 }
            } else {
                console.error(`[chartRenderer] Could not get 2D context for canvas ${canvas.id} (Metric: ${metricName})`);
                const errorP = document.createElement('p');
                errorP.textContent = `Error rendering chart for ${metricName}.`;
                errorP.className = 'text-danger';
                canvas.parentNode.replaceChild(errorP, canvas); // Replace canvas with error message
            }
        }, 0); 
    });
    console.log("[chartRenderer] Finished rendering all fund charts.");
} 
// Export necessary functions
// REMOVED: export { toggleSecondaryDataVisibility }; // Export toggle function
</file>

<file path="static/js/modules/ui/securityTableFilter.js">
// This file implements client-side filtering for the HTML table displaying security-level metrics.
// It enhances the user experience by allowing interactive filtering based on the values
// in specific static columns (e.g., Sector, Rating) without requiring a page reload.
// static/js/modules/ui/securityTableFilter.js
// This module handles client-side filtering for the securities table.
/**
 * Initializes the filtering functionality for the securities table.
 */
export function initSecurityTableFilter() {
    const filterSelects = document.querySelectorAll('.security-filter-select');
    const tableBody = document.getElementById('securities-table-body');
    if (!tableBody || filterSelects.length === 0) {
        console.log("Security table body or filter selects not found. Filtering disabled.");
        return; // Exit if necessary elements aren't present
    }
    // Store all original rows. Use querySelectorAll for robustness.
    const originalRows = Array.from(tableBody.querySelectorAll('tr'));
    if (originalRows.length === 0) {
        console.log("No rows found in the table body.");
        return; // Exit if no data rows
    }
    // Function to get current filter values
    const getCurrentFilters = () => {
        const filters = {};
        filterSelects.forEach(select => {
            if (select.value) { // Only add if a filter is selected (not 'All')
                filters[select.dataset.column] = select.value;
            }
        });
        return filters;
    };
    // Function to perform filtering and update the table
    const applyFilters = () => {
        const currentFilters = getCurrentFilters();
        const filterKeys = Object.keys(currentFilters);
        // Clear current table body content efficiently
        tableBody.innerHTML = ''; 
        originalRows.forEach(row => {
            let matches = true;
            // Get all cells in the current row
            const cells = row.querySelectorAll('td');
            // Assuming the order of cells matches the order of `column_order` from Python
            // We need a way to map filter column names to cell indices
            // Let's get the header names to map column names to indices
            const headerCells = document.querySelectorAll('#securities-table th');
            const columnNameToIndexMap = {};
            headerCells.forEach((th, index) => {
                columnNameToIndexMap[th.textContent.trim()] = index;
            });
            for (const column of filterKeys) {
                const columnIndex = columnNameToIndexMap[column];
                if (columnIndex !== undefined) {
                    const cellValue = cells[columnIndex]?.textContent.trim(); // Use optional chaining
                    // Strict comparison - ensure types match if needed, or use == for type coercion
                    if (cellValue !== currentFilters[column]) {
                        matches = false;
                        break; // No need to check other filters for this row
                    }
                }
                 else {
                      console.warn(`Column "${column}" not found in table header for filtering.`);
                      // Decide how to handle: skip filter, always fail match? Let's skip filter for robustness.
                 }
            }
            if (matches) {
                // Append the row if it matches all active filters
                tableBody.appendChild(row.cloneNode(true)); // Append a clone to avoid issues
            }
        });
        // Display a message if no rows match
        if (tableBody.children.length === 0) {
             const noMatchRow = tableBody.insertRow();
             const cell = noMatchRow.insertCell();
             cell.colSpan = headerCells.length; // Span across all columns
             cell.textContent = 'No securities match the current filter criteria.';
             cell.style.textAlign = 'center';
             cell.style.fontStyle = 'italic';
        }
    };
    // Add event listeners to all filter dropdowns
    filterSelects.forEach(select => {
        select.addEventListener('change', applyFilters);
    });
    console.log("Security table filtering initialized.");
}
</file>

<file path="static/js/modules/ui/tableSorter.js">
// static/js/modules/ui/tableSorter.js
// Purpose: Handles client-side sorting for HTML tables.
/**
 * Initializes sorting functionality for a specified table.
 * @param {string} tableId The ID of the table element to make sortable.
 */
export function initTableSorter(tableId) {
    const table = document.getElementById(tableId);
    if (!table) {
        console.warn(`Table sorter: Table with ID '${tableId}' not found.`);
        return;
    }
    const headers = table.querySelectorAll('thead th.sortable');
    const tbody = table.querySelector('tbody');
    if (!tbody) {
        console.warn(`Table sorter: Table with ID '${tableId}' does not have a tbody.`);
        return;
    }
    headers.forEach(header => {
        header.addEventListener('click', () => {
            // Get column name from data attribute
            const columnName = header.dataset.columnName;
            const currentIsAscending = header.classList.contains('sort-asc');
            const direction = currentIsAscending ? -1 : 1; // -1 for desc, 1 for asc
            // Find the index of the clicked column
            const columnIndex = Array.from(header.parentNode.children).indexOf(header);
            // Remove sorting indicators from other columns
            headers.forEach(h => {
                if (h !== header) {
                  h.classList.remove('sort-asc', 'sort-desc');
                }
            });
            // Set sorting indicator for the current column
            header.classList.toggle('sort-asc', !currentIsAscending);
            header.classList.toggle('sort-desc', currentIsAscending);
            // Sort the rows, passing the column name
            sortRows(tbody, columnIndex, direction, columnName);
        });
    });
    // --- Default Sort --- 
    const defaultSortHeader = table.querySelector('thead th[data-sort-default]');
    if (defaultSortHeader) {
        const defaultDirection = defaultSortHeader.dataset.sortDefault === 'asc' ? 1 : -1;
        const defaultColumnIndex = Array.from(defaultSortHeader.parentNode.children).indexOf(defaultSortHeader);
        const defaultColumnName = defaultSortHeader.dataset.columnName;
        console.log(`Table sorter: Applying default sort on column '${defaultColumnName}' (index ${defaultColumnIndex}), direction ${defaultDirection === 1 ? 'asc' : 'desc'}`);
        // Set initial visual indicators
        if (defaultDirection === 1) {
            defaultSortHeader.classList.add('sort-asc');
        } else {
            defaultSortHeader.classList.add('sort-desc');
        }
        // Perform the initial sort
        sortRows(tbody, defaultColumnIndex, defaultDirection, defaultColumnName);
    } else {
        console.log(`Table sorter: No default sort specified for table '${tableId}'.`);
    }
}
/**
 * Sorts the rows within a table body.
 * @param {HTMLElement} tbody The table body element containing the rows.
 * @param {number} columnIndex The index of the column to sort by.
 * @param {number} direction 1 for ascending, -1 for descending.
 * @param {string} columnName The name of the column being sorted.
 */
function sortRows(tbody, columnIndex, direction, columnName) {
    const rows = Array.from(tbody.querySelectorAll('tr'));
    // Get the correct comparison function, passing the column name
    const compareFunction = getCompareFunction(rows, columnIndex, columnName);
    // Sort the rows
    rows.sort((rowA, rowB) => {
        const cellA = rowA.children[columnIndex];
        const cellB = rowB.children[columnIndex];
        // Use data-value attribute primarily, fall back to textContent
        const valueA = cellA?.dataset.value ?? cellA?.textContent?.trim() ?? '';
        const valueB = cellB?.dataset.value ?? cellB?.textContent?.trim() ?? '';
        return compareFunction(valueA, valueB) * direction;
    });
    // Re-append sorted rows
    tbody.append(...rows); // More efficient way to re-append
}
/**
 * Determines the appropriate comparison function (numeric or text) based on column content.
 * @param {Array<HTMLElement>} rows Array of table row elements.
 * @param {number} columnIndex The index of the column to check.
 * @param {string} columnName The name of the column being sorted.
 * @returns {function(string, string): number} The comparison function.
 */
function getCompareFunction(rows, columnIndex, columnName) {
    // Check the first few rows (up to 5 data rows) to guess the data type
    let isNumeric = true;
    for (let i = 0; i < Math.min(rows.length, 5); i++) {
        const cell = rows[i].children[columnIndex];
        // Use data-value attribute primarily for checking type
        const value = cell?.dataset.value ?? cell?.textContent?.trim() ?? '';
        // Allow empty strings in numeric columns, but if we find something non-numeric (and not empty), switch to text sort
        if (value !== '' && isNaN(Number(value.replace(/,/g, '')))) {
            isNumeric = false;
            break;
        }
    }
    if (isNumeric) {
        // Check if it's the special column 'Change Z-Score'
        if (columnName === 'Change Z-Score') {
             // Use absolute value for comparison
            return (a, b) => {
                const numA = Math.abs(parseNumber(a));
                const numB = Math.abs(parseNumber(b));
                return numA - numB;
            };
        } else {
            // Standard numeric comparison for other numeric columns
            return (a, b) => {
                const numA = parseNumber(a);
                const numB = parseNumber(b);
                return numA - numB;
            };
        }
    } else {
        // Case-insensitive text comparison
        return (a, b) => a.toLowerCase().localeCompare(b.toLowerCase());
    }
}
/**
 * Helper to parse number, handling empty strings and NaN.
 * Returns -Infinity for values that cannot be parsed as numbers or are empty,
 * ensuring they sort consistently.
 * @param {string} val The string value to parse.
 * @returns {number}
 */
function parseNumber(val) {
    if (val === null || val === undefined || val.trim() === '') {
        return -Infinity; // Treat empty/null/undefined as very small
    }
    const num = Number(val.replace(/,/g, ''));
    // Treat non-numeric as very small. Math.abs(-Infinity) is Infinity, which might be desired
    // when sorting absolute values (non-numbers/empty go to the end when ascending by abs value).
    return isNaN(num) ? -Infinity : num;
}
</file>

<file path="static/js/modules/utils/helpers.js">
// This file contains general JavaScript utility functions that can be reused across different modules.
// It helps keep common tasks, like formatting numbers for display, consistent and DRY (Don't Repeat Yourself).
// static/js/modules/utils/helpers.js
// Utility functions
/**
 * Formats a number for display, handling null/undefined.
 * @param {number | null | undefined} value - The number to format.
 * @param {number} [digits=2] - Number of decimal places.
 * @returns {string} Formatted number or 'N/A'.
 */
export function formatNumber(value, digits = 2) {
    if (value === null || typeof value === 'undefined' || isNaN(value)) {
        return 'N/A';
    }
    return Number(value).toFixed(digits);
}
</file>

<file path="templates/attribution_charts.html">
{#
    Purpose: Attribution Residuals Chart Page. Visualizes residuals over time for Benchmark and Portfolio.
    - Two charts: Benchmark and Portfolio, each with bars for residuals (Prod and S&P) and a line for cumulative net residuals.
    - Toggle for net/absolute residuals.
    - Date range slider (two handles).
    - Filters for fund, characteristic, characteristic value.
    - Uses Chart.js for rendering.
#}
{% extends "base.html" %}
{% block title %}Attribution Residuals Charts{% endblock %}
{% block content %}
<div class="container-fluid mt-4">
    <h1>Attribution Residuals Charts</h1>
    <p class="text-muted">Visualize residuals over time for Benchmark and Portfolio. Use the filters and controls to explore the data.</p>
    <!-- Filters -->
    <form method="get" class="row g-3 align-items-end mb-4" id="filter-form">
        <div class="col-md-3">
            <label for="fund-select" class="form-label">Fund</label>
            <select class="form-select" id="fund-select" name="fund" onchange="this.form.submit()">
                <option value="" {% if not selected_fund %}selected{% endif %}>All Funds</option>
                {% for fund in available_funds %}
                    <option value="{{ fund }}" {% if fund == selected_fund %}selected{% endif %}>{{ fund }}</option>
                {% endfor %}
            </select>
        </div>
        <div class="col-md-3">
            <label for="characteristic-select" class="form-label">Group by Characteristic</label>
            <select class="form-select" id="characteristic-select" name="characteristic" onchange="this.form.submit()">
                {% for char in available_characteristics %}
                    <option value="{{ char }}" {% if char == selected_characteristic %}selected{% endif %}>{{ char }}</option>
                {% endfor %}
            </select>
        </div>
        <div class="col-md-3">
            <label for="characteristic-value-select" class="form-label">Filter by {{ selected_characteristic }}</label>
            <select class="form-select" id="characteristic-value-select" name="characteristic_value" onchange="this.form.submit()">
                <option value="" {% if not selected_characteristic_value %}selected{% endif %}>All</option>
                {% for val in available_characteristic_values %}
                    <option value="{{ val }}" {% if val == selected_characteristic_value %}selected{% endif %}>{{ val }}</option>
                {% endfor %}
            </select>
        </div>
    </form>
    <!-- Net/Absolute Toggle -->
    <div class="mb-3">
        <div class="form-check form-switch">
            <input class="form-check-input" type="checkbox" id="absToggle" {% if abs_toggle_default %}checked{% endif %}>
            <label class="form-check-label" for="absToggle">Show Absolute Residuals</label>
        </div>
    </div>
    <!-- Date Range Slider -->
    <div class="mb-4">
        <label for="date-range-slider" class="form-label">Date Range</label>
        <div>
            <input type="range" class="form-range" id="date-range-slider" min="0" max="0" value="0" step="1">
            <input type="range" class="form-range" id="date-range-slider-end" min="0" max="0" value="0" step="1">
            <div class="d-flex justify-content-between">
                <span id="start-date-label"></span>
                <span id="end-date-label"></span>
            </div>
        </div>
    </div>
    <!-- Charts -->
    <div class="row">
        <div class="col-md-2"></div>
        <div class="col-md-8 mb-5">
            <h3>Portfolio Residuals</h3>
            <canvas id="portChart" height="120"></canvas>
        </div>
        <div class="col-md-2"></div>
    </div>
    <div class="row">
        <div class="col-md-2"></div>
        <div class="col-md-8 mb-5">
            <h3>Benchmark Residuals</h3>
            <canvas id="benchChart" height="120"></canvas>
        </div>
        <div class="col-md-2"></div>
    </div>
</div>
{% endblock %}
{% block scripts %}
{{ super() }}
<script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
<script>
// Parse chart data from backend
const chartDataBench = JSON.parse({{ chart_data_bench_json|tojson|safe }});
const chartDataPort = JSON.parse({{ chart_data_port_json|tojson|safe }});
// Helper: get date labels
function getDateLabels(data) {
    return data.map(d => d.date);
}
// Helper: get values for a key
function getValues(data, key) {
    return data.map(d => d[key]);
}
// Chart state
let showAbs = {{ 'true' if abs_toggle_default else 'false' }};
let benchData = chartDataBench;
let portData = chartDataPort;
let dateLabels = getDateLabels(benchData);
let sliderMin = 0;
let sliderMax = dateLabels.length - 1;
let sliderStart = 0;
let sliderEnd = dateLabels.length - 1;
// DOM elements
const absToggle = document.getElementById('absToggle');
const sliderStartEl = document.getElementById('date-range-slider');
const sliderEndEl = document.getElementById('date-range-slider-end');
const startDateLabel = document.getElementById('start-date-label');
const endDateLabel = document.getElementById('end-date-label');
// Chart.js chart instances
let benchChart, portChart;
function renderCharts() {
    // Slice data for selected date range
    const dataSlice = (data) => data.slice(sliderStart, sliderEnd + 1);
    const benchSlice = dataSlice(benchData);
    const portSlice = dataSlice(portData);
    const labels = getDateLabels(benchSlice);
    // Datasets
    let benchBarProd, benchBarSP, benchLineProd, benchLineSP;
    let portBarProd, portBarSP, portLineProd, portLineSP;
    if (showAbs) {
        benchBarProd = getValues(benchSlice, 'abs_residual_prod');
        benchBarSP = getValues(benchSlice, 'abs_residual_sp');
        portBarProd = getValues(portSlice, 'abs_residual_prod');
        portBarSP = getValues(portSlice, 'abs_residual_sp');
        // Cumulative not shown for abs
        benchLineProd = benchLineSP = portLineProd = portLineSP = null;
    } else {
        benchBarProd = getValues(benchSlice, 'residual_prod');
        benchBarSP = getValues(benchSlice, 'residual_sp');
        portBarProd = getValues(portSlice, 'residual_prod');
        portBarSP = getValues(portSlice, 'residual_sp');
        benchLineProd = getValues(benchSlice, 'cum_residual_prod');
        benchLineSP = getValues(benchSlice, 'cum_residual_sp');
        portLineProd = getValues(portSlice, 'cum_residual_prod');
        portLineSP = getValues(portSlice, 'cum_residual_sp');
    }
    // Destroy old charts
    if (benchChart) benchChart.destroy();
    if (portChart) portChart.destroy();
    // Benchmark chart
    benchChart = new Chart(document.getElementById('benchChart').getContext('2d'), {
        type: 'bar',
        data: {
            labels: labels,
            datasets: [
                {
                    label: 'Residual (Prod)',
                    data: benchBarProd,
                    backgroundColor: 'rgba(54, 162, 235, 0.5)',
                    borderColor: 'rgba(54, 162, 235, 1)',
                    borderWidth: 1
                },
                {
                    label: 'Residual (S&P)',
                    data: benchBarSP,
                    backgroundColor: 'rgba(255, 99, 132, 0.5)',
                    borderColor: 'rgba(255, 99, 132, 1)',
                    borderWidth: 1
                },
                // Cumulative lines (only for net)
                ...(showAbs ? [] : [
                    {
                        label: 'Cumulative Residual (Prod)',
                        data: benchLineProd,
                        type: 'line',
                        borderColor: 'rgba(54, 162, 235, 1)',
                        backgroundColor: 'rgba(54, 162, 235, 0.1)',
                        fill: false,
                        yAxisID: 'y',
                        tension: 0.2
                    },
                    {
                        label: 'Cumulative Residual (S&P)',
                        data: benchLineSP,
                        type: 'line',
                        borderColor: 'rgba(255, 99, 132, 1)',
                        backgroundColor: 'rgba(255, 99, 132, 0.1)',
                        fill: false,
                        yAxisID: 'y',
                        tension: 0.2
                    }
                ])
            ]
        },
        options: {
            responsive: true,
            plugins: {
                legend: { position: 'top' },
                title: { display: false }
            },
            scales: {
                x: { stacked: false },
                y: { stacked: false, beginAtZero: true }
            }
        }
    });
    // Portfolio chart
    portChart = new Chart(document.getElementById('portChart').getContext('2d'), {
        type: 'bar',
        data: {
            labels: labels,
            datasets: [
                {
                    label: 'Residual (Prod)',
                    data: portBarProd,
                    backgroundColor: 'rgba(54, 162, 235, 0.5)',
                    borderColor: 'rgba(54, 162, 235, 1)',
                    borderWidth: 1
                },
                {
                    label: 'Residual (S&P)',
                    data: portBarSP,
                    backgroundColor: 'rgba(255, 99, 132, 0.5)',
                    borderColor: 'rgba(255, 99, 132, 1)',
                    borderWidth: 1
                },
                ...(showAbs ? [] : [
                    {
                        label: 'Cumulative Residual (Prod)',
                        data: portLineProd,
                        type: 'line',
                        borderColor: 'rgba(54, 162, 235, 1)',
                        backgroundColor: 'rgba(54, 162, 235, 0.1)',
                        fill: false,
                        yAxisID: 'y',
                        tension: 0.2
                    },
                    {
                        label: 'Cumulative Residual (S&P)',
                        data: portLineSP,
                        type: 'line',
                        borderColor: 'rgba(255, 99, 132, 1)',
                        backgroundColor: 'rgba(255, 99, 132, 0.1)',
                        fill: false,
                        yAxisID: 'y',
                        tension: 0.2
                    }
                ])
            ]
        },
        options: {
            responsive: true,
            plugins: {
                legend: { position: 'top' },
                title: { display: false }
            },
            scales: {
                x: { stacked: false },
                y: { stacked: false, beginAtZero: true }
            }
        }
    });
}
// Slider logic
function updateSliderLimits() {
    dateLabels = getDateLabels(benchData);
    sliderMin = 0;
    sliderMax = dateLabels.length - 1;
    sliderStartEl.min = sliderMin;
    sliderStartEl.max = sliderMax;
    sliderEndEl.min = sliderMin;
    sliderEndEl.max = sliderMax;
    sliderStartEl.value = sliderStart;
    sliderEndEl.value = sliderEnd;
    startDateLabel.textContent = dateLabels[sliderStart] || '';
    endDateLabel.textContent = dateLabels[sliderEnd] || '';
}
sliderStartEl.addEventListener('input', function() {
    sliderStart = Math.min(parseInt(sliderStartEl.value), parseInt(sliderEndEl.value));
    sliderEnd = Math.max(parseInt(sliderStartEl.value), parseInt(sliderEndEl.value));
    updateSliderLimits();
    renderCharts();
});
sliderEndEl.addEventListener('input', function() {
    sliderStart = Math.min(parseInt(sliderStartEl.value), parseInt(sliderEndEl.value));
    sliderEnd = Math.max(parseInt(sliderStartEl.value), parseInt(sliderEndEl.value));
    updateSliderLimits();
    renderCharts();
});
absToggle.addEventListener('change', function() {
    showAbs = absToggle.checked;
    renderCharts();
});
// Initialize
function init() {
    absToggle.checked = showAbs;
    sliderStart = 0;
    sliderEnd = dateLabels.length - 1;
    updateSliderLimits();
    renderCharts();
}
window.addEventListener('DOMContentLoaded', init);
</script>
{% endblock %}
</file>

<file path="templates/attribution_radar.html">
{#
    Purpose: Attribution Radar Chart Page. Visualizes aggregated L1 or L2 attribution factors (plus residual) for Portfolio and Benchmark as radar charts.
    - Two radar charts: Portfolio and Benchmark, each with Prod and S&P datasets.
    - Toggle for L1/L2 (default L2).
    - Date range slider and filters for fund, characteristic, characteristic value.
    - Uses Chart.js for rendering.
#}
{% extends "base.html" %}
{% block title %}Attribution Radar Charts{% endblock %}
{% block content %}
<div class="container-fluid mt-4">
    <h1>Attribution Radar Charts</h1>
    <p class="text-muted">Visualize aggregated attribution factors (L1 or L2 + Residual) for Portfolio and Benchmark as radar charts. Use the filters and controls to explore the data.</p>
    <!-- Filters -->
    <form method="get" class="row g-3 align-items-end mb-4" id="filter-form">
        <div class="col-md-3">
            <label for="fund-select" class="form-label">Fund</label>
            <select class="form-select" id="fund-select" name="fund" onchange="this.form.submit()">
                {% for fund in available_funds %}
                    <option value="{{ fund }}" {% if fund == selected_fund %}selected{% endif %}>{{ fund }}</option>
                {% endfor %}
            </select>
        </div>
        <div class="col-md-3">
            <label for="characteristic-select" class="form-label">Group by Characteristic</label>
            <select class="form-select" id="characteristic-select" name="characteristic" onchange="this.form.submit()">
                {% for char in available_characteristics %}
                    <option value="{{ char }}" {% if char == selected_characteristic %}selected{% endif %}>{{ char }}</option>
                {% endfor %}
            </select>
        </div>
        <div class="col-md-3">
            <label for="characteristic-value-select" class="form-label">Filter by {{ selected_characteristic }}</label>
            <select class="form-select" id="characteristic-value-select" name="characteristic_value" onchange="this.form.submit()">
                <option value="" {% if not selected_characteristic_value %}selected{% endif %}>All</option>
                {% for val in available_characteristic_values %}
                    <option value="{{ val }}" {% if val == selected_characteristic_value %}selected{% endif %}>{{ val }}</option>
                {% endfor %}
            </select>
        </div>
        <div class="col-md-3 d-flex align-items-center">
            <label for="level-toggle" class="form-label me-2 mb-0">Attribution Level</label>
            <div class="form-check form-switch">
                <input class="form-check-input" type="checkbox" id="level-toggle" name="level_toggle" {% if selected_level == 'L1' %}checked{% endif %} onchange="document.getElementById('level').value = this.checked ? 'L1' : 'L2'; this.form.submit();">
                <label class="form-check-label" for="level-toggle" id="level-toggle-label">L2</label>
            </div>
            <input type="hidden" name="level" id="level" value="{{ selected_level }}">
        </div>
        <!-- Hidden fields for date range -->
        <input type="hidden" name="start_date" id="start_date" value="{{ start_date.strftime('%Y-%m-%d') }}">
        <input type="hidden" name="end_date" id="end_date" value="{{ end_date.strftime('%Y-%m-%d') }}">
    </form>
    <!-- Date Range Slider -->
    <div class="mb-4">
        <label for="date-range-slider" class="form-label">Date Range</label>
        <div>
            <input type="range" class="form-range" id="date-range-slider" min="0" max="0" value="0" step="1">
            <input type="range" class="form-range" id="date-range-slider-end" min="0" max="0" value="0" step="1">
            <div class="d-flex justify-content-between">
                <span id="start-date-label"></span>
                <span id="end-date-label"></span>
            </div>
        </div>
    </div>
    <!-- Radar Charts -->
    <div class="row">
        <div class="col-md-1"></div>
        <div class="col-md-5 mb-5">
            <h3>Portfolio Attribution</h3>
            <canvas id="portfolioRadar" height="300"></canvas>
        </div>
        <div class="col-md-5 mb-5">
            <h3>Benchmark Attribution</h3>
            <canvas id="benchmarkRadar" height="300"></canvas>
        </div>
        <div class="col-md-1"></div>
    </div>
</div>
{% endblock %}
{% block scripts %}
{{ super() }}
<script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
<script>
// Parse radar data from backend
const radarData = JSON.parse({{ radar_data_json|tojson|safe }});
// Attribution Level Toggle Label
const levelToggle = document.getElementById('level-toggle');
const levelToggleLabel = document.getElementById('level-toggle-label');
if (levelToggle && levelToggleLabel) {
    levelToggleLabel.textContent = levelToggle.checked ? 'L1' : 'L2';
    levelToggle.addEventListener('change', function() {
        levelToggleLabel.textContent = this.checked ? 'L1' : 'L2';
    });
}
// Date slider logic
const minDate = new Date("{{ min_date.strftime('%Y-%m-%d') }}");
const maxDate = new Date("{{ max_date.strftime('%Y-%m-%d') }}");
const startDate = new Date("{{ start_date.strftime('%Y-%m-%d') }}");
const endDate = new Date("{{ end_date.strftime('%Y-%m-%d') }}");
// Generate all dates in range
function getDateArray(start, end) {
    const arr = [];
    let dt = new Date(start);
    while (dt <= end) {
        arr.push(new Date(dt));
        dt.setDate(dt.getDate() + 1);
    }
    return arr;
}
const allDates = getDateArray(minDate, maxDate);
const sliderStartEl = document.getElementById('date-range-slider');
const sliderEndEl = document.getElementById('date-range-slider-end');
const startDateLabel = document.getElementById('start-date-label');
const endDateLabel = document.getElementById('end-date-label');
const startDateInput = document.getElementById('start_date');
const endDateInput = document.getElementById('end_date');
sliderStartEl.min = 0;
sliderStartEl.max = allDates.length - 1;
sliderEndEl.min = 0;
sliderEndEl.max = allDates.length - 1;
// Find initial slider positions
let sliderStart = allDates.findIndex(d => d.getTime() === startDate.getTime());
let sliderEnd = allDates.findIndex(d => d.getTime() === endDate.getTime());
if (sliderStart === -1) sliderStart = 0;
if (sliderEnd === -1) sliderEnd = allDates.length - 1;
sliderStartEl.value = sliderStart;
sliderEndEl.value = sliderEnd;
function updateSliderLabels() {
    startDateLabel.textContent = allDates[sliderStart].toISOString().slice(0, 10);
    endDateLabel.textContent = allDates[sliderEnd].toISOString().slice(0, 10);
}
updateSliderLabels();
sliderStartEl.addEventListener('input', function() {
    sliderStart = Math.min(parseInt(sliderStartEl.value), parseInt(sliderEndEl.value));
    sliderEnd = Math.max(parseInt(sliderStartEl.value), parseInt(sliderEndEl.value));
    updateSliderLabels();
    startDateInput.value = allDates[sliderStart].toISOString().slice(0, 10);
    endDateInput.value = allDates[sliderEnd].toISOString().slice(0, 10);
});
sliderEndEl.addEventListener('input', function() {
    sliderStart = Math.min(parseInt(sliderStartEl.value), parseInt(sliderEndEl.value));
    sliderEnd = Math.max(parseInt(sliderStartEl.value), parseInt(sliderEndEl.value));
    updateSliderLabels();
    startDateInput.value = allDates[sliderStart].toISOString().slice(0, 10);
    endDateInput.value = allDates[sliderEnd].toISOString().slice(0, 10);
});
sliderStartEl.addEventListener('change', function() { document.getElementById('filter-form').submit(); });
sliderEndEl.addEventListener('change', function() { document.getElementById('filter-form').submit(); });
// Render Radar Chart
function renderRadarChart(canvasId, labels, prodData, spData, title) {
    const ctx = document.getElementById(canvasId).getContext('2d');
    if (!ctx) return;
    if (window[canvasId + '_chart']) {
        window[canvasId + '_chart'].destroy();
    }
    window[canvasId + '_chart'] = new Chart(ctx, {
        type: 'radar',
        data: {
            labels: labels,
            datasets: [
                {
                    label: 'Prod',
                    data: prodData,
                    backgroundColor: 'rgba(54, 162, 235, 0.0)',
                    borderColor: 'rgba(54, 162, 235, 1)',
                    pointBackgroundColor: 'rgba(54, 162, 235, 1)',
                    fill: false
                },
                {
                    label: 'S&P',
                    data: spData,
                    backgroundColor: 'rgba(255, 99, 132, 0.0)',
                    borderColor: 'rgba(255, 99, 132, 1)',
                    pointBackgroundColor: 'rgba(255, 99, 132, 1)',
                    fill: false
                }
            ]
        },
        options: {
            responsive: true,
            plugins: {
                legend: { position: 'top' },
                title: { display: true, text: title }
            },
            scales: {
                r: {
                    angleLines: { display: true },
                    suggestedMin: null,
                    suggestedMax: null
                }
            }
        }
    });
}
// On DOMContentLoaded, render both radar charts
window.addEventListener('DOMContentLoaded', function() {
    renderRadarChart(
        'portfolioRadar',
        radarData.labels,
        radarData.portfolio.prod,
        radarData.portfolio.sp,
        'Portfolio Attribution'
    );
    renderRadarChart(
        'benchmarkRadar',
        radarData.labels,
        radarData.benchmark.prod,
        radarData.benchmark.sp,
        'Benchmark Attribution'
    );
});
</script>
{% endblock %}
</file>

<file path="templates/attribution_security_page.html">
{#
    Purpose: Attribution Security-Level Page. Shows attribution data for each security (ISIN) for a selected date and fund.
    Filters: date, fund, type, bench/portfolio, MTD, normalize. Pagination (50 per page).
    Table columns: Security Name (linked), ISIN, Type, Returns (L0 Total), Original Residual, S&P Residual, Residual Diff, L1 values (Orig & S&P)
#}
{% extends 'base.html' %}
{% block title %}Attribution Security-Level Check{% endblock %}
{% block content %}
<div class="container-fluid mt-4">
    <div class="d-flex justify-content-between align-items-center mb-3">
        <h2>Attribution Security-Level Check</h2>
    </div>
    <p class="text-muted">Attribution data for each security. Filters and sorting are applied server-side. Returns = L0 Total.</p>
    {# --- Filter Form --- #}
    <form method="GET" action="{{ url_for('attribution_bp.attribution_security_page') }}" class="mb-3 p-3 border rounded bg-light" id="filter-form">
        <div class="row g-2 align-items-end">
            <div class="col-md-2">
                <label for="date" class="form-label">Date</label>
                <input type="date" name="date" id="date" class="form-control form-control-sm" value="{{ selected_date }}">
            </div>
            <div class="col-md-2">
                <label for="fund" class="form-label">Fund</label>
                <select name="fund" id="fund" class="form-select form-select-sm">
                    {% for fund in available_funds %}
                        <option value="{{ fund }}" {% if fund == selected_fund %}selected{% endif %}>{{ fund }}</option>
                    {% endfor %}
                </select>
            </div>
            <div class="col-md-2">
                <label for="type" class="form-label">Type</label>
                <select name="type" id="type" class="form-select form-select-sm">
                    <option value="">All</option>
                    {% for t in available_types %}
                        <option value="{{ t }}" {% if t == selected_type %}selected{% endif %}>{{ t }}</option>
                    {% endfor %}
                </select>
            </div>
            <div class="col-md-2">
                <label class="form-label">Benchmark/Portfolio</label>
                <div class="form-check form-switch">
                    <input class="form-check-input" type="checkbox" id="bench_or_port" name="bench_or_port" value="port" {% if bench_or_port == 'port' %}checked{% endif %} onchange="this.value = this.checked ? 'port' : 'bench'; this.form.submit();">
                    <label class="form-check-label" for="bench_or_port">Portfolio</label>
                </div>
            </div>
            <div class="col-md-2">
                <label class="form-label">Month-to-Date</label>
                <div class="form-check form-switch">
                    <input class="form-check-input" type="checkbox" id="mtd" name="mtd" value="on" {% if mtd %}checked{% endif %}>
                    <label class="form-check-label" for="mtd">MTD</label>
                </div>
            </div>
            <div class="col-md-2">
                <label class="form-label">Normalize</label>
                <div class="form-check form-switch">
                    <input class="form-check-input" type="checkbox" id="normalize" name="normalize" value="on" {% if normalize %}checked{% endif %}>
                    <label class="form-check-label" for="normalize">Normalize</label>
                </div>
            </div>
            <div class="col-md-auto">
                <button class="btn btn-primary btn-sm" type="submit">Apply Filters</button>
            </div>
        </div>
    </form>
    {# --- Data Table Section --- #}
    {% if rows %}
    <div class="table-responsive">
        <table class="table table-striped table-hover table-sm small caption-top" id="attribution-table">
            {% if pagination %}
            <caption class="pb-1">
                Displaying {{ rows|length }} of {{ pagination.total_items }} total securities. (Page {{ pagination.page }} of {{ pagination.total_pages }})
            </caption>
            {% endif %}
            <thead class="table-light sticky-top">
                <tr>
                    <th>Security Name</th>
                    <th>ISIN</th>
                    <th>Type</th>
                    <th>Returns (L0 Total)</th>
                    <th>Original Residual</th>
                    <th>S&amp;P Residual</th>
                    <th>Residual Diff</th>
                    <th>L1 Rates (Orig)</th>
                    <th>L1 Rates (S&amp;P)</th>
                    <th>L1 Credit (Orig)</th>
                    <th>L1 Credit (S&amp;P)</th>
                    <th>L1 FX (Orig)</th>
                    <th>L1 FX (S&amp;P)</th>
                </tr>
            </thead>
            <tbody>
                {% for row in rows %}
                <tr>
                    <td>
                        <a href="{{ url_for('security.security_details', metric_name='Attribution', security_id=row['ISIN']|urlencode) }}">
                            {{ row['Security Name'] }}
                        </a>
                    </td>
                    <td>{{ row['ISIN'] }}</td>
                    <td>{{ row['Type'] }}</td>
                    <td>{{ row['Returns']|round(3) }}</td>
                    <td>{{ row['Original Residual']|round(3) }}</td>
                    <td>{{ row["S&P Residual"]|round(3) }}</td>
                    <td>{{ row['Residual Diff']|round(3) }}</td>
                    <td>{{ row['L1 Values']['Rates'][0]|round(3) }}</td>
                    <td>{{ row['L1 Values']['Rates'][1]|round(3) }}</td>
                    <td>{{ row['L1 Values']['Credit'][0]|round(3) }}</td>
                    <td>{{ row['L1 Values']['Credit'][1]|round(3) }}</td>
                    <td>{{ row['L1 Values']['FX'][0]|round(3) }}</td>
                    <td>{{ row['L1 Values']['FX'][1]|round(3) }}</td>
                </tr>
                {% endfor %}
            </tbody>
        </table>
    </div>
    {% if pagination and pagination.total_pages > 1 %}
        <nav aria-label="Attribution security data navigation">
            <ul class="pagination pagination-sm justify-content-center">
                <li class="page-item {{ 'disabled' if not pagination.has_prev }}">
                    <a class="page-link" href="{{ pagination.url_for_page(pagination.prev_num) if pagination.has_prev else '#' }}" aria-label="Previous">
                        <span aria-hidden="true">&laquo;</span>
                    </a>
                </li>
                {% set start_page = pagination.start_page_display %}
                {% set end_page = pagination.end_page_display %}
                {% if start_page > 1 %}
                    <li class="page-item"><a class="page-link" href="{{ pagination.url_for_page(1) }}">1</a></li>
                    {% if start_page > 2 %}
                        <li class="page-item disabled"><span class="page-link">...</span></li>
                    {% endif %}
                {% endif %}
                {% for p in range(start_page, end_page + 1) %}
                    <li class="page-item {{ 'active' if p == pagination.page }}">
                        <a class="page-link" href="{{ pagination.url_for_page(p) }}">{{ p }}</a>
                    </li>
                {% endfor %}
                {% if end_page < pagination.total_pages %}
                    {% if end_page < pagination.total_pages - 1 %}
                        <li class="page-item disabled"><span class="page-link">...</span></li>
                    {% endif %}
                    <li class="page-item"><a class="page-link" href="{{ pagination.url_for_page(pagination.total_pages) }}">{{ pagination.total_pages }}</a></li>
                {% endif %}
                <li class="page-item {{ 'disabled' if not pagination.has_next }}">
                    <a class="page-link" href="{{ pagination.url_for_page(pagination.next_num) if pagination.has_next else '#' }}" aria-label="Next">
                        <span aria-hidden="true">&raquo;</span>
                    </a>
                </li>
            </ul>
        </nav>
    {% endif %}
    {% elif not rows %}
        <div class="alert alert-info mt-3" role="alert">
            No attribution data is currently available or matches the selected criteria.
        </div>
    {% endif %}
</div>
{% endblock %}
</file>

<file path="templates/attribution_summary.html">
{#
    Purpose: Attribution summary page. Shows two tables of residuals by fund and date for:
    - Benchmark (Prod, S&P)
    - Portfolio (Prod, S&P)
#}
{% extends "base.html" %}
{% block title %}Attribution Residuals Summary{% endblock %}
{% block content %}
<div class="container-fluid mt-4">
    <h1>Attribution Residuals Summary</h1>
    <p class="text-muted">Sum of residuals by fund and date. Residual = L0 Total - (L1 Rates + L1 Credit + L1 FX). Perfect attribution: residual = 0.</p>
    <!-- Level Toggle -->
    <form method="get" class="mb-3" id="level-toggle-form">
        <input type="hidden" name="fund" value="{{ selected_fund }}">
        <input type="hidden" name="characteristic" value="{{ selected_characteristic }}">
        <input type="hidden" name="start_date" value="{{ start_date.strftime('%Y-%m-%d') }}">
        <input type="hidden" name="end_date" value="{{ end_date.strftime('%Y-%m-%d') }}">
        <div class="btn-group" role="group" aria-label="Level toggle">
            <input type="radio" class="btn-check" name="level" id="level-l0" value="L0" autocomplete="off" {% if selected_level == 'L0' %}checked{% endif %} onchange="this.form.submit()">
            <label class="btn btn-outline-primary" for="level-l0">L0</label>
            <input type="radio" class="btn-check" name="level" id="level-l1" value="L1" autocomplete="off" {% if selected_level == 'L1' %}checked{% endif %} onchange="this.form.submit()">
            <label class="btn btn-outline-primary" for="level-l1">L1</label>
            <input type="radio" class="btn-check" name="level" id="level-l2" value="L2" autocomplete="off" {% if selected_level == 'L2' %}checked{% endif %} onchange="this.form.submit()">
            <label class="btn btn-outline-primary" for="level-l2">L2</label>
        </div>
    </form>
    <!-- Filter Form -->
    <form method="get" class="row g-3 align-items-end mb-4" id="filter-form">
        <div class="col-md-3">
            <label for="fund-select" class="form-label">Fund</label>
            <select class="form-select" id="fund-select" name="fund" onchange="this.form.submit()">
                <option value="" {% if not selected_fund %}selected{% endif %}>All Funds</option>
                {% for fund in available_funds %}
                    <option value="{{ fund }}" {% if fund == selected_fund %}selected{% endif %}>{{ fund }}</option>
                {% endfor %}
            </select>
        </div>
        <div class="col-md-3">
            <label for="characteristic-select" class="form-label">Group by Characteristic</label>
            <select class="form-select" id="characteristic-select" name="characteristic" onchange="this.form.submit()">
                {% for char in available_characteristics %}
                    <option value="{{ char }}" {% if char == selected_characteristic %}selected{% endif %}>{{ char }}</option>
                {% endfor %}
            </select>
        </div>
        <div class="col-md-3">
            <label for="characteristic-value-select" class="form-label">Filter by {{ selected_characteristic }}</label>
            <select class="form-select" id="characteristic-value-select" name="characteristic_value" onchange="this.form.submit()">
                <option value="" {% if not selected_characteristic_value %}selected{% endif %}>All</option>
                {% for val in available_characteristic_values %}
                    <option value="{{ val }}" {% if val == selected_characteristic_value %}selected{% endif %}>{{ val }}</option>
                {% endfor %}
            </select>
        </div>
        <div class="col-md-6">
            <label for="date-range-slider" class="form-label">Date Range</label>
            <div>
                <input type="range" class="form-range" id="date-range-slider" min="0" max="{{ (max_date - min_date).days }}" value="0" step="1">
                <input type="range" class="form-range" id="date-range-slider-end" min="0" max="{{ (max_date - min_date).days }}" value="{{ (end_date - min_date).days }}" step="1">
                <div class="d-flex justify-content-between">
                    <span id="start-date-label">{{ start_date.strftime('%Y-%m-%d') }}</span>
                    <span id="end-date-label">{{ end_date.strftime('%Y-%m-%d') }}</span>
                </div>
                <input type="hidden" name="start_date" id="start-date-input" value="{{ start_date.strftime('%Y-%m-%d') }}">
                <input type="hidden" name="end_date" id="end-date-input" value="{{ end_date.strftime('%Y-%m-%d') }}">
                <input type="hidden" name="level" value="{{ selected_level }}">
            </div>
        </div>
        <div class="col-md-3">
            <button type="submit" class="btn btn-primary">Apply Filters</button>
        </div>
    </form>
    <div class="mb-3">
        <strong>Current Filters:</strong>
        <span>Fund: {{ selected_fund if selected_fund else 'All' }}</span> |
        <span>Date: {{ start_date.strftime('%Y-%m-%d') }} to {{ end_date.strftime('%Y-%m-%d') }}</span> |
        <span>Characteristic: {{ selected_characteristic }}</span> |
        <span>Level: {{ selected_level }}</span>
    </div>
    <div class="row">
        <div class="col-md-12">
            <h3>Benchmark</h3>
            <div class="table-responsive">
                <table class="table table-striped table-bordered table-sm">
                    <thead>
                        <tr>
                            <th rowspan="2">Date</th>
                            <th rowspan="2">Fund</th>
                            {% if selected_characteristic %}<th rowspan="2">{{ selected_characteristic }}</th>{% endif %}
                            {% if selected_level == 'L0' or selected_level == None %}
                                <th colspan="2">Residual</th>
                                <th colspan="2">Abs Residual</th>
                            {% elif selected_level == 'L1' %}
                                <th colspan="2">L1 Rates</th>
                                <th colspan="2">L1 Credit</th>
                                <th colspan="2">L1 FX</th>
                            {% elif selected_level == 'L2' %}
                                <th colspan="2">Credit Spread Change</th>
                                <th colspan="2">Credit Convexity</th>
                                <th colspan="2">Credit Carry</th>
                                <th colspan="2">Credit Defaulted</th>
                                <th colspan="2">Rates Carry</th>
                                <th colspan="2">Rates Convexity</th>
                                <th colspan="2">Rates Curve</th>
                                <th colspan="2">Rates Duration</th>
                                <th colspan="2">Rates Roll</th>
                                <th colspan="2">FX Carry</th>
                                <th colspan="2">FX Change</th>
                            {% endif %}
                        </tr>
                        <tr>
                            {% if selected_level == 'L0' or selected_level == None %}
                                <th>Prod</th><th>S&amp;P</th>
                                <th>Prod</th><th>S&amp;P</th>
                            {% elif selected_level == 'L1' %}
                                <th>Prod</th><th>S&amp;P</th>
                                <th>Prod</th><th>S&amp;P</th>
                                <th>Prod</th><th>S&amp;P</th>
                            {% elif selected_level == 'L2' %}
                                <th>Prod</th><th>S&amp;P</th>
                                <th>Prod</th><th>S&amp;P</th>
                                <th>Prod</th><th>S&amp;P</th>
                                <th>Prod</th><th>S&amp;P</th>
                                <th>Prod</th><th>S&amp;P</th>
                                <th>Prod</th><th>S&amp;P</th>
                                <th>Prod</th><th>S&amp;P</th>
                                <th>Prod</th><th>S&amp;P</th>
                                <th>Prod</th><th>S&amp;P</th>
                                <th>Prod</th><th>S&amp;P</th>
                                <th>Prod</th><th>S&amp;P</th>
                            {% endif %}
                        </tr>
                    </thead>
                    <tbody>
                        {% for row in benchmark_results %}
                        <tr>
                            <td>{{ row.Date.strftime('%Y-%m-%d') if row.Date else '' }}</td>
                            <td>{{ row.Fund }}</td>
                            {% if selected_characteristic %}<td>{{ row[selected_characteristic] }}</td>{% endif %}
                            {% if selected_level == 'L0' or selected_level == None %}
                                <td>{{ '%.6f'|format(row.Residual_Prod) }}</td>
                                <td>{{ '%.6f'|format(row.Residual_SP) }}</td>
                                <td>{{ '%.6f'|format(row.AbsResidual_Prod) }}</td>
                                <td>{{ '%.6f'|format(row.AbsResidual_SP) }}</td>
                            {% elif selected_level == 'L1' %}
                                <td>{{ '%.6f'|format(row.L1Rates_Prod) }}</td>
                                <td>{{ '%.6f'|format(row.L1Rates_SP) }}</td>
                                <td>{{ '%.6f'|format(row.L1Credit_Prod) }}</td>
                                <td>{{ '%.6f'|format(row.L1Credit_SP) }}</td>
                                <td>{{ '%.6f'|format(row.L1FX_Prod) }}</td>
                                <td>{{ '%.6f'|format(row.L1FX_SP) }}</td>
                            {% elif selected_level == 'L2' %}
                                {% for key in row.L2ProdKeys %}
                                    <td>{{ '%.6f'|format(row.L2Prod[key]) }}</td>
                                    <td>{{ '%.6f'|format(row.L2SP[key]) }}</td>
                                {% endfor %}
                            {% endif %}
                        </tr>
                        {% endfor %}
                    </tbody>
                    <tfoot>
                        {% if (selected_level == 'L0' or selected_level == None) and benchmark_results %}
                        <tr class="table-secondary fw-bold">
                            <td colspan="{% if selected_characteristic %}3{% else %}2{% endif %}">Total</td>
                            <td>{{ '%.6f'|format(benchmark_results|sum(attribute='Residual_Prod')) }}</td>
                            <td>{{ '%.6f'|format(benchmark_results|sum(attribute='Residual_SP')) }}</td>
                            <td>{{ '%.6f'|format(benchmark_results|sum(attribute='AbsResidual_Prod')) }}</td>
                            <td>{{ '%.6f'|format(benchmark_results|sum(attribute='AbsResidual_SP')) }}</td>
                        </tr>
                        {% elif selected_level == 'L1' and benchmark_results %}
                        <tr class="table-secondary fw-bold">
                            <td colspan="{% if selected_characteristic %}3{% else %}2{% endif %}">Total</td>
                            <td>{{ '%.6f'|format(benchmark_results|sum(attribute='L1Rates_Prod')) }}</td>
                            <td>{{ '%.6f'|format(benchmark_results|sum(attribute='L1Rates_SP')) }}</td>
                            <td>{{ '%.6f'|format(benchmark_results|sum(attribute='L1Credit_Prod')) }}</td>
                            <td>{{ '%.6f'|format(benchmark_results|sum(attribute='L1Credit_SP')) }}</td>
                            <td>{{ '%.6f'|format(benchmark_results|sum(attribute='L1FX_Prod')) }}</td>
                            <td>{{ '%.6f'|format(benchmark_results|sum(attribute='L1FX_SP')) }}</td>
                        </tr>
                        {% elif selected_level == 'L2' and benchmark_results %}
                        <tr class="table-secondary fw-bold">
                            <td colspan="{% if selected_characteristic %}3{% else %}2{% endif %}">Total</td>
                            {% for key in benchmark_results[0].L2ProdKeys %}
                                <td>{{ '%.6f'|format(benchmark_results|map(attribute='L2Prod')|map(attribute=key)|sum) }}</td>
                                <td>{{ '%.6f'|format(benchmark_results|map(attribute='L2SP')|map(attribute=key)|sum) }}</td>
                            {% endfor %}
                        </tr>
                        {% endif %}
                    </tfoot>
                </table>
            </div>
        </div>
    </div>
    <div class="row mt-4">
        <div class="col-md-12">
            <h3>Portfolio</h3>
            <div class="table-responsive">
                <table class="table table-striped table-bordered table-sm">
                    <thead>
                        <tr>
                            <th rowspan="2">Date</th>
                            <th rowspan="2">Fund</th>
                            {% if selected_characteristic %}<th rowspan="2">{{ selected_characteristic }}</th>{% endif %}
                            {% if selected_level == 'L0' or selected_level == None %}
                                <th colspan="2">Residual</th>
                                <th colspan="2">Abs Residual</th>
                            {% elif selected_level == 'L1' %}
                                <th colspan="2">L1 Rates</th>
                                <th colspan="2">L1 Credit</th>
                                <th colspan="2">L1 FX</th>
                            {% elif selected_level == 'L2' %}
                                <th colspan="2">Credit Spread Change</th>
                                <th colspan="2">Credit Convexity</th>
                                <th colspan="2">Credit Carry</th>
                                <th colspan="2">Credit Defaulted</th>
                                <th colspan="2">Rates Carry</th>
                                <th colspan="2">Rates Convexity</th>
                                <th colspan="2">Rates Curve</th>
                                <th colspan="2">Rates Duration</th>
                                <th colspan="2">Rates Roll</th>
                                <th colspan="2">FX Carry</th>
                                <th colspan="2">FX Change</th>
                            {% endif %}
                        </tr>
                        <tr>
                            {% if selected_level == 'L0' or selected_level == None %}
                                <th>Prod</th><th>S&amp;P</th>
                                <th>Prod</th><th>S&amp;P</th>
                            {% elif selected_level == 'L1' %}
                                <th>Prod</th><th>S&amp;P</th>
                                <th>Prod</th><th>S&amp;P</th>
                                <th>Prod</th><th>S&amp;P</th>
                            {% elif selected_level == 'L2' %}
                                <th>Prod</th><th>S&amp;P</th>
                                <th>Prod</th><th>S&amp;P</th>
                                <th>Prod</th><th>S&amp;P</th>
                                <th>Prod</th><th>S&amp;P</th>
                                <th>Prod</th><th>S&amp;P</th>
                                <th>Prod</th><th>S&amp;P</th>
                                <th>Prod</th><th>S&amp;P</th>
                                <th>Prod</th><th>S&amp;P</th>
                                <th>Prod</th><th>S&amp;P</th>
                                <th>Prod</th><th>S&amp;P</th>
                                <th>Prod</th><th>S&amp;P</th>
                            {% endif %}
                        </tr>
                    </thead>
                    <tbody>
                        {% for row in portfolio_results %}
                        <tr>
                            <td>{{ row.Date.strftime('%Y-%m-%d') if row.Date else '' }}</td>
                            <td>{{ row.Fund }}</td>
                            {% if selected_characteristic %}<td>{{ row[selected_characteristic] }}</td>{% endif %}
                            {% if selected_level == 'L0' or selected_level == None %}
                                <td>{{ '%.6f'|format(row.Residual_Prod) }}</td>
                                <td>{{ '%.6f'|format(row.Residual_SP) }}</td>
                                <td>{{ '%.6f'|format(row.AbsResidual_Prod) }}</td>
                                <td>{{ '%.6f'|format(row.AbsResidual_SP) }}</td>
                            {% elif selected_level == 'L1' %}
                                <td>{{ '%.6f'|format(row.L1Rates_Prod) }}</td>
                                <td>{{ '%.6f'|format(row.L1Rates_SP) }}</td>
                                <td>{{ '%.6f'|format(row.L1Credit_Prod) }}</td>
                                <td>{{ '%.6f'|format(row.L1Credit_SP) }}</td>
                                <td>{{ '%.6f'|format(row.L1FX_Prod) }}</td>
                                <td>{{ '%.6f'|format(row.L1FX_SP) }}</td>
                            {% elif selected_level == 'L2' %}
                                {% for key in row.L2ProdKeys %}
                                    <td>{{ '%.6f'|format(row.L2Prod[key]) }}</td>
                                    <td>{{ '%.6f'|format(row.L2SP[key]) }}</td>
                                {% endfor %}
                            {% endif %}
                        </tr>
                        {% endfor %}
                    </tbody>
                    <tfoot>
                        {% if (selected_level == 'L0' or selected_level == None) and portfolio_results %}
                        <tr class="table-secondary fw-bold">
                            <td colspan="{% if selected_characteristic %}3{% else %}2{% endif %}">Total</td>
                            <td>{{ '%.6f'|format(portfolio_results|sum(attribute='Residual_Prod')) }}</td>
                            <td>{{ '%.6f'|format(portfolio_results|sum(attribute='Residual_SP')) }}</td>
                            <td>{{ '%.6f'|format(portfolio_results|sum(attribute='AbsResidual_Prod')) }}</td>
                            <td>{{ '%.6f'|format(portfolio_results|sum(attribute='AbsResidual_SP')) }}</td>
                        </tr>
                        {% elif selected_level == 'L1' and portfolio_results %}
                        <tr class="table-secondary fw-bold">
                            <td colspan="{% if selected_characteristic %}3{% else %}2{% endif %}">Total</td>
                            <td>{{ '%.6f'|format(portfolio_results|sum(attribute='L1Rates_Prod')) }}</td>
                            <td>{{ '%.6f'|format(portfolio_results|sum(attribute='L1Rates_SP')) }}</td>
                            <td>{{ '%.6f'|format(portfolio_results|sum(attribute='L1Credit_Prod')) }}</td>
                            <td>{{ '%.6f'|format(portfolio_results|sum(attribute='L1Credit_SP')) }}</td>
                            <td>{{ '%.6f'|format(portfolio_results|sum(attribute='L1FX_Prod')) }}</td>
                            <td>{{ '%.6f'|format(portfolio_results|sum(attribute='L1FX_SP')) }}</td>
                        </tr>
                        {% elif selected_level == 'L2' and portfolio_results %}
                        <tr class="table-secondary fw-bold">
                            <td colspan="{% if selected_characteristic %}3{% else %}2{% endif %}">Total</td>
                            {% for key in portfolio_results[0].L2ProdKeys %}
                                <td>{{ '%.6f'|format(portfolio_results|map(attribute='L2Prod')|map(attribute=key)|sum) }}</td>
                                <td>{{ '%.6f'|format(portfolio_results|map(attribute='L2SP')|map(attribute=key)|sum) }}</td>
                            {% endfor %}
                        </tr>
                        {% endif %}
                    </tfoot>
                </table>
            </div>
        </div>
    </div>
</div>
{% endblock %}
{% block scripts %}
{{ super() }}
<script>
// Date slider logic
(function() {
    const minDate = new Date("{{ min_date.strftime('%Y-%m-%d') }}");
    const maxDate = new Date("{{ max_date.strftime('%Y-%m-%d') }}");
    const startDateInput = document.getElementById('start-date-input');
    const endDateInput = document.getElementById('end-date-input');
    const startDateLabel = document.getElementById('start-date-label');
    const endDateLabel = document.getElementById('end-date-label');
    const sliderStart = document.getElementById('date-range-slider');
    const sliderEnd = document.getElementById('date-range-slider-end');
    function daysBetween(d1, d2) {
        return Math.round((d2 - d1) / (1000 * 60 * 60 * 24));
    }
    function addDays(date, days) {
        const d = new Date(date);
        d.setDate(d.getDate() + days);
        return d;
    }
    function formatDate(date) {
        return date.toISOString().slice(0, 10);
    }
    function updateLabelsAndInputs() {
        let startDays = parseInt(sliderStart.value, 10);
        let endDays = parseInt(sliderEnd.value, 10);
        if (endDays < startDays) {
            // Swap if needed
            [startDays, endDays] = [endDays, startDays];
            sliderStart.value = startDays;
            sliderEnd.value = endDays;
        }
        const start = addDays(minDate, startDays);
        const end = addDays(minDate, endDays);
        startDateLabel.textContent = formatDate(start);
        endDateLabel.textContent = formatDate(end);
        startDateInput.value = formatDate(start);
        endDateInput.value = formatDate(end);
    }
    sliderStart.addEventListener('input', updateLabelsAndInputs);
    sliderEnd.addEventListener('input', updateLabelsAndInputs);
    // Set initial slider positions based on current filter
    sliderStart.value = daysBetween(minDate, new Date("{{ start_date.strftime('%Y-%m-%d') }}"));
    sliderEnd.value = daysBetween(minDate, new Date("{{ end_date.strftime('%Y-%m-%d') }}"));
    updateLabelsAndInputs();
})();
</script>
{% endblock %}
</file>

<file path="templates/base.html">
<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>{% block title %}Data Checker{% endblock %}</title>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/css/bootstrap.min.css" rel="stylesheet">
    <style>
        /* Basic styles - can be expanded */
        body { padding-top: 6rem; } /* Increased top padding */
        .sticky-top {
            top: 56px; /* Adjust based on navbar height */
        }
        /* Add any custom global styles here */
        .table-danger {
            background-color: #f8d7da !important; /* Red for high Z */
        }
        .table-warning {
            background-color: #fff3cd !important; /* Yellow for medium Z */
        }
        /* Give chart canvases a default aspect ratio */
        .chart-canvas {
            aspect-ratio: 16 / 9; /* Default widescreen aspect ratio */
            width: 100%; /* Ensure it fills container width */
            max-width: 100%; /* Prevent overflow */
            min-height: 250px; /* Optional: Ensure a minimum height */
        }
        /* Navbar brand adjustments */
        .navbar-brand {
            display: flex; /* Use flexbox for alignment */
            align-items: center; /* Vertically center items */
            font-size: 1.5rem; /* Increase font size */
        }
        .navbar-brand img {
            height: 50px; /* Reduced logo height */
            margin-right: 0.5rem; /* Space between logo and text */
        }
    </style>
</head>
<body>
    <nav class="navbar navbar-expand-md navbar-dark bg-dark fixed-top">
        <div class="container-fluid">
            <a class="navbar-brand" href="{{ url_for('main.index') }}">
                <img src="{{ url_for('static', filename='images/bang.jpg') }}" alt="Logo">
                Data Checker
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarCollapse">
                <ul class="navbar-nav me-auto mb-2 mb-md-0">
                    <li class="nav-item">
                        <a class="nav-link" href="{{ url_for('main.index') }}">Time Series Dashboard</a>
                    </li>
                    <li class="nav-item dropdown">
                        <a class="nav-link dropdown-toggle" href="#" id="navbarDropdownChecks" role="button" data-bs-toggle="dropdown" aria-expanded="false">
                            Checks & Comparisons
                        </a>
                        <ul class="dropdown-menu" aria-labelledby="navbarDropdownChecks">
                            <li><a class="dropdown-item" href="{{ url_for('security.securities_page') }}">Securities Check</a></li>
                            <li><a class="dropdown-item" href="{{ url_for('weight.weight_check') }}">Weight Check</a></li>
                            <li><a class="dropdown-item" href="{{ url_for('curve_bp.curve_summary') }}">Yield Curve Check</a></li>
                            <li><a class="dropdown-item" href="{{ url_for('attribution_bp.attribution_summary') }}">Attribution Residuals</a></li>
                            <li><a class="dropdown-item" href="{{ url_for('attribution_bp.attribution_security_page') }}">Attribution Security-Level</a></li>
                            <li><a class="dropdown-item" href="{{ url_for('attribution_bp.attribution_radar') }}">Attribution Radar</a></li>
                            <li><hr class="dropdown-divider"></li>
                            <li><h6 class="dropdown-header">Comparisons</h6></li>
                            <li><a class="dropdown-item" href="{{ url_for('comparison_bp.summary') }}">Spread vs SpreadSP</a></li>
                            <li><a class="dropdown-item" href="{{ url_for('duration_comparison_bp.summary') }}">Duration vs DurationSP</a></li>
                            <li><a class="dropdown-item" href="{{ url_for('spread_duration_comparison_bp.summary') }}">Spread Duration vs SP</a></li>
                        </ul>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="{{ url_for('exclusion_bp.manage_exclusions') }}">Exclusions</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="{{ url_for('issue_bp.manage_issues') }}">Track Issues</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="{{ url_for('api_bp.get_data_page') }}">Data Management</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="{{ url_for('attribution_bp.attribution_charts') }}">Attribution Residuals Charts</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>
    <main role="main" class="container-fluid">
        {% block content %}
        {# Page specific content will go here #}
        {% endblock %}
    </main>
    <!-- Bootstrap Bundle with Popper -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/js/bootstrap.bundle.min.js"></script>
    <!-- Load Chart.js Library -->
    <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
    <!-- Load Date Adapter (e.g., date-fns) - MUST be after Chart.js -->
    <script src="https://cdn.jsdelivr.net/npm/chartjs-adapter-date-fns@3.0.0/dist/chartjs-adapter-date-fns.bundle.min.js"></script>
    <!-- Load Main Application JS (after libraries are loaded) -->
    <script type="module" src="{{ url_for('static', filename='js/main.js') }}"></script>
    {% block scripts %}
    {# Page specific scripts can go here #}
    {% endblock %}
</body>
</html>
</file>

<file path="templates/comparison_details_page.html">
{% extends "base.html" %}
{% block title %}Spread Comparison Details: {{ security_name }}{% endblock %}
{% block content %}
<div class="container mt-4">
    <nav aria-label="breadcrumb">
        <ol class="breadcrumb">
            <li class="breadcrumb-item"><a href="{{ url_for('comparison_bp.summary') }}">Comparison Summary</a></li>
            <li class="breadcrumb-item active" aria-current="page">{{ security_name }} ({{ security_id }})</li>
        </ol>
    </nav>
    <h1>Spread Comparison Details: {{ security_name }}</h1>
    <h5 class="text-muted">Security ID: {{ security_id }}</h5>
    <div class="row mt-4 mb-4">
        <div class="col-md-6">
            <h2>Comparison Statistics</h2>
            <ul class="list-group">
                <li class="list-group-item d-flex justify-content-between align-items-center">
                    Level Correlation
                    <span class="badge bg-primary rounded-pill">{{ "%.4f"|format(stats.Level_Correlation) if stats.Level_Correlation is not none else 'N/A' }}</span>
                </li>
                <li class="list-group-item d-flex justify-content-between align-items-center">
                    Change Correlation
                    <span class="badge bg-primary rounded-pill">{{ "%.4f"|format(stats.Change_Correlation) if stats.Change_Correlation is not none else 'N/A' }}</span>
                </li>
                <li class="list-group-item d-flex justify-content-between align-items-center">
                    Mean Absolute Difference
                    <span class="badge bg-secondary rounded-pill">{{ "%.2f"|format(stats.Mean_Abs_Diff) if stats.Mean_Abs_Diff is not none else 'N/A' }}</span>
                </li>
                <li class="list-group-item d-flex justify-content-between align-items-center">
                    Max Absolute Difference
                    <span class="badge bg-secondary rounded-pill">{{ "%.2f"|format(stats.Max_Abs_Diff) if stats.Max_Abs_Diff is not none else 'N/A' }}</span>
                </li>
                 <li class="list-group-item d-flex justify-content-between align-items-center">
                    Data Points (Original)
                    <span class="badge bg-info rounded-pill">{{ stats.Total_Points - stats.NaN_Count_Orig }} / {{ stats.Total_Points }}</span>
                </li>
                 <li class="list-group-item d-flex justify-content-between align-items-center">
                    Data Points (New)
                    <span class="badge bg-info rounded-pill">{{ stats.Total_Points - stats.NaN_Count_New }} / {{ stats.Total_Points }}</span>
                </li>
                 <li class="list-group-item d-flex justify-content-between align-items-center">
                    Same Date Range?
                    <span class="badge {{ 'bg-success' if stats.Same_Date_Range else 'bg-warning' }} rounded-pill">{{ 'Yes' if stats.Same_Date_Range else 'No' }}</span>
                </li>
            </ul>
        </div>
        {# Placeholder for additional stats or info if needed #}
        {# <div class="col-md-6">
             <h2>Other Info</h2>
        </div> #}
    </div>
    <h2>Time Series Comparison</h2>
    <p class="text-muted">Overlayed credit spreads from Original (sec_spread) and New (sec_spreadSP) datasets.</p>
    <div>
        <canvas id="comparisonChart"></canvas>
    </div>
    {# Embed chart data as JSON for JavaScript #}
    <script type="application/json" id="comparisonChartData">
        {{ chart_data | tojson | safe }}
    </script>
</div>
{% endblock %}
{% block scripts %}
{{ super() }}
{# We need Chart.js - ensure it's included in base.html or here #}
<script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
{# We also need the JS to render this specific chart #}
<script>
    document.addEventListener('DOMContentLoaded', function() {
        const chartDataElement = document.getElementById('comparisonChartData');
        const comparisonChartCanvas = document.getElementById('comparisonChart');
        if (chartDataElement && comparisonChartCanvas) {
            try {
                const chartData = JSON.parse(chartDataElement.textContent);
                const ctx = comparisonChartCanvas.getContext('2d');
                // --- REVERTING TO SIMPLE CONFIG --- 
                // Remove scriptable options for now
                /*
                // Define base colors (assuming these are the intended original colors)
                const baseColors = [
                    'rgba(13, 110, 253, 1)', // Bootstrap Blue (or COLOR_PALETTE[0])
                    'rgba(220, 53, 69, 1)'  // Bootstrap Red (or COLOR_PALETTE[1])
                ];
                const gapColor = 'rgba(150, 150, 150, 0.7)'; // Semi-transparent gray for gaps
                // Prepare datasets with scriptable borderColor
                console.log("Preparing datasets with scriptable colors...");
                const datasetsWithScriptableColors = chartData.datasets.map((dataset, index) => {
                     console.log(`Mapping dataset index: ${index}`);
                     return {
                        ...dataset, // Keep original label, data, tension, spanGaps etc.
                        borderColor: context => {
                            // Check if drawing a line segment and context is valid
                            if (context.type === 'segment' && context.p0 && context.p1) {
                                // --- NEW APPROACH: Check the 'skip' property of the points (safely) --- 
                                // Ensure p0 and p1 exist before accessing skip
                                const p0skip = context.p0 ? context.p0.skip : false;
                                const p1skip = context.p1 ? context.p1.skip : false;
                                if (p0skip || p1skip) {
                                     // --- DEBUG LOGGING START ---
                                    // Log when gap is detected using the 'skip' property
                                    console.log(`>>> Gap DETECTED (via skip): p0.skip=${p0skip}, p1.skip=${p1skip}, datasetIndex=${context.datasetIndex}. Applying gapColor.`);
                                    // --- DEBUG LOGGING END ---
                                    return gapColor;
                                }
                            }
                            // Default color for non-gap segments, points, legend
                            return baseColors[context.datasetIndex % baseColors.length];
                        },
                        // Ensure point colors match the line start/end unless hovered
                        pointBorderColor: context => baseColors[context.datasetIndex % baseColors.length],
                        pointBackgroundColor: context => baseColors[context.datasetIndex % baseColors.length],
                    }
                });
                console.log("Data prepared. Initializing Chart...");
                */
                // --- END REVERT --- 
                console.log("Initializing Chart with original data..."); // Log before init
                new Chart(ctx, {
                    type: 'line',
                    // Use the ORIGINAL datasets from Python/JSON
                    data: chartData, 
                    options: {
                        responsive: true,
                        maintainAspectRatio: true, // Adjust as needed
                        plugins: {
                            legend: {
                                position: 'top',
                            },
                            title: {
                                display: true,
                                text: 'Spread Comparison: {{ security_name|tojson }}'
                            }
                        },
                        scales: {
                            x: {
                                // Assuming labels are date strings, configure time scale if needed
                                // type: 'time',
                                // time: {
                                //     unit: 'day' // or week, month, etc.
                                // },
                                title: {
                                    display: true,
                                    text: 'Date'
                                }
                            },
                            y: {
                                title: {
                                    display: true,
                                    text: 'Spread'
                                }
                            }
                        },
                        interaction: {
                             intersect: false,
                             mode: 'index',
                        },
                        // Add other options from existing charts for consistency
                    }
                });
            } catch (error) {
                console.error("Error parsing chart data or rendering chart:", error);
                comparisonChartCanvas.parentElement.innerHTML = '<p class="text-danger">Error rendering chart.</p>';
            }
        } else {
             console.warn("Chart data or canvas element not found for comparison chart.");
        }
    });
</script>
{% endblock %}
</file>

<file path="templates/comparison_page.html">
{% extends "base.html" %}
{% block title %}Spread Comparison Summary{% endblock %}
{% block content %}
<div class="container-fluid mt-4"> {# Use container-fluid for wider view #}
    <h1>Spread Comparison: Original (sec_spread) vs. New (sec_spreadSP)</h1>
    <p class="text-muted">Comparing credit spreads between the two datasets. Click on a Security ID/Name to see details. Use filters or click column headers to sort. Pagination applied.</p>
    {# Display message if any #}
    {% if message %}
    <div class="alert alert-warning alert-dismissible fade show" role="alert">
        {{ message }}
        <button type="button" class="btn-close" data-bs-dismiss="alert" aria-label="Close"></button>
    </div>
    {% endif %}
    {# --- Filter Form --- #}
    {% if filter_options %}
    <form method="GET" action="{{ url_for('comparison_bp.summary') }}" class="mb-3 p-3 border rounded bg-light" id="filter-form">
        <h5>Filters</h5>
        <div class="row g-2 align-items-end">
            {% for column, options in filter_options.items() %}
            <div class="col-md-2 mb-2">
                <label for="filter-{{ column }}" class="form-label">{{ column }}</label>
                <select id="filter-{{ column }}" name="filter_{{ column }}" class="form-select form-select-sm">
                    <option value="">All</option>
                    {% for option in options %}
                    <option value="{{ option }}" {% if active_filters.get(column) == option|string %}selected{% endif %}>{{ option }}</option>
                    {% endfor %}
                </select>
            </div>
            {% endfor %}
            <div class="col-md-3 d-flex align-items-end">
                <div class="form-check form-switch mb-1">
                    <input class="form-check-input" type="checkbox" role="switch" id="showSoldToggle" name="show_sold" value="true" {% if show_sold %}checked{% endif %}>
                    <label class="form-check-label" for="showSoldToggle"><small>Show Sold Securities</small></label>
                </div>
            </div>
            <div class="col-md-auto">
                <button type="submit" class="btn btn-primary btn-sm">Apply Filters</button>
                {# Add a clear button only if filters are active #}
                {% if active_filters %}
                <a href="{{ url_for('comparison_bp.summary') }}" class="btn btn-secondary btn-sm">Clear Filters</a>
                {% endif %}
            </div>
        </div>
        {# Hidden fields to preserve current sort order when applying filters - page is implicitly reset #}
        <input type="hidden" name="sort_by" value="{{ current_sort_by }}" id="currentSortBy">
        <input type="hidden" name="sort_order" value="{{ current_sort_order }}" id="currentSortOrder">
    </form>
    {% endif %}
    {# --- Data Table --- #}
    <div class="table-responsive">
        <table class="table table-striped table-hover table-sm caption-top" id="comparison-table">
             {# Add table caption for summary #}
             {% if pagination %}
             <caption class="pb-1">
                 Displaying {{ table_data|length }} of {{ pagination.total_items }} total securities.
                 (Page {{ pagination.page }} of {{ pagination.total_pages }})
             </caption>
             {% endif %}
            <thead class="table-light">
                <tr>
                    {# Loop through the columns passed from the view #}
                    {% for col_name in columns_to_display %}
                        {% set is_sort_col = (col_name == current_sort_by) %}
                        {% set next_sort_order = 'asc' if is_sort_col and current_sort_order == 'desc' else 'desc' %}
                        {# Base arguments, including current filters #}
                        {% set sort_args = request.args.to_dict() %}
                        {% set _ = sort_args.pop('page', None) %}
                        {% set _ = sort_args.update({'sort_by': col_name, 'sort_order': next_sort_order}) %}
                        {# Generate URL for this header #}
                        {% set sort_url = url_for('comparison_bp.summary', **sort_args) %}
                        {# Add classes for styling and JS #}
                        <th class="sortable {{ 'sorted-' + current_sort_order if is_sort_col else '' }}" 
                            data-column-name="{{ col_name }}">
                            <a href="{{ sort_url }}" class="text-decoration-none text-dark">
                                {{ col_name.replace('_', ' ') | title }} 
                                {% if is_sort_col %}
                                    <span class="sort-indicator ms-1">{{ '▲' if current_sort_order == 'asc' else '▼' }}</span>
                                {% endif %}
                            </a>
                        </th>
                    {% endfor %}
                </tr>
            </thead>
            <tbody id="comparison-table-body">
                {% set id_col = id_column_name %}
                {% for row in table_data %}
                <tr>
                    {# Loop through the same columns to ensure order matches header #}
                    {% for col_name in columns_to_display %}
                        <td>
                            {% if col_name == id_col %}
                                <a href="{{ url_for('comparison_bp.comparison_details', security_id=row[id_col]|urlencode) }}">{{ row[id_col] }}</a>
                            {% elif col_name in ['Level_Correlation', 'Change_Correlation'] and row[col_name] is not none %}
                                {{ "%.3f"|format(row[col_name]) }}
                            {% elif col_name in ['Mean_Abs_Diff', 'Max_Abs_Diff'] and row[col_name] is not none %}
                                {{ "%.2f"|format(row[col_name]) }}
                            {% elif col_name == 'Same_Date_Range' %}
                                {{ 'Yes' if row[col_name] else 'No' }}
                            {% elif row[col_name] is number %}
                                {{ row[col_name]|round(3) }} {# General numeric formatting #}
                            {% else %}
                                {{ row[col_name] if row[col_name] is not none else '' }} {# Display strings or empty #}
                            {% endif %}
                        </td>
                    {% endfor %}
                </tr>
                {% else %}
                {# This message is now shown above if filtered_stats is empty #}
                {# <tr> <td colspan="{{ columns_to_display|length }}" class="text-center">No comparison data available matching the current filters.</td> </tr> #}
                {% endfor %}
            </tbody>
        </table>
    </div>
    {# --- Pagination Controls --- #}
    {% if pagination and pagination.total_pages > 1 %}
        <nav aria-label="Comparison data navigation">
            <ul class="pagination pagination-sm justify-content-center">
                {# Previous Page Link #}
                <li class="page-item {{ 'disabled' if not pagination.has_prev }}">
                    <a class="page-link" href="{{ pagination.url_for_page(pagination.prev_num) if pagination.has_prev else '#' }}" aria-label="Previous">&laquo;</a>
                </li>
                {# Page Number Links (using context variables calculated in view) #}
                 {% set start_page = pagination.start_page_display %}
                 {% set end_page = pagination.end_page_display %}
                 {% if start_page > 1 %}
                     <li class="page-item"><a class="page-link" href="{{ pagination.url_for_page(1) }}">1</a></li>
                     {% if start_page > 2 %}
                         <li class="page-item disabled"><span class="page-link">...</span></li>
                     {% endif %}
                 {% endif %}
                 {% for p in range(start_page, end_page + 1) %}
                    <li class="page-item {{ 'active' if p == pagination.page }}">
                        <a class="page-link" href="{{ pagination.url_for_page(p) }}">{{ p }}</a>
                    </li>
                {% endfor %}
                 {% if end_page < pagination.total_pages %}
                     {% if end_page < pagination.total_pages - 1 %}
                         <li class="page-item disabled"><span class="page-link">...</span></li>
                     {% endif %}
                     <li class="page-item"><a class="page-link" href="{{ pagination.url_for_page(pagination.total_pages) }}">{{ pagination.total_pages }}</a></li>
                 {% endif %}
                {# Next Page Link #}
                <li class="page-item {{ 'disabled' if not pagination.has_next }}">
                    <a class="page-link" href="{{ pagination.url_for_page(pagination.next_num) if pagination.has_next else '#' }}" aria-label="Next">&raquo;</a>
                </li>
            </ul>
        </nav>
    {% endif %}
</div>
{% endblock %}
{% block scripts %}
{{ super() }}
<script>
    document.addEventListener('DOMContentLoaded', function() {
        const filterForm = document.getElementById('filter-form');
        const currentSortBy = document.getElementById('currentSortBy');
        const currentSortOrder = document.getElementById('currentSortOrder');
        const showSoldToggle = document.getElementById('showSoldToggle');
        // Function to get current filters from the form
        function getCurrentFilters() {
            const formData = new FormData(filterForm);
            const filters = {};
            for (const [key, value] of formData.entries()) {
                if (key.startsWith('filter_') && value) {
                    filters[key] = value;
                }
            }
            // Include show_sold status explicitly
            filters['show_sold'] = showSoldToggle.checked ? 'true' : 'false';
            return filters;
        }
        // 1. Handle Form Submission (Apply Filters button)
        // The form naturally submits with all values, including the hidden sort fields and the toggle state.
        // 2. Handle Sorting Header Clicks
        document.querySelectorAll('.sortable-header').forEach(header => {
            header.addEventListener('click', function(e) {
                e.preventDefault();
                const sortBy = this.getAttribute('data-sort-by');
                let sortOrder = 'asc';
                if (currentSortBy.value === sortBy && currentSortOrder.value === 'asc') {
                    sortOrder = 'desc';
                }
                currentSortBy.value = sortBy;
                currentSortOrder.value = sortOrder;
                filterForm.submit(); // Submit the form with updated sort fields
            });
        });
        // 3. Handle Show Sold Toggle Change
        showSoldToggle.addEventListener('change', function() {
            // When toggle changes, reset page to 1 and submit form
            const url = new URL(filterForm.action);
            const params = new URLSearchParams(url.search);
            // Get existing filters
            const filters = getCurrentFilters(); // Includes new toggle state
            Object.entries(filters).forEach(([key, value]) => {
                params.set(key, value);
            });
            // Reset page to 1
            params.delete('page');
            // Keep current sort order
            params.set('sort_by', currentSortBy.value);
            params.set('sort_order', currentSortOrder.value);
            window.location.href = `${url.pathname}?${params.toString()}`;
        });
        // 4. Update Pagination Links to include all current filters and sort state
        document.querySelectorAll('.pagination-link').forEach(link => {
            if (!link.parentElement.classList.contains('disabled') && link.getAttribute('href') !== '#') {
                const url = new URL(link.href);
                const params = new URLSearchParams(url.search); // Existing params (includes page number)
                const filters = getCurrentFilters(); // Get current filters including toggle state
                // Add filters to pagination link
                Object.entries(filters).forEach(([key, value]) => {
                    params.set(key, value);
                });
                // Add sorting to pagination link
                params.set('sort_by', currentSortBy.value);
                params.set('sort_order', currentSortOrder.value);
                link.href = `${url.pathname}?${params.toString()}`;
            }
        });
    });
</script>
{% endblock %}
</file>

<file path="templates/curve_details.html">
{# templates/curve_details.html #}
{% extends "base.html" %}
{% block title %}Yield Curve Details - {{ currency }}{% endblock %}
{% block content %}
<div class="container mt-4">
    <div class="d-flex justify-content-between align-items-center mb-3">
        <h1>Yield Curve Details: <strong>{{ currency }}</strong></h1>
        <a href="{{ url_for('curve_bp.curve_summary') }}" class="btn btn-secondary">
            <i class="fas fa-arrow-left me-1"></i> Back to Summary
        </a>
    </div>
    {# Date and History Selection Row #}
    <div class="row mb-3 align-items-end">
        <div class="col-md-4">
            <label for="dateSelector" class="form-label">Select Date:</label>
            <select id="dateSelector" class="form-select">
                {% if available_dates %}
                    {% for date_str in available_dates %}
                        <option value="{{ date_str }}" {% if date_str == selected_date %}selected{% endif %}>
                            {{ date_str }}
                        </option>
                    {% endfor %}
                {% else %}
                    <option value="">No dates available</option>
                {% endif %}
            </select>
        </div>
        <div class="col-md-4">
            <label for="prevDaysSelector" class="form-label">Show Previous Days:</label>
            <select id="prevDaysSelector" class="form-select">
                {# Options for how many previous curves to show #}
                <option value="0" {% if num_prev_days == 0 %}selected{% endif %}>0 (None)</option>
                <option value="1" {% if num_prev_days == 1 %}selected{% endif %}>1</option>
                <option value="3" {% if num_prev_days == 3 %}selected{% endif %}>3</option>
                <option value="5" {% if num_prev_days == 5 %}selected{% endif %}>5</option>
            </select>
        </div>
    </div>
    {# Chart Card #}
    <div class="card shadow-sm mb-4">
        <div class="card-header">
            Yield Curve for {{ selected_date }}{% if num_prev_days > 0 %} (with {{ num_prev_days }} previous day(s)){% endif %}
        </div>
        <div class="card-body">
            {% if chart_data and chart_data.labels and chart_data.datasets %}
                <canvas id="yieldCurveChart" style="height: 750px; width: 100%;"></canvas>
            {% else %}
                <div class="alert alert-warning" role="alert">
                    No data available to display the chart for {{ currency }} on {{ selected_date }}.
                </div>
            {% endif %}
        </div>
    </div>
    {# Data Table Card for Selected Date #}
    {% if table_data %}
    <div class="card shadow-sm">
        <div class="card-header">
            Data for {{ selected_date }} (Compared to Previous Day)
        </div>
        <div class="card-body">
             <div class="table-responsive">
                <table class="table table-sm table-striped table-hover">
                    <thead>
                        <tr>
                            <th>Term</th>
                            <th>Term (Months, Approx)</th>
                            <th>Value</th>
                            <th>Daily Change</th>
                            <th>Deviation from Avg Shift</th>
                            <th>Deviation Z-Score</th>
                        </tr>
                    </thead>
                    <tbody>
                        {% for row in table_data %}
                            {# Apply conditional highlighting based on Z-score #}
                            {% set z_score = row.DeviationZScore | default(0, true) %}
                            {% set abs_z_score = z_score | abs %}
                            {% set row_class = '' %}
                            {% if abs_z_score > 3 %}
                                {% set row_class = 'table-danger' %}
                            {% elif abs_z_score > 2 %}
                                {% set row_class = 'table-warning' %}
                            {% endif %}
                            <tr class="{{ row_class }}">
                                <td>{{ row.Term }}</td>
                                <td>{{ row.TermMonths }}</td>
                                <td>{{ row.Value_Display | round(4) }}</td>
                                {# Format new columns, handle NaN with default filter #}
                                <td>{{ row.ValueChange | default('N/A', true) | round(4) }}</td>
                                <td>{{ row.ChangeDeviation | default('N/A', true) | round(4) }}</td>
                                <td>{{ z_score | default('N/A', true) | round(2) }}</td>
                            </tr>
                        {% endfor %}
                    </tbody>
                </table>
                <small class="text-muted">Highlighting: Yellow if |Z-Score| > 2, Red if |Z-Score| > 3. Z-Score measures how many standard deviations a term's daily change deviated from the average daily change of the whole curve.</small>
            </div>
        </div>
    </div>
    {% endif %}
</div> {# End container #}
{% endblock %}
{% block scripts %}
{{ super() }} {# Include scripts from base.html #}
<script src="https://cdn.jsdelivr.net/npm/chart.js"></script> {# Include Chart.js #}
<script>
document.addEventListener('DOMContentLoaded', function() {
    const chartData = {{ chart_data | tojson | safe }};
    const ctx = document.getElementById('yieldCurveChart');
    let yieldChart = null; // Reference to the chart instance
    function renderChart() {
        if (yieldChart) {
            yieldChart.destroy(); // Destroy previous chart instance if exists
        }
        if (ctx && chartData && chartData.labels && chartData.labels.length > 0 && chartData.datasets && chartData.datasets.length > 0) {
            console.log("Rendering chart with data:", chartData);
            yieldChart = new Chart(ctx, {
                type: 'line',
                data: chartData, // Now contains multiple datasets
                options: {
                    responsive: true,
                    maintainAspectRatio: false,
                    scales: {
                        x: {
                            title: {
                                display: true,
                                text: 'Term (Months)' // Already updated
                            },
                            type: 'linear',
                            position: 'bottom'
                        },
                        y: {
                            title: {
                                display: true,
                                text: 'Yield Value'
                            },
                            beginAtZero: false
                        }
                    },
                    plugins: {
                        tooltip: {
                            mode: 'index',
                            intersect: false,
                        },
                        title: {
                             display: true,
                             // Base title on the first dataset (selected date)
                             text: chartData.datasets[0].label ?
                                   `Yield Curves - ${chartData.datasets[0].label.split('(')[0].trim()}` :
                                   'Yield Curve'
                        },
                        legend: {
                            position: 'top',
                        }
                    }
                }
            });
        } else {
             console.log("Chart canvas not found or no chart data available.");
        }
    }
    renderChart(); // Initial rendering
    // Update URL and reload page when selectors change
    const dateSelector = document.getElementById('dateSelector');
    const prevDaysSelector = document.getElementById('prevDaysSelector');
    function updateUrlAndReload() {
        const selectedDate = dateSelector.value;
        const selectedPrevDays = prevDaysSelector.value;
        if (selectedDate) {
            const currentUrl = new URL(window.location.href);
            currentUrl.searchParams.set('date', selectedDate);
            currentUrl.searchParams.set('prev_days', selectedPrevDays);
            window.location.href = currentUrl.toString();
        }
    }
    if (dateSelector) {
        dateSelector.addEventListener('change', updateUrlAndReload);
    }
    if (prevDaysSelector) {
        prevDaysSelector.addEventListener('change', updateUrlAndReload);
    }
});
</script>
{% endblock %}
</file>

<file path="templates/curve_summary.html">
{# templates/curve_summary.html #}
{% extends "base.html" %}
{% block title %}Yield Curve Check Summary{% endblock %}
{% block content %}
<div class="container mt-4">
    <h1 class="mb-4">Yield Curve Check Summary</h1>
    <p class="lead">Summary of potential inconsistencies found in the yield curve data for the latest available date: <strong>{{ latest_date }}</strong>.</p>
    <p>Checks include basic monotonicity and comparison of daily change profiles against the previous day.</p>
    {% if summary %}
        <table class="table table-striped table-hover">
            <thead>
                <tr>
                    <th>Currency</th>
                    <th>Status / Issues Found</th>
                    <th>Actions</th>
                </tr>
            </thead>
            <tbody>
                {% for currency, issues in summary.items()|sort %}
                    <tr>
                        <td><strong>{{ currency }}</strong></td>
                        <td>
                            {% if issues == ["OK"] %}
                                <span class="badge bg-success">OK</span>
                            {% elif issues == ["Missing data for comparison"] %}
                                 <span class="badge bg-warning text-dark">Missing Data</span>
                                 <small class="text-muted ms-2">{{ issues[0] }}</small>
                            {% else %}
                                <span class="badge bg-danger">Check Required</span>
                                <ul class="list-unstyled mb-0 mt-1">
                                    {% for issue in issues %}
                                        <li><small><i class="fas fa-exclamation-triangle text-danger me-1"></i>{{ issue }}</small></li>
                                    {% endfor %}
                                </ul>
                            {% endif %}
                        </td>
                        <td>
                            <a href="{{ url_for('curve_bp.curve_details', currency=currency) }}" class="btn btn-sm btn-outline-primary">
                                View Details <i class="fas fa-chart-line ms-1"></i>
                            </a>
                        </td>
                    </tr>
                {% endfor %}
            </tbody>
        </table>
    {% else %}
        <div class="alert alert-warning" role="alert">
            No curve data loaded or no summary could be generated. Please check the data file (`Data/curves.csv`) and application logs.
        </div>
    {% endif %}
</div>
{% endblock %}
</file>

<file path="templates/delete_metric_page.html">
<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <title>{{ metric_name }} Check</title>
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
    <style>
        body { padding-top: 5rem; }
        .chart-container { margin-bottom: 15px; }
        .metrics-table { margin-top: 5px; margin-bottom: 25px; font-size: 0.9em; }
        .metrics-table th, .metrics-table td { padding: 4px 8px; border: 1px solid #dee2e6; }
        .missing-warning { color: red; font-weight: bold; }
        .high-z { background-color: #fff3cd; }
        .very-high-z { background-color: #f8d7da; font-weight: bold; }
    </style>
</head>
<body>
    <nav class="navbar navbar-expand-md navbar-dark bg-dark fixed-top">
        <a class="navbar-brand" href="/">Data Verification</a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarsExampleDefault" aria-controls="navbarsExampleDefault" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="navbarsExampleDefault">
            <ul class="navbar-nav mr-auto">
                <li class="nav-item">
                    <a class="nav-link" href="/">Dashboard</a>
                </li>
            </ul>
        </div>
    </nav>
    <main role="main" class="container">
        <h1>{{ metric_name }} Check</h1>
        <p>Latest Data Date: <strong>{{ latest_date }}</strong></p>
        <p>Charts sorted by the absolute Z-score of the latest <strong>Fund - Benchmark Spread</strong> (most deviation first).</p>
        {% if not missing_funds.empty %}
            <div class="alert alert-warning" role="alert">
                <strong>Warning:</strong> The following funds are missing data for the latest date ({{ latest_date }}):
                {{ missing_funds.index.tolist() | join(', ') }}
            </div>
        {% endif %}
        {% for fund_code, data in charts_data.items() %}
            {% set metrics = data.metrics %}
            {% set z_score = metrics['Spread Z-Score'] %}
            {% set z_class = 'high-z' if z_score and z_score|abs > 2 else ('very-high-z' if z_score and z_score|abs > 3 else '') %}
            <div class="chart-container {{ z_class }}">
                {{ data.chart_html|safe }}
            </div>
            <table class="table table-sm table-bordered metrics-table {{ z_class }}">
                <thead>
                    <tr>
                        <th>Metric</th>
                        <th>Latest Value ({{ latest_date }})</th>
                        <th>Change from Previous</th>
                        <th>Historical Spread</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Fund Value</td>
                        <td>{{ metrics['Latest Fund Value']|round(2) if metrics['Latest Fund Value'] is not none else 'N/A' }}</td>
                        <td>{{ metrics['Fund Value Change']|round(2) if metrics['Fund Value Change'] is not none else 'N/A' }}</td>
                        <td rowspan="2">Mean: {{ metrics['Historical Spread Mean']|round(2) if metrics['Historical Spread Mean'] is not none else 'N/A' }}</td>
                    </tr>
                    <tr>
                        <td>Benchmark Value</td>
                        <td>{{ metrics['Latest Benchmark Value']|round(2) if metrics['Latest Benchmark Value'] is not none else 'N/A' }}</td>
                        <td>N/A</td> {# Change not calculated for benchmark #}
                    </tr>
                    <tr>
                        <td>Fund - Benchmark Spread</td>
                        <td>{{ metrics['Latest Spread']|round(2) if metrics['Latest Spread'] is not none else 'N/A' }}</td>
                        <td>{{ metrics['Spread Change']|round(2) if metrics['Spread Change'] is not none else 'N/A' }}</td>
                        <td>Std Dev: {{ metrics['Historical Spread Std Dev']|round(2) if metrics['Historical Spread Std Dev'] is not none else 'N/A' }}</td>
                    </tr>
                     <tr>
                        <td><strong>Spread Z-Score</strong></td>
                        <td colspan="3"><strong>{{ z_score|round(2) if z_score is not none else 'N/A' }}</strong></td>
                    </tr>
                </tbody>
            </table>
            {# Conditionally add link to fund duration details page #}
            {% if metric_name == 'Duration' %}
                <div class="mb-4 text-right">
                     <a href="{{ url_for('fund_duration_details', fund_code=fund_code) }}" class="btn btn-info btn-sm">View Security Duration Changes for {{ fund_code }} &rarr;</a>
                </div>
            {% endif %}
        {% else %}
            <p>No data processed for this metric.</p>
        {% endfor %}
    </main>
    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.5.4/dist/umd/popper.min.js"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"></script>
</body>
</html>
</file>

<file path="templates/duration_comparison_details_page.html">
{% extends "base.html" %}
{# Note: security_name is not explicitly passed, using security_id for title/breadcrumb #}
{% block title %}Duration Comparison Details: {{ security_id }}{% endblock %}
{% block content %}
<div class="container mt-4">
    <nav aria-label="breadcrumb">
        <ol class="breadcrumb">
            <li class="breadcrumb-item"><a href="{{ url_for('duration_comparison_bp.summary') }}">Duration Comparison Summary</a></li> {# Updated Link #}
            {# Displaying ID as primary identifier if name isn't guaranteed #}
            <li class="breadcrumb-item active" aria-current="page">{{ security_id }}</li>
        </ol>
    </nav>
    <h1>Duration Comparison Details: {{ security_id }}</h1> {# Updated Title #}
    {# Add static info if available #}
    {% if static_info %}
        {% for key, value in static_info.items() %}
             {% if key != id_column_name %} {# Avoid repeating the ID #}
                <span class="text-muted me-3"><strong>{{ key }}:</strong> {{ value }}</span>
             {% endif %}
        {% endfor %}
    {% endif %}
    <div class="row mt-4 mb-4">
        <div class="col-md-6">
            <h2>Comparison Statistics</h2>
            {% if stats_summary %}
                <ul class="list-group">
                    <li class="list-group-item d-flex justify-content-between align-items-center">
                        Level Correlation
                        <span class="badge bg-primary rounded-pill">{{ stats_summary.Level_Correlation if stats_summary.Level_Correlation is not none else 'N/A' }}</span>
                    </li>
                    <li class="list-group-item d-flex justify-content-between align-items-center">
                        Change Correlation
                        <span class="badge bg-primary rounded-pill">{{ stats_summary.Change_Correlation if stats_summary.Change_Correlation is not none else 'N/A' }}</span>
                    </li>
                    <li class="list-group-item d-flex justify-content-between align-items-center">
                        Mean Absolute Difference
                        <span class="badge bg-secondary rounded-pill">{{ stats_summary.Mean_Abs_Diff if stats_summary.Mean_Abs_Diff is not none else 'N/A' }}</span>
                    </li>
                    <li class="list-group-item d-flex justify-content-between align-items-center">
                        Max Absolute Difference
                        <span class="badge bg-secondary rounded-pill">{{ stats_summary.Max_Abs_Diff if stats_summary.Max_Abs_Diff is not none else 'N/A' }}</span>
                    </li>
                    <li class="list-group-item d-flex justify-content-between align-items-center">
                        Data Points (Original)
                        <span class="badge bg-info rounded-pill">{{ stats_summary.Total_Points - stats_summary.NaN_Count_Orig }} / {{ stats_summary.Total_Points }}</span>
                    </li>
                    <li class="list-group-item d-flex justify-content-between align-items-center">
                        Data Points (New)
                        <span class="badge bg-info rounded-pill">{{ stats_summary.Total_Points - stats_summary.NaN_Count_New }} / {{ stats_summary.Total_Points }}</span>
                    </li>
                    <li class="list-group-item d-flex justify-content-between align-items-center">
                        Same Date Range?
                        <span class="badge {{ 'bg-success' if stats_summary.Same_Date_Range else 'bg-warning' }} rounded-pill">{{ 'Yes' if stats_summary.Same_Date_Range else 'No' }}</span>
                    </li>
                    {# Add date range details #}
                    <li class="list-group-item">
                        <small>Orig Range: {{ stats_summary.Start_Date_Orig or 'N/A' }} to {{ stats_summary.End_Date_Orig or 'N/A' }}</small><br>
                        <small>New Range: {{ stats_summary.Start_Date_New or 'N/A' }} to {{ stats_summary.End_Date_New or 'N/A' }}</small>
                    </li>
                </ul>
            {% else %}
                <p>No comparison statistics could be calculated.</p>
            {% endif %}
        </div>
        {# Placeholder for additional stats or info if needed #}
    </div>
    <h2>Time Series Comparison</h2>
    <p class="text-muted">Overlayed Duration from Original (sec_duration) and New (sec_durationSP) datasets.</p> {# Updated Text #}
    <div>
        <canvas id="comparisonChart"></canvas>
    </div>
    {# Embed chart data as JSON for JavaScript #}
    <script type="application/json" id="comparisonChartData">
        {{ chart_data | tojson | safe }}
    </script>
</div>
{% endblock %}
{% block scripts %}
{{ super() }}
{# We need Chart.js - ensure it's included in base.html or here #}
<script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
<script src="https://cdn.jsdelivr.net/npm/chartjs-adapter-date-fns/dist/chartjs-adapter-date-fns.bundle.min.js"></script> {# Include Date Adapter #}
<script>
    document.addEventListener('DOMContentLoaded', function() {
        const chartDataElement = document.getElementById('comparisonChartData');
        const comparisonChartCanvas = document.getElementById('comparisonChart');
        if (chartDataElement && comparisonChartCanvas) {
            try {
                const chartData = JSON.parse(chartDataElement.textContent);
                const ctx = comparisonChartCanvas.getContext('2d');
                console.log("Initializing Duration Comparison Chart...");
                new Chart(ctx, {
                    type: 'line',
                    data: chartData, // Direct use of data from JSON
                    options: {
                        responsive: true,
                        maintainAspectRatio: true,
                        plugins: {
                            legend: {
                                position: 'top',
                            },
                            title: {
                                display: true,
                                text: 'Duration Comparison: {{ security_id|tojson }}' // Use security_id
                            },
                            tooltip: {
                                mode: 'index', // Show tooltips for all datasets at the same index
                                intersect: false
                            }
                        },
                        scales: {
                            x: {
                                type: 'time', // Use time scale
                                time: {
                                    unit: 'day',
                                    tooltipFormat: 'yyyy-MM-dd', // Format for tooltip
                                    displayFormats: { // Formats for axis labels
                                        day: 'MMM d, yyyy'
                                    }
                                },
                                title: {
                                    display: true,
                                    text: 'Date'
                                }
                            },
                            y: {
                                title: {
                                    display: true,
                                    text: 'Duration' // Updated Axis Label
                                }
                            }
                        },
                        interaction: {
                             intersect: false,
                             mode: 'index',
                        },
                         elements: {
                            point:{ // Reduce point size for potentially dense data
                                radius: 2
                            }
                        }
                        // No complex scriptable options needed for basic display
                    }
                });
            } catch (error) {
                console.error("Error parsing duration chart data or rendering chart:", error);
                if (comparisonChartCanvas.parentElement) {
                    comparisonChartCanvas.parentElement.innerHTML = '<p class="text-danger">Error rendering duration chart.</p>';
                }
            }
        } else {
             console.warn("Chart data or canvas element not found for duration comparison chart.");
        }
    });
</script>
{% endblock %}
</file>

<file path="templates/duration_comparison_page.html">
{% extends "base.html" %}
{% block title %}Duration Comparison Summary{% endblock %}
{% block content %}
<div class="container-fluid mt-4"> {# Use container-fluid for wider view #}
    <h1>Duration Comparison: Original (sec_duration) vs. New (sec_durationSP)</h1> {# Updated Title #}
    <p class="text-muted">Comparing Duration between the two datasets. Click on a Security ID/Name to see details. Use filters or click column headers to sort. Pagination applied.</p> {# Updated Text #}
    {# Display message if any #}
    {% if message %}
    <div class="alert alert-warning alert-dismissible fade show" role="alert">
        {{ message }}
        <button type="button" class="btn-close" data-bs-dismiss="alert" aria-label="Close"></button>
    </div>
    {% endif %}
    {# --- Filter Form --- #}
    {% if filter_options %}
    <form method="GET" action="{{ url_for('duration_comparison_bp.summary') }}" class="mb-3 p-3 border rounded bg-light" id="filter-form"> {# Updated Action URL #}
        <h5>Filters</h5>
        <div class="row g-2 align-items-end">
            {% for column, options in filter_options.items() %}
            <div class="col-md-2 mb-2">
                <label for="filter-{{ column }}" class="form-label">{{ column }}</label>
                <select id="filter-{{ column }}" name="filter_{{ column }}" class="form-select form-select-sm">
                    <option value="">All</option>
                    {% for option in options %}
                    <option value="{{ option }}" {% if active_filters.get(column) == option|string %}selected{% endif %}>{{ option }}</option>
                    {% endfor %}
                </select>
            </div>
            {% endfor %}
            <div class="col-md-3 d-flex align-items-end">
                <div class="form-check form-switch mb-1">
                    <input class="form-check-input" type="checkbox" role="switch" id="showSoldToggle" name="show_sold" value="true" {% if show_sold %}checked{% endif %}>
                    <label class="form-check-label" for="showSoldToggle"><small>Show Sold Securities</small></label>
                </div>
            </div>
            <div class="col-md-auto">
                <button type="submit" class="btn btn-primary btn-sm">Apply Filters</button>
                {# Add a clear button only if filters are active #}
                {% if active_filters %}
                <a href="{{ url_for('duration_comparison_bp.summary') }}" class="btn btn-secondary btn-sm">Clear Filters</a> {# Updated Clear URL #}
                {% endif %}
            </div>
        </div>
        {# Hidden fields to preserve current sort order when applying filters - page is implicitly reset #}
        <input type="hidden" name="sort_by" value="{{ current_sort_by }}">
        <input type="hidden" name="sort_order" value="{{ current_sort_order }}">
    </form>
    {% endif %}
    {# --- Data Table --- #}
    <div class="table-responsive">
        <table class="table table-striped table-hover table-sm caption-top" id="duration-comparison-table"> {# Updated Table ID #}
             {# Add table caption for summary #}
             {% if pagination %}
             <caption class="pb-1">
                 Displaying {{ table_data|length }} of {{ pagination.total_items }} total securities.
                 (Page {{ pagination.page }} of {{ pagination.total_pages }})
             </caption>
             {% endif %}
            <thead class="table-light">
                <tr>
                    {# Loop through the columns passed from the view #}
                    {% for col_name in columns_to_display %}
                        {% set is_sort_col = (col_name == current_sort_by) %}
                        {% set next_sort_order = 'asc' if is_sort_col and current_sort_order == 'desc' else 'desc' %}
                        {# Base arguments, including current filters #}
                        {% set sort_args = request.args.to_dict() %}
                        {% set _ = sort_args.pop('page', None) %}
                        {% set _ = sort_args.update({'sort_by': col_name, 'sort_order': next_sort_order}) %}
                        {# Generate URL for this header #}
                        {% set sort_url = url_for('duration_comparison_bp.summary', **sort_args) %} {# Updated Sort URL #}
                        {# Add classes for styling and JS #}
                        <th class="sortable {{ 'sorted-' + current_sort_order if is_sort_col else '' }}"
                            data-column-name="{{ col_name }}">
                            <a href="{{ sort_url }}" class="text-decoration-none text-dark">
                                {{ col_name.replace('_', ' ') | title }}
                                {% if is_sort_col %}
                                    <span class="sort-indicator ms-1">{{ '▲' if current_sort_order == 'asc' else '▼' }}</span>
                                {% endif %}
                            </a>
                        </th>
                    {% endfor %}
                </tr>
            </thead>
            <tbody id="duration-comparison-table-body"> {# Updated tbody ID #}
                {% set id_col = id_column_name %}
                {% for row in table_data %}
                <tr>
                    {# Loop through the same columns to ensure order matches header #}
                    {% for col_name in columns_to_display %}
                        <td>
                            {% if col_name == id_col %}
                                <a href="{{ url_for('duration_comparison_bp.duration_comparison_details', security_id=row[id_col]|urlencode) }}">{{ row[id_col] }}</a> {# Updated Detail URL #}
                            {% elif col_name in ['Level_Correlation', 'Change_Correlation'] and row[col_name] is not none %}
                                {# Attempt to format as float, handle potential errors gracefully #}
                                {% set formatted_val = "%.3f"|format(row[col_name]|float) if row[col_name] is number else row[col_name] %}
                                {{ formatted_val }}
                            {% elif col_name in ['Mean_Abs_Diff', 'Max_Abs_Diff'] and row[col_name] is not none %}
                                {% set formatted_val = "%.2f"|format(row[col_name]|float) if row[col_name] is number else row[col_name] %}
                                {{ formatted_val }}
                             {% elif col_name == 'Same_Date_Range' %}
                                 <span class="badge {{ 'bg-success' if row[col_name] else 'bg-warning' }}">{{ 'Yes' if row[col_name] else 'No' }}</span>
                            {% elif col_name.endswith('_Date') and row[col_name] %}
                                 {# Assume date strings are already YYYY-MM-DD from view #}
                                {{ row[col_name] }}
                            {% elif row[col_name] is number %}
                                {{ row[col_name]|round(3) }} {# General numeric formatting #}
                            {% else %}
                                {{ row[col_name] if row[col_name] is not none else '' }} {# Display strings or empty #}
                            {% endif %}
                        </td>
                    {% endfor %}
                </tr>
                {% else %}
                <tr>
                    <td colspan="{{ columns_to_display|length }}" class="text-center">No duration comparison data available matching the current filters.</td> {# Updated Message #}
                </tr>
                {% endfor %}
            </tbody>
        </table>
    </div>
    {# --- Pagination Controls --- #}
    {% if pagination and pagination.total_pages > 1 %}
    <nav aria-label="Duration comparison data navigation">
        <ul class="pagination pagination-sm justify-content-center">
            {# Helper macro for generating pagination links #}
            {% macro page_link(page_num, text=None, is_disabled=False, is_active=False) %}
                {% set link_args = request.args.to_dict() %}
                {% set _ = link_args.update({'page': page_num, 'sort_by': current_sort_by, 'sort_order': current_sort_order}) %}
                {% set url = url_for('duration_comparison_bp.summary', **link_args) if page_num else '#' %} {# Updated URL #}
                <li class="page-item {{ 'disabled' if is_disabled }} {{ 'active' if is_active }}">
                    <a class="page-link" href="{{ url }}" {% if is_active %}aria-current="page"{% endif %}>{{ text or page_num }}</a>
                </li>
            {% endmacro %}
            {{ page_link(pagination.prev_num, '&laquo;', is_disabled=not pagination.has_prev) }}
            {# Simplified pagination display logic #}
            {% set window = 2 %}
            {% set start_page = [1, pagination.page - window] | max %}
            {% set end_page = [pagination.total_pages, pagination.page + window] | min %}
            {% if start_page > 1 %}
                {{ page_link(1) }}
                {% if start_page > 2 %}
                    <li class="page-item disabled"><span class="page-link">...</span></li>
                {% endif %}
            {% endif %}
            {% for p in range(start_page, end_page + 1) %}
                {{ page_link(p, is_active=(p == pagination.page)) }}
            {% endfor %}
            {% if end_page < pagination.total_pages %}
                {% if end_page < pagination.total_pages - 1 %}
                    <li class="page-item disabled"><span class="page-link">...</span></li>
                {% endif %}
                {{ page_link(pagination.total_pages) }}
            {% endif %}
            {{ page_link(pagination.next_num, '&raquo;', is_disabled=not pagination.has_next) }}
        </ul>
    </nav>
    {% endif %}
</div>
{% endblock %}
{% block scripts %}
{{ super() }}
{# No specific JS needed for this page unless client-side sorting is added back #}
{% endblock %}
</file>

<file path="templates/exclusions_page.html">
{% extends 'base.html' %}
{% block title %}Manage Security Exclusions{% endblock %}
{% block content %}
<div class="container mt-4">
    <h2>Manage Security Exclusions</h2>
    <hr>
    {# Display messages if any #}
    {% if message %}
        <div class="alert alert-{{ message_type }} alert-dismissible fade show" role="alert">
            {{ message }}
            <button type="button" class="btn-close" data-bs-dismiss="alert" aria-label="Close"></button>
        </div>
    {% endif %}
    <div class="row">
        {# Left Column: Display Current Exclusions #}
        <div class="col-md-7">
            <h4>Current Exclusions</h4>
            {% if exclusions %}
                <table class="table table-striped table-sm">
                    <thead>
                        <tr>
                            <th>Security ID</th>
                            <th>Date Added</th>
                            <th>End Date</th>
                            <th>Comment</th>
                            <th>Action</th>
                        </tr>
                    </thead>
                    <tbody>
                        {% for exclusion in exclusions %}
                            <tr>
                                <td>{{ exclusion.SecurityID }}</td>
                                <td>{{ exclusion.AddDate.strftime('%Y-%m-%d') if exclusion.AddDate else 'N/A' }}</td>
                                <td>{{ exclusion.EndDate.strftime('%Y-%m-%d') if exclusion.EndDate else '' }}</td>
                                <td>{{ exclusion.Comment }}</td>
                                <td>
                                    <form method="POST" action="{{ url_for('exclusion_bp.remove_exclusion_route') }}" style="display: inline;">
                                        <input type="hidden" name="security_id" value="{{ exclusion.SecurityID }}">
                                        <input type="hidden" name="add_date" value="{{ exclusion.AddDate.strftime('%Y-%m-%d') if exclusion.AddDate else '' }}">
                                        <button type="submit" class="btn btn-danger btn-sm" onclick="return confirm('Are you sure you want to remove this exclusion?');">Remove</button>
                                    </form>
                                </td>
                            </tr>
                        {% endfor %}
                    </tbody>
                </table>
            {% else %}
                <p>No securities are currently excluded.</p>
            {% endif %}
        </div>
        {# Right Column: Add New Exclusion Form #}
        <div class="col-md-5">
            <h4>Add New Exclusion</h4>
            <form method="POST" action="{{ url_for('exclusion_bp.manage_exclusions') }}">
                <div class="mb-3">
                    <label for="security-search-input" class="form-label">Search & Select Security ID:</label>
                    <input type="text" id="security-search-input" class="form-control mb-2" placeholder="Type to filter securities...">
                    <select class="form-select" id="security-select" name="security_id" required>
                        <option value="" disabled selected>Select a Security ID</option>
                        {% for sec_id in available_securities %}
                            <option value="{{ sec_id }}">{{ sec_id }}</option>
                        {% endfor %}
                    </select>
                </div>
                <div class="mb-3">
                    <label for="end_date" class="form-label">End Date (Optional):</label>
                    <input type="date" class="form-control" id="end_date" name="end_date">
                </div>
                <div class="mb-3">
                    <label for="comment" class="form-label">Comment (Required):</label>
                    <textarea class="form-control" id="comment" name="comment" rows="3" required></textarea>
                </div>
                <button type="submit" class="btn btn-primary">Add Exclusion</button>
            </form>
        </div>
    </div>
</div>
{% endblock %}
{% block scripts %}
{{ super() }} {# Include scripts from base.html #}
{# We will add specific JS for the dynamic dropdown here later #}
<script>
    // Basic dynamic filtering for the dropdown
    document.getElementById('security-search-input').addEventListener('input', function() {
        let filter = this.value.toLowerCase();
        let select = document.getElementById('security-select');
        let options = select.options;
        let firstVisibleOption = null;
        for (let i = 0; i < options.length; i++) {
            let option = options[i];
            // Skip the placeholder option
            if (option.value === "") {
                option.style.display = ""; // Always show placeholder if input is empty, hide otherwise
                option.style.display = filter ? "none" : "";
                continue;
            }
            let txtValue = option.textContent || option.innerText;
            if (txtValue.toLowerCase().indexOf(filter) > -1) {
                option.style.display = "";
                if (!firstVisibleOption) {
                     firstVisibleOption = option; // Keep track of the first match
                }
            } else {
                option.style.display = "none";
            }
        }
         // Optionally, select the first visible option if the user hasn't selected one manually
        // This part can be enhanced, maybe select only if input length > N or on specific event
        // if (filter && firstVisibleOption && select.selectedIndex <= 0) {
            // select.value = firstVisibleOption.value;
        // }
    });
    // Reset filter when dropdown is clicked (to show all options again initially)
    document.getElementById('security-select').addEventListener('mousedown', function(){
       // Optional: Uncomment below to clear search on dropdown click
       // document.getElementById('security-search-input').value = '';
       // let event = new Event('input');
       // document.getElementById('security-search-input').dispatchEvent(event);
    });
</script>
{% endblock %}
</file>

<file path="templates/fund_detail_page.html">
{% extends "base.html" %}
{% block title %}Fund Details: {{ fund_code }}{% endblock %}
{% block content %}
<div class="container mt-4">
    <h1 class="mb-4">Fund Details: {{ fund_code }}</h1>
    {% if message %}
        <div class="alert alert-info" role="alert">
            {{ message }}
        </div>
    {% endif %}
    {% if chart_data_json and chart_data_json != '[]' %}
        <!-- Toggle Switch for SP Data -->
        <div class="form-check form-switch mb-3">
            <input class="form-check-input" type="checkbox" role="switch" id="toggleSpData" checked>
            <label class="form-check-label" for="toggleSpData">Show SP Comparison Data</label>
        </div>
        <!-- Embed JSON data for JavaScript -->
        <script id="fundChartData" type="application/json">
            {{ chart_data_json | safe }}
        </script>
        <!-- Area where charts will be rendered by JavaScript -->
        <div id="fundChartsArea" class="row row-cols-1 row-cols-lg-2 g-4">
            <!-- Charts will be dynamically inserted here -->
        </div>
    {% elif not message %}
         <div class="alert alert-warning" role="alert">
            No chart data available to display for this fund.
        </div>
    {% endif %}
     <div class="mt-4">
        <a href="{{ url_for('main.index') }}" class="btn btn-secondary">Back to Dashboard</a>
    </div>
</div>
{% endblock %}
{% block scripts %}
{{ super() }}
<!-- Chart.js is already included in base.html -->
<!-- Make this a module to allow imports -->
<script type="module">
    // Import necessary functions from the chart renderer module
    import { renderFundCharts, toggleSecondaryDataVisibility } from '{{ url_for('static', filename='js/modules/ui/chartRenderer.js') }}';
    document.addEventListener('DOMContentLoaded', function () {
        const chartDataElement = document.getElementById('fundChartData');
        const chartsArea = document.getElementById('fundChartsArea');
        const toggleSwitch = document.getElementById('toggleSpData');
        if (!chartDataElement || !chartsArea || !toggleSwitch) {
            console.error('[Fund Detail Page] Required elements for chart rendering or toggle not found.');
            return;
        }
        try {
            const allChartData = JSON.parse(chartDataElement.textContent);
            // Check if any SP data exists across all metrics
            const anySpDataAvailable = allChartData.some(chartInfo => 
                chartInfo.datasets && chartInfo.datasets.some(ds => ds.isSpData === true)
            );
            // Initial setup of the toggle switch based on data availability
            if (anySpDataAvailable) {
                toggleSwitch.disabled = false;
                toggleSwitch.parentElement.querySelector('label').textContent = 'Show SP Comparison Data';
            } else {
                toggleSwitch.disabled = true;
                toggleSwitch.checked = false; // Ensure it's off if disabled
                toggleSwitch.parentElement.querySelector('label').textContent = 'Show SP Comparison Data (N/A)';
            }
            // Render the charts using the imported function
            // This function will now manage the chart instances internally
            renderFundCharts(chartsArea, allChartData);
            console.log("[Fund Detail Page] Called renderFundCharts.");
            // Add event listener for the toggle switch
            // This now calls the centralized toggle function from the module
            toggleSwitch.addEventListener('change', function() {
                const showSp = this.checked;
                console.log(`[Fund Detail Page] Toggle changed: Show SP Data = ${showSp}. Calling toggleSecondaryDataVisibility.`); 
                toggleSecondaryDataVisibility(showSp); // Call the imported function
            });
        } catch (error) {
            console.error('[Fund Detail Page] Error parsing chart data or setting up charts:', error);
            chartsArea.innerHTML = '<div class="alert alert-danger">Failed to load chart data.</div>';
            if (toggleSwitch) {
                toggleSwitch.disabled = true; // Disable toggle on error
                 toggleSwitch.parentElement.querySelector('label').textContent = 'Show SP Comparison Data (Error)';
            }
        }
    });
</script>
{% endblock %}
</file>

<file path="templates/fund_duration_details.html">
{% extends 'base.html' %}
{% block title %}Duration Change Details for {{ fund_code }}{% endblock %}
{% block content %}
<div class="container mt-4">
    <h2>Duration Change Details for Fund: {{ fund_code }}</h2>
    <p>Showing securities from <code>sec_duration.csv</code> held by <strong>{{ fund_code }}</strong>, sorted by the latest 1-day change in duration (largest change first).</p>
    {% if message %}
    <div class="alert alert-warning" role="alert">
        {{ message }}
    </div>
    {% endif %}
    {# Data Table Section #}
    {% if securities_data %}
    <div class="table-responsive">
        <table class="table table-striped table-hover table-sm small" id="fund-duration-table">
            <thead class="table-light">
                <tr>
                    {# Use the column_order provided by the backend #}
                    {% for col_name in column_order %}
                        {# Add sortable class and data attribute. Add default sort attribute if it matches. #}
                        <th class="sortable"
                            data-column-name="{{ col_name }}"
                            {% if col_name == 'Duration Contribution Change' %}data-sort-default="desc"{% endif %}>
                            {{ col_name }}
                        </th>
                    {% endfor %}
                </tr>
            </thead>
            <tbody id="fund-duration-table-body">
                {% for row in securities_data %}
                     {# Optionally add row highlighting based on the change magnitude if needed #}
                     {% set change_value = row['1 Day Duration Change'] %}
                     {% set row_class = '' %} {# Add logic here if desired e.g., based on change_value sign or magnitude #}
                     {# Example highlighting:
                     {% if change_value is not none %}
                        {% if change_value > 0.5 %}
                            {% set row_class = 'table-warning' %}
                        {% elif change_value < -0.5 %}
                             {% set row_class = 'table-info' %}
                        {% endif %}
                     {% endif %}
                     #}
                    <tr class="{{ row_class }}">
                        {% for col_name in column_order %}
                            {# Add data-value for numeric columns to aid sorting #}
                            <td {% if row[col_name] is number %}data-value="{{ row[col_name] }}"{% endif %}>
                                {# Special formatting for the change column or others if needed #}
                                {% if col_name == id_col_name %}
                                     {# Make the Security Name a link #}
                                     <a href="{{ url_for('security.security_details', metric_name='Duration', security_id=row[col_name]|urlencode) }}">{{ row[col_name] }}</a>
                                {% elif row[col_name] is number %}
                                    {# Format numeric columns, maybe specific format for change #}
                                     {{ "%.4f"|format(row[col_name]) }} {# Use 4 decimal places consistent with backend rounding #}
                                {% else %}
                                     {{ row[col_name] if row[col_name] is not none else '' }}
                                {% endif %}
                            </td>
                        {% endfor %}
                    </tr>
                {% endfor %}
            </tbody>
        </table>
    </div>
    {% elif not message %}
     <div class="alert alert-info" role="alert">
        No securities data to display for fund {{ fund_code }}.
    </div>
    {% endif %}
    <div class="mt-3">
         <a href="{{ url_for('metric.metric_page', metric_name='Duration') }}" class="btn btn-secondary btn-sm">&larr; Back to Duration Metric Page</a>
    </div>
</div>
{% endblock %}
</file>

<file path="templates/get_data.html">
{% extends "base.html" %}
{% block title %}Get Data via API{% endblock %}
{% block content %}
<div class="container mt-4">
    {# --- Display Data File Statuses --- #}
    <div class="card mb-4">
        <div class="card-header">
            Current Data File Status
        </div>
        <div class="card-body">
            <p class="card-text"><small class="text-muted">Data Folder: <code>{{ data_folder }}</code></small></p>
            {% if data_file_statuses %}
            <table class="table table-sm table-striped table-bordered">
                <thead>
                    <tr>
                        <th>File Name</th>
                        <th>Latest Data Date (in file)</th>
                        <th>File Last Modified</th>
                        <th>Funds Included</th>
                    </tr>
                </thead>
                <tbody>
                    {% for status in data_file_statuses %}
                    <tr>
                        <td>{{ status.filename }}</td>
                        <td>
                            {% if status.exists %}
                                {{ status.latest_data_date }}
                            {% else %}
                                <span class="text-muted">File Not Found</span>
                            {% endif %}
                        </td>
                        <td>
                             {% if status.exists %}
                                {{ status.last_modified }}
                            {% else %}
                                <span class="text-muted">N/A</span>
                            {% endif %}
                        </td>
                        <td>
                             {% if status.exists %}
                                {{ status.funds_included }}
                            {% else %}
                                <span class="text-muted">N/A</span>
                            {% endif %}
                        </td>
                    </tr>
                    {% endfor %}
                </tbody>
            </table>
            {% else %}
            <p class="text-muted">Could not retrieve data file statuses. Check QueryMap.csv or server logs.</p>
            {% endif %}
        </div>
    </div>
    {# --- End Display Data File Statuses --- #}
    <h2>Get Data via API</h2>
    <p>Select funds and date range to retrieve and process data using the API.</p>
    <p>The API calls and processing will be printed in the terminal where the Flask app is running.</p>
    <form id="get-data-form">
        <div class="row mb-3">
            <div class="col-md-4">
                <label for="daysBack" class="form-label">Days Back:</label>
                <input type="number" class="form-control" id="daysBack" name="days_back" value="30" required>
                <div class="form-text">Number of days of history to retrieve ending on the End Date.</div>
            </div>
            <div class="col-md-4">
                <label for="endDate" class="form-label">End Date:</label>
                <input type="date" class="form-control" id="endDate" name="end_date" value="{{ default_end_date }}" required>
                <div class="form-text">Defaults to the previous business day.</div>
            </div>
            <div class="col-md-4">
                <label class="form-label">Date Range Mode:</label>
                <div class="form-check">
                    <input class="form-check-input" type="radio" name="date_mode" id="dateModeQuick" value="quick" checked>
                    <label class="form-check-label" for="dateModeQuick">Use Days Back & End Date</label>
                </div>
                <div class="form-check">
                    <input class="form-check-input" type="radio" name="date_mode" id="dateModeRange" value="range">
                    <label class="form-check-label" for="dateModeRange">Use Custom Date Range</label>
                </div>
            </div>
        </div>
        <div class="row mb-3" id="custom-date-range-row" style="display: none;">
            <div class="col-md-4">
                <label for="startDate" class="form-label">Start Date:</label>
                <input type="date" class="form-control" id="startDate" name="start_date">
            </div>
            <div class="col-md-4">
                <label for="customEndDate" class="form-label">End Date:</label>
                <input type="date" class="form-control" id="customEndDate" name="custom_end_date">
            </div>
        </div>
        <div class="mb-3">
            <label class="form-label">Data Write Mode:</label>
            <div class="form-check">
                <input class="form-check-input" type="radio" name="write_mode" id="writeModeExpand" value="expand" checked>
                <label class="form-check-label" for="writeModeExpand">Expand (append new data, overwrite overlaps)</label>
            </div>
            <div class="form-check">
                <input class="form-check-input" type="radio" name="write_mode" id="writeModeOverwrite" value="overwrite_all">
                <label class="form-check-label" for="writeModeOverwrite">Run and Overwrite All (start every file from scratch)</label>
            </div>
        </div>
        <div class="mb-3">
            <label class="form-label">Select Funds:</label>
             <button type="button" class="btn btn-sm btn-outline-secondary ms-2" id="select-all-funds">Select All</button>
             <button type="button" class="btn btn-sm btn-outline-secondary ms-1" id="deselect-all-funds">Deselect All</button>
            <div id="fund-list" class="border p-3" style="max-height: 300px; overflow-y: auto;">
                {% for fund in funds %}
                <div class="form-check">
                    <input class="form-check-input fund-checkbox" type="checkbox" value="{{ fund['Fund Code'] }}" id="fund-{{ fund['Fund Code'] }}" name="funds"
                           {% if fund['Picked'] %}checked{% endif %}>
                    <label class="form-check-label" for="fund-{{ fund['Fund Code'] }}">
                        {{ fund['Fund Code'] }} (AUM: {{ fund['Total Asset Value USD']|int }})
                    </label>
                </div>
                {% else %}
                <p class="text-danger">No funds found or FundList.csv could not be loaded correctly.</p>
                {% endfor %}
            </div>
             <div class="form-text text-danger d-none" id="fund-selection-error">Please select at least one fund.</div>
        </div>
        <button type="submit" class="btn btn-primary">Run API Calls</button>
        <button type="button" id="run-overwrite-button" class="btn btn-warning ms-2">Run and Overwrite Data</button>
        <button type="button" id="run-cleanup-button" class="btn btn-secondary ms-2">Run Data Cleanup</button>
    </form>
    <div id="status-area" class="mt-4" style="display: none;">
        <h4>Processing Status</h4>
        <div class="progress mb-2" style="height: 20px;">
            <div id="progress-bar" class="progress-bar progress-bar-striped progress-bar-animated" role="progressbar" style="width: 0%;" aria-valuenow="0" aria-valuemin="0" aria-valuemax="100">0%</div>
        </div>
        <p id="status-message"></p>
        <div id="results-summary" class="mt-3">
            <h5>Results Summary</h5>
            <table class="table table-sm table-striped">
                <thead>
                    <tr>
                        <th>Query ID</th>
                        <th>File Name</th>
                        <th>Rows Returned</th>
                        <th>File Lines</th>
                        <th>Last Written</th>
                        <th>Status</th>
                        <th>Actions</th>
                    </tr>
                </thead>
                <tbody id="results-table-body">
                    <!-- Results will be populated here -->
                </tbody>
            </table>
        </div>
         <div id="error-message" class="alert alert-danger mt-3" style="display: none;">
             <!-- Errors shown here -->
         </div>
    </div>
    <!-- Scheduled API Calls Section -->
    <hr>
    <h2>Scheduled API Calls</h2>
    <p>Set up daily scheduled runs of the API calls by choosing funds, date range, overwrite mode, and time.</p>
    <form id="schedule-form" class="mb-4">
        <div class="row mb-3">
            <div class="col-md-3">
                <label for="scheduleTime" class="form-label">Run Time:</label>
                <input type="time" class="form-control" id="scheduleTime" name="schedule_time" required>
            </div>
            <div class="col-md-3">
                <label for="scheduleWriteMode" class="form-label">Write Mode:</label>
                <select class="form-select" id="scheduleWriteMode" name="write_mode">
                    <option value="expand" selected>Expand (append/merge)</option>
                    <option value="overwrite_all">Overwrite All</option>
                </select>
            </div>
            <div class="col-md-3">
                <label class="form-label">Date Mode:</label>
                <select class="form-select" id="scheduleDateMode" name="date_mode">
                    <option value="quick" selected>Quick (Days Back + End Date)</option>
                    <option value="range">Custom Range</option>
                </select>
            </div>
            <div class="col-md-3" id="scheduleDaysBackContainer">
                <label for="scheduleDaysBack" class="form-label">Days Back:</label>
                <input type="number" class="form-control" id="scheduleDaysBack" name="days_back" value="30">
            </div>
        </div>
        <div class="row mb-3" id="scheduleDatesContainer" style="display:none;">
            <div class="col-md-4">
                <label for="scheduleStartDate" class="form-label">Start Date:</label>
                <input type="date" class="form-control" id="scheduleStartDate" name="start_date">
            </div>
            <div class="col-md-4">
                <label for="scheduleEndDateCustom" class="form-label">End Date:</label>
                <input type="date" class="form-control" id="scheduleEndDateCustom" name="custom_end_date">
            </div>
        </div>
        <div class="mb-3">
            <label class="form-label">Funds:</label>
            <div id="schedule-fund-list" class="border p-3" style="max-height:150px; overflow-y:auto;">
                <!-- Reuse fund checkboxes from main form -->
                {% for fund in funds %}
                <div class="form-check form-check-inline">
                    <input class="form-check-input schedule-fund-checkbox" type="checkbox" value="{{ fund['Fund Code'] }}" id="sched-fund-{{ fund['Fund Code'] }}">
                    <label class="form-check-label" for="sched-fund-{{ fund['Fund Code'] }}">{{ fund['Fund Code'] }}</label>
                </div>
                {% endfor %}
            </div>
        </div>
        <div class="text-danger d-none" id="schedule-error">Please fill out all fields and select at least one fund.</div>
        <button type="submit" class="btn btn-success">Add Schedule</button>
    </form>
    <table class="table table-sm table-striped" id="schedule-table">
        <thead>
            <tr>
                <th>ID</th><th>Time</th><th>Funds</th><th>Date Mode</th><th>Params</th><th>Write Mode</th><th>Actions</th>
            </tr>
        </thead>
        <tbody></tbody>
    </table>
</div>
{% endblock %}
{% block scripts %}
{{ super() }} {# Include scripts from base.html #}
<script>
// .SYNOPSIS
//     JavaScript for Get Data via API page. Handles form logic, date range and write mode selection, API calls, and results table updates.
document.addEventListener('DOMContentLoaded', function() {
    const form = document.getElementById('get-data-form');
    const statusArea = document.getElementById('status-area');
    const statusMessage = document.getElementById('status-message');
    const progressBar = document.getElementById('progress-bar');
    const resultsTableBody = document.getElementById('results-table-body');
    const errorMessageDiv = document.getElementById('error-message');
    const fundSelectionError = document.getElementById('fund-selection-error');
    const cleanupButton = document.getElementById('run-cleanup-button');
    const runOverwriteButton = document.getElementById('run-overwrite-button');
    const cleanupStatus = document.createElement('div');
    cleanupStatus.id = 'cleanup-status';
    cleanupStatus.className = 'mt-2';
    cleanupButton.parentNode.insertBefore(cleanupStatus, cleanupButton.nextSibling);
    const fundCheckboxes = document.querySelectorAll('.fund-checkbox');
    const selectAllButton = document.getElementById('select-all-funds');
    const deselectAllButton = document.getElementById('deselect-all-funds');
    const dateModeQuick = document.getElementById('dateModeQuick');
    const dateModeRange = document.getElementById('dateModeRange');
    const customDateRangeRow = document.getElementById('custom-date-range-row');
    const startDateInput = document.getElementById('startDate');
    const customEndDateInput = document.getElementById('customEndDate');
    const writeModeExpand = document.getElementById('writeModeExpand');
    const writeModeOverwrite = document.getElementById('writeModeOverwrite');
    // Show/hide custom date range row based on radio selection
    function updateDateRangeVisibility() {
        if (dateModeRange.checked) {
            customDateRangeRow.style.display = '';
        } else {
            customDateRangeRow.style.display = 'none';
        }
    }
    dateModeQuick.addEventListener('change', updateDateRangeVisibility);
    dateModeRange.addEventListener('change', updateDateRangeVisibility);
    updateDateRangeVisibility(); // Initial state
    // Select/Deselect All Funds buttons
    selectAllButton.addEventListener('click', () => {
        fundCheckboxes.forEach(checkbox => checkbox.checked = true);
    });
    deselectAllButton.addEventListener('click', () => {
        fundCheckboxes.forEach(checkbox => checkbox.checked = false);
    });
    // --- Function to handle the API call logic ---
    async function handleApiCall(forceOverwriteButton = false) {
        // Clear previous results and errors
        statusArea.style.display = 'none';
        resultsTableBody.innerHTML = '';
        errorMessageDiv.style.display = 'none';
        errorMessageDiv.textContent = '';
        cleanupButton.style.display = 'none';
        cleanupStatus.textContent = '';
        cleanupStatus.className = 'mt-2';
        statusMessage.textContent = '';
        progressBar.style.width = '0%';
        progressBar.textContent = '0%';
        progressBar.classList.remove('bg-success', 'bg-danger');
        fundSelectionError.classList.add('d-none');
        // Get selected funds
        const selectedFunds = Array.from(document.querySelectorAll('input[name="funds"]:checked'))
                                 .map(cb => cb.value);
        if (selectedFunds.length === 0) {
            fundSelectionError.classList.remove('d-none');
            return;
        }
        // Date mode and values
        const dateMode = dateModeRange.checked ? 'range' : 'quick';
        let startDate = null, customEndDate = null, daysBack = null, endDate = null;
        if (dateMode === 'range') {
            startDate = startDateInput.value;
            customEndDate = customEndDateInput.value;
            if (!startDate || !customEndDate) {
                errorMessageDiv.textContent = 'Please select both Start Date and End Date for the custom range.';
                errorMessageDiv.style.display = 'block';
                return;
            }
        } else {
            daysBack = document.getElementById('daysBack').value;
            endDate = document.getElementById('endDate').value;
            if (!endDate) {
                errorMessageDiv.textContent = 'Please select an End Date.';
                errorMessageDiv.style.display = 'block';
                return;
            }
        }
        // Write mode
        const writeMode = writeModeOverwrite.checked ? 'overwrite_all' : 'expand';
        // Show status area and indicate processing
        statusArea.style.display = 'block';
        statusMessage.textContent = `Starting ${writeMode === 'overwrite_all' ? 'overwrite' : 'expand/merge'}...`;
        progressBar.classList.add('progress-bar-animated');
        progressBar.classList.remove('bg-success', 'bg-danger');
        progressBar.style.width = '5%';
        progressBar.textContent = '5%';
        try {
            // Prepare request body
            const requestBody = {
                date_mode: dateMode,
                write_mode: writeMode,
                funds: selectedFunds
            };
            if (dateMode === 'range') {
                requestBody.start_date = startDate;
                requestBody.custom_end_date = customEndDate;
            } else {
                requestBody.days_back = parseInt(daysBack, 10);
                requestBody.end_date = endDate;
            }
            const response = await fetch('{{ url_for("api_bp.run_api_calls") }}', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                },
                body: JSON.stringify(requestBody)
            });
            const result = await response.json();
            progressBar.classList.remove('progress-bar-animated');
            if (response.ok && (result.status === 'completed' || result.status === 'completed_with_errors')) {
                statusMessage.textContent = result.message;
                progressBar.style.width = '100%';
                progressBar.textContent = '100%';
                progressBar.classList.add(result.status === 'completed_with_errors' ? 'bg-warning' : 'bg-success');
                // Populate results table (add Last Written column)
                resultsTableBody.innerHTML = '';
                if (result.summary && result.summary.length > 0) {
                    result.summary.forEach(item => {
                        let rowsCellContent = 'N/A';
                        let linesCellContent = 'N/A';
                        if (item.actual_rows !== undefined && item.actual_rows !== null) {
                            rowsCellContent = item.actual_rows;
                            linesCellContent = item.actual_lines !== undefined ? item.actual_lines : (rowsCellContent > 0 ? rowsCellContent + 1 : 0);
                        } else if (item.simulated_rows !== undefined && item.simulated_rows !== null) {
                            rowsCellContent = item.simulated_rows;
                            linesCellContent = item.simulated_lines !== undefined ? item.simulated_lines : (rowsCellContent > 0 ? rowsCellContent + 1 : 0);
                        }
                        let fundCodeForRerun = item.fund_code || null;
                        if (!fundCodeForRerun && item.query_id && typeof item.query_id === 'string') {
                            const match = item.query_id.match(/some_pattern_to_extract_fund_code/);
                            if (match && match[1]) {
                                fundCodeForRerun = match[1];
                            }
                        }
                        const rerunButtonHtml = fundCodeForRerun
                            ? `<button class="btn btn-sm btn-outline-primary rerun-button" data-fund-code="${fundCodeForRerun}">Rerun</button>`
                            : `<span class="text-muted">Rerun N/A</span>`;
                        const row = `<tr data-query-id="${item.query_id}">
                                        <td>${item.query_id}</td>
                                        <td>${item.file_name}</td>
                                        <td>${rowsCellContent}</td>
                                        <td>${linesCellContent}</td>
                                        <td>${item.last_written ? item.last_written : 'N/A'}</td>
                                        <td><span class="badge ${item.status.includes('OK') || item.status.includes('Saved') ? 'bg-success' : (item.status.includes('Warning') ? 'bg-warning' : 'bg-danger')}">${item.status}</span></td>
                                        <td>${rerunButtonHtml}</td>
                                     </tr>`;
                        resultsTableBody.innerHTML += row;
                    });
                } else {
                    resultsTableBody.innerHTML = '<tr><td colspan="7">No summary data returned.</td></tr>';
                }
                errorMessageDiv.style.display = 'none';
                cleanupButton.style.display = 'inline-block';
            } else {
                statusMessage.textContent = 'Processing failed.';
                progressBar.style.width = '100%';
                progressBar.textContent = 'Error';
                progressBar.classList.add('bg-danger');
                errorMessageDiv.textContent = `Error: ${result.message || 'Unknown error'}`;
                errorMessageDiv.style.display = 'block';
            }
        } catch (error) {
            console.error("Fetch error:", error);
            progressBar.classList.remove('progress-bar-animated');
            progressBar.style.width = '100%';
            progressBar.textContent = 'Error';
            progressBar.classList.add('bg-danger');
            statusMessage.textContent = 'An error occurred during the request.';
            errorMessageDiv.textContent = 'Network error or server unreachable. Check console for details.';
            errorMessageDiv.style.display = 'block';
        }
    }
    // --- End of API call handler function ---
    // --- Event Listener for the original "Run API Calls" button (which is type="submit") ---
    form.addEventListener('submit', function(event) {
        event.preventDefault();
        handleApiCall(false);
    });
    runOverwriteButton.addEventListener('click', function() {
        handleApiCall(true);
    });
    // Add event listener for the Rerun buttons using event delegation
    resultsTableBody.addEventListener('click', async function(event) {
        if (event.target.classList.contains('rerun-button')) {
            const button = event.target;
            const row = button.closest('tr');
            const queryId = row.dataset.queryId;
            const fundCode = button.dataset.fundCode;
            const daysBack = document.getElementById('daysBack').value;
            const endDate = document.getElementById('endDate').value;
            const cells = row.cells; // Define cells here so it's available in all blocks
            if (!fundCode || !queryId) {
                console.error('Missing fund code or query ID for rerun');
                // Optionally display an error to the user near the button/row
                return;
            }
            // Provide visual feedback
            button.disabled = true;
            button.textContent = 'Running...';
            // You could also add a temporary status cell or highlight the row
            try {
                const response = await fetch('/rerun-api-call', { // New endpoint needed
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                    },
                    body: JSON.stringify({
                        query_id: queryId,
                        // Send fundCode as a list in the 'funds' key
                        funds: [fundCode],
                        days_back: parseInt(daysBack, 10),
                        end_date: endDate
                    })
                });
                const result = await response.json();
                // Use actual_rows/actual_lines as default, fallback to simulated if not present
                if (response.ok && result.status && result.status.includes('OK')) {
                    cells[2].textContent = result.actual_rows !== undefined ? result.actual_rows : (result.simulated_rows !== undefined ? result.simulated_rows : 'N/A');
                    cells[3].textContent = result.actual_lines !== undefined ? result.actual_lines : (result.simulated_lines !== undefined ? result.simulated_lines : 'N/A');
                    cells[4].innerHTML = `<span class="badge bg-success">${result.status}</span>`;
                    button.textContent = 'Rerun Success';
                    setTimeout(() => { button.textContent = 'Rerun'; }, 2000);
                } else {
                    cells[4].innerHTML = `<span class="badge bg-danger">Error</span>`;
                    console.error("Rerun failed:", result.message || 'Unknown error');
                    button.textContent = 'Rerun Failed';
                    setTimeout(() => { button.textContent = 'Rerun'; }, 3000);
                }
            } catch (error) {
                console.error("Rerun fetch error:", error);
                cells[4].innerHTML = `<span class="badge bg-danger">Network Error</span>`;
                button.textContent = 'Rerun Error';
                setTimeout(() => { button.textContent = 'Rerun'; }, 3000);
            } finally {
                button.disabled = false;
            }
        }
    });
    // Add event listener for the new Cleanup button
    cleanupButton.addEventListener('click', async function() {
        cleanupStatus.textContent = 'Starting cleanup process...';
        cleanupStatus.className = 'mt-2 alert alert-info';
        cleanupButton.disabled = true;
        try {
            const response = await fetch('/run-cleanup', {
                method: 'POST',
                 headers: {
                    'Content-Type': 'application/json',
                },
            });
            const result = await response.json();
            if (response.ok && result.status === 'success') {
                cleanupStatus.textContent = `Cleanup process finished successfully. Output:\n${result.output}`;
                cleanupStatus.className = 'mt-2 alert alert-success';
            } else {
                 cleanupStatus.textContent = `Cleanup process failed. Error:\n${result.error || result.message || 'Unknown error'}`;
                 cleanupStatus.className = 'mt-2 alert alert-danger';
            }
        } catch (error) {
            console.error("Cleanup fetch error:", error);
            cleanupStatus.textContent = 'Failed to trigger cleanup process. Network error or server unreachable.';
            cleanupStatus.className = 'mt-2 alert alert-danger';
        } finally {
             cleanupButton.disabled = false;
        }
    });
});
</script>
<!-- Scheduled API Calls JS -->
<script>
document.addEventListener('DOMContentLoaded', function() {
    const scheduleForm = document.getElementById('schedule-form');
    const scheduleTableBody = document.querySelector('#schedule-table tbody');
    const scheduleError = document.getElementById('schedule-error');
    const dateModeSelect = document.getElementById('scheduleDateMode');
    const daysBackContainer = document.getElementById('scheduleDaysBackContainer');
    const datesContainer = document.getElementById('scheduleDatesContainer');
    function toggleScheduleDateFields() {
        if (dateModeSelect.value === 'range') {
            datesContainer.style.display = '';
            daysBackContainer.style.display = 'none';
        } else {
            datesContainer.style.display = 'none';
            daysBackContainer.style.display = '';
        }
    }
    dateModeSelect.addEventListener('change', toggleScheduleDateFields);
    toggleScheduleDateFields();
    async function loadSchedules() {
        try {
            const res = await fetch('{{ url_for("api_bp.list_schedules") }}');
            const data = await res.json();
            scheduleTableBody.innerHTML = '';
            data.forEach(schedule => {
                const funds = schedule.funds.join(', ');
                const params = schedule.date_mode === 'quick'
                    ? `Days Back: ${schedule.days_back}, End Date: ${schedule.end_date}`
                    : `Start: ${schedule.start_date}, End: ${schedule.custom_end_date}`;
                const row = `
                    <tr data-id="${schedule.id}">
                        <td>${schedule.id}</td>
                        <td>${schedule.schedule_time}</td>
                        <td>${funds}</td>
                        <td>${schedule.date_mode}</td>
                        <td>${params}</td>
                        <td>${schedule.write_mode}</td>
                        <td><button class="btn btn-sm btn-danger delete-schedule">Delete</button></td>
                    </tr>`;
                scheduleTableBody.innerHTML += row;
            });
        } catch (e) {
            console.error('Error loading schedules', e);
        }
    }
    scheduleForm.addEventListener('submit', async function(e) {
        e.preventDefault();
        scheduleError.classList.add('d-none');
        const time = document.getElementById('scheduleTime').value;
        const writeMode = document.getElementById('scheduleWriteMode').value;
        const dateMode = dateModeSelect.value;
        const daysBack = document.getElementById('scheduleDaysBack').value;
        const endDate = document.getElementById('endDate').value;
        const startDate = document.getElementById('scheduleStartDate').value;
        const customEndDate = document.getElementById('scheduleEndDateCustom').value;
        const fundCheckboxes = document.querySelectorAll('.schedule-fund-checkbox:checked');
        if (!time || fundCheckboxes.length === 0 || (dateMode === 'quick' && !endDate) || (dateMode === 'range' && (!startDate || !customEndDate))) {
            scheduleError.textContent = 'Please fill out all fields and select at least one fund.';
            scheduleError.classList.remove('d-none');
            return;
        }
        const funds = Array.from(fundCheckboxes).map(cb => cb.value);
        const payload = { schedule_time: time, write_mode: writeMode, date_mode: dateMode, funds };
        if (dateMode === 'quick') { payload.days_back = parseInt(daysBack,10); payload.end_date = endDate; }
        else { payload.start_date = startDate; payload.custom_end_date = customEndDate; }
        try {
            const res = await fetch('{{ url_for("api_bp.add_schedule") }}', {
                method: 'POST', headers: { 'Content-Type': 'application/json' }, body: JSON.stringify(payload)
            });
            if (res.ok) {
                scheduleForm.reset();
                toggleScheduleDateFields();
                await loadSchedules();
            } else {
                const err = await res.json();
                scheduleError.textContent = err.message || 'Failed to add schedule.';
                scheduleError.classList.remove('d-none');
            }
        } catch (e) {
            console.error('Error adding schedule', e);
        }
    });
    scheduleTableBody.addEventListener('click', async function(e) {
        if (e.target.classList.contains('delete-schedule')) {
            const row = e.target.closest('tr');
            const id = row.dataset.id;
            if (!confirm('Delete schedule #' + id + '?')) return;
            try {
                const res = await fetch(`{{ url_for("api_bp.delete_schedule", schedule_id=0) }}`.replace('0', id), { method: 'DELETE' });
                if (res.ok) loadSchedules();
            } catch (e) {
                console.error('Error deleting schedule', e);
            }
        }
    });
    loadSchedules();
});
</script>
{% endblock %}
</file>

<file path="templates/index.html">
{% extends "base.html" %}
{% block title %}Data Verification Dashboard{% endblock %}
{% block content %}
    <div class="jumbotron mt-4"> {# Added mt-4 for spacing below fixed navbar #}
        <h1>Dashboard</h1>
        <p class="lead">Select a metric below to view the detailed checks, or see the latest Z-Score summary below.</p>
        <div class="row row-cols-1 row-cols-md-2 row-cols-lg-3 g-4">
            {% for metric in metrics %}
            <div class="col">
                <div class="card h-100 metric-card">
                    <div class="card-body d-flex flex-column">
                        <h5 class="card-title">{{ metric }}</h5>
                        <p class="card-text flex-grow-1">View details for {{ metric }}.</p>
                        {# Generate the URL using url_for #}
                        {% set metric_url = url_for('metric.metric_page', metric_name=metric) %}
                        <a href="{{ metric_url }}" class="btn btn-primary metric-link">View Details</a>
                        {# Debug: Display the generated URL #}
                        <span class="text-muted small mt-1">Debug URL: {{ metric_url }}</span>
                    </div>
                </div>
            </div>
            {% endfor %}
        </div>
    </div>
    <!-- Z-Score Summary Table -->
    {% if not summary_data.empty %}
    <h2>Latest Change Z-Score Summary</h2>
    <div class="table-responsive"> <!-- Make table scrollable on small screens -->
        <table class="table table-striped table-bordered table-hover table-sm">
            <thead class="thead-light"> {# thead-light might not be standard BS5, but harmless #}
                <tr>
                    <th>Fund Code</th>
                    {# Use the new summary_metrics list which contains combined names #}
                    {% for full_metric_name in summary_metrics %}
                    <th>{{ full_metric_name }}</th>
                    {% endfor %}
                </tr>
            </thead>
            <tbody>
                {% for fund_code, row in summary_data.iterrows() %}
                <tr>
                    {# Corrected url_for to point to the general fund detail page #}
                    <td><a href="{{ url_for('fund.fund_detail', fund_code=fund_code) }}" title="View all metrics for {{ fund_code }}">{{ fund_code }}</a></td>
                    {# Iterate through the same new list for data access #}
                    {% for full_metric_name in summary_metrics %}
                        {% set z_score = row[full_metric_name] %}
                        {% if z_score is none or z_score != z_score %}
                            {# Handle NaN/None - use base.html styling implicitly #}
                            <td class="text-muted fst-italic">N/A</td> {# Using BS5 classes #}
                        {% else %}
                            {# Apply conditional styling based on Z-score value - Use classes defined in base.html #}
                            {% set z_abs = z_score|abs %}
                            {% set cell_class = '' %}
                            {% if z_abs > 3.0 %}
                                {% set cell_class = 'table-danger' %}
                            {% elif z_abs > 2.0 %}
                                {% set cell_class = 'table-warning' %}
                            {% endif %}
                            <td class="{{ cell_class }}">{{ "%.2f"|format(z_score) }}</td>
                        {% endif %}
                    {% endfor %}
                </tr>
                {% endfor %}
            </tbody>
        </table>
    </div>
    {% else %}
    <div class="alert alert-warning" role="alert">
        No Z-score data could be generated for the summary table. Check the console logs for errors.
    </div>
    {% endif %}
    <!-- Add a link to the new API Data Retrieval page - Kept original url_for -->
    <div class="mt-4 mb-4 p-3 border rounded bg-light">
        <h5>Get Data via API (Simulated)</h5>
        <p>Select funds and dates to simulate retrieving data from the Rex API.</p>
        <a href="{{ url_for('api_bp.get_data_page') }}" class="btn btn-success btn-sm">Go to Get Data Page</a>
    </div>
    <!-- Add a link to the Securities page - url_for already matches base.html -->
    <div class="mt-4 p-3 border rounded bg-light">
        <h5>Securities Data Check</h5>
        <p>View checks for individual securities based on latest daily changes.</p>
        <a href="{{ url_for('security.securities_page') }}" class="btn btn-info btn-sm">View Securities Check</a>
    </div>
{% endblock %}
{% block scripts %}
{# Add any page-specific scripts here if needed in the future #}
{% endblock %}
</file>

<file path="templates/issues_page.html">
{% extends 'base.html' %}
{% block title %}Track Data Issues{% endblock %}
{% block content %}
<div class="container mt-4">
    <h2>Track Data Issues</h2>
    <hr>
    {# Display messages if any (using Flask flashing) #}
    {% with messages = get_flashed_messages(with_categories=true) %}
      {% if messages %}
        {% for category, message in messages %}
          <div class="alert alert-{{ category }} alert-dismissible fade show" role="alert">
            {{ message }}
            <button type="button" class="btn-close" data-bs-dismiss="alert" aria-label="Close"></button>
          </div>
        {% endfor %}
      {% endif %}
    {% endwith %}
    <div class="row">
        {# Left Column: Add New Issue Form #}
        <div class="col-md-5">
            <h4>Raise New Issue</h4>
            <form method="POST" action="{{ url_for('issue_bp.manage_issues') }}">
                <div class="mb-3">
                    <label for="raised_by" class="form-label">Raised By (Your Name/ID):</label>
                    <input type="text" class="form-control" id="raised_by" name="raised_by" required>
                </div>
                <div class="mb-3">
                    <label for="issue_date" class="form-label">Date of Issue:</label>
                    <input type="date" class="form-control" id="issue_date" name="issue_date" required>
                </div>
                 <div class="mb-3">
                    <label for="fund_impacted" class="form-label">Fund Impacted:</label>
                    <select class="form-select" id="fund_impacted" name="fund_impacted" required>
                        <option value="" disabled selected>Select a Fund</option>
                        {% for fund_code in available_funds %}
                            <option value="{{ fund_code }}">{{ fund_code }}</option>
                        {% endfor %}
                    </select>
                </div>
                 <div class="mb-3">
                    <label class="form-label">Data Source:</label><br>
                    {% for source in data_sources %}
                        <div class="form-check form-check-inline">
                            <input class="form-check-input" type="radio" name="data_source" id="source_{{ source|lower }}" value="{{ source }}" required>
                            <label class="form-check-label" for="source_{{ source|lower }}">{{ source }}</label>
                        </div>
                    {% endfor %}
                </div>
                <div class="mb-3">
                    <label for="description" class="form-label">Description of Issue:</label>
                    <textarea class="form-control" id="description" name="description" rows="4" required></textarea>
                </div>
                <button type="submit" class="btn btn-primary">Raise Issue</button>
            </form>
        </div>
        {# Right Column: Display Open Issues #}
        <div class="col-md-7">
            <h4>Open Issues</h4>
            {% if open_issues %}
                <div class="table-responsive">
                <table class="table table-striped table-sm table-bordered">
                    <thead class="table-light">
                        <tr>
                            <th>ID</th>
                            <th>Raised</th>
                            <th>By</th>
                            <th>Issue Date</th>
                            <th>Fund</th>
                            <th>Source</th>
                            <th>Description</th>
                            <th>Action</th>
                        </tr>
                    </thead>
                    <tbody>
                        {% for issue in open_issues %}
                            <tr>
                                <td>{{ issue.IssueID }}</td>
                                <td>{{ issue.DateRaised.strftime('%Y-%m-%d') if issue.DateRaised else 'N/A' }}</td>
                                <td>{{ issue.RaisedBy }}</td>
                                <td>{{ issue.IssueDate.strftime('%Y-%m-%d') if issue.IssueDate else 'N/A' }}</td>
                                <td>{{ issue.FundImpacted }}</td>
                                <td>{{ issue.DataSource }}</td>
                                <td style="white-space: pre-wrap; word-wrap: break-word;">{{ issue.Description }}</td>
                                <td>
                                    {# Button to trigger modal for closing #}
                                    <button type="button" class="btn btn-success btn-sm" data-bs-toggle="modal" data-bs-target="#closeIssueModal-{{ issue.IssueID }}">
                                        Close
                                    </button>
                                    {# Modal for Closing Issue #}
                                    <div class="modal fade" id="closeIssueModal-{{ issue.IssueID }}" tabindex="-1" aria-labelledby="closeIssueModalLabel-{{ issue.IssueID }}" aria-hidden="true">
                                        <div class="modal-dialog">
                                            <div class="modal-content">
                                                <div class="modal-header">
                                                    <h5 class="modal-title" id="closeIssueModalLabel-{{ issue.IssueID }}">Close Issue: {{ issue.IssueID }}</h5>
                                                    <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
                                                </div>
                                                <form method="POST" action="{{ url_for('issue_bp.close_issue_route') }}">
                                                    <div class="modal-body">
                                                        <input type="hidden" name="issue_id" value="{{ issue.IssueID }}">
                                                        <div class="mb-3">
                                                            <label for="closed_by-{{ issue.IssueID }}" class="form-label">Closed By (Your Name/ID):</label>
                                                            <input type="text" class="form-control" id="closed_by-{{ issue.IssueID }}" name="closed_by" required>
                                                        </div>
                                                        <div class="mb-3">
                                                            <label for="resolution_comment-{{ issue.IssueID }}" class="form-label">Resolution Comment:</label>
                                                            <textarea class="form-control" id="resolution_comment-{{ issue.IssueID }}" name="resolution_comment" rows="3" required></textarea>
                                                        </div>
                                                    </div>
                                                    <div class="modal-footer">
                                                        <button type="button" class="btn btn-secondary" data-bs-dismiss="modal">Cancel</button>
                                                        <button type="submit" class="btn btn-success">Mark as Closed</button>
                                                    </div>
                                                </form>
                                            </div>
                                        </div>
                                    </div>
                                </td>
                            </tr>
                        {% endfor %}
                    </tbody>
                </table>
                </div>
            {% else %}
                <p>No open issues found.</p>
            {% endif %}
        </div>
    </div>
    <hr class="my-4">
    {# Section for Closed Issues #}
    <h4>Closed Issues</h4>
     {% if closed_issues %}
        <div class="table-responsive">
        <table class="table table-striped table-sm table-bordered table-secondary">
            <thead class="table-light">
                <tr>
                     <th>ID</th>
                     <th>Raised</th>
                     <th>By</th>
                     <th>Issue Date</th>
                     <th>Fund</th>
                     <th>Source</th>
                     <th>Description</th>
                     <th>Closed</th>
                     <th>By</th>
                     <th>Resolution</th>
                </tr>
            </thead>
            <tbody>
                {% for issue in closed_issues %}
                    <tr>
                        <td>{{ issue.IssueID }}</td>
                        <td>{{ issue.DateRaised.strftime('%Y-%m-%d') if issue.DateRaised else 'N/A' }}</td>
                        <td>{{ issue.RaisedBy }}</td>
                        <td>{{ issue.IssueDate.strftime('%Y-%m-%d') if issue.IssueDate else 'N/A' }}</td>
                        <td>{{ issue.FundImpacted }}</td>
                        <td>{{ issue.DataSource }}</td>
                        <td style="white-space: pre-wrap; word-wrap: break-word;">{{ issue.Description }}</td>
                        <td>{{ issue.DateClosed.strftime('%Y-%m-%d') if issue.DateClosed else 'N/A' }}</td>
                        <td>{{ issue.ClosedBy }}</td>
                        <td style="white-space: pre-wrap; word-wrap: break-word;">{{ issue.ResolutionComment }}</td>
                    </tr>
                {% endfor %}
            </tbody>
        </table>
        </div>
    {% else %}
        <p>No closed issues found.</p>
    {% endif %}
</div>
{% endblock %}
{% block scripts %}
{{ super() }} {# Include scripts from base.html #}
{# No specific JS needed for this page currently, modals are handled by Bootstrap #}
{% endblock %}
</file>

<file path="templates/metric_page_js.html">
<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <title>{{ metric_name }} Check (JS)</title>
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
    <!-- Include Chart.js and the date adapter -->
    <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-adapter-date-fns/dist/chartjs-adapter-date-fns.bundle.min.js"></script>
    <style>
        body { padding-top: 5rem; }
        .chart-container-wrapper { margin-bottom: 15px; padding: 10px; border: 1px solid #eee; }
        .chart-canvas { max-height: 400px; }
        .metrics-table { margin-top: 15px; margin-bottom: 25px; font-size: 0.9em; }
        .metrics-table th, .metrics-table td { padding: 4px 8px; border: 1px solid #dee2e6; }
        .missing-warning { color: red; font-weight: bold; }
        .high-z { background-color: #fff3cd; }
        .very-high-z { background-color: #f8d7da; font-weight: bold; }
    </style>
</head>
<body>
    <nav class="navbar navbar-expand-md navbar-dark bg-dark fixed-top">
        <a class="navbar-brand" href="/">Data Verification</a>
        <!-- Navbar content -->
    </nav>
    <main role="main" class="container-fluid">
        <h1>{{ metric_name }} Check</h1>
        <p>Latest Data Date: <strong>{{ latest_date }}</strong></p>
        <p>Charts sorted by the maximum absolute <strong>Change Z-Score</strong> across all columns for the fund (most deviation first).</p>
        <!-- Toggle Switch for S&P Data -->
        <div id="sp-toggle-container" class="form-group" style="display: none;"> <!-- Initially hidden, JS will show if needed -->
            <div class="form-check form-switch"><input class="form-check-input" type="checkbox" role="switch" id="toggleSpData">
                <label class="form-check-label" for="toggleSpData">Show S&P Comparison Data</label>
            </div>
        </div>
        <!-- End Toggle Switch -->
        {% if not missing_funds.empty %}
            <div class="alert alert-warning" role="alert">
                <strong>Warning:</strong> The following funds are missing data for the latest date ({{ latest_date }}):
                {{ missing_funds.index.tolist() | join(', ') }}
            </div>
        {% endif %}
        {% if error_message %}
        <div class="alert alert-danger" role="alert">
          {{ error_message }}
        </div>
        {% endif %}
        <!-- Data passed from Flask, embedded as JSON -->
        <script type="application/json" id="chartData">
            {{ charts_data_json | safe }}
        </script>
        <div id="chartsArea">
            <!-- Charts will be rendered here by JavaScript -->
        </div>
        <!-- END: Metrics Table -->
    </main>
    <!-- Link to the external JavaScript module -->
    <script type="module" src="{{ url_for('static', filename='js/main.js') }}"></script>
    <!-- Bootstrap JS (optional) -->
    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.5.4/dist/umd/popper.min.js"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"></script>
</body>
</html>
</file>

<file path="templates/securities_page.html">
lockdown-install.js:1 Removing unpermitted intrinsics
Alpha002:181 Initializing Chart with original data...
main.js:29 DOM fully loaded and parsed
{% extends 'base.html' %}
{% block title %}Security Data Check{% endblock %}
{% block head_extra %}
  {# Link the new CSS file #}
  <link rel="stylesheet" href="{{ url_for('static', filename='css/style.css') }}">
{% endblock %}
{% block content %}
<div class="container-fluid mt-4"> {# Use container-fluid for wider tables #}
    <div class="d-flex justify-content-between align-items-center mb-3">
        <h2>Security Data Check</h2>
        <a href="{{ url_for('exclusion_bp.manage_exclusions') }}" class="btn btn-outline-secondary btn-sm">Manage Exclusions</a>
    </div>
    <p class="text-muted">Potential data issues based on the latest daily change Z-score. Filters and sorting are applied server-side.</p>
    {% if message %}
    <div class="alert alert-warning alert-dismissible fade show" role="alert">
        {{ message }}
        <button type="button" class="btn-close" data-bs-dismiss="alert" aria-label="Close"></button>
    </div>
    {% endif %}
    {# --- Search and Filter Form --- #}
    {# Combined into one form submitting GET requests #}
    <form method="GET" action="{{ url_for('security.securities_page') }}" class="mb-3 p-3 border rounded bg-light" id="filter-form">
        <div class="row g-2 align-items-end">
            {# Search Box #}
            <div class="col-md-4">
                <label for="search_term" class="form-label">Search by {{ id_col_name }}</label>
                <input type="text" name="search_term" id="search_term" class="form-control form-control-sm" placeholder="Enter search term..." value="{{ search_term or '' }}">
            </div>
            {# Dynamic Filters #}
            {% if filter_options %}
                {% for column, options in filter_options.items() %}
                <div class="col-md-2">
                    <label for="filter-{{ column|replace(' ', '_') }}" class="form-label">{{ column }}</label>
                    <select id="filter-{{ column|replace(' ', '_') }}" name="filter_{{ column }}" class="form-select form-select-sm">
                        <option value="">All</option>
                        {% for option in options %}
                        <option value="{{ option }}" {% if active_filters.get(column) == option|string %}selected{% endif %}>{{ option }}</option>
                        {% endfor %}
                    </select>
                </div>
                {% endfor %}
            {% endif %}
            {# Buttons #}
            <div class="col-md-auto">
                <button class="btn btn-primary btn-sm" type="submit">Apply Filters</button>
                {# Clear button redirects to the base URL without filters/search #}
                 {% if search_term or active_filters %}
                    <a href="{{ url_for('security.securities_page') }}" class="btn btn-secondary btn-sm">Clear All</a>
                {% endif %}
            </div>
        </div>
        {# Hidden fields to preserve pagination/sorting state if needed (though typically sorting/filtering resets page to 1) #}
        {# <input type="hidden" name="sort_by" value="{{ current_sort_by }}"> #}
        {# <input type="hidden" name="sort_order" value="{{ current_sort_order }}"> #}
    </form>
    {# --- Data Table Section --- #}
    {% if securities_data %}
    <div class="table-responsive">
        <table class="table table-striped table-hover table-sm small caption-top" id="securities-table">
            {# Add table caption for summary #}
            {% if pagination %}
            <caption class="pb-1">
                Displaying {{ securities_data|length }} of {{ pagination.total_items }} total securities. 
                (Page {{ pagination.page }} of {{ pagination.total_pages }})
            </caption>
            {% endif %}
            <thead class="table-light sticky-top"> {# Make header sticky #}
                <tr>
                    {# Generate sortable headers #}
                    {% for col_name in column_order %}
                        {% set is_sort_col = (col_name == current_sort_by or (col_name == 'Change Z-Score' and current_sort_by is none)) %}
                        {# Determine next sort order: flip if current column, default to asc otherwise #}
                        {% set next_sort_order = 'asc' if is_sort_col and current_sort_order == 'desc' else 'desc' %}
                        {# Base arguments, including current search and filters #}
                        {% set sort_args = request.args.to_dict() %}
                        {% set _ = sort_args.pop('page', None) %}
                        {% set _ = sort_args.update({'sort_by': col_name, 'sort_order': next_sort_order}) %}
                        {# Construct the URL for the sort link #}
                        {% set sort_url = url_for('security.securities_page', **sort_args) %}
                        {# Add classes for styling and potential JS hooks #}
                        <th class="sortable {{ 'sorted-' + current_sort_order if is_sort_col else '' }}" 
                            data-column-name="{{ col_name }}">
                            <a href="{{ sort_url }}" class="text-decoration-none text-dark">
                                {{ col_name }} 
                                {% if is_sort_col %}
                                    <span class="sort-indicator ms-1">{{ '▲' if current_sort_order == 'asc' else '▼' }}</span>
                                {% endif %}
                            </a>
                        </th>
                    {% endfor %}
                </tr>
            </thead>
            <tbody id="securities-table-body">
                {# Add an empty spacer row to prevent header overlap with first data row #}
                <tr class="spacer-row" style="height: 45px; background: transparent; border: none;">
                    <td colspan="{{ column_order|length }}" style="padding: 0; border: none;"></td>
                </tr>
                {% for row in securities_data %}
                    {% set z_score = row['Change Z-Score'] %}
                    {% set abs_z_score = z_score|abs if z_score is not none else 0 %} {# Calculate here #}
                    {% set row_class = 'table-danger' if abs_z_score >= 3 else ('table-warning' if abs_z_score >= 2 else '') %}
                    <tr class="{{ row_class }}">
                        {% for col_name in column_order %}
                            <td>
                                {# Link for ID column (which should now be ISIN) #}
                                {% if col_name == id_col_name %}
                                    {# The row object should contain the ISIN value under the key specified by id_col_name #}
                                    <a href="{{ url_for('security.security_details', metric_name='Spread', security_id=row[id_col_name]|urlencode) }}">
                                        {{ row[id_col_name] }}
                                    </a>
                                {# Formatting for numeric columns #}
                                {% elif col_name == 'Change Z-Score' and row[col_name] is not none %}
                                    {{ "%.2f"|format(row[col_name]) }}
                                {% elif row[col_name] is number %}
                                    {# Consider if specific formatting is needed for other numbers #}
                                    {{ row[col_name]|round(3) }}
                                {# Display other values (string, None) #}
                                {% else %}
                                    {{ row[col_name] if row[col_name] is not none else '' }} {# Display empty for None #}
                                {% endif %}
                            </td>
                        {% endfor %}
                    </tr>
                {% endfor %}
            </tbody>
        </table>
    </div>
    {# --- Pagination Controls --- #}
    {% if pagination and pagination.total_pages > 1 %}
        <nav aria-label="Security data navigation">
            <ul class="pagination pagination-sm justify-content-center">
                {# Previous Page Link #}
                <li class="page-item {{ 'disabled' if not pagination.has_prev }}">
                    <a class="page-link" href="{{ pagination.url_for_page(pagination.prev_num) if pagination.has_prev else '#' }}" aria-label="Previous">
                        <span aria-hidden="true">&laquo;</span>
                    </a>
                </li>
                {# Page Number Links (using context variables calculated in view) #}
                 {% set start_page = pagination.start_page_display %}
                 {% set end_page = pagination.end_page_display %}
                 {% if start_page > 1 %}
                     <li class="page-item"><a class="page-link" href="{{ pagination.url_for_page(1) }}">1</a></li>
                     {% if start_page > 2 %}
                         <li class="page-item disabled"><span class="page-link">...</span></li>
                     {% endif %}
                 {% endif %}
                 {% for p in range(start_page, end_page + 1) %}
                    <li class="page-item {{ 'active' if p == pagination.page }}">
                        <a class="page-link" href="{{ pagination.url_for_page(p) }}">{{ p }}</a>
                    </li>
                {% endfor %}
                 {% if end_page < pagination.total_pages %}
                     {% if end_page < pagination.total_pages - 1 %}
                         <li class="page-item disabled"><span class="page-link">...</span></li>
                     {% endif %}
                     <li class="page-item"><a class="page-link" href="{{ pagination.url_for_page(pagination.total_pages) }}">{{ pagination.total_pages }}</a></li>
                 {% endif %}
                {# Next Page Link #}
                <li class="page-item {{ 'disabled' if not pagination.has_next }}">
                    <a class="page-link" href="{{ pagination.url_for_page(pagination.next_num) if pagination.has_next else '#' }}" aria-label="Next">
                        <span aria-hidden="true">&raquo;</span>
                    </a>
                </li>
            </ul>
        </nav>
    {% endif %}
    {# Handle case where filters resulted in no data (message displayed above) #}
    {% elif not message %}
     <div class="alert alert-info mt-3" role="alert">
        No security metrics data is currently available or matches the selected criteria.
    </div>
    {% endif %}
</div>
{% endblock %}
{% block scripts %}
{{ super() }}
{# Remove client-side filter script block - All filtering/sorting is server-side #}
{# Optional: Add JS for minor enhancements like highlighting sort column, but core logic is server-side #}
{% endblock %}
</file>

<file path="templates/security_details_page.html">
{% extends 'base.html' %}
{% block title %}Security Details: {{ security_id }} - {{ metric_name }}{% endblock %}
{% block content %}
<div class="container mt-4">
    <h1>{{ security_id }} - {{ metric_name }}</h1>
    <p>Latest data as of: <strong>{{ latest_date }}</strong></p>
    {# Display Static Info #}
    {% if static_info %}
    <div class="mb-3 p-3 border rounded bg-light small">
        <strong>Static Details:</strong> 
        {% for key, value in static_info.items() %}
            <span class="me-3">{{ key }}: {{ value }}</span>
        {% endfor %}
    </div>
    {% endif %}
    {# Primary Chart Area (Metric + Price Overlay) #}
    <h2>{{ metric_name }} and Price Time Series</h2>
    <div id="primary-chart-container" class="mb-4" style="height: 450px; position: relative;">
        <canvas id="primarySecurityChart"></canvas>
    </div>
    {# Duration Chart Area #}
    <h2>Duration Time Series</h2>
    <div id="duration-chart-container" class="mb-4" style="height: 450px; position: relative;">
        <canvas id="durationSecurityChart"></canvas>
    </div>
    {# Spread Duration Chart Area #}
    <h2>Spread Duration Time Series</h2>
    <div id="spread-duration-chart-container" class="mb-4" style="height: 450px; position: relative;">
        <canvas id="spreadDurationChart"></canvas>
    </div>
    {# Spread Chart Area #}
    <h2>Spread Time Series</h2>
    <div id="spread-chart-container" class="mb-4" style="height: 450px; position: relative;">
        <canvas id="spreadChart"></canvas>
    </div>
    {# Hidden script tag for chart data #}
    <script id="chartJsonData" type="application/json">
        {{ chart_data_json|safe }}
    </script>
</div>
{% endblock %}
{% block scripts %}
{# Include Chart.js library (Likely inherited from base.html, but included here for safety/explicitness if needed) #}
{# If base.html already includes it, this line can be removed #}
<script src="https://cdn.jsdelivr.net/npm/chart.js"></script> 
<script>
    document.addEventListener('DOMContentLoaded', function() {
        // Get the chart data passed from Flask
        const chartDataElement = document.getElementById('chartJsonData');
        const chartData = JSON.parse(chartDataElement.textContent);
        /* --- REMOVED: Helper function to replace 0 with null --- 
        const processDatasetData = (dataset) => {
            if (dataset && dataset.data) {
                dataset.data = dataset.data.map(value => (value === 0 ? null : value));
            }
            return dataset; // Return the processed dataset (or original if no data)
        };
        */
        // --- REMOVED: Processing datasets before creating charts --- 
        // No longer needed as backend uses NaN
        /*
        if (chartData.primary_datasets) {
            chartData.primary_datasets = chartData.primary_datasets.map(processDatasetData);
        }
        if (chartData.duration_dataset) {
            chartData.duration_dataset = processDatasetData(chartData.duration_dataset);
        }
        // Add processing for new datasets if they were added
        if (chartData.sp_duration_dataset) { chartData.sp_duration_dataset = processDatasetData(chartData.sp_duration_dataset); }
        if (chartData.spread_duration_dataset) { chartData.spread_duration_dataset = processDatasetData(chartData.spread_duration_dataset); }
        if (chartData.sp_spread_duration_dataset) { chartData.sp_spread_duration_dataset = processDatasetData(chartData.sp_spread_duration_dataset); }
        if (chartData.spread_dataset) { chartData.spread_dataset = processDatasetData(chartData.spread_dataset); }
        if (chartData.sp_spread_dataset) { chartData.sp_spread_dataset = processDatasetData(chartData.sp_spread_dataset); }
        */
        // --- Render Primary Chart (Metric + Price) --- 
        const primaryCtx = document.getElementById('primarySecurityChart').getContext('2d');
        if (chartData.primary_datasets && chartData.primary_datasets.length > 0) {
            // Check if *any* primary dataset has actual data (nulls are ignored by this check)
            const hasPrimaryData = chartData.primary_datasets.some(dataset => 
                dataset.data && dataset.data.some(value => value !== null)
            );
            if (hasPrimaryData) {
                new Chart(primaryCtx, {
                    type: 'line',
                    data: {
                        labels: chartData.labels,
                        datasets: chartData.primary_datasets // Use datasets directly
                    },
                    options: {
                        responsive: true,
                        maintainAspectRatio: false,
                        spanGaps: true, // Let Chart.js handle gaps where data is null
                        scales: {
                            x: {
                                title: {
                                    display: true,
                                    text: 'Date'
                                }
                            },
                            y: { // Primary Y-axis (for the main metric)
                                position: 'left',
                                title: {
                                    display: true,
                                    text: {{ metric_name|tojson }} + ' Value' 
                                }
                            },
                            y1: { // Secondary Y-axis (for Price)
                                position: 'right',
                                title: {
                                    display: true,
                                    text: 'Price'
                                },
                                grid: {
                                    drawOnChartArea: false, 
                                },
                            }
                        },
                        plugins: {
                            legend: {
                                position: 'top',
                            },
                            tooltip: {
                                mode: 'index',
                                intersect: false,
                            }
                        }
                    }
                });
            } else {
                 document.getElementById('primary-chart-container').innerHTML = '<p class="text-info">No primary data available for this security.</p>';
            }
        } else {
            console.error('No primary datasets found for the primary chart.');
            document.getElementById('primary-chart-container').innerHTML = '<p class="text-danger">Could not render primary chart: No data available.</p>';
        }
        // --- Render Duration Chart (Now includes SP) ---
        const durationCtx = document.getElementById('durationSecurityChart').getContext('2d');
        const durationDatasets = [];
        if (chartData.duration_dataset) {
            // Check if there's actual data (not null)
            if (chartData.duration_dataset.data && chartData.duration_dataset.data.some(value => value !== null)) {
                durationDatasets.push(chartData.duration_dataset);
            }
        }
        if (chartData.sp_duration_dataset) {
             if (chartData.sp_duration_dataset.data && chartData.sp_duration_dataset.data.some(value => value !== null)) {
                durationDatasets.push(chartData.sp_duration_dataset);
            }
        }
        if (durationDatasets.length > 0) {
            new Chart(durationCtx, {
                type: 'line',
                data: {
                    labels: chartData.labels,
                    datasets: durationDatasets // Use combined list
                },
                options: {
                    responsive: true,
                    maintainAspectRatio: false,
                    spanGaps: true, 
                    scales: {
                        x: { title: { display: true, text: 'Date' } },
                        y: { title: { display: true, text: 'Duration Value' } }
                    },
                    plugins: { /* legend, tooltip */ }
                }
            });
        } else {
             console.log('No non-null data points found for Duration or SP Duration chart.');
             document.getElementById('duration-chart-container').innerHTML = '<p class="text-info">No duration data available for this security.</p>';
        }
        // --- Render Spread Duration Chart (Includes SP) ---
        const spreadDurationCtx = document.getElementById('spreadDurationChart').getContext('2d');
        const spreadDurationDatasets = [];
        if (chartData.spread_duration_dataset && chartData.spread_duration_dataset.data && chartData.spread_duration_dataset.data.some(v => v !== null)) {
            spreadDurationDatasets.push(chartData.spread_duration_dataset);
        }
        if (chartData.sp_spread_duration_dataset && chartData.sp_spread_duration_dataset.data && chartData.sp_spread_duration_dataset.data.some(v => v !== null)) {
            spreadDurationDatasets.push(chartData.sp_spread_duration_dataset);
        }
        if (spreadDurationDatasets.length > 0) {
            new Chart(spreadDurationCtx, {
                type: 'line',
                data: { labels: chartData.labels, datasets: spreadDurationDatasets },
                options: { 
                    responsive: true, maintainAspectRatio: false, spanGaps: true, 
                    scales: { x: { title: { display: true, text: 'Date' } }, y: { title: { display: true, text: 'Spread Duration Value' } } },
                    plugins: { /* legend, tooltip */ }
                }
            });
        } else {
            console.log('No data found for Spread Duration or SP Spread Duration chart.');
            document.getElementById('spread-duration-chart-container').innerHTML = '<p class="text-info">Spread Duration data not available for this security.</p>';
        }
        // --- Render Spread Chart (Includes SP) ---
        const spreadCtx = document.getElementById('spreadChart').getContext('2d');
        const spreadDatasets = [];
        if (chartData.spread_dataset && chartData.spread_dataset.data && chartData.spread_dataset.data.some(v => v !== null)) {
            spreadDatasets.push(chartData.spread_dataset);
        }
        if (chartData.sp_spread_dataset && chartData.sp_spread_dataset.data && chartData.sp_spread_dataset.data.some(v => v !== null)) {
            spreadDatasets.push(chartData.sp_spread_dataset);
        }
        if (spreadDatasets.length > 0) {
            new Chart(spreadCtx, {
                type: 'line',
                data: { labels: chartData.labels, datasets: spreadDatasets },
                options: {
                    responsive: true, maintainAspectRatio: false, spanGaps: true, 
                    scales: { x: { title: { display: true, text: 'Date' } }, y: { title: { display: true, text: 'Spread Value' } } },
                    plugins: { /* legend, tooltip */ }
                }
            });
        } else {
             console.log('No data found for Spread or SP Spread chart.');
             document.getElementById('spread-chart-container').innerHTML = '<p class="text-info">Spread data not available for this security.</p>';
        }
    });
</script>
{% endblock %}
</file>

<file path="templates/spread_duration_comparison_details_page.html">
{% extends "base.html" %}
{# Note: security_name is not explicitly passed, using security_id for title/breadcrumb #}
{% block title %}Spread Duration Comparison Details: {{ security_id }}{% endblock %}
{% block content %}
<div class="container mt-4">
    <nav aria-label="breadcrumb">
        <ol class="breadcrumb">
            <li class="breadcrumb-item"><a href="{{ url_for('spread_duration_comparison_bp.summary') }}">Spread Duration Comparison Summary</a></li> {# Updated Link #}
            {# Displaying ID as primary identifier if name isn't guaranteed #}
            <li class="breadcrumb-item active" aria-current="page">{{ security_id }}</li>
        </ol>
    </nav>
    <h1>Spread Duration Comparison Details: {{ security_id }}</h1> {# Updated Title #}
    {# Add static info if available #}
    {% if static_info %}
        {% for key, value in static_info.items() %}
             {% if key != id_column_name %} {# Avoid repeating the ID #}
                <span class="text-muted me-3"><strong>{{ key }}:</strong> {{ value }}</span>
             {% endif %}
        {% endfor %}
    {% endif %}
    <div class="row mt-4 mb-4">
        <div class="col-md-6">
            <h2>Comparison Statistics</h2>
            {% if stats_summary %}
                <ul class="list-group">
                    <li class="list-group-item d-flex justify-content-between align-items-center">
                        Level Correlation
                        <span class="badge bg-primary rounded-pill">{{ stats_summary.Level_Correlation if stats_summary.Level_Correlation is not none else 'N/A' }}</span>
                    </li>
                    <li class="list-group-item d-flex justify-content-between align-items-center">
                        Change Correlation
                        <span class="badge bg-primary rounded-pill">{{ stats_summary.Change_Correlation if stats_summary.Change_Correlation is not none else 'N/A' }}</span>
                    </li>
                    <li class="list-group-item d-flex justify-content-between align-items-center">
                        Mean Absolute Difference
                        <span class="badge bg-secondary rounded-pill">{{ stats_summary.Mean_Abs_Diff if stats_summary.Mean_Abs_Diff is not none else 'N/A' }}</span>
                    </li>
                    <li class="list-group-item d-flex justify-content-between align-items-center">
                        Max Absolute Difference
                        <span class="badge bg-secondary rounded-pill">{{ stats_summary.Max_Abs_Diff if stats_summary.Max_Abs_Diff is not none else 'N/A' }}</span>
                    </li>
                    <li class="list-group-item d-flex justify-content-between align-items-center">
                        Data Points (Original)
                        <span class="badge bg-info rounded-pill">{{ stats_summary.Total_Points - stats_summary.NaN_Count_Orig }} / {{ stats_summary.Total_Points }}</span>
                    </li>
                    <li class="list-group-item d-flex justify-content-between align-items-center">
                        Data Points (New)
                        <span class="badge bg-info rounded-pill">{{ stats_summary.Total_Points - stats_summary.NaN_Count_New }} / {{ stats_summary.Total_Points }}</span>
                    </li>
                    <li class="list-group-item d-flex justify-content-between align-items-center">
                        Same Date Range?
                        <span class="badge {{ 'bg-success' if stats_summary.Same_Date_Range else 'bg-warning' }} rounded-pill">{{ 'Yes' if stats_summary.Same_Date_Range else 'No' }}</span>
                    </li>
                    {# Add date range details #}
                    <li class="list-group-item">
                        <small>Orig Range: {{ stats_summary.Start_Date_Orig or 'N/A' }} to {{ stats_summary.End_Date_Orig or 'N/A' }}</small><br>
                        <small>New Range: {{ stats_summary.Start_Date_New or 'N/A' }} to {{ stats_summary.End_Date_New or 'N/A' }}</small>
                    </li>
                </ul>
            {% else %}
                <p>No comparison statistics could be calculated.</p>
            {% endif %}
        </div>
        {# Placeholder for additional stats or info if needed #}
    </div>
    <h2>Time Series Comparison</h2>
    <p class="text-muted">Overlayed Spread Duration from Original (sec_Spread duration) and New (sec_Spread durationSP) datasets.</p> {# Updated Text #}
    <div>
        <canvas id="comparisonChart"></canvas>
    </div>
    {# Embed chart data as JSON for JavaScript #}
    <script type="application/json" id="comparisonChartData">
        {{ chart_data | tojson | safe }}
    </script>
</div>
{% endblock %}
{% block scripts %}
{{ super() }}
{# We need Chart.js - ensure it's included in base.html or here #}
<script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
<script src="https://cdn.jsdelivr.net/npm/chartjs-adapter-date-fns/dist/chartjs-adapter-date-fns.bundle.min.js"></script> {# Include Date Adapter #}
<script>
    document.addEventListener('DOMContentLoaded', function() {
        const chartDataElement = document.getElementById('comparisonChartData');
        const comparisonChartCanvas = document.getElementById('comparisonChart');
        if (chartDataElement && comparisonChartCanvas) {
            try {
                const chartData = JSON.parse(chartDataElement.textContent);
                const ctx = comparisonChartCanvas.getContext('2d');
                console.log("Initializing Spread Duration Comparison Chart...");
                new Chart(ctx, {
                    type: 'line',
                    data: chartData, // Direct use of data from JSON
                    options: {
                        responsive: true,
                        maintainAspectRatio: true,
                        plugins: {
                            legend: {
                                position: 'top',
                            },
                            title: {
                                display: true,
                                text: 'Spread Duration Comparison: {{ security_id|tojson }}' // Use security_id
                            },
                            tooltip: {
                                mode: 'index', // Show tooltips for all datasets at the same index
                                intersect: false
                            }
                        },
                        scales: {
                            x: {
                                type: 'time', // Use time scale
                                time: {
                                    unit: 'day',
                                    tooltipFormat: 'yyyy-MM-dd', // Format for tooltip
                                    displayFormats: { // Formats for axis labels
                                        day: 'MMM d, yyyy'
                                    }
                                },
                                title: {
                                    display: true,
                                    text: 'Date'
                                }
                            },
                            y: {
                                title: {
                                    display: true,
                                    text: 'Spread Duration' // Updated Axis Label
                                }
                            }
                        },
                        interaction: {
                             intersect: false,
                             mode: 'index',
                        },
                         elements: {
                            point:{ // Reduce point size for potentially dense data
                                radius: 2
                            }
                        }
                        // No complex scriptable options needed for basic display
                    }
                });
            } catch (error) {
                console.error("Error parsing spread duration chart data or rendering chart:", error);
                if (comparisonChartCanvas.parentElement) {
                    comparisonChartCanvas.parentElement.innerHTML = '<p class="text-danger">Error rendering spread duration chart.</p>';
                }
            }
        } else {
             console.warn("Chart data or canvas element not found for spread duration comparison chart.");
        }
    });
</script>
{% endblock %}
</file>

<file path="templates/spread_duration_comparison_page.html">
{% extends "base.html" %}
{% block title %}Spread Duration Comparison Summary{% endblock %}
{% block content %}
<div class="container-fluid mt-4"> {# Use container-fluid for wider view #}
    <h1>Spread Duration Comparison: Original (sec_Spread duration) vs. New (sec_Spread durationSP)</h1> {# Updated Title #}
    <p class="text-muted">Comparing Spread Duration between the two datasets. Click on a Security ID/Name to see details. Use filters or click column headers to sort. Pagination applied.</p> {# Updated Text #}
    {# Display message if any #}
    {% if message %}
    <div class="alert alert-warning alert-dismissible fade show" role="alert">
        {{ message }}
        <button type="button" class="btn-close" data-bs-dismiss="alert" aria-label="Close"></button>
    </div>
    {% endif %}
    {# --- Filter Form --- #}
    {% if filter_options %}
    <form method="GET" action="{{ url_for('spread_duration_comparison_bp.summary') }}" class="mb-3 p-3 border rounded bg-light" id="filter-form"> {# Updated Action URL #}
        <h5>Filters</h5>
        <div class="row g-2 align-items-end">
            {% for column, options in filter_options.items() %}
            <div class="col-md-2 mb-2">
                <label for="filter-{{ column }}" class="form-label">{{ column }}</label>
                <select id="filter-{{ column }}" name="filter_{{ column }}" class="form-select form-select-sm">
                    <option value="">All</option>
                    {% for option in options %}
                    <option value="{{ option }}" {% if active_filters.get(column) == option|string %}selected{% endif %}>{{ option }}</option>
                    {% endfor %}
                </select>
            </div>
            {% endfor %}
            <div class="col-md-3 d-flex align-items-end">
                <div class="form-check form-switch mb-1">
                    <input class="form-check-input" type="checkbox" role="switch" id="showSoldToggle" name="show_sold" value="true" {% if show_sold %}checked{% endif %}>
                    <label class="form-check-label" for="showSoldToggle"><small>Show Sold Securities</small></label>
                </div>
            </div>
            <div class="col-md-auto">
                <button type="submit" class="btn btn-primary btn-sm">Apply Filters</button>
                {# Add a clear button only if filters are active #}
                {% if active_filters %}
                <a href="{{ url_for('spread_duration_comparison_bp.summary') }}" class="btn btn-secondary btn-sm">Clear Filters</a> {# Updated Clear URL #}
                {% endif %}
            </div>
        </div>
        {# Hidden fields to preserve current sort order when applying filters - page is implicitly reset #}
        <input type="hidden" name="sort_by" value="{{ current_sort_by }}">
        <input type="hidden" name="sort_order" value="{{ current_sort_order }}">
    </form>
    {% endif %}
    {# --- Data Table --- #}
    <div class="table-responsive">
        <table class="table table-striped table-hover table-sm caption-top" id="spread-duration-comparison-table"> {# Updated Table ID #}
             {# Add table caption for summary #}
             {% if pagination %}
             <caption class="pb-1">
                 Displaying {{ table_data|length }} of {{ pagination.total_items }} total securities.
                 (Page {{ pagination.page }} of {{ pagination.total_pages }})
             </caption>
             {% endif %}
            <thead class="table-light">
                <tr>
                    {# Loop through the columns passed from the view #}
                    {% for col_name in columns_to_display %}
                        {% set is_sort_col = (col_name == current_sort_by) %}
                        {% set next_sort_order = 'asc' if is_sort_col and current_sort_order == 'desc' else 'desc' %}
                        {# Base arguments, including current filters #}
                        {% set sort_args = request.args.to_dict() %}
                        {% set _ = sort_args.pop('page', None) %}
                        {% set _ = sort_args.update({'sort_by': col_name, 'sort_order': next_sort_order}) %}
                        {# Generate URL for this header #}
                        {% set sort_url = url_for('spread_duration_comparison_bp.summary', **sort_args) %} {# Updated Sort URL #}
                        {# Add classes for styling and JS #}
                        <th class="sortable {{ 'sorted-' + current_sort_order if is_sort_col else '' }}"
                            data-column-name="{{ col_name }}">
                            <a href="{{ sort_url }}" class="text-decoration-none text-dark">
                                {{ col_name.replace('_', ' ') | title }}
                                {% if is_sort_col %}
                                    <span class="sort-indicator ms-1">{{ '▲' if current_sort_order == 'asc' else '▼' }}</span>
                                {% endif %}
                            </a>
                        </th>
                    {% endfor %}
                </tr>
            </thead>
            <tbody id="spread-duration-comparison-table-body"> {# Updated tbody ID #}
                {% set id_col = id_column_name %}
                {% for row in table_data %}
                <tr>
                    {# Loop through the same columns to ensure order matches header #}
                    {% for col_name in columns_to_display %}
                        <td>
                            {% if col_name == id_col %}
                                <a href="{{ url_for('spread_duration_comparison_bp.spread_duration_comparison_details', security_id=row[id_col]|urlencode) }}">{{ row[id_col] }}</a> {# Updated Detail URL #}
                            {% elif col_name in ['Level_Correlation', 'Change_Correlation'] and row[col_name] is not none %}
                                {# Attempt to format as float, handle potential errors gracefully #}
                                {% set formatted_val = "%.3f"|format(row[col_name]|float) if row[col_name] is number else row[col_name] %}
                                {{ formatted_val }}
                            {% elif col_name in ['Mean_Abs_Diff', 'Max_Abs_Diff'] and row[col_name] is not none %}
                                {% set formatted_val = "%.2f"|format(row[col_name]|float) if row[col_name] is number else row[col_name] %}
                                {{ formatted_val }}
                             {% elif col_name == 'Same_Date_Range' %}
                                 <span class="badge {{ 'bg-success' if row[col_name] else 'bg-warning' }}">{{ 'Yes' if row[col_name] else 'No' }}</span>
                            {% elif col_name.endswith('_Date') and row[col_name] %}
                                 {# Assume date strings are already YYYY-MM-DD from view #}
                                {{ row[col_name] }}
                            {% elif row[col_name] is number %}
                                {{ row[col_name]|round(3) }} {# General numeric formatting #}
                            {% else %}
                                {{ row[col_name] if row[col_name] is not none else '' }} {# Display strings or empty #}
                            {% endif %}
                        </td>
                    {% endfor %}
                </tr>
                {% else %}
                <tr>
                    <td colspan="{{ columns_to_display|length }}" class="text-center">No spread duration comparison data available matching the current filters.</td> {# Updated Message #}
                </tr>
                {% endfor %}
            </tbody>
        </table>
    </div>
    {# --- Pagination Controls --- #}
    {% if pagination and pagination.total_pages > 1 %}
    <nav aria-label="Spread duration comparison data navigation">
        <ul class="pagination pagination-sm justify-content-center">
            {# Helper macro for generating pagination links #}
            {% macro page_link(page_num, text=None, is_disabled=False, is_active=False) %}
                {% set link_args = request.args.to_dict() %}
                {% set _ = link_args.update({'page': page_num, 'sort_by': current_sort_by, 'sort_order': current_sort_order}) %}
                {% set url = url_for('spread_duration_comparison_bp.summary', **link_args) if page_num else '#' %} {# Updated URL #}
                <li class="page-item {{ 'disabled' if is_disabled }} {{ 'active' if is_active }}">
                    <a class="page-link" href="{{ url }}" {% if is_active %}aria-current="page"{% endif %}>{{ text or page_num }}</a>
                </li>
            {% endmacro %}
            {{ page_link(pagination.prev_num, '&laquo;', is_disabled=not pagination.has_prev) }}
            {# Simplified pagination display logic #}
            {% set window = 2 %}
            {% set start_page = [1, pagination.page - window] | max %}
            {% set end_page = [pagination.total_pages, pagination.page + window] | min %}
            {% if start_page > 1 %}
                {{ page_link(1) }}
                {% if start_page > 2 %}
                    <li class="page-item disabled"><span class="page-link">...</span></li>
                {% endif %}
            {% endif %}
            {% for p in range(start_page, end_page + 1) %}
                {{ page_link(p, is_active=(p == pagination.page)) }}
            {% endfor %}
            {% if end_page < pagination.total_pages %}
                {% if end_page < pagination.total_pages - 1 %}
                    <li class="page-item disabled"><span class="page-link">...</span></li>
                {% endif %}
                {{ page_link(pagination.total_pages) }}
            {% endif %}
            {{ page_link(pagination.next_num, '&raquo;', is_disabled=not pagination.has_next) }}
        </ul>
    </nav>
    {% endif %}
</div>
{% endblock %}
{% block scripts %}
{{ super() }}
{# No specific JS needed for this page unless client-side sorting is added back #}
{% endblock %}
</file>

<file path="templates/weight_check_page.html">
{% extends "base.html" %}
{% block title %}Weight Check (100% Target){% endblock %}
{% block content %}
<div class="container-fluid mt-4">
    <h1 class="mb-4">Weight Check (100% Target)</h1>
    <p>The following tables show weights from <code>{{ fund_filename }}</code> and <code>{{ bench_filename }}</code>. Cells highlighted in <span class="text-danger fw-bold">red</span> indicate weights that are not exactly 100.00%.</p>
    {% macro render_weight_table(title, filename, data, date_headers) %}
        <h2 class="mt-4">{{ title }} <small class="text-muted fs-6">({{ filename }})</small></h2>
        {% if data %}
            <div class="table-responsive">
                <table class="table table-bordered table-sm table-hover weight-check-table">
                    <thead class="thead-light">
                        <tr>
                            <th>Fund Code</th>
                            {# Reverse the date headers so newest is first #}
                            {% for date in date_headers | reverse %}
                                <th class="text-center">{{ date }}</th>
                            {% endfor %}
                        </tr>
                    </thead>
                    <tbody>
                        {% for fund_code, date_values in data.items()|sort %}
                            <tr>
                                <td class="fw-bold">{{ fund_code }}</td>
                                {# Reverse the date headers again to match the header order #}
                                {% for date in date_headers | reverse %}
                                    {% set cell_data = date_values.get(date) %}
                                    {% if cell_data %}
                                        {% set value_percent_str = cell_data.value_percent_str %}
                                        {% set is_100 = cell_data.is_100 %}
                                        <td class="text-center {{ 'table-danger' if not is_100 }}">
                                            {{ value_percent_str }}
                                        </td>
                                    {% else %}
                                        <td class="text-center text-muted">-</td> {# Data missing for this date #}
                                    {% endif %}
                                {% endfor %}
                            </tr>
                        {% else %}
                            <tr>
                                <td colspan="{{ date_headers|length + 1 }}" class="text-center">No data found in {{ filename }}.</td>
                            </tr>
                        {% endfor %}
                    </tbody>
                </table>
            </div>
        {% else %}
            <div class="alert alert-warning" role="alert">
                Could not load or process data from <code>{{ filename }}</code>. Check server logs.
            </div>
        {% endif %}
    {% endmacro %}
    {{ render_weight_table("Fund Weights", fund_filename, fund_data, date_headers) }}
    {{ render_weight_table("Benchmark Weights", bench_filename, bench_data, date_headers) }}
</div>
<style>
/* Optional: Adjust table layout for wide tables */
.weight-check-table th,
.weight-check-table td {
    white-space: nowrap; /* Prevent wrapping in cells */
    font-size: 0.85rem; /* Slightly smaller font */
    padding: 0.3rem 0.5rem; /* Adjust padding */
}
.weight-check-table th:first-child,
.weight-check-table td:first-child {
    position: sticky;
    left: 0;
    background-color: #f8f9fa; /* Light background for sticky column */
    z-index: 1;
}
.weight-check-table thead th {
    position: sticky;
    top: 0; /* Stick headers to top */
    background-color: #e9ecef;
    z-index: 2;
}
</style>
{% endblock %}
</file>

<file path="utils.py">
# This file contains utility functions used throughout the Simple Data Checker application.
# These functions provide common helper functionalities like parsing specific string formats
# or validating data types, helping to keep the main application logic cleaner.
"""
Utility functions for the Flask application.
"""
import re
import pandas as pd
import os
import logging
# Configure logging
# Removed basicConfig - logging is now configured centrally in app.py
# logger = logging.getLogger(__name__) # Get logger for this module
DEFAULT_RELATIVE_PATH = 'Data'
def _is_date_like(column_name):
    """Check if a column name looks like a date (e.g., YYYY-MM-DD or DD/MM/YYYY).
    Updated regex to match both common formats.
    Ensures the pattern matches the entire string.
    """
    # Regex explanation:
    # ^            - Start of string
    # (\d{4}-\d{2}-\d{2}) - Group 1: YYYY-MM-DD format
    # |            - OR
    # (\d{2}/\d{2}/\d{4}) - Group 2: DD/MM/YYYY format
    # $            - End of string
    pattern = r'^((\d{4}-\d{2}-\d{2})|(\d{2}/\d{2}/\d{4}))$'
    return bool(re.match(pattern, str(column_name)))
def parse_fund_list(fund_string):
    """Safely parses the fund list string like '[FUND1,FUND2]' or '[FUND1]' into a list.
       Handles potential errors and variations in spacing.
    """
    if not isinstance(fund_string, str) or not fund_string.startswith('[') or not fund_string.endswith(']'):
        return [] # Return empty list if format is unexpected
    try:
        # Remove brackets and split by comma
        content = fund_string[1:-1]
        # Split by comma, strip whitespace from each element
        funds = [f.strip() for f in content.split(',') if f.strip()]
        return funds
    except Exception as e:
        print(f"Error parsing fund string '{fund_string}': {e}")
        return []
def get_data_folder_path(app_root_path=None):
    """
    Retrieves the data folder path, prioritizing config.py, then a default.
    Resolves the path to an absolute path relative to the provided
    app_root_path or the current working directory.
    Args:
        app_root_path (str, optional): The root path of the application or script.
                                      If None, os.getcwd() is used. Defaults to None.
    Returns:
        str: The absolute path to the data folder.
    """
    chosen_path_source = f"default ('{DEFAULT_RELATIVE_PATH}')"
    chosen_path = DEFAULT_RELATIVE_PATH
    try:
        # Attempt to import the path from config.py
        # This approach allows config.py to be optional or lack the variable
        from config import DATA_FOLDER
        if isinstance(DATA_FOLDER, str) and DATA_FOLDER.strip():
            chosen_path = DATA_FOLDER.strip()
            chosen_path_source = "config.py (DATA_FOLDER)"
        else:
            logging.warning(f"DATA_FOLDER in config.py is not a valid non-empty string. Falling back to default '{DEFAULT_RELATIVE_PATH}'.")
    except ImportError:
        logging.info("config.py not found or DATA_FOLDER not defined. Using default path.")
    except Exception as e:
        logging.error(f"An unexpected error occurred while trying to read DATA_FOLDER from config.py: {e}. Falling back to default.")
    # Determine the base path for resolving relative paths
    if app_root_path:
        base_path = app_root_path
        base_path_source = "provided app_root_path"
    else:
        # Use current working directory if no app_root_path is provided
        # This is suitable for standalone scripts run from the project root,
        # but can be unreliable otherwise. Providing app_root_path is safer.
        base_path = os.getcwd()
        base_path_source = "os.getcwd()"
        logging.warning(f"No app_root_path provided to get_data_folder_path. Using current working directory ({base_path}) as base. Ensure this is the intended behavior.")
    # Resolve the chosen path (relative or absolute) to an absolute path
    try:
        if os.path.isabs(chosen_path):
            absolute_path = chosen_path
            logging.info(f"Using absolute path from {chosen_path_source}: {absolute_path}")
        else:
            absolute_path = os.path.abspath(os.path.join(base_path, chosen_path))
            logging.info(f"Resolved relative path from {chosen_path_source} ('{chosen_path}') relative to {base_path_source} ('{base_path}') to absolute path: {absolute_path}")
        # Basic check: does the directory exist? Log a warning if not.
        # Consider adding creation logic if needed, but for now, just check.
        if not os.path.isdir(absolute_path):
             logging.warning(f"The determined data folder path does not exist or is not a directory: {absolute_path}")
        return absolute_path
    except Exception as e:
        logging.error(f"Failed to resolve data folder path '{chosen_path}' relative to '{base_path}'. Error: {e}. Falling back to default relative path '{DEFAULT_RELATIVE_PATH}' resolved against '{base_path}'.")
        # Fallback resolution in case of error during primary resolution
        try:
             fallback_path = os.path.abspath(os.path.join(base_path, DEFAULT_RELATIVE_PATH))
             logging.info(f"Using fallback absolute path: {fallback_path}")
             if not os.path.isdir(fallback_path):
                 logging.warning(f"The fallback data folder path does not exist or is not a directory: {fallback_path}")
             return fallback_path
        except Exception as final_e:
            logging.critical(f"CRITICAL: Failed even to resolve fallback path. Returning '.'. Error: {final_e}")
            # Absolute last resort
            return '.'
# Example usage (for testing purposes, typically called from app.py or scripts)
# if __name__ == '__main__':
#     # Simulate being called from an app context
#     script_dir = os.path.dirname(os.path.abspath(__file__))
#     project_root = os.path.dirname(script_dir) # Assuming utils.py is one level down from root
#     print(f"Simulating call with project root: {project_root}")
#     data_path_from_app = get_data_folder_path(app_root_path=project_root)
#     print(f"Data path (app context): {data_path_from_app}")
#
#     # Simulate being called from a standalone script without app_root_path
#     print("\nSimulating call without providing app_root_path (uses CWD):")
#     data_path_standalone = get_data_folder_path()
#     print(f"Data path (standalone context): {data_path_standalone}")
</file>

<file path="Version2.md">
Python Unit testing
Attribution Errors
Join comparison views .py files get rid of duplication
S&P vs Prod page
Fund Groups
Schedulers
Revamp look use tail winds, 538 style graphs
Speed up security level views, move processing to backend

S&P bad securities email
Monitor of missing S&P data
Add Security level checks - Min yield etc
Better handing in charts and zscoores when positioning is changing
get rid of div/0 debugs when looking at comparison
fund drop down on security should be a a list not a concat
add ability to link Jira tickets
Name Drop down on issues
redo navigation bar
Every PY file fhould be below 500 lines
Add a checks sum calculation for each security to see if needs rerunning.

New Features https://gemini.google.com/app/6c64ecfa19410785

Load up the dataframes in the background to speed up the page
Fix teh Status Bar
Add SQLite with enhanced file locking
Static completeness checks
Update requirements file
Watch list for securities - Gregs issue

Utilise TAVU
Add no fund as an option in the fund drop down of teh issues page
add rimes as an issue on the issues page
Exclude NaN values from the zscore calculations and teh first page where there is no S&P data
speed up comparision views by using the backend to pre calc pages
Filter by fund Group
Crete an IVP upload File
Bring in PCLID key
Create upload process
Add Is distressed  as a filter
Improve Fake TQS data
Fund Optimation Algorithm

done:
Get rid of the (Rex) refereneces and simulate
</file>

<file path="views/__init__.py">
"""
This file makes the 'views' directory a Python package.
"""
# You can leave this file empty or use it to import blueprints
# for easier registration in the app factory, though explicit imports
# in the factory as done currently are also perfectly fine.
</file>

<file path="views/api_core.py">
'''
Defines core functionality for the API views, including shared helper functions,
the Flask Blueprint, and feature switch configurations.
'''
import os
import pandas as pd
from flask import Blueprint, current_app
import datetime
#from tqs import tqs_query as tqs
# --- Feature Switch ---
# Set to True to attempt real API calls, validation, and saving.
# Set to False to only simulate the API call (print to console).
USE_REAL_TQS_API = False
# ----------------------
# Blueprint Configuration
api_bp = Blueprint(
    'api_bp', __name__,
    template_folder='../templates',
    static_folder='../static'
)
def _simulate_and_print_tqs_call(QueryID, FundCodeList, StartDate, EndDate):
    '''Simulates calling the TQS API by printing the call signature.
    This function is used when USE_REAL_TQS_API is False.
    It does NOT interact with any external API.
    Returns:
        int: A simulated number of rows for status reporting.
    '''
    # Format the call signature exactly as requested: tqs(QueryID,[FundList],StartDate,EndDate)
    call_signature = f"tqs({QueryID}, {FundCodeList}, {StartDate}, {EndDate})"
    print(f"--- SIMULATING TQS API CALL (USE_REAL_TQS_API = False) ---")
    print(call_signature)
    print(f"--------------------------------------------------------")
    # Return a simulated row count for the summary table
    simulated_row_count = len(FundCodeList) * 10 if FundCodeList else 0 # Dummy calculation
    return simulated_row_count
def _fetch_real_tqs_data(QueryID, FundCodeList, StartDate, EndDate):
    '''Fetches real data from the TQS API.
    This function is called when USE_REAL_TQS_API is True.
    Replace the placeholder logic with the actual API interaction code.
    Args:
        QueryID: The query identifier.
        FundCodeList: List of fund codes.
        StartDate: Start date string (YYYY-MM-DD).
        EndDate: End date string (YYYY-MM-DD).
    Returns:
        pd.DataFrame or None: The DataFrame containing the fetched data,
                              or None if the API call fails or returns no data.
    '''
    current_app.logger.info(f"Attempting real TQS API call for QueryID: {QueryID}")
    print(f"--- EXECUTING REAL TQS API CALL (USE_REAL_TQS_API = True) --- ")
    print(f"tqs({QueryID}, {FundCodeList}, {StartDate}, {EndDate})")
    print(f"--------------------------------------------------------")
    dataframe = None # Initialize dataframe to None
    try:
        # --- !!! Replace this comment and the line below with the actual API call !!! ---
        # Ensure the `tqs` function/library is imported (commented out at the top)
        # dataframe = tqs.get_data(QueryID, FundCodeList, StartDate, EndDate) # Example real call
        print(dataframe.head()) if dataframe is not None else print("No data to display")
        pass # Remove this pass when uncommenting the line above
        # --- End of section to replace ---
        # Check if the API returned valid data (e.g., a DataFrame)
        if dataframe is not None and isinstance(dataframe, pd.DataFrame):
            current_app.logger.info(f"Real TQS API call successful for QueryID: {QueryID}, Rows: {len(dataframe)}")
            return dataframe
        elif dataframe is None:
             # Explicitly handle the case where the API call itself returned None (e.g., planned failure or empty result coded as None)
             current_app.logger.warning(f"Real TQS API call for QueryID: {QueryID} returned None.")
             return None
        else:
            # Handle cases where the API returned something unexpected (not a DataFrame)
            current_app.logger.warning(f"Real TQS API call for QueryID: {QueryID} returned an unexpected data type: {type(dataframe)}.")
            return None # Treat unexpected types as failure
    except NameError as ne:
         # Specific handling if the tqs function isn't defined (import is commented out)
         current_app.logger.error(f"Real TQS API call failed for QueryID: {QueryID}. TQS function not imported/defined. Error: {ne}")
         print(f"     ERROR: TQS function not available. Ensure 'from tqs import tqs_query as tqs' is uncommented and the library is installed.")
         return None
    except Exception as e:
        # Handle API call errors (timeout, connection issues, authentication, etc.)
        current_app.logger.error(f"Real TQS API call failed for QueryID: {QueryID}. Error: {e}", exc_info=True)
        print(f"     ERROR during real API call: {e}")
        return None
# --- Helper function to find key columns ---
def _find_key_columns(df, filename):
    """Attempts to find the date and fund/identifier columns."""
    date_col = None
    fund_col = None
    # Date column candidates (add more if needed)
    date_candidates = ['Date', 'date', 'AsOfDate', 'ASOFDATE', 'Effective Date', 'Trade Date', 'Position Date']
    # Fund/ID column candidates
    fund_candidates = ['Code', 'Fund Code', 'Fundcode', 'security id', 'SecurityID', 'Security Name'] # Broadened list
    found_cols = df.columns.str.strip().str.lower()
    for candidate in date_candidates:
        if candidate.lower() in found_cols:
            # Find the original casing
            original_cols = [col for col in df.columns if col.strip().lower() == candidate.lower()]
            if original_cols:
                date_col = original_cols[0]
                current_app.logger.info(f"[{filename}] Found date column: '{date_col}'")
                break
    for candidate in fund_candidates:
        if candidate.lower() in found_cols:
            # Find the original casing
            original_cols = [col for col in df.columns if col.strip().lower() == candidate.lower()]
            if original_cols:
                fund_col = original_cols[0]
                current_app.logger.info(f"[{filename}] Found fund/ID column: '{fund_col}'")
                break
    if not date_col:
        current_app.logger.warning(f"[{filename}] Could not reliably identify a date column from candidates: {date_candidates}")
    if not fund_col:
        current_app.logger.warning(f"[{filename}] Could not reliably identify a fund/ID column from candidates: {fund_candidates}")
    return date_col, fund_col
# --- Helper Function to Get File Statuses ---
def get_data_file_statuses(data_folder):
    """
    Scans the data folder based on QueryMap.csv and returns status for each file.
    """
    statuses = []
    query_map_path = os.path.join(data_folder, 'QueryMap.csv')
    if not os.path.exists(query_map_path):
        current_app.logger.warning(f"QueryMap.csv not found at {query_map_path} for status check.")
        return statuses # Return empty list if map is missing
    try:
        query_map_df = pd.read_csv(query_map_path)
        if 'FileName' not in query_map_df.columns:
             current_app.logger.warning(f"QueryMap.csv at {query_map_path} is missing 'FileName' column.")
             return statuses
        date_column_candidates = ['Date', 'date', 'AsOfDate', 'ASOFDATE', 'Effective Date', 'Trade Date', 'Position Date'] # Add more candidates if needed
        for index, row in query_map_df.iterrows():
            filename = row['FileName']
            file_path = os.path.join(data_folder, filename)
            status_info = {
                'filename': filename,
                'exists': False,
                'last_modified': 'N/A',
                'latest_data_date': 'N/A',
                'funds_included': 'N/A' # Initialize new key
            }
            if os.path.exists(file_path):
                status_info['exists'] = True
                try:
                    # Get file modification time
                    mod_timestamp = os.path.getmtime(file_path)
                    status_info['last_modified'] = datetime.datetime.fromtimestamp(mod_timestamp).strftime('%Y-%m-%d %H:%M:%S')
                    # Try to read the CSV and find the latest date
                    try:
                        df = pd.read_csv(file_path, low_memory=False) # low_memory=False can help with mixed types
                        df_head = df.head()
                        # Determine the actual date column name
                        date_col = None
                        # --- Update Date Column Candidates ---
                        date_column_candidates = ['Date', 'date', 'AsOfDate', 'ASOFDATE', 'Effective Date', 'Trade Date', 'Position Date']
                        found_cols = df_head.columns.str.strip()
                        current_app.logger.info(f"[{filename}] Checking for date columns: {date_column_candidates} in columns {found_cols.tolist()}")
                        for candidate in date_column_candidates:
                            # Case-insensitive check
                            matching_cols = [col for col in found_cols if col.lower() == candidate.lower()]
                            if matching_cols:
                                date_col = matching_cols[0] # Use the actual name found
                                current_app.logger.info(f"[{filename}] Found date column: '{date_col}'")
                                break # Found the first match
                        if date_col:
                            try:
                                # --- FIX: Use the full DataFrame's date column ---
                                if date_col not in df.columns:
                                    # Handle case where column name from head differs slightly after full read (e.g., whitespace)
                                    # Find it again in the full df columns, case-insensitively
                                    corrected_date_col = None
                                    for col in df.columns:
                                        if col.strip().lower() == date_col.lower():
                                            corrected_date_col = col
                                            break
                                    if not corrected_date_col:
                                         raise ValueError(f"Date column '{date_col}' found in header but not in full DataFrame columns: {df.columns.tolist()}")
                                    date_col = corrected_date_col # Use the name from the full df
                                date_series = df[date_col] # Use the full series from the complete DataFrame
                                # --------------------------------------------------
                                current_app.logger.info(f"[{filename}] Attempting to parse full date column '{date_col}' (length: {len(date_series)}). Top 5 values: {date_series.head().to_list()}")
                                # Try standard YYYY-MM-DD first
                                parsed_dates = pd.to_datetime(date_series, format='%Y-%m-%d', errors='coerce')
                                # If all are NaT, try DD/MM/YYYY
                                if parsed_dates.isnull().all():
                                    current_app.logger.info(f"[{filename}] Format YYYY-MM-DD failed, trying DD/MM/YYYY...")
                                    parsed_dates = pd.to_datetime(date_series, format='%d/%m/%Y', errors='coerce')
                                # If still all NaT, try inferring (less reliable but fallback)
                                if parsed_dates.isnull().all():
                                     current_app.logger.warning(f"[{filename}] Both specific formats failed, trying to infer date format...")
                                     parsed_dates = pd.to_datetime(date_series, errors='coerce', infer_datetime_format=True)
                                # Check if any dates were successfully parsed
                                if not parsed_dates.isnull().all():
                                    latest_date = parsed_dates.max()
                                    if pd.notna(latest_date):
                                        status_info['latest_data_date'] = latest_date.strftime('%Y-%m-%d')
                                        current_app.logger.info(f"[{filename}] Successfully found latest date: {status_info['latest_data_date']}")
                                    else:
                                        status_info['latest_data_date'] = 'No Valid Dates Found'
                                        current_app.logger.warning(f"[{filename}] Parsed dates but found no valid max date (all NaT?).")
                                else:
                                    status_info['latest_data_date'] = 'Date Parsing Failed'
                                    current_app.logger.warning(f"[{filename}] All parsing attempts failed for date column '{date_col}'.")
                            except Exception as date_err:
                                current_app.logger.error(f"Error parsing date column '{date_col}' in {file_path}: {date_err}", exc_info=True)
                                status_info['latest_data_date'] = f'Error Parsing Date ({date_col})'
                        else:
                            status_info['latest_data_date'] = 'No Date Column Found/Parsed'
                            current_app.logger.warning(f"[{filename}] Could not find a suitable date column.")
                        # --- Add Fund Code Extraction ---
                        code_col = None
                        # FIX: Search for 'code' OR 'fund code' (case-insensitive)
                        code_candidates = ['code', 'fund code']
                        found_code_col_name = None
                        for candidate in code_candidates:
                              matches = [c for c in df.columns if c.strip().lower() == candidate]
                              if matches:
                                  found_code_col_name = matches[0] # Use the actual column name found
                                  break # Stop searching once found
                        if found_code_col_name:
                            code_col = found_code_col_name # Assign the found name to code_col
                            current_app.logger.info(f"[{filename}] Found Code column: '{code_col}'")
                            if not df.empty and code_col in df:
                                try:
                                    unique_funds = sorted([str(f) for f in df[code_col].unique() if pd.notna(f)])
                                    if unique_funds:
                                        if len(unique_funds) <= 5:
                                            status_info['funds_included'] = ', '.join(unique_funds)
                                        else:
                                            status_info['funds_included'] = ', '.join(unique_funds[:5]) + f' ... ({len(unique_funds)} total)'
                                        current_app.logger.info(f"[{filename}] Found funds: {status_info['funds_included']}")
                                    else:
                                        status_info['funds_included'] = 'No Codes Found'
                                except Exception as fund_err:
                                     current_app.logger.error(f"[{filename}] Error extracting funds from column '{code_col}': {fund_err}")
                                     status_info['funds_included'] = 'Error Extracting Funds'
                            else:
                                status_info['funds_included'] = 'Code Column Empty?' # Should be covered by EmptyDataError usually
                        else:
                            status_info['funds_included'] = 'Code Column Missing'
                            current_app.logger.warning(f"[{filename}] Code column ('Code' or 'Fund Code') not found.")
                        # --- End Fund Code Extraction ---
                    except pd.errors.EmptyDataError:
                         status_info['latest_data_date'] = 'File is Empty'
                         status_info['funds_included'] = 'File is Empty' # Also set for funds
                         current_app.logger.warning(f"CSV file is empty: {file_path}")
                    except Exception as read_err:
                        status_info['latest_data_date'] = 'Read Error'
                        current_app.logger.error(f"Error reading CSV {file_path} for status check: {read_err}", exc_info=True)
                except Exception as file_err:
                       current_app.logger.error(f"Error accessing file properties for {file_path}: {file_err}", exc_info=True)
                       status_info['last_modified'] = 'Error Accessing File'
            statuses.append(status_info)
    except Exception as e:
        current_app.logger.error(f"Failed to process QueryMap.csv for file statuses: {e}", exc_info=True)
        # Optionally return a status indicating the map couldn't be processed
        return [{'filename': 'QueryMap Error', 'exists': False, 'last_modified': str(e), 'latest_data_date': '', 'funds_included': ''}]
    return statuses
# --- End Helper Function ---
</file>

<file path="views/api_routes_call.py">
'''
Defines the Flask routes for API data calls, including:
- Running API calls with simulation or real API calls
- Rerunning individual API queries
'''
import os
import pandas as pd
from flask import request, current_app, jsonify
import datetime
import time
import json
# Import from our local modules
from views.api_core import api_bp, _simulate_and_print_tqs_call, _fetch_real_tqs_data, _find_key_columns, USE_REAL_TQS_API
# Import the validation function from data_validation
from data_validation import validate_data
@api_bp.route('/run_api_calls', methods=['POST'])
def run_api_calls():
    '''Handles the form submission to trigger API calls (real or simulated).
    Now supports:
    - date_mode: 'quick' (days_back + end_date) or 'range' (start_date + custom_end_date)
    - write_mode: 'expand' (append/overwrite overlaps) or 'overwrite_all' (start every file from scratch)
    '''
    try:
        # Get data from form
        data = request.get_json()
        date_mode = data.get('date_mode', 'quick')
        write_mode = data.get('write_mode', 'expand')
        days_back = int(data.get('days_back', 30)) # Default to 30 days if not provided
        end_date_str = data.get('end_date')
        start_date_str = data.get('start_date')
        custom_end_date_str = data.get('custom_end_date')
        selected_funds = data.get('funds', [])
        # Determine overwrite mode
        overwrite_mode = (write_mode == 'overwrite_all')
        # Date range logic
        if date_mode == 'range':
            if not start_date_str or not custom_end_date_str:
                return jsonify({"status": "error", "message": "Start and end date are required for custom range."}), 400
            start_date = pd.to_datetime(start_date_str)
            end_date = pd.to_datetime(custom_end_date_str)
        else:
            if not end_date_str:
                return jsonify({"status": "error", "message": "End date is required."}), 400
            end_date = pd.to_datetime(end_date_str)
            start_date = end_date - pd.Timedelta(days=days_back)
        # Format dates as YYYY-MM-DD for the TQS call
        start_date_tqs_str = start_date.strftime('%Y-%m-%d')
        end_date_tqs_str = end_date.strftime('%Y-%m-%d')
        if not selected_funds:
            return jsonify({"status": "error", "message": "At least one fund must be selected."}), 400
        # --- Get Query Map ---
        data_folder = current_app.config.get('DATA_FOLDER', 'Data')
        query_map_path = os.path.join(data_folder, 'QueryMap.csv')
        if not os.path.exists(query_map_path):
            return jsonify({"status": "error", "message": f"QueryMap.csv not found at {query_map_path}"}), 500
        query_map_df = pd.read_csv(query_map_path)
        if not {'QueryID', 'FileName'}.issubset(query_map_df.columns):
            return jsonify({"status": "error", "message": "QueryMap.csv missing required columns (QueryID, FileName)."}), 500
        # Sort queries: ts_*, pre_*, others
        def sort_key(query):
            filename = query.get('FileName', '').lower()
            if filename.startswith('ts_'):
                return 0
            elif filename.startswith('pre_'):
                return 1
            else:
                return 2
        queries_with_indices = list(enumerate(query_map_df.to_dict('records')))
        def sort_key_with_index(item):
            index, query = item
            filename = query.get('FileName', '').lower()
            if filename.startswith('ts_'):
                return (0, index)
            elif filename.startswith('pre_'):
                return (1, index)
            else:
                return (2, index)
        queries_with_indices.sort(key=sort_key_with_index)
        queries = [item[1] for item in queries_with_indices]
        current_app.logger.info(f"Processing order after sorting: {[q.get('FileName', 'N/A') for q in queries]}")
        results_summary = []
        total_queries = len(queries)
        completed_queries = 0
        all_ts_files_succeeded = True
        current_mode_desc = "SIMULATED mode" if not USE_REAL_TQS_API else ("REAL API mode (Overwrite Enabled)" if overwrite_mode else "REAL API mode (Merge/Append)")
        current_app.logger.info(f"--- Starting /run_api_calls in {current_mode_desc} ---")
        # Loop through sorted queries
        for query_info in queries:
            query_id = query_info.get('QueryID')
            file_name = query_info.get('FileName')
            if not query_id or not file_name:
                current_app.logger.warning(f"Skipping entry due to missing QueryID or FileName: {query_info}")
                summary = {
                    "query_id": query_id or "N/A", "file_name": file_name or "N/A",
                    "status": "Skipped (Missing QueryID/FileName)",
                    "simulated_rows": None, "simulated_lines": None,
                    "actual_rows": None, "actual_lines": None,
                    "save_action": "N/A", "validation_status": "Not Run",
                    "last_written": None
                }
                results_summary.append(summary)
                continue
            output_path = os.path.join(data_folder, file_name)
            summary = {
                "query_id": query_id,
                "file_name": file_name,
                "status": "Pending",
                "simulated_rows": None,
                "simulated_lines": None,
                "actual_rows": None,
                "actual_lines": None,
                "save_action": "N/A",
                "validation_status": "Not Run",
                "last_written": None
            }
            file_type = 'other'
            if file_name.lower().startswith('ts_'):
                file_type = 'ts'
            elif file_name.lower().startswith('pre_'):
                file_type = 'pre'
            current_app.logger.info(f"--- Starting Process for QueryID: {query_id}, File: {file_name} (Type: {file_type}) ---")
            if file_type == 'pre' and not all_ts_files_succeeded:
                current_app.logger.warning(f"[{file_name}] Skipping pre_ file because a previous ts_ file failed processing.")
                summary['status'] = 'Skipped (Previous TS Failure)'
                summary['validation_status'] = 'Not Run'
                summary['save_action'] = 'Skipped'
                results_summary.append(summary)
                completed_queries += 1
                continue
            try:
                if USE_REAL_TQS_API:
                    df_new = None
                    df_to_save = None
                    force_overwrite = overwrite_mode
                    try:
                        df_new = _fetch_real_tqs_data(query_id, selected_funds, start_date_tqs_str, end_date_tqs_str)
                        now_str = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                        if df_new is None:
                            current_app.logger.warning(f"[{file_name}] No data returned from API call for QueryID {query_id}.")
                            summary['status'] = 'Warning - No data returned from API'
                            summary['validation_status'] = 'Skipped (API Returned None)'
                            summary['last_written'] = now_str
                            if file_type == 'ts': all_ts_files_succeeded = False
                        elif df_new.empty:
                            current_app.logger.warning(f"[{file_name}] Empty DataFrame returned from API call for QueryID {query_id}.")
                            summary['status'] = 'Warning - Empty data returned from API'
                            summary['validation_status'] = 'OK (Empty Data)'
                            df_to_save = df_new
                            summary['actual_rows'] = 0
                            summary['last_written'] = now_str
                        else:
                            current_app.logger.info(f"[{file_name}] Fetched {len(df_new)} new rows.")
                            summary['actual_rows'] = len(df_new)
                            df_to_save = df_new
                            summary['last_written'] = now_str
                        if df_new is not None:
                            if file_type == 'ts':
                                current_app.logger.info(f"[{file_name}] Processing as ts_ file (Overwrite Mode: {force_overwrite}).")
                                try:
                                    if not df_new.empty:
                                        date_col_new, fund_col_new = _find_key_columns(df_new, f"{file_name} (New TS Data)")
                                        if not date_col_new or not fund_col_new:
                                            err_msg = f"Could not find essential date/fund columns in fetched ts_ data for {file_name}. Cannot proceed."
                                            current_app.logger.error(f"[{file_name}] {err_msg}")
                                            raise ValueError(err_msg)
                                    else:
                                        date_col_new, fund_col_new = None, None
                                        current_app.logger.info(f"[{file_name}] Skipping key column check for empty TS data.")
                                    if force_overwrite:
                                        current_app.logger.info(f"[{file_name}] Overwrite Mode enabled. Skipping check for existing file.")
                                        if os.path.exists(output_path):
                                            summary['save_action'] = 'Overwritten (User Request)'
                                        else:
                                            summary['save_action'] = 'Created (Overwrite Mode)'
                                    elif os.path.exists(output_path):
                                        current_app.logger.info(f"[{file_name}] TS file exists. Reading existing data for merge/append.")
                                        try:
                                            df_existing = pd.read_csv(output_path, low_memory=False)
                                            if not df_existing.empty:
                                                if date_col_new and fund_col_new:
                                                    date_col_existing, fund_col_existing = _find_key_columns(df_existing, f"{file_name} (Existing TS)")
                                                    if date_col_existing == date_col_new and fund_col_existing == fund_col_new:
                                                        date_col = date_col_new
                                                        fund_col = fund_col_new
                                                        df_existing[fund_col] = df_existing[fund_col].astype(str)
                                                        df_new[fund_col] = df_new[fund_col].astype(str)
                                                        funds_in_new_data = [str(f) for f in df_new[fund_col].unique()]
                                                        if date_col in df_existing.columns and date_col in df_new.columns:
                                                            dates_in_new = df_new[date_col].astype(str).unique()
                                                            mask = ~((df_existing[fund_col].isin(funds_in_new_data)) & (df_existing[date_col].astype(str).isin(dates_in_new)))
                                                            df_existing_filtered = df_existing[mask]
                                                        else:
                                                            df_existing_filtered = df_existing[~df_existing[fund_col].isin(funds_in_new_data)]
                                                        df_combined = pd.concat([df_existing_filtered, df_new], ignore_index=True)
                                                        try:
                                                            df_combined = df_combined.sort_values(by=[date_col, fund_col])
                                                        except Exception as sort_err:
                                                            current_app.logger.warning(f"[{file_name}] Could not sort combined data: {sort_err}")
                                                        df_to_save = df_combined
                                                        summary['save_action'] = 'Combined (Append/Overwrite)'
                                                        summary['last_written'] = now_str
                                                        current_app.logger.info(f"[{file_name}] Prepared combined data ({len(df_to_save)} rows).")
                                                    else:
                                                        current_app.logger.warning(f"[{file_name}] Key columns mismatch between existing ({date_col_existing}, {fund_col_existing}) and new ({date_col_new}, {fund_col_new}). Overwriting entire file.")
                                                        summary['save_action'] = 'Overwritten (Column Mismatch)'
                                                        summary['last_written'] = now_str
                                                else:
                                                    current_app.logger.warning(f"[{file_name}] Cannot merge TS data as key columns were not identified in new data. Overwriting.")
                                                    summary['save_action'] = 'Overwritten (Merge Skipped)'
                                                    summary['last_written'] = now_str
                                            else:
                                                current_app.logger.warning(f"[{file_name}] Existing TS file is empty. Overwriting.")
                                                summary['save_action'] = 'Overwritten (Existing Empty)'
                                                summary['last_written'] = now_str
                                        except pd.errors.EmptyDataError:
                                            current_app.logger.warning(f"[{file_name}] Existing TS file is empty (EmptyDataError). Overwriting.")
                                            summary['save_action'] = 'Overwritten (Existing Empty)'
                                            summary['last_written'] = now_str
                                        except Exception as read_err:
                                            current_app.logger.error(f"[{file_name}] Error reading existing TS file: {read_err}. Overwriting.", exc_info=True)
                                            summary['save_action'] = 'Overwritten (Read Error)'
                                            summary['last_written'] = now_str
                                            all_ts_files_succeeded = False
                                    else:
                                        current_app.logger.info(f"[{file_name}] TS file does not exist (or overwrite mode is on and file was absent). Creating new file.")
                                        summary['save_action'] = 'Created'
                                        summary['last_written'] = now_str
                                except ValueError as ve:
                                    current_app.logger.error(f"[{file_name}] TS validation failed: {ve}")
                                    summary['status'] = f'Error - TS Validation Failed: {ve}'
                                    summary['validation_status'] = 'Failed (Missing Columns)'
                                    summary['last_written'] = now_str
                                    all_ts_files_succeeded = False
                                    df_to_save = None
                            elif file_type == 'pre':
                                current_app.logger.info(f"[{file_name}] Processing as pre_ file (checking column count).")
                                if os.path.exists(output_path):
                                    try:
                                        if not df_new.empty:
                                            existing_header_df = pd.read_csv(output_path, nrows=0, low_memory=False)
                                            existing_cols = existing_header_df.columns.tolist()
                                            new_cols = df_new.columns.tolist()
                                            if force_overwrite or len(existing_cols) != len(new_cols) or set(existing_cols) != set(new_cols):
                                                if force_overwrite:
                                                    current_app.logger.info(f"[{file_name}] Overwriting pre_ file as requested by user.")
                                                    summary['save_action'] = 'Overwritten (User Request)'
                                                else:
                                                    current_app.logger.warning(f"[{file_name}] Column count/names mismatch between existing pre_ file ({len(existing_cols)} cols: {existing_cols}) and new data ({len(new_cols)} cols: {new_cols}). Overwriting.")
                                                    summary['save_action'] = 'Overwritten (Column Mismatch)'
                                            else:
                                                current_app.logger.info(f"[{file_name}] Existing pre_ file found with matching columns. Overwriting.")
                                                summary['save_action'] = 'Overwritten'
                                        else:
                                            current_app.logger.info(f"[{file_name}] New data for pre_ file is empty. Overwriting existing file.")
                                            summary['save_action'] = 'Overwritten (New Data Empty)'
                                    except pd.errors.EmptyDataError:
                                        current_app.logger.warning(f"[{file_name}] Existing pre_ file is empty (EmptyDataError). Overwriting.")
                                        summary['save_action'] = 'Overwritten (Existing Empty)'
                                    except Exception as read_err:
                                        current_app.logger.error(f"[{file_name}] Error reading existing pre_ file header: {read_err}. Overwriting.", exc_info=True)
                                        summary['save_action'] = 'Overwritten (Read Error)'
                                else:
                                    current_app.logger.info(f"[{file_name}] Pre_ file does not exist. Creating new file.")
                                    summary['save_action'] = 'Created'
                            else:
                                current_app.logger.info(f"[{file_name}] Processing as 'other' file type (using column count check).")
                                if os.path.exists(output_path):
                                    try:
                                        if not df_new.empty:
                                            existing_header_df = pd.read_csv(output_path, nrows=0, low_memory=False)
                                            existing_cols = existing_header_df.columns.tolist()
                                            new_cols = df_new.columns.tolist()
                                            if force_overwrite or len(existing_cols) != len(new_cols) or set(existing_cols) != set(new_cols):
                                                if force_overwrite:
                                                    current_app.logger.info(f"[{file_name}] Overwriting 'other' file as requested by user.")
                                                    summary['save_action'] = 'Overwritten (User Request)'
                                                else:
                                                    current_app.logger.warning(f"[{file_name}] Column count/names mismatch for 'other' file. Overwriting.")
                                                    summary['save_action'] = 'Overwritten (Column Mismatch)'
                                            else:
                                                current_app.logger.info(f"[{file_name}] Existing 'other' file found with matching columns. Overwriting.")
                                                summary['save_action'] = 'Overwritten'
                                        else:
                                            current_app.logger.info(f"[{file_name}] New data for 'other' file is empty. Overwriting existing file.")
                                            summary['save_action'] = 'Overwritten (New Data Empty)'
                                    except pd.errors.EmptyDataError:
                                        current_app.logger.warning(f"[{file_name}] Existing 'other' file is empty (EmptyDataError). Overwriting.")
                                        summary['save_action'] = 'Overwritten (Existing Empty)'
                                    except Exception as read_err:
                                        current_app.logger.error(f"[{file_name}] Error reading existing 'other' file header: {read_err}. Overwriting.", exc_info=True)
                                        summary['save_action'] = 'Overwritten (Read Error)'
                                else:
                                    current_app.logger.info(f"[{file_name}] 'Other' file does not exist. Creating new file.")
                                    summary['save_action'] = 'Created'
                            # 4. Save the Final DataFrame (Common step, if df_to_save is valid)
                            if df_to_save is not None:
                                current_app.logger.info(f"[{file_name}] Attempting to save {len(df_to_save)} rows to {output_path} (Action: {summary['save_action']})")
                                try:
                                    df_to_save.to_csv(output_path, index=False, header=True)
                                    current_app.logger.info(f"[{file_name}] Successfully saved data to {output_path}")
                                    summary['status'] = 'OK - Data Saved'
                                    try:
                                        with open(output_path, 'r', encoding='utf-8') as f:
                                            summary['actual_lines'] = sum(1 for line in f)
                                    except Exception:
                                        summary['actual_lines'] = 'N/A'
                                    summary['validation_status'] = validate_data(df_to_save, file_name)
                                    current_app.logger.info(f"[{file_name}] Validation status: {summary['validation_status']})")
                                except Exception as write_err:
                                    current_app.logger.error(f"[{file_name}] Error writing final data to {output_path}: {write_err}", exc_info=True)
                                    summary['status'] = f'Error - Failed to save file: {write_err}'
                                    summary['validation_status'] = 'Failed (Save Error)'
                                    if file_type == 'ts': all_ts_files_succeeded = False
                            elif df_new is None:
                                pass
                            elif df_to_save is None and file_type == 'ts':
                                pass
                            else:
                                current_app.logger.error(f"[{file_name}] Reached unexpected state where df_to_save is None but no prior error logged.")
                                summary['status'] = 'Error - Internal Logic Error (df_to_save is None)'
                    except Exception as proc_err:
                        current_app.logger.error(f"Error processing real data for QueryID {query_id}, File {file_name}: {proc_err}", exc_info=True)
                        summary['status'] = f'Error - Processing failed: {proc_err}'
                        summary['validation_status'] = 'Failed (Processing Error)'
                        summary['last_written'] = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                        if file_type == 'ts': all_ts_files_succeeded = False
                else:
                    # Simulation Mode
                    simulated_rows = _simulate_and_print_tqs_call(query_id, selected_funds, start_date_tqs_str, end_date_tqs_str)
                    summary['simulated_rows'] = simulated_rows
                    summary['simulated_lines'] = simulated_rows + 1 if simulated_rows > 0 else 0
                    summary['status'] = 'Simulated OK'
                    summary['save_action'] = 'Not Applicable'
                    summary['validation_status'] = 'Not Run (Simulated)'
                    summary['last_written'] = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            except Exception as outer_err:
                current_app.logger.error(f"Unexpected outer error processing QueryID {query_id} ({file_name}): {outer_err}", exc_info=True)
                if summary['status'] == 'Pending' or summary['status'].startswith('Warning'):
                    summary['status'] = f"Outer Processing Error: {outer_err}"
                if file_type == 'ts': all_ts_files_succeeded = False
                summary['last_written'] = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            results_summary.append(summary)
            completed_queries += 1
            if USE_REAL_TQS_API and completed_queries < total_queries:
                print(f"Pausing for 3 seconds before next real API call ({completed_queries}/{total_queries})...")
                time.sleep(3)
        mode_message = "SIMULATED mode" if not USE_REAL_TQS_API else ("REAL API mode (Overwrite Enabled)" if overwrite_mode else "REAL API mode (Merge/Append)")
        final_status = "completed"
        if USE_REAL_TQS_API and not all_ts_files_succeeded:
            completion_message = f"Processed {completed_queries}/{total_queries} API calls ({mode_message}). WARNING: One or more ts_ files failed processing or validation."
            final_status = "completed_with_errors"
        else:
            completion_message = f"Processed {completed_queries}/{total_queries} API calls ({mode_message})."
        return jsonify({
            "status": final_status,
            "message": completion_message,
            "summary": results_summary
        })
    except ValueError as ve:
        current_app.logger.error(f"Value error in /run_api_calls: {ve}", exc_info=True)
        return jsonify({"status": "error", "message": f"Invalid input value: {ve}"}), 400
    except FileNotFoundError as fnf:
        current_app.logger.error(f"File not found error in /run_api_calls: {fnf}", exc_info=True)
        return jsonify({"status": "error", "message": f"Required file not found: {fnf}"}), 500
    except Exception as e:
        current_app.logger.error(f"Unexpected error in /run_api_calls: {e}", exc_info=True)
        return jsonify({"status": "error", "message": f"An unexpected error occurred: {e}"}), 500
@api_bp.route('/rerun-api-call', methods=['POST'])
def rerun_api_call():
    '''Handles the request to rerun a single API call (real or simulated).'''
    try:
        data = request.get_json()
        query_id = data.get('query_id')
        days_back = int(data.get('days_back', 30))
        end_date_str = data.get('end_date')
        selected_funds = data.get('funds', []) # Get the list of funds
        overwrite_mode = data.get('overwrite_mode', False) # Get the new overwrite flag
        # --- Basic Input Validation ---
        if not query_id:
            return jsonify({"status": "error", "message": "Query ID is required."}), 400
        if not end_date_str:
            return jsonify({"status": "error", "message": "End date is required."}), 400
        if not selected_funds:
             # Allow rerunning even if no funds are selected? Decide based on API behavior.
             # For now, let's require funds similar to the initial run.
             return jsonify({"status": "error", "message": "At least one fund must be selected."}), 400
        # --- Calculate Dates ---
        end_date = pd.to_datetime(end_date_str)
        start_date = end_date - pd.Timedelta(days=days_back)
        start_date_tqs_str = start_date.strftime('%Y-%m-%d')
        end_date_tqs_str = end_date.strftime('%Y-%m-%d')
        # --- Find FileName from QueryMap ---
        data_folder = current_app.config.get('DATA_FOLDER', 'Data')
        query_map_path = os.path.join(data_folder, 'QueryMap.csv')
        if not os.path.exists(query_map_path):
            return jsonify({"status": "error", "message": f"QueryMap.csv not found at {query_map_path}"}), 500
        query_map_df = pd.read_csv(query_map_path)
        # Ensure comparison is string vs string
        query_map_df['QueryID'] = query_map_df['QueryID'].astype(str)
        if 'QueryID' not in query_map_df.columns or 'FileName' not in query_map_df.columns:
             return jsonify({"status": "error", "message": "QueryMap.csv missing required columns (QueryID, FileName)."}), 500
        # Compare string query_id from request with string QueryID column
        query_row = query_map_df[query_map_df['QueryID'] == query_id]
        if query_row.empty:
            # Log the types for debugging if it still fails
            current_app.logger.warning(f"QueryID '{query_id}' (type: {type(query_id)}) not found in QueryMap QueryIDs (types: {query_map_df['QueryID'].apply(type).unique()}).")
            return jsonify({"status": "error", "message": f"QueryID '{query_id}' not found in QueryMap.csv."}), 404
        file_name = query_row.iloc[0]['FileName']
        output_path = os.path.join(data_folder, file_name)
        # --- Execute Single API Call (Simulated or Real) ---
        status = "Rerun Error: Unknown"
        rows_returned = 0
        lines_in_file = 0
        actual_df = None
        simulated_rows = None # Initialize simulation keys too
        simulated_lines = None
        try:
            if USE_REAL_TQS_API:
                # --- Real API Call, Validation, and Save ---
                actual_df = _fetch_real_tqs_data(query_id, selected_funds, start_date_tqs_str, end_date_tqs_str)
                if actual_df is not None and isinstance(actual_df, pd.DataFrame):
                    rows_returned = len(actual_df)
                    if actual_df.empty:
                        current_app.logger.info(f"(Rerun) API returned empty DataFrame for {query_id} ({file_name}). Saving empty file.")
                        status = "Saved OK (Empty)"
                    else:
                        is_valid, validation_errors = validate_data(actual_df, file_name)
                        if not is_valid:
                            current_app.logger.warning(f"(Rerun) Data validation failed for {file_name}: {validation_errors}")
                            status = f"Validation Failed: {'; '.join(validation_errors)}"
                            lines_in_file = 0
                        # else: Validation passed
                    if not status.startswith("Validation Failed"):
                        try:
                            os.makedirs(os.path.dirname(output_path), exist_ok=True)
                            actual_df.to_csv(output_path, index=False)
                            current_app.logger.info(f"(Rerun) Successfully saved data to {output_path}")
                            lines_in_file = rows_returned + 1
                            if status != "Saved OK (Empty)":
                                status = "Saved OK"
                        except Exception as e:
                            current_app.logger.error(f"(Rerun) Error saving DataFrame to {output_path}: {e}", exc_info=True)
                            status = f"Save Error: {e}"
                            lines_in_file = 0
                elif actual_df is None:
                    current_app.logger.warning(f"(Rerun) Real API call/fetch for {query_id} ({file_name}) returned None.")
                    status = "No Data / API Error / TQS Missing"
                    rows_returned = 0
                    lines_in_file = 0
                else:
                    current_app.logger.error(f"(Rerun) Real API fetch for {query_id} ({file_name}) returned unexpected type: {type(actual_df)}.")
                    status = "API Returned Invalid Type"
                    rows_returned = 0
                    lines_in_file = 0
            else:
                # --- Simulate API Call ---
                simulated_rows = _simulate_and_print_tqs_call(query_id, selected_funds, start_date_tqs_str, end_date_tqs_str)
                rows_returned = simulated_rows
                lines_in_file = simulated_rows + 1 if simulated_rows > 0 else 0
                status = "Simulated OK"
        except Exception as e:
            current_app.logger.error(f"Error during single rerun for query {query_id} ({file_name}): {e}", exc_info=True)
            status = f"Processing Error: {e}"
            rows_returned = 0
            lines_in_file = 0
        # --- Return Result for the Single Query ---
        result_data = {
            "status": status,
             # Provide consistent keys for the frontend to update the table
            "simulated_rows": simulated_rows, # Value if simulated, None otherwise
            "actual_rows": rows_returned if USE_REAL_TQS_API else None, # Value if real, None otherwise
            "simulated_lines": simulated_lines, # Value if simulated, None otherwise
            "actual_lines": lines_in_file if USE_REAL_TQS_API else None # Value if real, None otherwise
        }
        return jsonify(result_data)
    except ValueError as ve:
        current_app.logger.error(f"Value error in /rerun-api-call: {ve}", exc_info=True)
        return jsonify({"status": "error", "message": f"Invalid input value: {ve}"}), 400
    except FileNotFoundError as fnf:
        current_app.logger.error(f"File not found error in /rerun-api-call: {fnf}", exc_info=True)
        return jsonify({"status": "error", "message": f"Required file not found: {fnf}"}), 500
    except Exception as e:
        current_app.logger.error(f"Unexpected error in /rerun-api-call: {e}", exc_info=True)
        return jsonify({"status": "error", "message": f"An unexpected error occurred: {e}"}), 500
def get_schedules_file():
    return os.path.join(current_app.instance_path, 'schedules.json')
def load_schedules():
    file = get_schedules_file()
    if not os.path.exists(file):
        return []
    try:
        with open(file, 'r') as f:
            return json.load(f)
    except Exception as e:
        current_app.logger.error(f"Error loading schedules: {e}", exc_info=True)
        return []
def save_schedules(schedules):
    file = get_schedules_file()
    try:
        with open(file, 'w') as f:
            json.dump(schedules, f)
    except Exception as e:
        current_app.logger.error(f"Error saving schedules: {e}", exc_info=True)
@api_bp.route('/schedules', methods=['GET'])
def list_schedules():
    return jsonify(load_schedules())
@api_bp.route('/schedules', methods=['POST'])
def add_schedule():
    data = request.get_json() or {}
    schedule_time = data.get('schedule_time')
    write_mode = data.get('write_mode')
    date_mode = data.get('date_mode')
    funds = data.get('funds')
    if not schedule_time or not write_mode or not date_mode or not isinstance(funds, list) or not funds:
        return jsonify({'message': 'Missing or invalid schedule fields'}), 400
    if date_mode == 'quick':
        days_back = data.get('days_back')
        end_date = data.get('end_date')
        if days_back is None or end_date is None:
            return jsonify({'message': 'days_back and end_date are required for quick mode'}), 400
    else:
        start_date = data.get('start_date')
        custom_end_date = data.get('custom_end_date')
        if start_date is None or custom_end_date is None:
            return jsonify({'message': 'start_date and custom_end_date are required for range mode'}), 400
    schedules = load_schedules()
    new_id = max((s['id'] for s in schedules), default=0) + 1
    sched = {
        'id': new_id,
        'schedule_time': schedule_time,
        'write_mode': write_mode,
        'date_mode': date_mode,
        'funds': funds
    }
    if date_mode == 'quick':
        sched['days_back'] = int(days_back)
        sched['end_date'] = end_date
    else:
        sched['start_date'] = start_date
        sched['custom_end_date'] = custom_end_date
    schedules.append(sched)
    save_schedules(schedules)
    return jsonify(sched), 201
@api_bp.route('/schedules/<int:schedule_id>', methods=['DELETE'])
def delete_schedule(schedule_id):
    schedules = load_schedules()
    new_list = [s for s in schedules if s['id'] != schedule_id]
    if len(new_list) == len(schedules):
        return jsonify({'message': 'Schedule not found'}), 404
    save_schedules(new_list)
    return '', 204
</file>

<file path="views/api_routes_data.py">
'''
Defines the Flask route for data retrieval interface and related functionality.
'''
import os
import pandas as pd
from flask import render_template, request, current_app, jsonify
import datetime
from pandas.tseries.offsets import BDay
# Import from our local modules
from views.api_core import api_bp, get_data_file_statuses
@api_bp.route('/get_data')
def get_data_page():
    '''Renders the page for users to select parameters for API data retrieval.'''
    try:
        # Construct the path to FundList.csv relative to the app's instance path or root
        # Assuming DATA_FOLDER is configured relative to the app root
        data_folder = current_app.config.get('DATA_FOLDER', 'Data')
        fund_list_path = os.path.join(data_folder, 'FundList.csv')
        if not os.path.exists(fund_list_path):
            current_app.logger.error(f"FundList.csv not found at {fund_list_path}")
            return "Error: FundList.csv not found.", 500
        fund_df = pd.read_csv(fund_list_path)
        # Ensure required columns exist
        if not {'Fund Code', 'Total Asset Value USD', 'Picked'}.issubset(fund_df.columns):
             current_app.logger.error(f"FundList.csv is missing required columns.")
             return "Error: FundList.csv is missing required columns (Fund Code, Total Asset Value USD, Picked).", 500
        # Convert Total Asset Value to numeric, coercing errors
        fund_df['Total Asset Value USD'] = pd.to_numeric(fund_df['Total Asset Value USD'], errors='coerce')
        fund_df.dropna(subset=['Total Asset Value USD'], inplace=True) # Remove rows where conversion failed
        # Sort by Total Asset Value USD descending
        fund_df = fund_df.sort_values(by='Total Asset Value USD', ascending=False)
        # Convert Picked to boolean
        fund_df['Picked'] = fund_df['Picked'].astype(bool)
        # Prepare fund data for the template
        funds = fund_df.to_dict('records')
        # Calculate default end date (previous business day)
        default_end_date = (datetime.datetime.today() - BDay(1)).strftime('%Y-%m-%d')
        # --- Get Data File Statuses ---
        data_file_statuses = get_data_file_statuses(data_folder)
        # --- End Get Data File Statuses ---
    except Exception as e:
        current_app.logger.error(f"Error preparing get_data page: {e}", exc_info=True)
        # Provide a user-friendly error message, specific details are logged
        return f"An error occurred while preparing the data retrieval page: {e}", 500
    return render_template('get_data.html', funds=funds, default_end_date=default_end_date, data_file_statuses=data_file_statuses)
</file>

<file path="views/api_views.py">
'''
Main entry point for API views.
This file now imports from the split modules to maintain backward compatibility.
'''
# Import all components from the split files
from views.api_core import (
    api_bp,
    USE_REAL_TQS_API,
    _simulate_and_print_tqs_call,
    _fetch_real_tqs_data,
    _find_key_columns,
    get_data_file_statuses
)
from views.api_routes_data import get_data_page
from views.api_routes_call import run_api_calls, rerun_api_call
# The api_bp Blueprint is now available from this module as before
# All the previously defined functions are also available through this import structure
</file>

<file path="views/attribution_views.py">
# Purpose: Defines the Attribution summary view for the Simple Data Checker app.
# This blueprint provides a page that loads att_factors.csv and displays the sum of residuals
# for each fund and day, for two cases: Benchmark and Portfolio, with both Prod and S&P columns.
# Residuals are calculated as L0 Total - (L1 Rates + L1 Credit + L1 FX), with L1s computed from L2s.
from flask import Blueprint, render_template, current_app, request, jsonify, url_for
import pandas as pd
import os
import json
attribution_bp = Blueprint('attribution_bp', __name__, url_prefix='/attribution')
def _is_date_like(column_name):
    import re
    if not isinstance(column_name, str):
        return False
    date_patterns = [
        r'\d{2}/\d{2}/\d{4}',  # DD/MM/YYYY
        r'\d{2}-\d{2}-\d{4}',  # DD-MM-YYYY
        r'\d{4}/\d{2}/\d{2}',  # YYYY/MM/DD
        r'\d{4}-\d{2}-\d{2}',  # YYYY-MM-DD
        r'\d{1,2}/\d{1,2}/\d{2,4}',  # D/M/YY or D/M/YYYY
        r'\d{1,2}-\d{1,2}-\d{2,4}',  # D-M-YY or D-M-YYYY
    ]
    return any(re.search(pattern, column_name) for pattern in date_patterns)
@attribution_bp.route('/summary')
def attribution_summary():
    """
    Loads att_factors.csv and computes residuals for each fund and day for Benchmark and Portfolio,
    showing both Prod and S&P (SPv3) columns. Supports grouping/filtering by characteristic.
    Table columns: Date, Fund, (Group by), Residual (Prod), Residual (S&P), Abs Residual (Prod), Abs Residual (S&P)
    Also supports L1 and L2 breakdowns via a 3-way toggle.
    """
    data_folder = current_app.config['DATA_FOLDER']
    file_path = os.path.join(data_folder, 'att_factors.csv')
    if not os.path.exists(file_path):
        return "att_factors.csv not found", 404
    # Load data
    df = pd.read_csv(file_path)
    df.columns = df.columns.str.strip()  # Remove accidental spaces
    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')
    df = df.dropna(subset=['Date', 'Fund'])
    # --- Load w_secs.csv and extract static characteristics ---
    wsecs_path = os.path.join(data_folder, 'w_secs.csv')
    wsecs = pd.read_csv(wsecs_path)
    wsecs.columns = wsecs.columns.str.strip()
    static_cols = [col for col in wsecs.columns if col not in ['ISIN'] and not _is_date_like(col)]
    wsecs_static = wsecs.drop_duplicates(subset=['ISIN'])[['ISIN'] + static_cols]
    # --- Characteristic selection ---
    available_characteristics = static_cols
    selected_characteristic = request.args.get('characteristic', default='Type', type=str)
    if selected_characteristic not in available_characteristics:
        selected_characteristic = available_characteristics[0] if available_characteristics else None
    # Join att_factors with w_secs static info
    df = df.merge(wsecs_static, on='ISIN', how='left')
    # Get available funds and date range for UI
    available_funds = sorted(df['Fund'].dropna().unique())
    min_date = df['Date'].min()
    max_date = df['Date'].max()
    # Get filter parameters from query string
    selected_fund = request.args.get('fund', default='', type=str)
    start_date_str = request.args.get('start_date', default=None, type=str)
    end_date_str = request.args.get('end_date', default=None, type=str)
    selected_level = request.args.get('level', default='L0', type=str)
    selected_characteristic_value = request.args.get('characteristic_value', default='', type=str)
    # Default to first fund if none selected
    if not selected_fund and available_funds:
        selected_fund = available_funds[0]
    # Parse date range
    start_date = pd.to_datetime(start_date_str, errors='coerce') if start_date_str else min_date
    end_date = pd.to_datetime(end_date_str, errors='coerce') if end_date_str else max_date
    # Helper: L2 column names for each group
    l2_credit = [
        'Credit Spread Change Daily', 'Credit Convexity Daily', 'Credit Carry Daily', 'Credit Defaulted'
    ]
    l2_rates = [
        'Rates Carry Daily', 'Rates Convexity Daily', 'Rates Curve Daily', 'Rates Duration Daily', 'Rates Roll Daily'
    ]
    l2_fx = [
        'FX Carry Daily', 'FX Change Daily'
    ]
    l2_all = l2_credit + l2_rates + l2_fx
    def sum_l2s(row, prefix):
        credit = sum([row.get(f'{prefix}{col}', 0) for col in l2_credit])
        rates = sum([row.get(f'{prefix}{col}', 0) for col in l2_rates])
        fx = sum([row.get(f'{prefix}{col}', 0) for col in l2_fx])
        return credit, rates, fx
    def compute_residual(row, l0_col, l2_prefix):
        l0 = row.get(l0_col, 0)
        credit, rates, fx = sum_l2s(row, l2_prefix)
        l1_total = credit + rates + fx
        return l0 - l1_total
    # Prepare results: group by Date, Fund, and selected characteristic
    group_cols = ['Date', 'Fund']
    if selected_characteristic:
        group_cols.append(selected_characteristic)
    benchmark_results = []
    portfolio_results = []
    # Compute available values for the selected characteristic
    if selected_characteristic:
        available_characteristic_values = sorted(df[selected_characteristic].dropna().unique())
    else:
        available_characteristic_values = []
    # L1 factor groupings
    l1_groups = {
        'Rates': [
            'Rates Carry Daily', 'Rates Convexity Daily', 'Rates Curve Daily', 'Rates Duration Daily', 'Rates Roll Daily'
        ],
        'Credit': [
            'Credit Spread Change Daily', 'Credit Convexity Daily', 'Credit Carry Daily', 'Credit Defaulted'
        ],
        'FX': [
            'FX Carry Daily', 'FX Change Daily'
        ]
    }
    for group_keys, group in df.groupby(group_cols):
        if selected_characteristic:
            date, fund, char_val = group_keys
        else:
            date, fund = group_keys
            char_val = None
        if selected_fund and fund != selected_fund:
            continue
        if not (start_date <= date <= end_date):
            continue
        # Filter by characteristic value if set
        if selected_characteristic and selected_characteristic_value:
            if char_val != selected_characteristic_value:
                continue
        # L0: Residuals and Abs Residuals
        if selected_level == 'L0' or not selected_level:
            bench_prod_res = group.apply(lambda row: compute_residual(row, 'L0 Bench Total Daily', 'L2 Bench '), axis=1)
            bench_sp_res = group.apply(lambda row: compute_residual(row, 'L0 Bench Total Daily', 'SPv3_L2 Bench '), axis=1)
            port_prod_res = group.apply(lambda row: compute_residual(row, 'L0 Port Total Daily ', 'L2 Port '), axis=1)
            port_sp_res = group.apply(lambda row: compute_residual(row, 'L0 Port Total Daily ', 'SPv3_L2 Port '), axis=1)
            bench_prod = bench_prod_res.sum()
            bench_sp = bench_sp_res.sum()
            port_prod = port_prod_res.sum()
            port_sp = port_sp_res.sum()
            bench_prod_abs = bench_prod_res.abs().sum()
            bench_sp_abs = bench_sp_res.abs().sum()
            port_prod_abs = port_prod_res.abs().sum()
            port_sp_abs = port_sp_res.abs().sum()
            row_info_bench = {
                'Date': date, 'Fund': fund, 'Residual_Prod': bench_prod, 'Residual_SP': bench_sp,
                'AbsResidual_Prod': bench_prod_abs, 'AbsResidual_SP': bench_sp_abs
            }
            row_info_port = {
                'Date': date, 'Fund': fund, 'Residual_Prod': port_prod, 'Residual_SP': port_sp,
                'AbsResidual_Prod': port_prod_abs, 'AbsResidual_SP': port_sp_abs
            }
            if selected_characteristic:
                row_info_bench[selected_characteristic] = char_val
                row_info_port[selected_characteristic] = char_val
            benchmark_results.append(row_info_bench)
            portfolio_results.append(row_info_port)
        # L1: Show L1 Rates, Credit, FX (Prod and S&P)
        elif selected_level == 'L1':
            # Aggregate L1s for Prod and S&P
            bench_l1_credit_prod = group.apply(lambda row: sum([row.get(f'L2 Bench {col}', 0) for col in l2_credit]), axis=1).sum()
            bench_l1_credit_sp = group.apply(lambda row: sum([row.get(f'SPv3_L2 Bench {col}', 0) for col in l2_credit]), axis=1).sum()
            bench_l1_rates_prod = group.apply(lambda row: sum([row.get(f'L2 Bench {col}', 0) for col in l2_rates]), axis=1).sum()
            bench_l1_rates_sp = group.apply(lambda row: sum([row.get(f'SPv3_L2 Bench {col}', 0) for col in l2_rates]), axis=1).sum()
            bench_l1_fx_prod = group.apply(lambda row: sum([row.get(f'L2 Bench {col}', 0) for col in l2_fx]), axis=1).sum()
            bench_l1_fx_sp = group.apply(lambda row: sum([row.get(f'SPv3_L2 Bench {col}', 0) for col in l2_fx]), axis=1).sum()
            port_l1_credit_prod = group.apply(lambda row: sum([row.get(f'L2 Port {col}', 0) for col in l2_credit]), axis=1).sum()
            port_l1_credit_sp = group.apply(lambda row: sum([row.get(f'SPv3_L2 Port {col}', 0) for col in l2_credit]), axis=1).sum()
            port_l1_rates_prod = group.apply(lambda row: sum([row.get(f'L2 Port {col}', 0) for col in l2_rates]), axis=1).sum()
            port_l1_rates_sp = group.apply(lambda row: sum([row.get(f'SPv3_L2 Port {col}', 0) for col in l2_rates]), axis=1).sum()
            port_l1_fx_prod = group.apply(lambda row: sum([row.get(f'L2 Port {col}', 0) for col in l2_fx]), axis=1).sum()
            port_l1_fx_sp = group.apply(lambda row: sum([row.get(f'SPv3_L2 Port {col}', 0) for col in l2_fx]), axis=1).sum()
            row_info_bench = {
                'Date': date, 'Fund': fund,
                'L1Rates_Prod': bench_l1_rates_prod, 'L1Rates_SP': bench_l1_rates_sp,
                'L1Credit_Prod': bench_l1_credit_prod, 'L1Credit_SP': bench_l1_credit_sp,
                'L1FX_Prod': bench_l1_fx_prod, 'L1FX_SP': bench_l1_fx_sp
            }
            row_info_port = {
                'Date': date, 'Fund': fund,
                'L1Rates_Prod': port_l1_rates_prod, 'L1Rates_SP': port_l1_rates_sp,
                'L1Credit_Prod': port_l1_credit_prod, 'L1Credit_SP': port_l1_credit_sp,
                'L1FX_Prod': port_l1_fx_prod, 'L1FX_SP': port_l1_fx_sp
            }
            if selected_characteristic:
                row_info_bench[selected_characteristic] = char_val
                row_info_port[selected_characteristic] = char_val
            benchmark_results.append(row_info_bench)
            portfolio_results.append(row_info_port)
        # L2: Show all L2 values (Prod and S&P) side by side
        elif selected_level == 'L2':
            # For each L2, sum for group, for both Prod and S&P
            l2prod = {col: group.apply(lambda row: row.get(f'L2 Bench {col}', 0), axis=1).sum() for col in l2_all}
            l2sp = {col: group.apply(lambda row: row.get(f'SPv3_L2 Bench {col}', 0), axis=1).sum() for col in l2_all}
            l2prod_port = {col: group.apply(lambda row: row.get(f'L2 Port {col}', 0), axis=1).sum() for col in l2_all}
            l2sp_port = {col: group.apply(lambda row: row.get(f'SPv3_L2 Port {col}', 0), axis=1).sum() for col in l2_all}
            row_info_bench = {
                'Date': date, 'Fund': fund,
                'L2Prod': l2prod, 'L2SP': l2sp, 'L2ProdKeys': l2_all
            }
            row_info_port = {
                'Date': date, 'Fund': fund,
                'L2Prod': l2prod_port, 'L2SP': l2sp_port, 'L2ProdKeys': l2_all
            }
            if selected_characteristic:
                row_info_bench[selected_characteristic] = char_val
                row_info_port[selected_characteristic] = char_val
            benchmark_results.append(row_info_bench)
            portfolio_results.append(row_info_port)
    # Sort
    benchmark_results = sorted(benchmark_results, key=lambda x: (x['Date'], x['Fund'], x.get(selected_characteristic, '')))
    portfolio_results = sorted(portfolio_results, key=lambda x: (x['Date'], x['Fund'], x.get(selected_characteristic, '')))
    return render_template(
        'attribution_summary.html',
        benchmark_results=benchmark_results,
        portfolio_results=portfolio_results,
        available_funds=available_funds,
        selected_fund=selected_fund,
        min_date=min_date,
        max_date=max_date,
        start_date=start_date,
        end_date=end_date,
        available_characteristics=available_characteristics,
        selected_characteristic=selected_characteristic,
        selected_level=selected_level,
        available_characteristic_values=available_characteristic_values,
        selected_characteristic_value=selected_characteristic_value
    )
@attribution_bp.route('/charts')
def attribution_charts():
    """
    Attribution Residuals Chart Page: Shows residuals over time for Benchmark and Portfolio (Prod and S&P),
    with filters and grouping as in the summary view. Data is prepared for JS charts.
    """
    data_folder = current_app.config['DATA_FOLDER']
    file_path = os.path.join(data_folder, 'att_factors.csv')
    if not os.path.exists(file_path):
        return "att_factors.csv not found", 404
    # Load data
    df = pd.read_csv(file_path)
    df.columns = df.columns.str.strip()
    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')
    df = df.dropna(subset=['Date', 'Fund'])
    # --- Load w_secs.csv and extract static characteristics ---
    wsecs_path = os.path.join(data_folder, 'w_secs.csv')
    wsecs = pd.read_csv(wsecs_path)
    wsecs.columns = wsecs.columns.str.strip()
    static_cols = [col for col in wsecs.columns if col not in ['ISIN'] and not _is_date_like(col)]
    wsecs_static = wsecs.drop_duplicates(subset=['ISIN'])[['ISIN'] + static_cols]
    # --- Characteristic selection ---
    available_characteristics = static_cols
    selected_characteristic = request.args.get('characteristic', default='Type', type=str)
    if selected_characteristic not in available_characteristics:
        selected_characteristic = available_characteristics[0] if available_characteristics else None
    selected_characteristic_value = request.args.get('characteristic_value', default='', type=str)
    # Join att_factors with w_secs static info
    df = df.merge(wsecs_static, on='ISIN', how='left')
    # Get available funds and date range for UI
    available_funds = sorted(df['Fund'].dropna().unique())
    min_date = df['Date'].min()
    max_date = df['Date'].max()
    # Get filter parameters from query string
    selected_fund = request.args.get('fund', default='', type=str)
    start_date_str = request.args.get('start_date', default=None, type=str)
    end_date_str = request.args.get('end_date', default=None, type=str)
    # Default to first fund if none selected
    if not selected_fund and available_funds:
        selected_fund = available_funds[0]
    # Parse date range
    start_date = pd.to_datetime(start_date_str, errors='coerce') if start_date_str else min_date
    end_date = pd.to_datetime(end_date_str, errors='coerce') if end_date_str else max_date
    # Compute available values for the selected characteristic
    if selected_characteristic:
        available_characteristic_values = sorted(df[selected_characteristic].dropna().unique())
    else:
        available_characteristic_values = []
    # Filter by characteristic value if set
    if selected_characteristic and selected_characteristic_value:
        df = df[df[selected_characteristic] == selected_characteristic_value]
    # Filter by fund
    if selected_fund:
        df = df[df['Fund'] == selected_fund]
    # Filter by date range
    df = df[(df['Date'] >= start_date) & (df['Date'] <= end_date)]
    # Helper: L2 column names for each group
    l2_credit = [
        'Credit Spread Change Daily', 'Credit Convexity Daily', 'Credit Carry Daily', 'Credit Defaulted'
    ]
    l2_rates = [
        'Rates Carry Daily', 'Rates Convexity Daily', 'Rates Curve Daily', 'Rates Duration Daily', 'Rates Roll Daily'
    ]
    l2_fx = [
        'FX Carry Daily', 'FX Change Daily'
    ]
    def sum_l2s(row, prefix):
        credit = sum([row.get(f'{prefix}{col}', 0) for col in l2_credit])
        rates = sum([row.get(f'{prefix}{col}', 0) for col in l2_rates])
        fx = sum([row.get(f'{prefix}{col}', 0) for col in l2_fx])
        return credit, rates, fx
    def compute_residual(row, l0_col, l2_prefix):
        l0 = row.get(l0_col, 0)
        credit, rates, fx = sum_l2s(row, l2_prefix)
        l1_total = credit + rates + fx
        return l0 - l1_total
    # Group by Date (and characteristic if grouping)
    group_cols = ['Date']
    if selected_characteristic:
        group_cols.append(selected_characteristic)
    # Prepare time series data for charts
    chart_data_bench = []
    chart_data_port = []
    # Sort by date for cumulative
    for group_keys, group in df.groupby(group_cols):
        if selected_characteristic:
            date, char_val = group_keys
        else:
            date, = group_keys
            char_val = None
        # Aggregate residuals for this day (sum over all ISINs)
        bench_prod_res = group.apply(lambda row: compute_residual(row, 'L0 Bench Total Daily', 'L2 Bench '), axis=1)
        bench_sp_res = group.apply(lambda row: compute_residual(row, 'L0 Bench Total Daily', 'SPv3_L2 Bench '), axis=1)
        port_prod_res = group.apply(lambda row: compute_residual(row, 'L0 Port Total Daily ', 'L2 Port '), axis=1)
        port_sp_res = group.apply(lambda row: compute_residual(row, 'L0 Port Total Daily ', 'SPv3_L2 Port '), axis=1)
        # Net and abs
        bench_prod = bench_prod_res.sum()
        bench_sp = bench_sp_res.sum()
        port_prod = port_prod_res.sum()
        port_sp = port_sp_res.sum()
        bench_prod_abs = bench_prod_res.abs().sum()
        bench_sp_abs = bench_sp_res.abs().sum()
        port_prod_abs = port_prod_res.abs().sum()
        port_sp_abs = port_sp_res.abs().sum()
        chart_data_bench.append({
            'date': date.strftime('%Y-%m-%d'),
            'residual_prod': bench_prod,
            'residual_sp': bench_sp,
            'abs_residual_prod': bench_prod_abs,
            'abs_residual_sp': bench_sp_abs
        })
        chart_data_port.append({
            'date': date.strftime('%Y-%m-%d'),
            'residual_prod': port_prod,
            'residual_sp': port_sp,
            'abs_residual_prod': port_prod_abs,
            'abs_residual_sp': port_sp_abs
        })
    # Sort by date
    chart_data_bench = sorted(chart_data_bench, key=lambda x: x['date'])
    chart_data_port = sorted(chart_data_port, key=lambda x: x['date'])
    # Add cumulative net residuals
    cum_bench_prod = 0
    cum_bench_sp = 0
    cum_port_prod = 0
    cum_port_sp = 0
    for d in chart_data_bench:
        cum_bench_prod += d['residual_prod']
        cum_bench_sp += d['residual_sp']
        d['cum_residual_prod'] = cum_bench_prod
        d['cum_residual_sp'] = cum_bench_sp
    for d in chart_data_port:
        cum_port_prod += d['residual_prod']
        cum_port_sp += d['residual_sp']
        d['cum_residual_prod'] = cum_port_prod
        d['cum_residual_sp'] = cum_port_sp
    # Pass as JSON for JS charts
    chart_data_bench_json = json.dumps(chart_data_bench)
    chart_data_port_json = json.dumps(chart_data_port)
    return render_template(
        'attribution_charts.html',
        chart_data_bench_json=chart_data_bench_json,
        chart_data_port_json=chart_data_port_json,
        available_funds=available_funds,
        selected_fund=selected_fund,
        min_date=min_date,
        max_date=max_date,
        start_date=start_date,
        end_date=end_date,
        available_characteristics=available_characteristics,
        selected_characteristic=selected_characteristic,
        available_characteristic_values=available_characteristic_values,
        selected_characteristic_value=selected_characteristic_value,
        abs_toggle_default=False
    )
@attribution_bp.route('/radar')
def attribution_radar():
    """
    Attribution Radar Chart Page: Aggregates L1 or L2 factors (plus residual) for Portfolio and Benchmark
    for a single fund, over a selected date range and characteristic. Data is prepared for radar charts.
    """
    data_folder = current_app.config['DATA_FOLDER']
    file_path = os.path.join(data_folder, 'att_factors.csv')
    if not os.path.exists(file_path):
        return "att_factors.csv not found", 404
    # Load data
    df = pd.read_csv(file_path)
    df.columns = df.columns.str.strip()
    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')
    df = df.dropna(subset=['Date', 'Fund'])
    # --- Load w_secs.csv and extract static characteristics ---
    wsecs_path = os.path.join(data_folder, 'w_secs.csv')
    wsecs = pd.read_csv(wsecs_path)
    wsecs.columns = wsecs.columns.str.strip()
    static_cols = [col for col in wsecs.columns if col not in ['ISIN'] and not _is_date_like(col)]
    wsecs_static = wsecs.drop_duplicates(subset=['ISIN'])[['ISIN'] + static_cols]
    # --- Characteristic selection ---
    available_characteristics = static_cols
    selected_characteristic = request.args.get('characteristic', default='Type', type=str)
    if selected_characteristic not in available_characteristics:
        selected_characteristic = available_characteristics[0] if available_characteristics else None
    selected_characteristic_value = request.args.get('characteristic_value', default='', type=str)
    # Join att_factors with w_secs static info
    df = df.merge(wsecs_static, on='ISIN', how='left')
    # Get available funds and date range for UI
    available_funds = sorted(df['Fund'].dropna().unique())
    min_date = df['Date'].min()
    max_date = df['Date'].max()
    # Get filter parameters from query string
    selected_fund = request.args.get('fund', default='', type=str)
    start_date_str = request.args.get('start_date', default=None, type=str)
    end_date_str = request.args.get('end_date', default=None, type=str)
    selected_level = request.args.get('level', default='L2', type=str)  # Default to L2
    # Default to first fund if none selected
    if not selected_fund and available_funds:
        selected_fund = available_funds[0]
    # Parse date range
    start_date = pd.to_datetime(start_date_str, errors='coerce') if start_date_str else min_date
    end_date = pd.to_datetime(end_date_str, errors='coerce') if end_date_str else max_date
    # Compute available values for the selected characteristic
    if selected_characteristic:
        available_characteristic_values = sorted(df[selected_characteristic].dropna().unique())
    else:
        available_characteristic_values = []
    # Filter by characteristic value if set
    if selected_characteristic and selected_characteristic_value:
        df = df[df[selected_characteristic] == selected_characteristic_value]
    # Filter by fund (only one fund allowed)
    if selected_fund:
        df = df[df['Fund'] == selected_fund]
    # Filter by date range
    df = df[(df['Date'] >= start_date) & (df['Date'] <= end_date)]
    # Helper: L2 column names for each group
    l2_credit = [
        'Credit Spread Change Daily', 'Credit Convexity Daily', 'Credit Carry Daily', 'Credit Defaulted'
    ]
    l2_rates = [
        'Rates Carry Daily', 'Rates Convexity Daily', 'Rates Curve Daily', 'Rates Duration Daily', 'Rates Roll Daily'
    ]
    l2_fx = [
        'FX Carry Daily', 'FX Change Daily'
    ]
    l2_all = l2_credit + l2_rates + l2_fx
    # L1 groupings
    l1_groups = {
        'Rates': [
            'Rates Carry Daily', 'Rates Convexity Daily', 'Rates Curve Daily', 'Rates Duration Daily', 'Rates Roll Daily'
        ],
        'Credit': [
            'Credit Spread Change Daily', 'Credit Convexity Daily', 'Credit Carry Daily', 'Credit Defaulted'
        ],
        'FX': [
            'FX Carry Daily', 'FX Change Daily'
        ]
    }
    # --- Aggregation for Radar Chart ---
    def sum_l2s_block(df_block, prefix, l2_cols):
        return [df_block[f'{prefix}{col}'].sum() if f'{prefix}{col}' in df_block else 0 for col in l2_cols]
    def sum_l1s_block(df_block, prefix):
        return [df_block[[f'{prefix}{col}' for col in l2s if f'{prefix}{col}' in df_block]].sum().sum() for l2s in l1_groups.values()]
    def compute_residual_block(df_block, l0_col, l2_prefix, l2_cols):
        l0 = df_block[l0_col].sum() if l0_col in df_block else 0
        l2_sum = sum([df_block[f'{l2_prefix}{col}'].sum() if f'{l2_prefix}{col}' in df_block else 0 for col in l2_cols])
        return l0 - l2_sum
    # Prepare radar data for Portfolio and Benchmark, Prod and S&P
    radar_labels = []
    port_prod = []
    port_sp = []
    bench_prod = []
    bench_sp = []
    residual_labels = ['Residual']
    if selected_level == 'L1':
        radar_labels = list(l1_groups.keys()) + residual_labels
        # Portfolio
        port_l1_prod = sum_l1s_block(df, 'L2 Port ', l2_all)
        port_l1_sp = sum_l1s_block(df, 'SPv3_L2 Port ', l2_all)
        port_resid_prod = compute_residual_block(df, 'L0 Port Total Daily ', 'L2 Port ', l2_all)
        port_resid_sp = compute_residual_block(df, 'L0 Port Total Daily ', 'SPv3_L2 Port ', l2_all)
        port_prod = port_l1_prod + [port_resid_prod]
        port_sp = port_l1_sp + [port_resid_sp]
        # Benchmark
        bench_l1_prod = sum_l1s_block(df, 'L2 Bench ', l2_all)
        bench_l1_sp = sum_l1s_block(df, 'SPv3_L2 Bench ', l2_all)
        bench_resid_prod = compute_residual_block(df, 'L0 Bench Total Daily', 'L2 Bench ', l2_all)
        bench_resid_sp = compute_residual_block(df, 'L0 Bench Total Daily', 'SPv3_L2 Bench ', l2_all)
        bench_prod = bench_l1_prod + [bench_resid_prod]
        bench_sp = bench_l1_sp + [bench_resid_sp]
    else:  # L2 (default)
        radar_labels = l2_all + residual_labels
        # Portfolio
        port_l2_prod = sum_l2s_block(df, 'L2 Port ', l2_all)
        port_l2_sp = sum_l2s_block(df, 'SPv3_L2 Port ', l2_all)
        port_resid_prod = compute_residual_block(df, 'L0 Port Total Daily ', 'L2 Port ', l2_all)
        port_resid_sp = compute_residual_block(df, 'L0 Port Total Daily ', 'SPv3_L2 Port ', l2_all)
        port_prod = port_l2_prod + [port_resid_prod]
        port_sp = port_l2_sp + [port_resid_sp]
        # Benchmark
        bench_l2_prod = sum_l2s_block(df, 'L2 Bench ', l2_all)
        bench_l2_sp = sum_l2s_block(df, 'SPv3_L2 Bench ', l2_all)
        bench_resid_prod = compute_residual_block(df, 'L0 Bench Total Daily', 'L2 Bench ', l2_all)
        bench_resid_sp = compute_residual_block(df, 'L0 Bench Total Daily', 'SPv3_L2 Bench ', l2_all)
        bench_prod = bench_l2_prod + [bench_resid_prod]
        bench_sp = bench_l2_sp + [bench_resid_sp]
    # Prepare output for Chart.js radar chart
    radar_data = {
        'labels': radar_labels,
        'portfolio': {
            'prod': port_prod,
            'sp': port_sp
        },
        'benchmark': {
            'prod': bench_prod,
            'sp': bench_sp
        }
    }
    return render_template(
        'attribution_radar.html',
        radar_data_json=json.dumps(radar_data),
        available_funds=available_funds,
        selected_fund=selected_fund,
        min_date=min_date,
        max_date=max_date,
        start_date=start_date,
        end_date=end_date,
        available_characteristics=available_characteristics,
        selected_characteristic=selected_characteristic,
        available_characteristic_values=available_characteristic_values,
        selected_characteristic_value=selected_characteristic_value,
        selected_level=selected_level
    )
@attribution_bp.route('/security')
def attribution_security_page():
    """
    Attribution Security-Level Page: Shows attribution data for each security (ISIN) for a selected date and fund.
    Supports filtering by date, fund, type, bench/portfolio toggle, MTD aggregation, normalization, and pagination.
    Table columns: Security Name (linked), ISIN, Type, Returns (L0 Total), Original Residual, S&P Residual, Residual Diff, L1 values (Orig & S&P)
    """
    import numpy as np
    from pandas.tseries.offsets import BDay
    data_folder = current_app.config['DATA_FOLDER']
    att_path = os.path.join(data_folder, 'att_factors.csv')
    wsecs_path = os.path.join(data_folder, 'w_secs.csv')
    if not os.path.exists(att_path) or not os.path.exists(wsecs_path):
        return "Required data files not found", 404
    # Load data
    df = pd.read_csv(att_path)
    df.columns = df.columns.str.strip()
    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')
    df = df.dropna(subset=['Date', 'Fund', 'ISIN'])
    wsecs = pd.read_csv(wsecs_path)
    wsecs.columns = wsecs.columns.str.strip()
    static_cols = [col for col in wsecs.columns if col not in ['ISIN'] and not _is_date_like(col)]
    wsecs_static = wsecs.drop_duplicates(subset=['ISIN'])[['ISIN'] + static_cols]
    # --- UI Controls ---
    # Date picker: default to previous business day
    max_date = df['Date'].max()
    prev_bday = (max_date if max_date is not pd.NaT else pd.Timestamp.today())
    prev_bday = prev_bday if prev_bday.weekday() < 5 else prev_bday - BDay(1)
    selected_date_str = request.args.get('date', default=prev_bday.strftime('%Y-%m-%d'), type=str)
    selected_date = pd.to_datetime(selected_date_str, errors='coerce')
    # Fund dropdown: default to first fund
    available_funds = sorted(df['Fund'].dropna().unique())
    selected_fund = request.args.get('fund', default=available_funds[0] if available_funds else '', type=str)
    # Type filter
    available_types = sorted(wsecs_static['Type'].dropna().unique())
    selected_type = request.args.get('type', default='', type=str)
    # Bench/Portfolio toggle
    bench_or_port = request.args.get('bench_or_port', default='bench', type=str)  # 'bench' or 'port'
    # MTD toggle
    mtd = request.args.get('mtd', default='off', type=str) == 'on'
    # Normalize toggle
    normalize = request.args.get('normalize', default='off', type=str) == 'on'
    # Pagination
    page = request.args.get('page', 1, type=int)
    per_page = 50
    # --- Filter and Join Data ---
    df = df[df['Fund'] == selected_fund]
    if selected_type:
        # Join to get type for filtering
        df = df.merge(wsecs_static[['ISIN', 'Type']], on='ISIN', how='left')
        df = df[df['Type'] == selected_type]
    else:
        df = df.merge(wsecs_static[['ISIN', 'Type']], on='ISIN', how='left')
    # Date filtering/aggregation
    if mtd:
        # Get all dates in month up to selected_date
        if selected_date is pd.NaT:
            selected_date = max_date
        month_start = selected_date.replace(day=1)
        mtd_dates = pd.date_range(month_start, selected_date, freq='B')
        df = df[df['Date'].isin(mtd_dates)]
        # Fill missing days for each ISIN with zeros
        all_isins = df['ISIN'].unique()
        all_dates = mtd_dates
        idx = pd.MultiIndex.from_product([all_isins, all_dates], names=['ISIN', 'Date'])
        df = df.set_index(['ISIN', 'Date'])
        df = df.reindex(idx, fill_value=0).reset_index().merge(df.reset_index(), on=['ISIN', 'Date'], how='left', suffixes=('', '_orig'))
        # Use original columns where available, otherwise zeros
        for col in df.columns:
            if col.endswith('_orig'):
                base = col[:-5]
                df[base] = df[col].combine_first(df[base])
        # Aggregate (sum) over month for each ISIN
        group_cols = ['ISIN']
        # Only sum numeric columns, keep first for non-numeric
        numeric_cols = df.select_dtypes(include=['number']).columns.tolist()
        agg_dict = {col: 'sum' for col in numeric_cols if col not in group_cols}
        # For non-numeric columns (like 'Type', 'Security Name'), keep the first value
        for col in ['Type', 'Security Name']:
            if col in df.columns:
                agg_dict[col] = 'first'
        df = df.groupby(group_cols).agg(agg_dict).reset_index()
        # Add back static info
        df = df.merge(wsecs_static, on='ISIN', how='left')
    else:
        df = df[df['Date'] == selected_date]
    # --- Normalization ---
    if bench_or_port == 'bench':
        weight_col = 'Bench Weight'
        l0_col = 'L0 Bench Total Daily'
        l1_prefix = 'L2 Bench '
        l1_sp_prefix = 'SPv3_L2 Bench '
    else:
        weight_col = 'Port Exp Wgt'
        l0_col = 'L0 Port Total Daily '
        l1_prefix = 'L2 Port '
        l1_sp_prefix = 'SPv3_L2 Port '
    # L1 factor names
    l1_factors = [
        'Rates Carry Daily', 'Rates Convexity Daily', 'Rates Curve Daily', 'Rates Duration Daily', 'Rates Roll Daily',
        'Credit Spread Change Daily', 'Credit Convexity Daily', 'Credit Carry Daily', 'Credit Defaulted',
        'FX Carry Daily', 'FX Change Daily'
    ]
    # L1 groupings (must be in this scope)
    l1_groups = {
        'Rates': [
            'Rates Carry Daily', 'Rates Convexity Daily', 'Rates Curve Daily', 'Rates Duration Daily', 'Rates Roll Daily'
        ],
        'Credit': [
            'Credit Spread Change Daily', 'Credit Convexity Daily', 'Credit Carry Daily', 'Credit Defaulted'
        ],
        'FX': [
            'FX Carry Daily', 'FX Change Daily'
        ]
    }
    # Residual calculation
    def calc_residual(row, l0, l1_prefix):
        l1_sum = sum([row.get(f'{l1_prefix}{f}', 0) for f in l1_factors])
        return row.get(l0, 0) - l1_sum
    # Normalization
    def norm(row, col, weight_col):
        w = row.get(weight_col, 0)
        v = row.get(col, 0)
        if normalize and w:
            return v / w
        return v
    # Prepare table rows
    table_rows = []
    for _, row in df.iterrows():
        returns = norm(row, l0_col, weight_col)
        orig_resid = calc_residual(row, l0_col, l1_prefix)
        sp_resid = calc_residual(row, l0_col, l1_sp_prefix)
        orig_resid = norm(row, None, weight_col) if normalize and weight_col else orig_resid
        sp_resid = norm(row, None, weight_col) if normalize and weight_col else sp_resid
        resid_diff = orig_resid - sp_resid
        # Calculate L1 values as sum of L2s for each group
        l1_vals = {}
        for l1_name, l2_list in l1_groups.items():
            orig_sum = sum([norm(row, f'{l1_prefix}{l2}', weight_col) for l2 in l2_list])
            sp_sum = sum([norm(row, f'{l1_sp_prefix}{l2}', weight_col) for l2 in l2_list])
            l1_vals[l1_name] = (orig_sum, sp_sum)
        table_rows.append({
            'Security Name': row.get('Security Name', ''),
            'ISIN': row['ISIN'],
            'Type': row.get('Type', ''),
            'Returns': returns,
            'Original Residual': orig_resid,
            'S&P Residual': sp_resid,
            'Residual Diff': resid_diff,
            'L1 Values': l1_vals
        })
    # Sort by abs(Original Residual)
    table_rows = sorted(table_rows, key=lambda r: abs(r['Original Residual']), reverse=True)
    # Pagination
    total_items = len(table_rows)
    total_pages = max(1, (total_items + per_page - 1) // per_page)
    page = max(1, min(page, total_pages))
    start = (page - 1) * per_page
    end = start + per_page
    page_rows = table_rows[start:end]
    # Pagination object for template
    class Pagination:
        def __init__(self, page, per_page, total_items):
            self.page = page
            self.per_page = per_page
            self.total_items = total_items
            self.total_pages = max(1, (total_items + per_page - 1) // per_page)
            self.has_prev = page > 1
            self.has_next = page < self.total_pages
            self.prev_num = page - 1
            self.next_num = page + 1
            self.start_page_display = max(1, page - 2)
            self.end_page_display = min(self.total_pages, page + 2)
        def url_for_page(self, p):
            args = request.args.to_dict()
            args['page'] = p
            return url_for('attribution_bp.attribution_security_page', **args)
    pagination = Pagination(page, per_page, total_items)
    return render_template(
        'attribution_security_page.html',
        rows=page_rows,
        pagination=pagination,
        available_funds=available_funds,
        selected_fund=selected_fund,
        available_types=available_types,
        selected_type=selected_type,
        selected_date=selected_date.strftime('%Y-%m-%d') if selected_date is not pd.NaT else '',
        bench_or_port=bench_or_port,
        mtd=mtd,
        normalize=normalize
    )
</file>

<file path="views/comparison_views.py">
# views/comparison_views.py
# This module defines the Flask Blueprint for comparing two security spread datasets.
# It includes routes for a summary view listing securities with comparison metrics
# and a detail view showing overlayed time-series charts and statistics for a single security.
from flask import Blueprint, render_template, request, current_app, jsonify, url_for
import pandas as pd
import os
import logging
import math # Add math import for pagination calculation
from pathlib import Path
import re
from datetime import datetime
# Assuming security_processing and utils are in the parent directory or configured in PYTHONPATH
try:
    from security_processing import load_and_process_security_data, calculate_security_latest_metrics # May need adjustments
    from utils import parse_fund_list # Example utility
    from config import COLOR_PALETTE # Still need colors
except ImportError:
    # Handle potential import errors if the structure is different
    logging.error("Could not import required modules from parent directory.")
    # Add fallback imports or path adjustments if necessary
    # Example: sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
    from ..security_processing import load_and_process_security_data, calculate_security_latest_metrics
    from ..utils import parse_fund_list
    from ..config import COLOR_PALETTE
# Import utils module properly
try: 
    from utils import _is_date_like # Try direct import
except ImportError:
    # Define our own function if import fails
    def _is_date_like(column_name):
        """Check if a column name looks like a date.
        Returns True for formats like: YYYY-MM-DD, DD/MM/YYYY, DD-MM-YYYY, etc."""
        if not isinstance(column_name, str):
            return False
        # Match common date formats in column names
        date_patterns = [
            r'\d{2}/\d{2}/\d{4}',  # DD/MM/YYYY
            r'\d{2}-\d{2}-\d{4}',  # DD-MM-YYYY
            r'\d{4}/\d{2}/\d{2}',  # YYYY/MM/DD
            r'\d{4}-\d{2}-\d{2}',  # YYYY-MM-DD
            r'\d{1,2}/\d{1,2}/\d{2,4}',  # D/M/YY or D/M/YYYY
            r'\d{1,2}-\d{1,2}-\d{2,4}',  # D-M-YY or D-M-YYYY
        ]
        # Return True if any pattern matches
        return any(re.search(pattern, column_name) for pattern in date_patterns)
comparison_bp = Blueprint('comparison_bp', __name__,
                        template_folder='../templates',
                        static_folder='../static')
# Configure logging
# log = logging.getLogger(__name__)
PER_PAGE_COMPARISON = 50 # Items per page for comparison summary
# --- Data Loading and Processing ---
def load_weights_and_held_status(data_folder: str, weights_filename: str = 'w_secs.csv', id_col_override: str = 'ISIN') -> pd.Series:
    """
    Loads the weights file (e.g., w_secs.csv), identifies the latest date,
    and returns a boolean Series indicating which securities (indexed by ISIN)
    have a non-zero weight on that date (i.e., are currently held).
    Args:
        data_folder: The absolute path to the data directory.
        weights_filename: The name of the weights file.
        id_col_override: The specific column name in the weights file expected to contain the ISINs for joining.
    Returns:
        A pandas Series where the index is the Security ID (ISIN) and the value
        is True if the security is held on the latest date, False otherwise.
        Returns an empty Series if the file cannot be loaded or processed.
    """
    current_app.logger.info(f"--- Entering load_weights_and_held_status for {weights_filename} ---")
    weights_filepath = Path(data_folder) / weights_filename
    if not weights_filepath.exists():
        current_app.logger.warning(f"Weights file not found: {weights_filepath}")
        return pd.Series(dtype=bool)
    try:
        current_app.logger.info(f"Loading weights data from: {weights_filepath}")
        # Load without setting index initially to easily find date/ID columns
        weights_df = pd.read_csv(weights_filepath, low_memory=False)
        weights_df.columns = weights_df.columns.str.strip() # Clean column names
        # --- Identify Date and ID columns ---
        date_col = next((col for col in weights_df.columns if 'date' in col.lower()), None)
        # Prioritize the explicitly provided id_col_override, then look for ISIN/SecurityID
        id_col_in_file = id_col_override if id_col_override in weights_df.columns else \
                         next((col for col in weights_df.columns if col.lower() in ['isin', 'securityid']), None)
        # Check for ID column - required
        if not id_col_in_file:
            current_app.logger.error(f"Could not automatically identify ID column in {weights_filepath}. Columns found: {weights_df.columns.tolist()}")
            return pd.Series(dtype=bool)
        current_app.logger.info(f"Weights file ID column identified: '{id_col_in_file}'")
        # --- Identify and Melt Date Columns ---
        date_columns = [col for col in weights_df.columns if _is_date_like(col)]
        if not date_columns:
            # If no date-like columns were found and no explicit date column exists
            if not date_col:
                current_app.logger.error(f"No date column or date-like columns found in {weights_filepath}")
                return pd.Series(dtype=bool)
            # Try to use the explicitly found date column
            current_app.logger.info(f"No date-like columns found. Attempting to use explicit date column: '{date_col}'")
            try:
                weights_df[date_col] = pd.to_datetime(weights_df[date_col], errors='coerce')
                if weights_df[date_col].isnull().all():
                    raise ValueError("Date column parsing failed.")
                # If successful, proceed as if it was a long-format file from the start
                value_col = next((col for col in weights_df.columns if col.lower() == 'value'), 'Value') # Assume a 'Value' column exists
                if value_col not in weights_df.columns:
                    # If no obvious value column, assume last column is value
                    value_col = weights_df.columns[-1]
                    current_app.logger.warning(f"No 'Value' column found in long-format weights file, assuming last column '{value_col}' holds weights.")
                # Rename for consistency
                weights_df = weights_df.rename(columns={date_col: 'Date', id_col_in_file: 'ISIN', value_col: 'Value'})
                weights_df['Value'] = pd.to_numeric(weights_df['Value'], errors='coerce')
            except Exception as e:
                current_app.logger.error(f"Failed to process weights file {weights_filepath} as long format after date column detection failed: {e}")
                return pd.Series(dtype=bool)
        else:
            current_app.logger.info(f"Found {len(date_columns)} date-like columns in {weights_filename}: {date_columns[:5]}{'...' if len(date_columns) > 5 else ''}")
            # Wide format: Melt the DataFrame
            id_vars = [col for col in weights_df.columns if col not in date_columns]
            melted_weights = weights_df.melt(id_vars=id_vars, value_vars=date_columns, var_name='Date', value_name='Value')
            # Attempt to convert 'Date' column to datetime objects after melting
            melted_weights['Date'] = pd.to_datetime(melted_weights['Date'], errors='coerce')
            melted_weights['Value'] = pd.to_numeric(melted_weights['Value'], errors='coerce')
            # Rename the identified ID column to 'ISIN' for consistency
            melted_weights = melted_weights.rename(columns={id_col_in_file: 'ISIN'})
            weights_df = melted_weights # Use the melted df going forward
        # Find the latest date in the entire dataset
        latest_date = weights_df['Date'].max()
        if pd.isna(latest_date):
            current_app.logger.warning(f"Could not determine the latest date in {weights_filepath}.")
            return pd.Series(dtype=bool)
        current_app.logger.info(f"Latest date in weights file '{weights_filepath}': {latest_date}")
        # Filter for the latest date and where Value is not NaN and > 0
        latest_weights = weights_df[(weights_df['Date'] == latest_date) & (weights_df['Value'].notna()) & (weights_df['Value'] > 0)].copy()
        # --- Determine Held Status using the correct ID column ---
        # We now use the *renamed* 'ISIN' column, which originally came from id_col_in_file (our target)
        held_status_col = 'ISIN'
        if held_status_col not in latest_weights.columns:
             current_app.logger.error(f"The target ID column '{held_status_col}' (derived from '{id_col_in_file}') not found after processing {weights_filepath}. Columns: {latest_weights.columns.tolist()}")
             return pd.Series(dtype=bool)
        current_app.logger.info(f"Using '{held_status_col}' column from {weights_filename} for held_status index.")
        # Create the boolean Series: index is the ISIN, value is True
        # Drop duplicates in case a security appears multiple times on the same date (e.g., different funds)
        held_ids = latest_weights.drop_duplicates(subset=[held_status_col])[held_status_col]
        held_status = pd.Series(True, index=held_ids)
        held_status.index.name = 'ISIN' # Ensure the index name is 'ISIN' for merging
        # --- Logging for Debugging ---
        current_app.logger.debug(f"Weights data before setting index (first 5 rows using '{held_status_col}'):\n{latest_weights[[held_status_col, 'Value']].head().to_string()}")
        sample_values = latest_weights[held_status_col].unique()[:5]
        current_app.logger.debug(f"Sample values from '{held_status_col}' column to be used for index: {sample_values}")
        # This renaming logic is now less critical as we set the index correctly, but keep for consistency check
        if held_status.index.name != id_col_override:
             current_app.logger.warning(f"Renaming held_status index from '{held_status.index.name}' to '{id_col_override}' to ensure merge compatibility.")
             held_status.index.name = id_col_override # Explicitly set to ISIN for clarity
        current_app.logger.debug(f"Resulting held_status Series index name after potential rename: '{held_status.index.name}'")
        current_app.logger.debug(f"Held status index preview (first 5 values): {held_status.index[:5].tolist()}")
        current_app.logger.debug(f"Held status values preview (first 5): {held_status.head().to_dict()}")
        current_app.logger.info(f"Determined held status for {len(held_status)} IDs based on weights on {latest_date}.")
        return held_status
    except Exception as e:
        current_app.logger.error(f"Error loading or processing weights file {weights_filepath}: {e}", exc_info=True)
        return pd.Series(dtype=bool)
def load_comparison_data(data_folder_path: str, file1='sec_spread.csv', file2='sec_spreadSP.csv'):
    log = current_app.logger # Use app logger
    log.info(f"--- Entering load_comparison_data: {file1}, {file2} ---") # ENTRY LOG
    """Loads, processes, merges data from two security spread files, and gets held status.
    Args:
        data_folder_path (str): The absolute path to the data folder.
        file1 (str, optional): Filename for the first dataset. Defaults to 'sec_spread.csv'.
        file2 (str, optional): Filename for the second dataset. Defaults to 'sec_spreadSP.csv'.
    Returns:
        tuple: (merged_df, static_data, common_static_cols, id_col_name, held_status)
               Returns (pd.DataFrame(), pd.DataFrame(), [], None, pd.Series(dtype=bool)) on error.
    """
    log.info(f"Loading comparison data: {file1} and {file2} from {data_folder_path}")
    if not data_folder_path:
        log.error("No data_folder_path provided to load_comparison_data.")
        return pd.DataFrame(), pd.DataFrame(), [], None, pd.Series(dtype=bool)
    # Pass the absolute data folder path to the loading functions
    df1, static_cols1 = load_and_process_security_data(file1, data_folder_path)
    log.info(f"Loaded {file1}. Shape: {df1.shape if not df1.empty else 'Empty'}. Static cols: {static_cols1}")
    df2, static_cols2 = load_and_process_security_data(file2, data_folder_path)
    log.info(f"Loaded {file2}. Shape: {df2.shape if not df2.empty else 'Empty'}. Static cols: {static_cols2}")
    # Reset index to make Date and ID columns accessible
    if not df1.empty:
        df1 = df1.reset_index()
    if not df2.empty:
        df2 = df2.reset_index()
    # Load held status, passing the data folder path
    held_status = load_weights_and_held_status(data_folder_path)
    log.info(f"Loaded held_status. Is empty: {held_status.empty}")
    if df1.empty or df2.empty:
        log.warning(f"One or both dataframes are empty. File1 empty: {df1.empty}, File2 empty: {df2.empty}")
        return pd.DataFrame(), pd.DataFrame(), [], None, held_status # Still return status
    common_static_cols = list(set(static_cols1) & set(static_cols2))
    # Identify ID column - check for 'ISIN' first
    if 'ISIN' in df1.columns:
        id_col_name = 'ISIN'
        log.info(f"Identified ID column from columns: {id_col_name}")
    # Explicitly check if there are columns before accessing index 0
    elif not df1.empty and len(df1.columns) > 0:
        potential_id = df1.columns[0]
        log.warning(f"\'ISIN\' column not found in df1. Attempting to use the first column \'{potential_id}\' as ID. This might be incorrect.")
        id_col_name = potential_id
        if id_col_name not in df2.columns:
            log.error(f"Fallback ID column '{id_col_name}' from df1 not found in df2.")
            return pd.DataFrame(), pd.DataFrame(), [], None, held_status
    else:
        log.error("Failed to identify ID column. \'ISIN\' not found and DataFrame might be empty or malformed.")
        return pd.DataFrame(), pd.DataFrame(), [], None, held_status
    if id_col_name in common_static_cols:
        common_static_cols.remove(id_col_name)
        log.debug(f"Removed ID column '{id_col_name}' from common_static_cols.")
    try:
        df1_merge = df1[[id_col_name, 'Date', 'Value'] + common_static_cols].rename(columns={'Value': 'Value_Orig'})
        if id_col_name not in df2.columns:
             log.error(f"ID column '{id_col_name}' identified in df1 not found in df2 columns: {df2.columns.tolist()}")
             raise KeyError(f"ID column '{id_col_name}' not found in second dataframe")
        df2_merge = df2[[id_col_name, 'Date', 'Value']].rename(columns={'Value': 'Value_New'})
    except KeyError as e:
        log.error(f"Missing required column for merge preparation: {e}. Df1 cols: {df1.columns.tolist()}, Df2 cols: {df2.columns.tolist()}")
        return pd.DataFrame(), pd.DataFrame(), [], None, held_status
    merged_df = pd.merge(df1_merge, df2_merge, on=[id_col_name, 'Date'], how='outer')
    merged_df = merged_df.sort_values(by=[id_col_name, 'Date'])
    merged_df['Change_Orig'] = merged_df.groupby(id_col_name)['Value_Orig'].diff()
    merged_df['Change_New'] = merged_df.groupby(id_col_name)['Value_New'].diff()
    static_data = merged_df.groupby(id_col_name)[common_static_cols].last().reset_index()
    log.info(f"Successfully merged data. Shape: {merged_df.shape}")
    # Return held_status along with other data
    log.info(f"--- Exiting load_comparison_data. Merged shape: {merged_df.shape if not merged_df.empty else 'Empty'}. ID col: {id_col_name} ---") # EXIT LOG
    return merged_df, static_data, common_static_cols, id_col_name, held_status
def calculate_comparison_stats(merged_df, static_data, id_col):
    log = current_app.logger # Use app logger
    log.info(f"--- Entering calculate_comparison_stats. Input shape: {merged_df.shape if not merged_df.empty else 'Empty'} ---") # ENTRY LOG
    """Calculates comparison statistics for each security.
    Args:
        merged_df (pd.DataFrame): The merged dataframe of original and new values.
        static_data (pd.DataFrame): DataFrame with static info per security.
        id_col (str): The name of the column containing the Security ID/Name.
    """
    if merged_df.empty:
        return pd.DataFrame()
    if id_col not in merged_df.columns:
        log.error(f"Specified id_col '{id_col}' not found in merged_df columns: {merged_df.columns.tolist()}")
        return pd.DataFrame() # Cannot group without the ID column
    log.info(f"Calculating comparison statistics using ID column: {id_col}...")
    stats_list = []
    # Use the passed id_col here
    for sec_id, group in merged_df.groupby(id_col):
        sec_stats = {id_col: sec_id} # Use actual id_col name
        # Filter out rows where both values are NaN for overall analysis period
        group_valid_overall = group.dropna(subset=['Value_Orig', 'Value_New'], how='all')
        overall_min_date = group_valid_overall['Date'].min()
        overall_max_date = group_valid_overall['Date'].max()
        # Filter out rows where EITHER value is NaN for correlation/diff calculations
        valid_comparison = group.dropna(subset=['Value_Orig', 'Value_New'])
        # 1. Correlation of Levels
        if len(valid_comparison) >= 2: # Need at least 2 points for correlation
            # Use the NaN-dropped dataframe for correlation
            level_corr = valid_comparison['Value_Orig'].corr(valid_comparison['Value_New'])
            sec_stats['Level_Correlation'] = level_corr if pd.notna(level_corr) else None
        else:
             sec_stats['Level_Correlation'] = None
        # 2. Max / Min (use original group to get true max/min including non-overlapping points)
        sec_stats['Max_Orig'] = group['Value_Orig'].max()
        sec_stats['Min_Orig'] = group['Value_Orig'].min()
        sec_stats['Max_New'] = group['Value_New'].max()
        sec_stats['Min_New'] = group['Value_New'].min()
        # 3. Date Range Comparison - Refined Logic
        # Find min/max dates within the MERGED data where each series is individually valid
        min_date_orig_idx = group['Value_Orig'].first_valid_index()
        max_date_orig_idx = group['Value_Orig'].last_valid_index()
        min_date_new_idx = group['Value_New'].first_valid_index()
        max_date_new_idx = group['Value_New'].last_valid_index()
        sec_stats['Start_Date_Orig'] = group.loc[min_date_orig_idx, 'Date'] if min_date_orig_idx is not None else None
        sec_stats['End_Date_Orig'] = group.loc[max_date_orig_idx, 'Date'] if max_date_orig_idx is not None else None
        sec_stats['Start_Date_New'] = group.loc[min_date_new_idx, 'Date'] if min_date_new_idx is not None else None
        sec_stats['End_Date_New'] = group.loc[max_date_new_idx, 'Date'] if max_date_new_idx is not None else None
        # Check if the start and end dates MATCH for the valid periods of EACH series
        same_start = pd.Timestamp(sec_stats['Start_Date_Orig']) == pd.Timestamp(sec_stats['Start_Date_New']) if sec_stats['Start_Date_Orig'] and sec_stats['Start_Date_New'] else False
        same_end = pd.Timestamp(sec_stats['End_Date_Orig']) == pd.Timestamp(sec_stats['End_Date_New']) if sec_stats['End_Date_Orig'] and sec_stats['End_Date_New'] else False
        sec_stats['Same_Date_Range'] = same_start and same_end
        # Add overall date range for info
        sec_stats['Overall_Start_Date'] = overall_min_date
        sec_stats['Overall_End_Date'] = overall_max_date
        # 4. Correlation of Daily Changes (Volatility Alignment)
        # Use the dataframe where BOTH values are non-NaN to calculate changes for correlation
        valid_comparison = valid_comparison.copy() # Avoid SettingWithCopyWarning
        valid_comparison['Change_Orig_Corr'] = valid_comparison['Value_Orig'].diff()
        valid_comparison['Change_New_Corr'] = valid_comparison['Value_New'].diff()
        # Drop NaNs created by the diff() itself (first row)
        valid_changes = valid_comparison.dropna(subset=['Change_Orig_Corr', 'Change_New_Corr'])
        # --- Debug Logging Start ---
        # if sec_id == 'Alpha001': # Log only for a specific security to avoid flooding
        #     log.debug(f"Debug {sec_id} - valid_changes DataFrame (first 5 rows):\n{valid_changes.head()}")
        #     log.debug(f"Debug {sec_id} - valid_changes count: {len(valid_changes)}")
        # --- Debug Logging End ---
        if len(valid_changes) >= 2:
            change_corr = valid_changes['Change_Orig_Corr'].corr(valid_changes['Change_New_Corr'])
            sec_stats['Change_Correlation'] = change_corr if pd.notna(change_corr) else None
        else:
            sec_stats['Change_Correlation'] = None
            # Log why correlation is None
            log.debug(f"Cannot calculate Change_Correlation for {sec_id}. Need >= 2 valid change pairs, found {len(valid_changes)}.")
        # 5. Difference Statistics (use the valid_comparison df where both values exist)
        valid_comparison['Abs_Diff'] = (valid_comparison['Value_Orig'] - valid_comparison['Value_New']).abs()
        sec_stats['Mean_Abs_Diff'] = valid_comparison['Abs_Diff'].mean() # Mean diff where both values exist
        sec_stats['Max_Abs_Diff'] = valid_comparison['Abs_Diff'].max() # Max diff where both values exist
        # Count NaNs - use original group
        sec_stats['NaN_Count_Orig'] = group['Value_Orig'].isna().sum()
        sec_stats['NaN_Count_New'] = group['Value_New'].isna().sum()
        sec_stats['Total_Points'] = len(group)
        stats_list.append(sec_stats)
    summary_df = pd.DataFrame(stats_list)
    # Merge static data back
    if not static_data.empty and id_col in static_data.columns and id_col in summary_df.columns:
        summary_df = pd.merge(summary_df, static_data, on=id_col, how='left')
    elif not static_data.empty:
         log.warning(f"Could not merge static data back. ID column '{id_col}' missing from static_data ({id_col in static_data.columns}) or summary_df ({id_col in summary_df.columns}).")
    log.info(f"Finished calculating stats. Summary shape: {summary_df.shape}")
    log.info(f"--- Exiting calculate_comparison_stats. Output shape: {summary_df.shape if summary_df is not None and not summary_df.empty else 'Empty'} ---") # EXIT LOG
    return summary_df
# --- Routes ---
@comparison_bp.route('/comparison/logtest')
def log_test():
    current_app.logger.info("--- !!! HIT /comparison/logtest ROUTE !!! ---")
    return "Log test route executed. Check instance/app.log.", 200
@comparison_bp.route('/comparison/summary')
def summary():
    current_app.logger.info("--- Starting Spread Comparison Summary Request ---")
    data_folder = current_app.config.get('DATA_FOLDER', './Data')
    page = request.args.get('page', 1, type=int)
    sort_by = request.args.get('sort_by', 'Max_Abs_Diff') # Default sort
    sort_order = request.args.get('sort_order', 'desc')
    # Get filters from query string (e.g., ?filter_Type=Corp&filter_Currency=USD)
    active_filters = {k.replace('filter_', ''): v for k, v in request.args.items() if k.startswith('filter_') and v}
    # Get show_sold status
    show_sold_str = request.args.get('show_sold', 'false') # Default to false (only show held)
    show_sold = show_sold_str.lower() == 'true'
    current_app.logger.info(f"Request Params: Page={page}, SortBy={sort_by}, Order={sort_order}, Filters={active_filters}, ShowSold={show_sold}")
    # --- Load Weights/Held Status ---
    # Ensure we use the 'ISIN' column from w_secs.csv
    held_status = load_weights_and_held_status(data_folder, id_col_override='ISIN')
    if held_status.empty:
        current_app.logger.warning("Could not load held status information for comparison.")
    # --- Load and Process Comparison Data ---
    file1='sec_spread.csv'
    file2='sec_spreadSP.csv'
    id_col_name_actual = 'ISIN' # Assume ISIN as default/target
    try:
        current_app.logger.info(f"Loading comparison data: {file1} and {file2} from {data_folder}")
        # Use helper function to load, standardize, and identify columns
        merged_df, static_data, static_cols_actual, id_col_name_actual, held_status_loaded = load_comparison_data(
            data_folder, file1, file2)
        if merged_df.empty:
            raise ValueError("Failed to load one or both comparison files.")
        current_app.logger.info(f"Actual ID column identified for comparison data: '{id_col_name_actual}'")
        current_app.logger.info(f"Calculating comparison statistics using ID column: {id_col_name_actual}...")
        # Calculate stats using the identified columns
        summary_stats = calculate_comparison_stats(merged_df, static_data, id_col=id_col_name_actual)
        current_app.logger.info(f"Finished calculating stats. Summary shape: {summary_stats.shape}")
        # Prepare filter options based on actual static columns found in the summary
        filter_options = {col: sorted(summary_stats[col].dropna().unique().tolist()) 
                          for col in static_cols_actual if col in summary_stats.columns}
        # --- Merge Held Status ---
        if not held_status.empty and id_col_name_actual in summary_stats.columns:
            # Ensure merge keys are the same type (convert to string as a safe bet)
            if summary_stats[id_col_name_actual].dtype != held_status.index.dtype:
                current_app.logger.info(f"Converted merge keys to string. Original dtypes: summary_stats['{id_col_name_actual}']: {summary_stats[id_col_name_actual].dtype}, held_status.index: {held_status.index.dtype}")
                try:
                    summary_stats[id_col_name_actual] = summary_stats[id_col_name_actual].astype(str)
                    held_status.index = held_status.index.astype(str)
                except Exception as e:
                     current_app.logger.error(f"Failed to convert merge keys to string: {e}")
            current_app.logger.info(f"Attempting to merge held_status (index name: '{held_status.index.name}') with comparison summary_stats on column '{id_col_name_actual}'")
            debug_cols_before = summary_stats.columns.tolist()
            current_app.logger.debug(f"summary_stats columns before merge: {debug_cols_before}")
            current_app.logger.debug(f"held_status index preview before merge: {held_status.index[:5].tolist()}")
            current_app.logger.debug(f"summary_stats ID column ('{id_col_name_actual}') preview before merge: {summary_stats[id_col_name_actual].unique()[:5].tolist()}")
            # Rename the Series when merging
            summary_stats = pd.merge(summary_stats, held_status.rename('is_held'), 
                                    left_on=id_col_name_actual, right_index=True, how='left')
            debug_cols_after = summary_stats.columns.tolist()
            current_app.logger.debug(f"Columns after merge attempt: {debug_cols_after}")
            held_count = summary_stats['is_held'].notna().sum()
            current_app.logger.info(f"Merged held status. Comparison stats shape: {summary_stats.shape}. 'is_held' column has {held_count} non-NA values.")
            if held_count > 0:
                current_app.logger.debug(f"Preview of 'is_held' after merge (first 5 non-NA): {summary_stats[summary_stats['is_held'].notna()]['is_held'].head().to_dict()}")
            else:
                current_app.logger.debug("Preview of 'is_held' after merge (first 5 non-NA): {}")
            # Fill NaN in 'is_held'
            if 'is_held' in summary_stats.columns:
                summary_stats['is_held'] = summary_stats['is_held'].fillna(False)
                current_app.logger.info("Filled NA in 'is_held' with False for comparison summary.")
            else:
                current_app.logger.error("'is_held' column not found after comparison merge!")
                summary_stats['is_held'] = False # Add the column as False if merge failed entirely
        else:
            current_app.logger.warning("Skipping merge with held_status: held_status is empty or ID column mismatch.")
            summary_stats['is_held'] = False # Assume not held if we can't merge
        # --- Apply Filters ---
        filtered_stats = summary_stats.copy()
        # Apply holding filter first if needed
        if not show_sold:
             held_count_before = len(filtered_stats)
             filtered_stats = filtered_stats[filtered_stats['is_held'] == True]
             held_count_after = len(filtered_stats)
             current_app.logger.info(f"Applied 'Show Held Only' filter. Kept {held_count_after} out of {held_count_before} securities.")
             if held_count_after == 0:
                 current_app.logger.warning("No securities remaining after applying holding status filter.")
        # Apply other active filters
        for col, value in active_filters.items():
            if col in filtered_stats.columns:
                count_before = len(filtered_stats)
                try:
                    # Try direct comparison first
                    filtered_stats = filtered_stats[filtered_stats[col] == value]
                except TypeError:
                     try:
                         # If direct comparison fails (mixed types), try converting both to string
                         filtered_stats = filtered_stats[filtered_stats[col].astype(str) == str(value)]
                         current_app.logger.warning(f"Applied filter {col}={value} after converting column to string due to type mismatch.")
                     except Exception as e_filter:
                         current_app.logger.error(f"Could not apply filter {col}={value}: {e_filter}")
                count_after = len(filtered_stats)
                current_app.logger.info(f"Applied filter: {col} == '{value}'. Kept {count_after}/{count_before} rows.")
        # --- Handle No Data ---
        if filtered_stats.empty:
            message = "No comparison data available matching the current filters."
            # Try to provide a more helpful message
            if not show_sold and 'is_held' in summary_stats.columns and summary_stats['is_held'].any(): # Check if any securities were held *before* filtering
                 message = "No *currently held* securities match the other filters. Try enabling 'Show Sold Securities'."
            elif not active_filters and not show_sold:
                  message = "No currently held securities found in the comparison data. Try enabling 'Show Sold Securities'."
            elif not active_filters and show_sold:
                   message = "No comparison data found in the source files."
            current_app.logger.warning(message)
            return render_template('comparison_page.html',
                                   table_data=[],
                                   columns_to_display=[],
                                   id_column_name=id_col_name_actual,
                                   message=message,
                                   pagination=None,
                                   current_sort_by=sort_by,
                                   current_sort_order=sort_order,
                                   filter_options=filter_options,
                                   active_filters=active_filters,
                                   show_sold=show_sold)
        # --- Sorting ---
        if sort_by in filtered_stats.columns:
            try:
                # Define numeric columns for proper sorting
                numeric_cols = ['Level_Correlation', 'Change_Correlation', 'Mean_Abs_Diff', 'Max_Abs_Diff', 'Max_Orig', 'Min_Orig', 'Max_New', 'Min_New']
                if sort_by in numeric_cols:
                    filtered_stats[sort_by] = pd.to_numeric(filtered_stats[sort_by], errors='coerce')
                    # Ensure NaNs are handled consistently
                    na_position = 'last' if sort_order == 'asc' else 'first' # Sort NaNs last for asc, first for desc
                    filtered_stats = filtered_stats.sort_values(by=sort_by, ascending=(sort_order == 'asc'), na_position=na_position)
                else:
                     # Sort non-numeric columns case-insensitively
                     filtered_stats = filtered_stats.sort_values(by=sort_by, ascending=(sort_order == 'asc'), key=lambda col: col.astype(str).str.lower())
                current_app.logger.info(f"Sorted data by '{sort_by}' ({sort_order}).")
            except Exception as e:
                current_app.logger.error(f"Error sorting data by {sort_by}: {e}")
        # --- Pagination ---
        per_page = 50 # Number of items per page
        total_items = len(filtered_stats)
        total_pages = math.ceil(total_items / per_page)
        total_pages = max(1, total_pages)
        page = max(1, min(page, total_pages))
        start_index = (page - 1) * per_page
        end_index = start_index + per_page
        paginated_data = filtered_stats.iloc[start_index:end_index]
        current_app.logger.info(f"Pagination: Total items={total_items}, Total pages={total_pages}, Current page={page}, Displaying items {start_index}-{end_index-1}")
        page_window = 2
        start_page_display = max(1, page - page_window)
        end_page_display = min(total_pages, page + page_window)
        # Create pagination context dictionary
        pagination = {
            'page': page,
            'per_page': per_page,
            'total_items': total_items,
            'total_pages': total_pages,
            'has_prev': page > 1,
            'has_next': page < total_pages,
            'prev_num': page - 1,
            'next_num': page + 1,
            'start_page_display': start_page_display,
            'end_page_display': end_page_display,
            # Function to generate URLs for pagination links, preserving state
            'url_for_page': lambda p: url_for('comparison_bp.summary', 
                                            page=p, 
                                            sort_by=sort_by, 
                                            sort_order=sort_order, 
                                            show_sold=str(show_sold).lower(),
                                            **{f'filter_{k}': v for k, v in active_filters.items()})
        }
        # Convert paginated data to list of dicts for the template
        table_data_dict = paginated_data.to_dict(orient='records')
        # Define columns to display in the summary table (ensure they exist in the data)
        columns_to_display = [id_col_name_actual, 'Type', 'Currency', 'Funds', 'Level_Correlation', 'Change_Correlation', 'Mean_Abs_Diff', 'Max_Abs_Diff', 'Same_Date_Range', 'is_held']
        columns_to_display = [col for col in columns_to_display if col in paginated_data.columns]
        current_app.logger.info(f"Rendering comparison summary page {page}/{pagination['total_pages']} with {len(table_data_dict)} rows.")
        return render_template('comparison_page.html',
                               table_data=table_data_dict,
                               columns_to_display=columns_to_display,
                               id_column_name=id_col_name_actual, # Pass the actual ID column name
                               pagination=pagination,
                               current_sort_by=sort_by,
                               current_sort_order=sort_order,
                               filter_options=filter_options,
                               active_filters=active_filters,
                               show_sold=show_sold,
                               message=None) # Clear message if data is found
    except FileNotFoundError as e:
        current_app.logger.error(f"Comparison file not found: {e}", exc_info=True)
        flash(f"Error: Required comparison file not found ({e.filename}). Please check the Data folder.", 'danger')
        return render_template('comparison_page.html', table_data=[], columns_to_display=[], pagination=None, message=f"Error: Could not find file {e.filename}.", id_column_name='ISIN', filter_options={}, active_filters={}, show_sold=show_sold)
    except Exception as e:
        current_app.logger.error(f"Error generating comparison summary: {e}", exc_info=True)
        flash(f"An unexpected error occurred during comparison: {e}", 'danger')
        return render_template('comparison_page.html', table_data=[], columns_to_display=[], pagination=None, message=f"An unexpected error occurred: {e}", id_column_name='ISIN', filter_options={}, active_filters={}, show_sold=show_sold)
@comparison_bp.route('/comparison/details/<path:security_id>')
def comparison_details(security_id):
    """Displays side-by-side historical charts for a specific security."""
    current_app.logger.info(f"--- Starting Comparison Detail Request for Security ID: {security_id} ---")
    # Retrieve the configured absolute data folder path
    data_folder = current_app.config['DATA_FOLDER']
    if not data_folder:
        current_app.logger.error("DATA_FOLDER is not configured in the application.")
        return "Internal Server Error: Data folder not configured", 500
    try:
        # Pass the absolute data folder path here
        merged_df, static_data, common_static_cols, id_col, _ = load_comparison_data(data_folder)
        if merged_df.empty:
            current_app.logger.warning("Merged data is empty, cannot show details.")
            return "Error: Could not load comparison data.", 404
        if id_col is None or id_col not in merged_df.columns:
             current_app.logger.error(f"ID column ('{id_col}') not found in merged data for details view.")
             return "Error: Could not identify security ID column in data.", 500
        # Filter using the actual ID column name
        security_data = merged_df[merged_df[id_col] == security_id].copy()
        if security_data.empty:
            return "Security ID not found", 404
        # Get the static data for this specific security
        sec_static_data = static_data[static_data[id_col] == security_id]
        # Recalculate detailed stats for this security, passing the correct ID column
        stats_df = calculate_comparison_stats(security_data.copy(), sec_static_data, id_col=id_col)
        security_stats = stats_df.iloc[0].where(pd.notnull(stats_df.iloc[0]), None).to_dict() if not stats_df.empty else {}
        # Prepare chart data
        security_data['Date_Str'] = security_data['Date'].dt.strftime('%Y-%m-%d')
        # Convert NaN to None using list comprehension after .tolist()
        data_orig = security_data['Value_Orig'].tolist()
        data_orig_processed = [None if pd.isna(x) else x for x in data_orig]
        data_new = security_data['Value_New'].tolist()
        data_new_processed = [None if pd.isna(x) else x for x in data_new]
        chart_data = {
            'labels': security_data['Date_Str'].tolist(),
            'datasets': [
                {
                    'label': 'Original Spread (Sec_spread)',
                    'data': data_orig_processed, # Use processed list
                    'borderColor': COLOR_PALETTE[0 % len(COLOR_PALETTE)],
                    'tension': 0.1
                },
                {
                    'label': 'New Spread (Sec_spreadSP)',
                    'data': data_new_processed, # Use processed list
                    'borderColor': COLOR_PALETTE[1 % len(COLOR_PALETTE)],
                    'tension': 0.1
                }
            ]
        }
        # Get static attributes for display (use actual_id_col if it's 'Security Name')
        # Best to get from security_stats which should now include merged static data
        security_name_display = security_stats.get('Security Name', security_id) if id_col == 'Security Name' else security_id
        # If 'Security Name' is not the ID, try to get it from stats
        if id_col != 'Security Name' and 'Security Name' in security_stats:
             security_name_display = security_stats.get('Security Name', security_id)
        return render_template('comparison_details_page.html',
                               security_id=security_id,
                               security_name=security_name_display,
                               chart_data=chart_data, # Pass as JSONifiable dict
                               stats=security_stats, # Pass comparison stats
                               id_column_name=id_col) # Pass actual ID col name
    except Exception as e:
        current_app.logger.exception(f"Error generating comparison details page for {security_id}.")
        return f"An error occurred: {e}", 500
</file>

<file path="views/curve_views.py">
# Purpose: Defines the Flask Blueprint for yield curve analysis views.
# Stdlib imports
from datetime import datetime
# Third-party imports
from flask import Blueprint, render_template, request, jsonify, current_app
import pandas as pd
import numpy as np
# Local imports
from curve_processing import load_curve_data, check_curve_inconsistencies, get_latest_curve_date
from config import COLOR_PALETTE
curve_bp = Blueprint('curve_bp', __name__, template_folder='../templates')
# --- Routes ---
@curve_bp.route('/curve/summary')
def curve_summary():
    """Displays a summary of yield curve checks for the latest date."""
    # Retrieve the absolute data folder path
    data_folder = current_app.config['DATA_FOLDER']
    if not data_folder:
        current_app.logger.error("DATA_FOLDER is not configured in the application.")
        return "Internal Server Error: Data folder not configured", 500
    current_app.logger.info("Loading curve data for summary...")
    curve_df = load_curve_data(data_folder_path=data_folder)
    if curve_df.empty:
        current_app.logger.warning("Curve data is empty or failed to load.")
        summary = {}
        latest_date_str = "N/A"
    else:
        current_app.logger.info("Checking curve inconsistencies...")
        summary = check_curve_inconsistencies(curve_df)
        latest_date = get_latest_curve_date(curve_df)
        latest_date_str = latest_date.strftime('%Y-%m-%d') if latest_date else "N/A"
        current_app.logger.info(f"Inconsistency summary generated for date: {latest_date_str}")
    return render_template('curve_summary.html',
                           summary=summary,
                           latest_date=latest_date_str)
@curve_bp.route('/curve/details/<currency>')
def curve_details(currency):
    """Displays the yield curve chart for a specific currency and date, with historical overlays."""
    current_app.logger.info(f"Loading curve data for currency: {currency}")
    # Retrieve the absolute data folder path
    data_folder = current_app.config['DATA_FOLDER']
    if not data_folder:
        current_app.logger.error("DATA_FOLDER is not configured in the application.")
        return "Internal Server Error: Data folder not configured", 500
    # Pass the data folder path to the loading function
    curve_df = load_curve_data(data_folder_path=data_folder)
    # Get parameters from request
    selected_date_str = request.args.get('date')
    try:
        num_prev_days = int(request.args.get('prev_days', 1))
    except ValueError:
        num_prev_days = 1
    available_dates = []
    curve_table_data = []
    chart_data = {'labels': [], 'datasets': []}
    selected_date = None
    if curve_df.empty:
        current_app.logger.warning(f"Curve data is empty for currency details: {currency}")
    else:
        # --- Get Available Dates for Currency ---
        try:
            if currency in curve_df.index.get_level_values('Currency'):
                available_dates = sorted(
                    curve_df.loc[currency].index.get_level_values('Date').unique(),
                    reverse=True
                )
            else:
                current_app.logger.warning(f"Currency '{currency}' not found in curve data index.")
                available_dates = []
        except Exception as e:
            current_app.logger.error(f"Error getting dates for {currency}: {e}", exc_info=True)
            available_dates = []
        # --- Determine Selected Date and Previous Date ---
        latest_date = available_dates[0] if available_dates else None
        previous_date = None # For daily change calculation
        if selected_date_str:
            try:
                selected_date = pd.to_datetime(selected_date_str).normalize()
                if selected_date not in available_dates:
                    current_app.logger.warning(f"Requested date {selected_date_str} not available for {currency}, falling back to latest.")
                    selected_date = latest_date
                else:
                    # Find the actual previous date in the available list
                    selected_date_index = available_dates.index(selected_date)
                    if selected_date_index + 1 < len(available_dates):
                        previous_date = available_dates[selected_date_index + 1]
                        current_app.logger.info(f"Previous date for change calc: {previous_date}")
                    else:
                        current_app.logger.info("Selected date is the oldest available, no previous date for change calc.")
            except ValueError:
                current_app.logger.warning(f"Invalid date format '{selected_date_str}', falling back to latest.")
                selected_date = latest_date
        else:
            selected_date = latest_date
            # If defaulting to latest, find the previous date
            if len(available_dates) > 1:
                 previous_date = available_dates[1]
                 current_app.logger.info(f"Defaulting to latest date. Previous date for change calc: {previous_date}")
        # Update selected_date_str after determining selected_date
        selected_date_str = selected_date.strftime('%Y-%m-%d') if selected_date else "N/A"
        # --- Prepare Data for Chart and Table ---
        if selected_date and available_dates:
            # 1. Process Selected Date for Labels, Chart, and Table Base
            curve_for_selected_date_df = pd.DataFrame() # Ensure it's defined
            try:
                mask_selected = (curve_df.index.get_level_values('Currency') == currency) & \
                                (curve_df.index.get_level_values('Date') == selected_date)
                curve_for_selected_date_df = curve_df[mask_selected].reset_index()
                if not curve_for_selected_date_df.empty:
                    curve_for_selected_date_df['TermMonths'] = (curve_for_selected_date_df['TermDays'] / 30).round(1)
                    curve_for_selected_date_df = curve_for_selected_date_df.sort_values('TermDays')
                    chart_data['labels'] = curve_for_selected_date_df['TermMonths'].tolist()
                    # Base table data on selected date
                    curve_table_df = curve_for_selected_date_df[['Term', 'TermDays', 'TermMonths', 'Value']].copy()
                else:
                    current_app.logger.warning(f"No data for selected date {selected_date_str} to generate labels/table.")
            except Exception as e:
                current_app.logger.error(f"Error processing selected date {selected_date_str} for labels/table base: {e}", exc_info=True)
            # 2. Calculate Daily Changes (if previous date exists)
            if previous_date and not curve_for_selected_date_df.empty:
                try:
                    mask_previous = (curve_df.index.get_level_values('Currency') == currency) & \
                                    (curve_df.index.get_level_values('Date') == previous_date)
                    curve_for_previous_date = curve_df[mask_previous].reset_index()
                    if not curve_for_previous_date.empty:
                        # Merge selected and previous date data on TermDays
                        merged_changes = pd.merge(
                            curve_for_selected_date_df[['TermDays', 'Value']],
                            curve_for_previous_date[['TermDays', 'Value']],
                            on='TermDays',
                            suffixes=('_selected', '_prev'),
                            how='left' # Keep all terms from selected date
                        )
                        merged_changes['ValueChange'] = merged_changes['Value_selected'] - merged_changes['Value_prev']
                        # Calculate deviation from average shift
                        average_curve_shift = merged_changes['ValueChange'].mean()
                        merged_changes['ChangeDeviation'] = merged_changes['ValueChange'] - average_curve_shift
                        # Calculate Z-score of the deviation
                        deviation_std = merged_changes['ChangeDeviation'].std()
                        if deviation_std != 0 and pd.notna(deviation_std): # Avoid division by zero
                             merged_changes['DeviationZScore'] = (merged_changes['ChangeDeviation'] - merged_changes['ChangeDeviation'].mean()) / deviation_std
                        else:
                             merged_changes['DeviationZScore'] = 0.0 # Or np.nan if preferred
                        # Merge calculated changes back into the main table DataFrame
                        curve_table_df = pd.merge(
                             curve_table_df,
                             merged_changes[['TermDays', 'ValueChange', 'ChangeDeviation', 'DeviationZScore']],
                             on='TermDays',
                             how='left' # Keep all terms from selected date
                        )
                    else:
                         current_app.logger.warning(f"No data found for previous date {previous_date.strftime('%Y-%m-%d')}")
                         # Add empty columns if previous data missing
                         curve_table_df['ValueChange'] = np.nan
                         curve_table_df['ChangeDeviation'] = np.nan
                         curve_table_df['DeviationZScore'] = np.nan
                except Exception as e:
                     current_app.logger.error(f"Error calculating daily changes: {e}", exc_info=True)
                     curve_table_df['ValueChange'] = np.nan
                     curve_table_df['ChangeDeviation'] = np.nan
                     curve_table_df['DeviationZScore'] = np.nan
            else:
                # Add empty columns if no previous date
                if 'ValueChange' not in curve_table_df.columns:
                    curve_table_df['ValueChange'] = np.nan
                    curve_table_df['ChangeDeviation'] = np.nan
                    curve_table_df['DeviationZScore'] = np.nan
            # Prepare final table data (convert df to dict)
            # Rename Value to Value_Display after calculations are done
            curve_table_df.rename(columns={'Value': 'Value_Display'}, inplace=True)
            curve_table_data = curve_table_df[['Term', 'TermMonths', 'Value_Display', 'ValueChange', 'ChangeDeviation', 'DeviationZScore']].to_dict('records')
            # 3. Fetch and prepare datasets for Chart
            # Determine the range of plot dates based on num_prev_days
            selected_date_index_for_plot = available_dates.index(selected_date)
            start_index = selected_date_index_for_plot
            end_index = min(len(available_dates), selected_date_index_for_plot + num_prev_days + 1)
            dates_to_plot = available_dates[start_index:end_index]
            current_app.logger.info(f"Plotting dates for {currency}: {dates_to_plot}")
            for i, plot_date in enumerate(dates_to_plot):
                try:
                    mask = (curve_df.index.get_level_values('Currency') == currency) & \
                           (curve_df.index.get_level_values('Date') == plot_date)
                    curve_for_plot_date_filtered = curve_df[mask]
                    if not curve_for_plot_date_filtered.empty:
                        curve_for_plot_date = curve_for_plot_date_filtered.reset_index().sort_values('TermDays')
                        # Align data points to selected date's TermDays
                        if not curve_for_selected_date_df.empty:
                             aligned_curve = curve_for_plot_date.set_index('TermDays')[['Value']].reindex(curve_for_selected_date_df['TermDays'])
                             plot_data_values = aligned_curve['Value'].fillna(np.nan).tolist()
                        else:
                             plot_data_values = []
                        # Determine color and style
                        color_index = i % len(COLOR_PALETTE)
                        border_color = COLOR_PALETTE[color_index]
                        if i > 0:
                            try:
                                r, g, b = int(border_color[1:3], 16), int(border_color[3:5], 16), int(border_color[5:7], 16)
                                border_color = f'rgba({r},{g},{b},0.4)'
                            except (IndexError, ValueError):
                                current_app.logger.warning(f"Could not parse color {border_color} for fading, using default.")
                        dataset = {
                            'label': f'{currency} ({plot_date.strftime("%Y-%m-%d")})',
                            'data': plot_data_values,
                            'borderColor': border_color,
                            'backgroundColor': border_color,
                            'fill': False,
                            'tension': 0.1,
                            'borderWidth': 2 if i == 0 else 1.5
                        }
                        chart_data['datasets'].append(dataset)
                    else:
                        current_app.logger.warning(f"No data found for {currency} on {plot_date.strftime('%Y-%m-%d')}")
                except Exception as e:
                    current_app.logger.error(f"Error processing plot data for {currency} on {plot_date.strftime('%Y-%m-%d')}: {e}", exc_info=True)
    # --- Final Rendering --- 
    return render_template('curve_details.html',
                           currency=currency,
                           chart_data=chart_data,
                           table_data=curve_table_data,
                           available_dates=[d.strftime('%Y-%m-%d') for d in available_dates],
                           selected_date=selected_date_str,
                           num_prev_days=num_prev_days,
                           color_palette=COLOR_PALETTE
                           )
</file>

<file path="views/duration_comparison_views.py">
# views/duration_comparison_views.py
# This module defines the Flask Blueprint for comparing two security duration datasets.
# It includes routes for a summary view listing securities with comparison metrics
# and a detail view showing overlayed time-series charts and statistics for a single security.
from flask import Blueprint, render_template, request, current_app, jsonify, url_for
import pandas as pd
import os
import logging
import math # Add math for pagination calculation
from pathlib import Path
import re
from datetime import datetime
# Import utils module properly
try: 
    from utils import _is_date_like # Try direct import
except ImportError:
    # Define our own function if import fails
    def _is_date_like(column_name):
        """Check if a column name looks like a date.
        Returns True for formats like: YYYY-MM-DD, DD/MM/YYYY, DD-MM-YYYY, etc."""
        if not isinstance(column_name, str):
            return False
        # Match common date formats in column names
        date_patterns = [
            r'\d{2}/\d{2}/\d{4}',  # DD/MM/YYYY
            r'\d{2}-\d{2}-\d{4}',  # DD-MM-YYYY
            r'\d{4}/\d{2}/\d{2}',  # YYYY/MM/DD
            r'\d{4}-\d{2}-\d{2}',  # YYYY-MM-DD
            r'\d{1,2}/\d{1,2}/\d{2,4}',  # D/M/YY or D/M/YYYY
            r'\d{1,2}-\d{1,2}-\d{2,4}',  # D-M-YY or D-M-YYYY
        ]
        # Return True if any pattern matches
        return any(re.search(pattern, column_name) for pattern in date_patterns)
# Assuming security_processing and utils are in the parent directory or configured in PYTHONPATH
try:
    from security_processing import load_and_process_security_data # May need adjustments
    from utils import parse_fund_list # Example utility
    from config import COLOR_PALETTE
except ImportError:
    # Handle potential import errors if the structure is different
    logging.error("Could not import required modules from parent directory.")
    # Add fallback imports or path adjustments if necessary
    # Example: sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
    from ..security_processing import load_and_process_security_data
    from ..utils import parse_fund_list
    from ..config import COLOR_PALETTE
duration_comparison_bp = Blueprint('duration_comparison_bp', __name__,
                        template_folder='../templates',
                        static_folder='../static')
# Configure logging
# log = logging.getLogger(__name__)
PER_PAGE_COMPARISON = 50 # Items per page for comparison summary
# --- Data Loading and Processing ---
def load_weights_and_held_status(data_folder: str, weights_filename: str = 'w_secs.csv', id_col_override: str = 'ISIN') -> pd.Series:
    """
    Loads the weights file (e.g., w_secs.csv), identifies the latest date,
    and returns a boolean Series indicating which securities (indexed by ISIN)
    have a non-zero weight on that date (i.e., are currently held).
    Args:
        data_folder: The absolute path to the data directory.
        weights_filename: The name of the weights file.
        id_col_override: The specific column name in the weights file expected to contain the ISINs for joining.
    Returns:
        A pandas Series where the index is the Security ID (ISIN) and the value
        is True if the security is held on the latest date, False otherwise.
        Returns an empty Series if the file cannot be loaded or processed.
    """
    current_app.logger.info(f"--- Entering load_weights_and_held_status for {weights_filename} ---")
    weights_filepath = Path(data_folder) / weights_filename
    if not weights_filepath.exists():
        current_app.logger.warning(f"Weights file not found: {weights_filepath}")
        return pd.Series(dtype=bool)
    try:
        current_app.logger.info(f"Loading weights data from: {weights_filepath}")
        # Load without setting index initially to easily find date/ID columns
        weights_df = pd.read_csv(weights_filepath, low_memory=False)
        weights_df.columns = weights_df.columns.str.strip() # Clean column names
        # --- Identify Date and ID columns ---
        date_col = next((col for col in weights_df.columns if 'date' in col.lower()), None)
        # Prioritize the explicitly provided id_col_override, then look for ISIN/SecurityID
        id_col_in_file = id_col_override if id_col_override in weights_df.columns else \
                         next((col for col in weights_df.columns if col.lower() in ['isin', 'securityid']), None)
        # Check for ID column - required
        if not id_col_in_file:
            current_app.logger.error(f"Could not automatically identify ID column in {weights_filepath}. Columns found: {weights_df.columns.tolist()}")
            return pd.Series(dtype=bool)
        current_app.logger.info(f"Weights file ID column identified: '{id_col_in_file}'")
        # --- Identify and Melt Date Columns ---
        # Use our local _is_date_like function
        date_columns = [col for col in weights_df.columns if _is_date_like(col)]
        if not date_columns:
            # If no date-like columns were found and no explicit date column exists
            if not date_col:
                current_app.logger.error(f"No date column or date-like columns found in {weights_filepath}")
                return pd.Series(dtype=bool)
            # Try to use the explicitly found date column
            current_app.logger.info(f"No date-like columns found. Attempting to use explicit date column: '{date_col}'")
            try:
                weights_df[date_col] = pd.to_datetime(weights_df[date_col], errors='coerce')
                if weights_df[date_col].isnull().all():
                    raise ValueError("Date column parsing failed.")
                # If successful, proceed as if it was a long-format file from the start
                value_col = next((col for col in weights_df.columns if col.lower() == 'value'), 'Value') # Assume a 'Value' column exists
                if value_col not in weights_df.columns:
                    # If no obvious value column, assume last column is value
                    value_col = weights_df.columns[-1]
                    current_app.logger.warning(f"No 'Value' column found in long-format weights file, assuming last column '{value_col}' holds weights.")
                # Rename for consistency
                weights_df = weights_df.rename(columns={date_col: 'Date', id_col_in_file: 'ISIN', value_col: 'Value'})
                weights_df['Value'] = pd.to_numeric(weights_df['Value'], errors='coerce')
            except Exception as e:
                current_app.logger.error(f"Failed to process weights file {weights_filepath} as long format after date column detection failed: {e}")
                return pd.Series(dtype=bool)
        else:
            current_app.logger.info(f"Found {len(date_columns)} date-like columns in {weights_filename}: {date_columns[:5]}{'...' if len(date_columns) > 5 else ''}")
            # Wide format: Melt the DataFrame
            id_vars = [col for col in weights_df.columns if col not in date_columns]
            melted_weights = weights_df.melt(id_vars=id_vars, value_vars=date_columns, var_name='Date', value_name='Value')
            # Attempt to convert 'Date' column to datetime objects after melting
            melted_weights['Date'] = pd.to_datetime(melted_weights['Date'], errors='coerce')
            melted_weights['Value'] = pd.to_numeric(melted_weights['Value'], errors='coerce')
            # Rename the identified ID column to 'ISIN' for consistency
            melted_weights = melted_weights.rename(columns={id_col_in_file: 'ISIN'})
            weights_df = melted_weights # Use the melted df going forward
        # Find the latest date in the entire dataset
        latest_date = weights_df['Date'].max()
        if pd.isna(latest_date):
            current_app.logger.warning(f"Could not determine the latest date in {weights_filepath}.")
            return pd.Series(dtype=bool)
        current_app.logger.info(f"Latest date in weights file '{weights_filepath}': {latest_date}")
        # Filter for the latest date and where Value is not NaN and > 0
        latest_weights = weights_df[(weights_df['Date'] == latest_date) & (weights_df['Value'].notna()) & (weights_df['Value'] > 0)].copy()
        # --- Determine Held Status using the correct ID column ---
        # We now use the *renamed* 'ISIN' column, which originally came from id_col_in_file (our target)
        held_status_col = 'ISIN'
        if held_status_col not in latest_weights.columns:
             current_app.logger.error(f"The target ID column '{held_status_col}' (derived from '{id_col_in_file}') not found after processing {weights_filepath}. Columns: {latest_weights.columns.tolist()}")
             return pd.Series(dtype=bool)
        current_app.logger.info(f"Using '{held_status_col}' column from {weights_filename} for held_status index.")
        # Create the boolean Series: index is the ISIN, value is True
        # Drop duplicates in case a security appears multiple times on the same date (e.g., different funds)
        held_ids = latest_weights.drop_duplicates(subset=[held_status_col])[held_status_col]
        held_status = pd.Series(True, index=held_ids)
        held_status.index.name = 'ISIN' # Ensure the index name is 'ISIN' for merging
        # --- Logging for Debugging ---
        current_app.logger.debug(f"Weights data before setting index (first 5 rows using '{held_status_col}'):\n{latest_weights[[held_status_col, 'Value']].head().to_string()}")
        sample_values = latest_weights[held_status_col].unique()[:5]
        current_app.logger.debug(f"Sample values from '{held_status_col}' column to be used for index: {sample_values}")
        # This renaming logic is now less critical as we set the index correctly, but keep for consistency check
        if held_status.index.name != id_col_override:
             current_app.logger.warning(f"Renaming held_status index from '{held_status.index.name}' to '{id_col_override}' to ensure merge compatibility.")
             held_status.index.name = id_col_override # Explicitly set to ISIN for clarity
        current_app.logger.debug(f"Resulting held_status Series index name after potential rename: '{held_status.index.name}'")
        current_app.logger.debug(f"Held status index preview (first 5 values): {held_status.index[:5].tolist()}")
        current_app.logger.debug(f"Held status values preview (first 5): {held_status.head().to_dict()}")
        current_app.logger.info(f"Determined held status for {len(held_status)} IDs based on weights on {latest_date}.")
        return held_status
    except Exception as e:
        current_app.logger.error(f"Error loading or processing weights file {weights_filepath}: {e}", exc_info=True)
        return pd.Series(dtype=bool)
def load_duration_comparison_data(data_folder_path: str, file1='sec_duration.csv', file2='sec_durationSP.csv'):
    """Loads, processes, merges data from two security duration files, and gets held status.
    Args:
        data_folder_path (str): The absolute path to the data folder.
        file1 (str, optional): Filename for the first dataset. Defaults to 'sec_duration.csv'.
        file2 (str, optional): Filename for the second dataset. Defaults to 'sec_durationSP.csv'.
    Returns:
        tuple: (merged_df, static_data, common_static_cols, id_col_name, held_status)
               Returns (pd.DataFrame(), pd.DataFrame(), [], None, pd.Series(dtype=bool)) on error.
    """
    log = current_app.logger # Use app logger
    log.info(f"Loading duration comparison data: {file1} and {file2} from {data_folder_path}")
    if not data_folder_path:
        log.error("No data_folder_path provided to load_duration_comparison_data.")
        return pd.DataFrame(), pd.DataFrame(), [], None, pd.Series(dtype=bool)
    # Load held status first (uses its own corrected loading logic)
    held_status = load_weights_and_held_status(data_folder_path)
    # Pass the absolute data folder path to the loading functions
    df1, static_cols1 = load_and_process_security_data(file1, data_folder_path)
    df2, static_cols2 = load_and_process_security_data(file2, data_folder_path)
    if df1.empty or df2.empty:
        log.warning(f"One or both duration dataframes are empty after loading. File1 empty: {df1.empty}, File2 empty: {df2.empty}")
        # Return held_status even if data is empty, as it might be needed
        return pd.DataFrame(), pd.DataFrame(), [], None, held_status
    # --- Verify Index and Get Actual Names ---
    if df1.index.nlevels != 2 or df2.index.nlevels != 2:
        log.error("One or both duration dataframes do not have the expected 2 index levels after loading.")
        return pd.DataFrame(), pd.DataFrame(), [], None, held_status
    # Assume index names are consistent between df1 and df2 as they use the same loader
    date_level_name, id_level_name = df1.index.names
    log.info(f"Duration data index levels identified: Date='{date_level_name}', ID='{id_level_name}'")
    # --- Reset Index ---
    df1 = df1.reset_index()
    df2 = df2.reset_index()
    log.debug(f"Duration df1 columns after reset: {df1.columns.tolist()}")
    log.debug(f"Duration df2 columns after reset: {df2.columns.tolist()}")
    # --- Check Required Columns (Post-Reset) ---
    required_cols_df1 = [id_level_name, date_level_name, 'Value']
    required_cols_df2 = [id_level_name, date_level_name, 'Value'] # Assuming Value is standard output name
    missing_cols_df1 = [col for col in required_cols_df1 if col not in df1.columns]
    missing_cols_df2 = [col for col in required_cols_df2 if col not in df2.columns]
    if missing_cols_df1 or missing_cols_df2:
        log.error(f"Missing required columns after index reset. Df1 missing: {missing_cols_df1}, Df2 missing: {missing_cols_df2}")
        return pd.DataFrame(), pd.DataFrame(), [], None, held_status
    # Common static columns (excluding the ID column which is now standard)
    common_static_cols = list(set(static_cols1) & set(static_cols2))
    if id_level_name in common_static_cols:
        common_static_cols.remove(id_level_name)
        log.debug(f"Removed ID column '{id_level_name}' from common_static_cols list.")
    # Ensure 'Value' is not accidentally in common_static_cols
    if 'Value' in common_static_cols:
        common_static_cols.remove('Value')
    # --- Merge Preparation (Using Correct Column Names) ---
    try:
        # Select using the dynamically identified date and id column names
        df1_merge = df1[[id_level_name, date_level_name, 'Value'] + common_static_cols].rename(columns={'Value': 'Value_Orig'})
        df2_merge = df2[[id_level_name, date_level_name, 'Value']].rename(columns={'Value': 'Value_New'})
    except KeyError as e:
        # This should be less likely now, but keep for safety
        log.error(f"KeyError during merge preparation using dynamic names '{id_level_name}', '{date_level_name}': {e}. Df1 cols: {df1.columns.tolist()}, Df2 cols: {df2.columns.tolist()}")
        return pd.DataFrame(), pd.DataFrame(), [], None, held_status
    # --- Perform Merge ---
    # Merge using the dynamic date and id column names
    merged_df = pd.merge(df1_merge, df2_merge, on=[id_level_name, date_level_name], how='outer')
    merged_df = merged_df.sort_values(by=[id_level_name, date_level_name])
    # Calculate changes using the dynamic ID column name for grouping
    merged_df['Change_Orig'] = merged_df.groupby(id_level_name)['Value_Orig'].diff()
    merged_df['Change_New'] = merged_df.groupby(id_level_name)['Value_New'].diff()
    # Extract static data using the dynamic ID column name
    static_data = merged_df.groupby(id_level_name)[common_static_cols].last().reset_index()
    log.info(f"Successfully merged duration data. Shape: {merged_df.shape}")
    # Return the dynamic ID column name for use in later functions
    return merged_df, static_data, common_static_cols, id_level_name, held_status
def calculate_comparison_stats(merged_df, static_data, id_col):
    """Calculates comparison statistics for each security's duration.
    Args:
        merged_df (pd.DataFrame): The merged dataframe of original and new duration values.
        static_data (pd.DataFrame): DataFrame with static info per security.
        id_col (str): The name of the column containing the Security ID/Name.
    """
    log = current_app.logger # Use app logger
    if merged_df.empty:
        return pd.DataFrame()
    if id_col not in merged_df.columns:
        log.error(f"Specified id_col '{id_col}' not found in merged_df columns: {merged_df.columns.tolist()}")
        return pd.DataFrame() # Cannot group without the ID column
    log.info(f"Calculating duration comparison statistics using ID column: {id_col}...")
    stats_list = []
    # Use the passed id_col here
    for sec_id, group in merged_df.groupby(id_col):
        sec_stats = {id_col: sec_id} # Use actual id_col name
        # Filter out rows where both values are NaN for overall analysis period
        group_valid_overall = group.dropna(subset=['Value_Orig', 'Value_New'], how='all')
        overall_min_date = group_valid_overall['Date'].min()
        overall_max_date = group_valid_overall['Date'].max()
        # Filter out rows where EITHER value is NaN for correlation/diff calculations
        valid_comparison = group.dropna(subset=['Value_Orig', 'Value_New'])
        # 1. Correlation of Levels
        if len(valid_comparison) >= 2: # Need at least 2 points for correlation
            # Use the NaN-dropped dataframe for correlation
            level_corr = valid_comparison['Value_Orig'].corr(valid_comparison['Value_New'])
            sec_stats['Level_Correlation'] = level_corr if pd.notna(level_corr) else None
        else:
             sec_stats['Level_Correlation'] = None
        # 2. Max / Min (use original group to get true max/min including non-overlapping points)
        sec_stats['Max_Orig'] = group['Value_Orig'].max()
        sec_stats['Min_Orig'] = group['Value_Orig'].min()
        sec_stats['Max_New'] = group['Value_New'].max()
        sec_stats['Min_New'] = group['Value_New'].min()
        # 3. Date Range Comparison - Refined Logic
        # Find min/max dates within the MERGED data where each series is individually valid
        min_date_orig_idx = group['Value_Orig'].first_valid_index()
        max_date_orig_idx = group['Value_Orig'].last_valid_index()
        min_date_new_idx = group['Value_New'].first_valid_index()
        max_date_new_idx = group['Value_New'].last_valid_index()
        sec_stats['Start_Date_Orig'] = group.loc[min_date_orig_idx, 'Date'] if min_date_orig_idx is not None else None
        sec_stats['End_Date_Orig'] = group.loc[max_date_orig_idx, 'Date'] if max_date_orig_idx is not None else None
        sec_stats['Start_Date_New'] = group.loc[min_date_new_idx, 'Date'] if min_date_new_idx is not None else None
        sec_stats['End_Date_New'] = group.loc[max_date_new_idx, 'Date'] if max_date_new_idx is not None else None
        # Check if the start and end dates MATCH for the valid periods of EACH series
        same_start = pd.Timestamp(sec_stats['Start_Date_Orig']) == pd.Timestamp(sec_stats['Start_Date_New']) if sec_stats['Start_Date_Orig'] and sec_stats['Start_Date_New'] else False
        same_end = pd.Timestamp(sec_stats['End_Date_Orig']) == pd.Timestamp(sec_stats['End_Date_New']) if sec_stats['End_Date_Orig'] and sec_stats['End_Date_New'] else False
        sec_stats['Same_Date_Range'] = same_start and same_end
        # Add overall date range for info
        sec_stats['Overall_Start_Date'] = overall_min_date
        sec_stats['Overall_End_Date'] = overall_max_date
        # 4. Correlation of Daily Changes (Volatility Alignment)
        # Use the dataframe where BOTH values are non-NaN to calculate changes for correlation
        valid_comparison = valid_comparison.copy() # Avoid SettingWithCopyWarning
        valid_comparison['Change_Orig_Corr'] = valid_comparison['Value_Orig'].diff()
        valid_comparison['Change_New_Corr'] = valid_comparison['Value_New'].diff()
        # Drop NaNs created by the diff() itself (first row)
        valid_changes = valid_comparison.dropna(subset=['Change_Orig_Corr', 'Change_New_Corr'])
        if len(valid_changes) >= 2:
            change_corr = valid_changes['Change_Orig_Corr'].corr(valid_changes['Change_New_Corr'])
            sec_stats['Change_Correlation'] = change_corr if pd.notna(change_corr) else None
        else:
            sec_stats['Change_Correlation'] = None
            log.debug(f"Cannot calculate Duration Change_Correlation for {sec_id}. Need >= 2 valid change pairs, found {len(valid_changes)}.")
        # 5. Difference Statistics (use the valid_comparison df where both values exist)
        valid_comparison['Abs_Diff'] = (valid_comparison['Value_Orig'] - valid_comparison['Value_New']).abs()
        sec_stats['Mean_Abs_Diff'] = valid_comparison['Abs_Diff'].mean() # Mean diff where both values exist
        sec_stats['Max_Abs_Diff'] = valid_comparison['Abs_Diff'].max() # Max diff where both values exist
        # Count NaNs - use original group
        sec_stats['NaN_Count_Orig'] = group['Value_Orig'].isna().sum()
        sec_stats['NaN_Count_New'] = group['Value_New'].isna().sum()
        sec_stats['Total_Points'] = len(group)
        stats_list.append(sec_stats)
    summary_df = pd.DataFrame(stats_list)
    # Merge static data back
    if not static_data.empty and id_col in static_data.columns and id_col in summary_df.columns:
        summary_df = pd.merge(summary_df, static_data, on=id_col, how='left')
    elif not static_data.empty:
         log.warning(f"Could not merge static data back for duration comparison. ID column '{id_col}' missing from static_data ({id_col in static_data.columns}) or summary_df ({id_col in summary_df.columns}).")
    log.info(f"Finished calculating duration stats. Summary shape: {summary_df.shape}")
    return summary_df
# --- Routes ---
@duration_comparison_bp.route('/duration_comparison/summary') # Updated route
def summary():
    """Displays the duration comparison summary page with server-side filtering, sorting, and pagination."""
    current_app.logger.info("--- Starting Duration Comparison Summary Request ---")
    # Retrieve the configured absolute data folder path
    data_folder = current_app.config['DATA_FOLDER']
    if not data_folder:
        current_app.logger.error("DATA_FOLDER is not configured in the application.")
        return "Internal Server Error: Data folder not configured", 500
    try:
        # --- Get Request Parameters ---
        page = request.args.get('page', 1, type=int)
        sort_by = request.args.get('sort_by', 'Change_Correlation') # Default sort
        sort_order = request.args.get('sort_order', 'desc').lower()
        if sort_order not in ['asc', 'desc']:
            sort_order = 'desc'
        ascending = sort_order == 'asc'
        # NEW: Get holding status filter
        show_sold = request.args.get('show_sold', 'false').lower() == 'true'
        # Get active filters (ensuring keys are correct)
        active_filters = {k.replace('filter_', ''): v
                          for k, v in request.args.items()
                          if k.startswith('filter_') and v}
        current_app.logger.info(f"Request Params: Page={page}, SortBy={sort_by}, Order={sort_order}, Filters={active_filters}, ShowSold={show_sold}")
        # --- Load and Prepare Data ---
        merged_data, static_data, static_cols, actual_id_col, held_status = load_duration_comparison_data(data_folder)
        if actual_id_col is None:
            current_app.logger.error("Failed to get ID column name during duration data loading.")
            return "Error loading duration comparison data: Could not determine ID column.", 500
        else:
            current_app.logger.info(f"Actual ID column identified for duration comparison data: '{actual_id_col}'")
        summary_stats = calculate_comparison_stats(merged_data, static_data, id_col=actual_id_col)
        if summary_stats.empty:
            current_app.logger.info("No duration summary statistics could be calculated.")
            return render_template('duration_comparison_page.html', # Updated template
                                   table_data=[],
                                   columns_to_display=[],
                                   id_column_name=actual_id_col,
                                   filter_options={},
                                   active_filters={},
                                   current_sort_by=sort_by,
                                   current_sort_order=sort_order,
                                   pagination=None,
                                   show_sold=show_sold, # Pass filter status
                                   message="No duration comparison data available.")
        # --- Merge Held Status ---
        if not held_status.empty and actual_id_col in summary_stats.columns:
            # --- Convert merge keys to string BEFORE merging --- 
            try:
                original_dtype_stats = summary_stats[actual_id_col].dtype
                original_dtype_held = held_status.index.dtype
                summary_stats[actual_id_col] = summary_stats[actual_id_col].astype(str)
                held_status.index = held_status.index.astype(str)
                current_app.logger.info(f"Converted merge keys to string for duration. Original dtypes: summary_stats['{actual_id_col}']: {original_dtype_stats}, held_status.index: {original_dtype_held}")
            except Exception as e:
                current_app.logger.error(f"Error converting duration merge keys to string: {e}. Merge might fail.")
            # --- Add PRE-MERGE logging ---
            current_app.logger.info(f"PRE-MERGE CHECK: summary_stats ID column ('{actual_id_col}') dtype: {summary_stats[actual_id_col].dtype}, held_status index ('{held_status.index.name}') dtype: {held_status.index.dtype}")
            current_app.logger.info(f"PRE-MERGE CHECK: summary_stats ID column name: '{actual_id_col}', held_status index name: '{held_status.index.name}'")
            if actual_id_col != held_status.index.name:
                 current_app.logger.error(f"CRITICAL: Mismatch between duration summary_stats merge column ('{actual_id_col}') and held_status index name ('{held_status.index.name}'). Merge will likely fail or produce incorrect results.")
            # --- End PRE-MERGE logging ---
            current_app.logger.info(f"Attempting to merge held_status (index name: '{held_status.index.name}') with duration summary_stats on column '{actual_id_col}'")
            debug_cols_before = summary_stats.columns.tolist()
            current_app.logger.debug(f"summary_stats columns before merge: {debug_cols_before}")
            current_app.logger.debug(f"held_status index preview before merge: {held_status.index[:5].tolist()}")
            current_app.logger.debug(f"summary_stats ID column ('{actual_id_col}') preview before merge: {summary_stats[actual_id_col].unique()[:5].tolist()}")
            # Add explicit rename to 'is_held' when merging
            summary_stats = pd.merge(summary_stats, held_status.rename('is_held'), 
                                     left_on=actual_id_col, right_index=True, how='left')
            debug_cols_after = summary_stats.columns.tolist()
            current_app.logger.debug(f"Columns after merge attempt: {debug_cols_after}")
            held_count = summary_stats['is_held'].notna().sum()
            current_app.logger.info(f"Merged held status. Duration stats shape: {summary_stats.shape}. 'is_held' column has {held_count} non-NA values.")
            if held_count > 0:
                current_app.logger.debug(f"Preview of 'is_held' after merge (first 5 non-NA): {summary_stats[summary_stats['is_held'].notna()]['is_held'].head().to_dict()}")
            else:
                current_app.logger.debug("Preview of 'is_held' after merge (first 5 non-NA): {}")
            # Fill NaN in 'is_held'
            if 'is_held' in summary_stats.columns:
                summary_stats['is_held'] = summary_stats['is_held'].fillna(False)
                current_app.logger.info(f"Filled NA in 'is_held' with False for duration summary.")
            else:
                current_app.logger.error("'is_held' column not found after duration merge!")
                summary_stats['is_held'] = False # Add the column as False if merge failed entirely
        else:
            current_app.logger.warning(f"Could not merge held status for duration data. held_status empty: {held_status.empty}, '{actual_id_col}' in summary_stats: {actual_id_col in summary_stats.columns}")
            summary_stats['is_held'] = False
        # --- Apply Holding Status Filter ---
        original_count = len(summary_stats)
        if not show_sold:
            summary_stats = summary_stats[summary_stats['is_held'] == True]
            current_app.logger.info(f"Applied 'Show Held Only' filter. Kept {len(summary_stats)} out of {original_count} securities.")
        else:
            current_app.logger.info("Skipping 'Show Held Only' filter (show_sold is True).")
        if summary_stats.empty:
             current_app.logger.info("No securities remaining after applying holding status filter.")
             return render_template('duration_comparison_page.html', # Updated template
                                    table_data=[],
                                    columns_to_display=[actual_id_col] + static_cols, # Show basic cols
                                    id_column_name=actual_id_col,
                                    filter_options={},
                                    active_filters={},
                                    current_sort_by=sort_by,
                                    current_sort_order=sort_order,
                                    pagination=None,
                                    show_sold=show_sold, # Pass filter status
                                    message="No currently held securities found.")
        # --- Collect Filter Options (From Data *After* Holding Filter) --- 
        filter_options = {}
        potential_filter_cols = static_cols 
        for col in potential_filter_cols:
            if col in summary_stats.columns:
                unique_vals = summary_stats[col].dropna().unique().tolist()
                try:
                    sorted_vals = sorted(unique_vals, key=lambda x: (isinstance(x, (int, float)), x))
                except TypeError:
                    sorted_vals = sorted(unique_vals, key=str)
                filter_options[col] = sorted_vals
        final_filter_options = dict(sorted(filter_options.items())) # Sort filter dropdowns alphabetically
        current_app.logger.info(f"Filter options generated: {list(final_filter_options.keys())}") # Use final_filter_options
        # --- Apply Static Column Filters --- 
        filtered_data = summary_stats.copy()
        if active_filters:
            current_app.logger.info(f"Applying static column filters: {active_filters}")
            for col, value in active_filters.items():
                if col in filtered_data.columns and value:
                    try:
                        # Robust string comparison
                         filtered_data = filtered_data[filtered_data[col].astype(str).str.lower() == str(value).lower()]
                    except Exception as e:
                        current_app.logger.warning(f"Could not apply filter for column '{col}' with value '{value}'. Error: {e}. Skipping filter.")
                else:
                    current_app.logger.warning(f"Filter column '{col}' not found in data. Skipping filter.")
            current_app.logger.info(f"Data shape after static filtering: {filtered_data.shape}")
        else:
            current_app.logger.info("No active static column filters.")
        if filtered_data.empty:
             current_app.logger.info("No data remaining after applying static column filters.")
             return render_template('duration_comparison_page.html', # Updated template
                                    table_data=[],
                                    columns_to_display=[actual_id_col] + static_cols, # Show basic cols
                                    id_column_name=actual_id_col,
                                    filter_options=final_filter_options, # Show filter options
                                    active_filters=active_filters,
                                    current_sort_by=sort_by,
                                    current_sort_order=sort_order,
                                    pagination=None,
                                    show_sold=show_sold, # Pass filter status
                                    message="No data matches the current filters.")
        # --- Apply Sorting ---
        if sort_by in filtered_data.columns:
            current_app.logger.info(f"Sorting by '{sort_by}' ({'Ascending' if ascending else 'Descending'})")
            na_position = 'last' 
            try:
                filtered_data = filtered_data.sort_values(by=sort_by, ascending=ascending, na_position=na_position)
            except Exception as e:
                current_app.logger.error(f"Error during sorting by '{sort_by}': {e}. Falling back to default sort.")
                sort_by = 'Change_Correlation' 
                ascending = False
                filtered_data = filtered_data.sort_values(by=sort_by, ascending=ascending, na_position=na_position)
        else:
            current_app.logger.warning(f"Sort column '{sort_by}' not found. Using default ID sort.")
            sort_by = actual_id_col 
            ascending = True
            filtered_data = filtered_data.sort_values(by=actual_id_col, ascending=ascending, na_position='last')
        # --- Pagination ---
        total_items = len(filtered_data)
        safe_per_page = max(1, PER_PAGE_COMPARISON)
        total_pages = math.ceil(total_items / safe_per_page)
        total_pages = max(1, total_pages)
        page = max(1, min(page, total_pages))
        start_index = (page - 1) * safe_per_page
        end_index = start_index + safe_per_page
        paginated_data = filtered_data.iloc[start_index:end_index]
        current_app.logger.info(f"Pagination: Total items={total_items}, Total pages={total_pages}, Current page={page}, Displaying items {start_index}-{end_index-1}")
        page_window = 2
        start_page_display = max(1, page - page_window)
        end_page_display = min(total_pages, page + page_window)
        # --- Prepare for Template ---
        base_cols = [
            'Level_Correlation', 'Change_Correlation',
            'Mean_Abs_Diff', 'Max_Abs_Diff',
            'NaN_Count_Orig', 'NaN_Count_New', 'Total_Points',
            'Same_Date_Range',
            # Add/remove columns as needed
        ]
        columns_to_display = [actual_id_col] + \
                             [col for col in static_cols if col != actual_id_col and col in paginated_data.columns] + \
                             [col for col in base_cols if col in paginated_data.columns]
        table_data = paginated_data.to_dict(orient='records')
        # Format specific columns 
        for row in table_data:
            for col in ['Level_Correlation', 'Change_Correlation']:
                 if col in row and pd.notna(row[col]):
                    row[col] = f"{row[col]:.4f}" 
            # Add date formatting if needed for stats cols
        # Create pagination object
        pagination_context = {
            'page': page,
            'per_page': safe_per_page,
            'total_items': total_items,
            'total_pages': total_pages,
            'has_prev': page > 1,
            'has_next': page < total_pages,
            'prev_num': page - 1,
            'next_num': page + 1,
            'start_page_display': start_page_display,
            'end_page_display': end_page_display,
            # Function to generate URLs for pagination links, preserving state
             'url_for_page': lambda p: url_for('duration_comparison_bp.summary', 
                                              page=p, 
                                              sort_by=sort_by, 
                                              sort_order=sort_order, 
                                              show_sold=str(show_sold).lower(), # Pass holding status
                                              **{f'filter_{k}': v for k, v in active_filters.items()})
        }
        current_app.logger.info("--- Successfully Prepared Data for Duration Comparison Template ---")
        return render_template('duration_comparison_page.html', # Updated template
                               table_data=table_data,
                               columns_to_display=columns_to_display,
                               id_column_name=actual_id_col, # Pass the ID column name
                               filter_options=final_filter_options,
                               active_filters=active_filters,
                               current_sort_by=sort_by,
                               current_sort_order=sort_order,
                               pagination=pagination_context,
                               show_sold=show_sold, # Pass holding filter status
                               message=None) # No message if data is present
    except FileNotFoundError as e:
        current_app.logger.error(f"Duration comparison file not found: {e}")
        return f"Error: Required duration comparison file not found ({e.filename}). Check the Data folder.", 404
    except Exception as e:
        current_app.logger.exception("An unexpected error occurred in the duration comparison summary view.") # Log full traceback
        return render_template('duration_comparison_page.html', 
                               message=f"An unexpected error occurred: {e}",
                               table_data=[], pagination=None, filter_options={}, 
                               active_filters={}, show_sold=show_sold, columns_to_display=[], 
                               id_column_name='Security') # Include show_sold in error template
@duration_comparison_bp.route('/duration_comparison/details/<path:security_id>')
def duration_comparison_details(security_id):
    """Displays side-by-side historical duration charts for a specific security."""
    current_app.logger.info(f"--- Starting Duration Comparison Detail Request for Security ID: {security_id} ---")
    # Retrieve the configured absolute data folder path
    data_folder = current_app.config['DATA_FOLDER']
    if not data_folder:
        current_app.logger.error("DATA_FOLDER is not configured in the application.")
        return "Internal Server Error: Data folder not configured", 500
    try:
        # Pass the absolute data folder path
        merged_data, static_data, common_static_cols, id_col_name, _ = load_duration_comparison_data(data_folder)
        if id_col_name is None:
             current_app.logger.error(f"Failed to get ID column name for details view (Security: {security_id}).")
             return "Error loading duration comparison data: Could not determine ID column.", 500
        if merged_data.empty:
            current_app.logger.warning(f"Merged duration data is empty for details view (Security: {security_id}).")
            return f"No merged duration data found for Security ID: {security_id}", 404
        # Filter data for the specific security using the correct ID column name
        security_data = merged_data[merged_data[id_col_name] == security_id].copy() # Use .copy()
        if security_data.empty:
            current_app.logger.warning(f"No duration data found for the specific Security ID: {security_id}")
            # Consider checking if the ID exists in the original files?
            return f"Duration data not found for Security ID: {security_id}", 404
        # Get static info for this security (handle potential multiple rows if ID isn't unique, take first)
        static_info = security_data[[id_col_name] + common_static_cols].iloc[0].to_dict() if not security_data.empty else {}
        # Sort by date for charting
        security_data = security_data.sort_values(by='Date')
        # Prepare data for Chart.js
        # Ensure 'Date' is in the correct string format for JSON/JS
        security_data['Date_Str'] = security_data['Date'].dt.strftime('%Y-%m-%d')
        chart_data = {
            'labels': security_data['Date_Str'].tolist(),
            'datasets': [
                {
                    'label': 'Original Duration',
                    'data': security_data['Value_Orig'].where(pd.notna(security_data['Value_Orig']), None).tolist(), # Replace NaN with None for JSON
                    'borderColor': COLOR_PALETTE[0 % len(COLOR_PALETTE)],
                    'fill': False,
                    'tension': 0.1
                },
                {
                    'label': 'New Duration',
                    'data': security_data['Value_New'].where(pd.notna(security_data['Value_New']), None).tolist(), # Replace NaN with None for JSON
                    'borderColor': COLOR_PALETTE[1 % len(COLOR_PALETTE)],
                    'fill': False,
                    'tension': 0.1
                }
            ]
        }
        # Calculate overall statistics for this security
        stats_summary = calculate_comparison_stats(security_data, pd.DataFrame([static_info]), id_col=id_col_name) # Pass single security data
        stats_dict = stats_summary.iloc[0].to_dict() if not stats_summary.empty else {}
         # Format dates and numbers in stats_dict before passing
        for key, value in stats_dict.items():
            if isinstance(value, pd.Timestamp):
                stats_dict[key] = value.strftime('%Y-%m-%d')
            elif isinstance(value, (int, float)):
                 if 'Correlation' in key and pd.notna(value):
                     stats_dict[key] = f"{value:.4f}"
                 elif 'Diff' in key and pd.notna(value):
                      stats_dict[key] = f"{value:.2f}" # Adjust formatting as needed
        current_app.logger.info(f"Successfully prepared data for duration details template (Security: {security_id})")
        return render_template('duration_comparison_details_page.html', # Updated template
                               security_id=security_id,
                               static_info=static_info, # Pass static info
                               chart_data=chart_data,
                               stats_summary=stats_dict) # Pass calculated stats
    except FileNotFoundError as e:
        current_app.logger.error(f"Duration comparison file not found for details view: {e} (Security: {security_id})")
        return f"Error: Required duration comparison file not found ({e.filename}). Check the Data folder.", 404
    except KeyError as e:
         current_app.logger.error(f"KeyError accessing data for security '{security_id}': {e}. ID column used: '{id_col_name}'")
         return f"Error accessing data for security '{security_id}'. It might be missing required columns or have unexpected formatting.", 500
    except Exception as e:
        current_app.logger.exception(f"An unexpected error occurred in the duration comparison details view for security '{security_id}'.") # Log full traceback
        return f"An internal error occurred while processing details for security '{security_id}': {e}", 500
</file>

<file path="views/exclusion_views.py">
"""
This module defines the Flask Blueprint for handling security exclusion management.
It provides routes to view the current exclusion list and add new securities
to the list.
"""
import os
import pandas as pd
from flask import Blueprint, render_template, request, redirect, url_for, current_app
from datetime import datetime
import logging
# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
# Define the Blueprint
exclusion_bp = Blueprint('exclusion_bp', __name__, template_folder='../templates')
EXCLUSIONS_FILE = 'exclusions.csv'
# Assuming sec_spread.csv contains the list of all possible securities
# We need to determine the correct file and column name for security IDs
# Let's tentatively use sec_spread.csv and 'Security ID'
SECURITIES_SOURCE_FILE = 'sec_spread.csv' # Adjust if needed
SECURITY_ID_COLUMN = 'Security Name' # Corrected column name
def load_exclusions(data_folder_path: str):
    """Loads the current list of exclusions from the CSV file.
    Args:
        data_folder_path (str): The absolute path to the data folder.
    Returns:
        list[dict]: A list of dictionaries representing the exclusions, or [] if error.
    """
    if not data_folder_path:
        logging.error("No data_folder_path provided to load_exclusions.")
        return []
    exclusions_path = os.path.join(data_folder_path, EXCLUSIONS_FILE)
    try:
        if os.path.exists(exclusions_path) and os.path.getsize(exclusions_path) > 0:
            df = pd.read_csv(exclusions_path, parse_dates=['AddDate', 'EndDate'], dayfirst=False) # Specify date format if needed
            # Ensure correct types after loading
            df['AddDate'] = pd.to_datetime(df['AddDate'], errors='coerce')
            df['EndDate'] = pd.to_datetime(df['EndDate'], errors='coerce')
            df['SecurityID'] = df['SecurityID'].astype(str)
            df['Comment'] = df['Comment'].astype(str)
            df = df.sort_values(by='AddDate', ascending=False)
            return df.to_dict('records')
        else:
            logging.info(f"'{EXCLUSIONS_FILE}' is empty or does not exist. Returning empty list.")
            return []
    except Exception as e:
        logging.error(f"Error loading exclusions from {exclusions_path}: {e}")
        return [] # Return empty list on error
def load_available_securities(data_folder_path: str):
    """Loads the list of available security IDs from the source file.
    Args:
        data_folder_path (str): The absolute path to the data folder.
    Returns:
        list[str]: A sorted list of unique available security IDs, or [] if error.
    """
    if not data_folder_path:
        logging.error("No data_folder_path provided to load_available_securities.")
        return []
    securities_file_path = os.path.join(data_folder_path, SECURITIES_SOURCE_FILE)
    try:
        if os.path.exists(securities_file_path):
            # Load only the necessary column
            # Use security_processing logic if more complex loading is needed
            df_securities = pd.read_csv(securities_file_path, usecols=[SECURITY_ID_COLUMN], encoding_errors='replace', on_bad_lines='skip')
            df_securities.dropna(subset=[SECURITY_ID_COLUMN], inplace=True)
            security_ids = df_securities[SECURITY_ID_COLUMN].astype(str).unique().tolist()
            security_ids.sort() # Sort for dropdown consistency
            return security_ids
        else:
            logging.warning(f"Securities source file '{SECURITIES_SOURCE_FILE}' not found at {securities_file_path}.")
            return []
    except KeyError:
        logging.error(f"Column '{SECURITY_ID_COLUMN}' not found in '{SECURITIES_SOURCE_FILE}'. Cannot load available securities.")
        return []
    except Exception as e:
        logging.error(f"Error loading available securities from {securities_file_path}: {e}")
        return []
def add_exclusion(data_folder_path: str, security_id, end_date_str, comment):
    """Adds a new exclusion to the CSV file.
    Args:
        data_folder_path (str): The absolute path to the data folder.
        security_id:
        end_date_str:
        comment:
    Returns:
        tuple[bool, str]: (Success status, Message)
    """
    if not data_folder_path:
        logging.error("No data_folder_path provided to add_exclusion.")
        return False, "Internal Server Error: Data folder path not configured."
    exclusions_path = os.path.join(data_folder_path, EXCLUSIONS_FILE)
    try:
        # Basic validation
        if not security_id or not comment:
            logging.warning("Attempted to add exclusion with missing SecurityID or Comment.")
            return False, "Security ID and Comment are required."
        add_date = datetime.now().strftime('%Y-%m-%d')
        # Parse end_date, allow it to be empty
        end_date = pd.to_datetime(end_date_str, errors='coerce').strftime('%Y-%m-%d') if end_date_str else ''
        new_exclusion = pd.DataFrame({
            'SecurityID': [str(security_id)],
            'AddDate': [add_date],
            'EndDate': [end_date],
            'Comment': [str(comment)]
        })
        # Append to CSV, create header if file doesn't exist or is empty
        file_exists = os.path.exists(exclusions_path)
        is_empty = file_exists and os.path.getsize(exclusions_path) == 0
        write_header = not file_exists or is_empty
        new_exclusion.to_csv(exclusions_path, mode='a', header=write_header, index=False)
        logging.info(f"Added exclusion for SecurityID: {security_id}")
        return True, "Exclusion added successfully."
    except Exception as e:
        logging.error(f"Error adding exclusion to {exclusions_path}: {e}")
        return False, "An error occurred while saving the exclusion."
def remove_exclusion(data_folder_path: str, security_id_to_remove, add_date_str_to_remove):
    """Removes a specific exclusion entry from the CSV file based on SecurityID and AddDate.
    Args:
        data_folder_path (str): The absolute path to the data folder.
        security_id_to_remove:
        add_date_str_to_remove:
    Returns:
        tuple[bool, str]: (Success status, Message)
    """
    if not data_folder_path:
        logging.error("No data_folder_path provided to remove_exclusion.")
        return False, "Internal Server Error: Data folder path not configured."
    exclusions_path = os.path.join(data_folder_path, EXCLUSIONS_FILE)
    try:
        if not os.path.exists(exclusions_path) or os.path.getsize(exclusions_path) == 0:
            logging.warning(f"Attempted to remove exclusion, but '{EXCLUSIONS_FILE}' is empty or does not exist.")
            return False, "Exclusion file is empty or missing."
        df = pd.read_csv(exclusions_path)
        # Ensure columns used for matching are strings
        df['SecurityID'] = df['SecurityID'].astype(str)
        # Keep AddDate as string for direct comparison with the string from the form
        df['AddDate'] = df['AddDate'].astype(str)
        security_id_to_remove = str(security_id_to_remove)
        original_count = len(df)
        # Filter out the row(s) to remove
        # Match both SecurityID and the AddDate string
        df_filtered = df[~((df['SecurityID'] == security_id_to_remove) & (df['AddDate'] == add_date_str_to_remove))]
        if len(df_filtered) == original_count:
            logging.warning(f"Exclusion entry for SecurityID '{security_id_to_remove}' with AddDate '{add_date_str_to_remove}' not found for removal.")
            return False, "Exclusion entry not found."
        # Overwrite the CSV with the filtered data
        df_filtered.to_csv(exclusions_path, index=False)
        logging.info(f"Removed exclusion entry for SecurityID: {security_id_to_remove}, AddDate: {add_date_str_to_remove}")
        return True, "Exclusion removed successfully."
    except Exception as e:
        logging.error(f"Error removing exclusion from {exclusions_path}: {e}")
        return False, "An error occurred while removing the exclusion."
@exclusion_bp.route('/exclusions', methods=['GET', 'POST'])
def manage_exclusions():
    """
    Handles viewing and adding security exclusions.
    GET: Displays the list of current exclusions and the form to add new ones.
    POST: Processes the form submission to add a new exclusion.
    """
    # Retrieve the configured absolute data folder path
    data_folder = current_app.config['DATA_FOLDER']
    if not data_folder:
        current_app.logger.error("DATA_FOLDER is not configured in the application.")
        return "Internal Server Error: Data folder not configured", 500
    message = None
    message_type = 'info' # Can be 'success' or 'error'
    if request.method == 'POST':
        security_id = request.form.get('security_id')
        end_date_str = request.form.get('end_date')
        comment = request.form.get('comment')
        # Pass the absolute data_folder path to the helper function
        success, msg = add_exclusion(data_folder, security_id, end_date_str, comment)
        if success:
            # Redirect to the same page using GET to prevent form resubmission
            return redirect(url_for('exclusion_bp.manage_exclusions', _external=True))
        else:
            message = msg
            message_type = 'error'
            # Fall through to render the page again with the error message
    # For both GET requests and POST failures, load data and render template
    # Pass the absolute data_folder path to the helper functions
    current_exclusions = load_exclusions(data_folder)
    available_securities = load_available_securities(data_folder)
    return render_template('exclusions_page.html',
                           exclusions=current_exclusions,
                           available_securities=available_securities,
                           message=message,
                           message_type=message_type)
@exclusion_bp.route('/exclusions/remove', methods=['POST'])
def remove_exclusion_route():
    """Handles the POST request to remove an exclusion."""
    # Retrieve the configured absolute data folder path
    data_folder = current_app.config['DATA_FOLDER']
    if not data_folder:
        current_app.logger.error("DATA_FOLDER is not configured in the application.")
        # Redirect back with an error indication (flash message would be better)
        return redirect(url_for('exclusion_bp.manage_exclusions'))
    security_id = request.form.get('security_id')
    add_date_str = request.form.get('add_date') # Get AddDate as string
    if not security_id or not add_date_str:
        # Handle missing identifiers (shouldn't happen with hidden fields but good practice)
        # Redirect back with an error message (using flash or query params)
        logging.warning("Remove exclusion attempt missing SecurityID or AddDate.")
        # For simplicity, redirect back to the main page; flash messages would be better
        return redirect(url_for('exclusion_bp.manage_exclusions'))
    # Pass the absolute data_folder path to the helper function
    success, msg = remove_exclusion(data_folder, security_id, add_date_str)
    # Regardless of success/failure, redirect back to the main exclusions page.
    # Consider using flash messages to display the success/error message after redirect.
    # For now, the message `msg` is logged but not shown to the user on redirect.
    return redirect(url_for('exclusion_bp.manage_exclusions'))
</file>

<file path="views/fund_views.py">
"""Blueprint for fund-specific routes, including duration details and a general fund overview page."""
from flask import Blueprint, render_template, current_app, jsonify
import os
import pandas as pd
import traceback
import logging # Added for logging
import glob # Added for finding files
import re # Added for extracting metric name
import numpy as np
# Import necessary functions from other modules
from utils import _is_date_like, parse_fund_list # Import required utils
# Updated import to include data loader
from data_loader import load_and_process_data
from security_processing import load_and_process_security_data, calculate_security_latest_metrics # For fund_duration_details
# Define the blueprint
fund_bp = Blueprint('fund', __name__, url_prefix='/fund')
@fund_bp.route('/duration_details/<fund_code>') # Corresponds to /fund/duration_details/...
def fund_duration_details(fund_code):
    """Renders a page showing duration changes for securities held by a specific fund."""
    # Retrieve the absolute data folder path from the app context
    data_folder = current_app.config['DATA_FOLDER']
    if not data_folder:
        current_app.logger.error("DATA_FOLDER is not configured in the application.")
        return "Internal Server Error: Data folder not configured", 500
    duration_filename = "sec_duration.csv"
    # Construct absolute path
    data_filepath = os.path.join(data_folder, duration_filename)
    current_app.logger.info(f"--- Requesting Duration Details for Fund: {fund_code} --- File: {data_filepath}")
    if not os.path.exists(data_filepath):
        print(f"Error: Duration file '{duration_filename}' not found.")
        return f"Error: Data file '{duration_filename}' not found.", 404
    try:
        # 1. Load the duration data (only header first for column identification)
        header_df = pd.read_csv(data_filepath, nrows=0, encoding='utf-8')
        all_cols = [col.strip() for col in header_df.columns.tolist()]
        # Define ID column (specific to this file/route)
        id_col_name = 'ISIN'
        if id_col_name not in all_cols:
            print(f"Error: Expected ID column '{id_col_name}' not found in {duration_filename}.")
            return f"Error: Required ID column '{id_col_name}' not found in '{duration_filename}'.", 500
        # 2. Identify static and date columns dynamically
        date_cols = []
        static_cols = []
        for col in all_cols:
            if col == id_col_name:
                continue # Skip the ID column
            if col == 'Security Name':
                continue # Skip the old ID column if it exists
            if _is_date_like(col): # Use the helper function from utils
                date_cols.append(col)
            else:
                static_cols.append(col) # Treat others as static
        print(f"Dynamically identified Static Cols: {static_cols}")
        print(f"Dynamically identified Date Cols (first 5): {date_cols[:5]}...")
        if not date_cols or len(date_cols) < 2:
             print("Error: Not enough date columns found in duration file to calculate change.")
             return f"Error: Insufficient date columns in '{duration_filename}' to calculate change.", 500
        # Now read the full data
        df = pd.read_csv(data_filepath, encoding='utf-8')
        df.columns = df.columns.str.strip() # Strip again after full read
        # Ensure the Funds column exists (still needed for filtering)
        funds_col = 'Funds' # Keep this assumption for now as it's key to filtering
        if funds_col not in static_cols:
             print(f"Warning: Expected column '{funds_col}' for filtering not found among static columns.")
             # Decide how to handle this - error or proceed without fund filtering? Let's error for now.
             return f"Error: Required column '{funds_col}' for fund filtering not found.", 500
        # Ensure date columns are sortable (attempt conversion if needed, basic check)
        try:
            # Check and sort date columns using the correct YYYY-MM-DD format
            pd.to_datetime(date_cols, format='%Y-%m-%d', errors='raise')
            date_cols = sorted(date_cols, key=lambda d: pd.to_datetime(d, format='%Y-%m-%d'))
            print(f"Identified and sorted date columns (YYYY-MM-DD): {date_cols[-5:]} (last 5 shown)")
        except ValueError:
            print("Warning: Could not parse all date columns using YYYY-MM-DD format. Using original order.")
            # Fallback remains, but hopefully won't be needed as often
        # Identify last two date columns based on sorted list (or original if parsing failed)
        if len(date_cols) < 2: # Double check after potential parsing failure
            return f"Error: Insufficient valid date columns in '{duration_filename}' to calculate change after sorting attempt.", 500
        last_date_col = date_cols[-1]
        second_last_date_col = date_cols[-2]
        print(f"Using dates for change calculation: {second_last_date_col} and {last_date_col}")
        # Ensure the relevant date columns are numeric for calculation
        df[last_date_col] = pd.to_numeric(df[last_date_col], errors='coerce')
        df[second_last_date_col] = pd.to_numeric(df[second_last_date_col], errors='coerce')
        # 3. Filter by Fund Code
        # Apply the parsing function from utils to the 'Funds' column
        fund_lists = df['Funds'].apply(parse_fund_list)
        # Create a boolean mask to filter rows where the fund_code is in the parsed list
        mask = fund_lists.apply(lambda funds: fund_code in funds)
        filtered_df = df[mask].copy() # Use copy to avoid SettingWithCopyWarning
        # --- Load and Process Weight Data (w_secs.csv) --- 
        weights_filename = "w_secs.csv"
        # Construct absolute path for weights file
        weights_filepath = os.path.join(data_folder, weights_filename)
        contribution_calculated = False # Flag to track if calculation was successful
        new_contribution_cols = []
        if not os.path.exists(weights_filepath):
            print(f"Warning: Weight file '{weights_filename}' not found. Skipping duration contribution calculation.")
        else:
            try:
                print(f"Loading weight file: {weights_filename}")
                weights_df = pd.read_csv(weights_filepath, encoding='utf-8')
                weights_df.columns = weights_df.columns.str.strip()
                # Define expected columns in weights file
                weight_id_col = 'Security Name'
                weight_fund_col = 'Funds'
                # Check if necessary columns exist in weights_df
                # Convert the dates from duration file (YYYY-MM-DD) to the format in weights file (DD/MM/YYYY)
                try:
                    last_date_dt = pd.to_datetime(last_date_col, format='%Y-%m-%d')
                    second_last_date_dt = pd.to_datetime(second_last_date_col, format='%Y-%m-%d')
                    last_date_col_weights_fmt = last_date_dt.strftime('%d/%m/%Y')
                    second_last_date_col_weights_fmt = second_last_date_dt.strftime('%d/%m/%Y')
                    print(f"Looking for weight columns: {last_date_col_weights_fmt}, {second_last_date_col_weights_fmt}")
                except ValueError:
                     print(f"Error: Could not convert duration dates ({last_date_col}, {second_last_date_col}) to datetime objects for weight lookup. Skipping contribution.")
                     last_date_col_weights_fmt = None # Ensure it skips if conversion fails
                # Check using the formatted date strings
                required_weight_cols = [weight_id_col, weight_fund_col]
                if last_date_col_weights_fmt:
                    required_weight_cols.extend([last_date_col_weights_fmt, second_last_date_col_weights_fmt])
                else:
                    # Skip if dates couldn't be formatted
                    print(f"Skipping weight check due to date format conversion error.")
                    all_cols_exist = False
                all_cols_exist = all(col in weights_df.columns for col in required_weight_cols)
                if not all_cols_exist:
                    missing_cols = [col for col in required_weight_cols if col not in weights_df.columns]
                    print(f"Warning: Weight file '{weights_filename}' is missing required columns (needed: {required_weight_cols}, missing: {missing_cols}). Skipping contribution calculation.")
                else:
                    print(f"Filtering weights for fund: {fund_code}")
                    # Filter weights by fund code (assuming direct match in 'Funds' column)
                    fund_weights_df = weights_df[weights_df[weight_fund_col] == fund_code].copy()
                    if fund_weights_df.empty:
                        print(f"Warning: No weights found for fund '{fund_code}' in {weights_filename}. Contribution will be zero.")
                        # Create empty df with correct columns to avoid merge errors later if we still want zero cols
                        weights_to_merge = pd.DataFrame(columns=[weight_id_col, 'Weight Last Date', 'Weight Second Last Date'])
                        weights_to_merge = weights_to_merge.astype({weight_id_col: 'object', 'Weight Last Date': 'float64', 'Weight Second Last Date': 'float64'})
                    else:
                         # Select and rename relevant weight columns using the CORRECT formatted date strings
                        weights_to_merge = fund_weights_df[[weight_id_col, last_date_col_weights_fmt, second_last_date_col_weights_fmt]].copy()
                        weights_to_merge.rename(columns={
                            last_date_col_weights_fmt: 'Weight Last Date',
                            second_last_date_col_weights_fmt: 'Weight Second Last Date'
                        }, inplace=True)
                         # Ensure weight columns are numeric
                        weights_to_merge['Weight Last Date'] = pd.to_numeric(weights_to_merge['Weight Last Date'], errors='coerce')
                        weights_to_merge['Weight Second Last Date'] = pd.to_numeric(weights_to_merge['Weight Second Last Date'], errors='coerce')
                    # Merge weights with filtered duration data
                    print(f"Merging duration data with weights on '{weight_id_col}'")
                    filtered_df = pd.merge(filtered_df, weights_to_merge, on=weight_id_col, how='left')
                    # Fill missing weights with 0 and calculate contribution
                    filtered_df['Weight Last Date'].fillna(0, inplace=True)
                    filtered_df['Weight Second Last Date'].fillna(0, inplace=True)
                    contrib_last_col = 'Duration Contribution Last Date'
                    contrib_second_last_col = 'Duration Contribution Second Last Date'
                    contrib_change_col = 'Duration Contribution Change'
                    filtered_df[contrib_last_col] = filtered_df[last_date_col] * filtered_df['Weight Last Date']
                    filtered_df[contrib_second_last_col] = filtered_df[second_last_date_col] * filtered_df['Weight Second Last Date']
                    filtered_df[contrib_change_col] = filtered_df[contrib_last_col] - filtered_df[contrib_second_last_col]
                    contribution_calculated = True
                    new_contribution_cols = [contrib_second_last_col, contrib_last_col, contrib_change_col]
                    print("Duration contribution calculated successfully.")
            except Exception as weight_err:
                 print(f"Error processing weight file '{weights_filename}': {weight_err}. Skipping contribution calculation.")
                 traceback.print_exc()
        # --- End Weight Data Processing ---
        if filtered_df.empty:
            print(f"No securities found for fund '{fund_code}' in {duration_filename} after initial filtering.")
            # Render a template indicating no data found for this fund
            return render_template('fund_duration_details.html',
                                   fund_code=fund_code,
                                   securities_data=[],
                                   column_order=[],
                                   id_col_name=None,
                                   message=f"No securities found held by fund '{fund_code}' in {duration_filename}.")
        print(f"Found {len(filtered_df)} securities for fund '{fund_code}'. Calculating duration changes...")
        # 4. Calculate 1-day Duration Change (already done if contribution wasn't skipped)
        change_col_name = '1 Day Duration Change'
        if change_col_name not in filtered_df.columns:
            filtered_df[change_col_name] = filtered_df[last_date_col] - filtered_df[second_last_date_col]
        # 5. Sort by Duration Change (descending, NaN last)
        filtered_df.sort_values(by=change_col_name, ascending=False, na_position='last', inplace=True)
        print(f"Sorted securities by {change_col_name}.")
        # 6. Prepare data for template
        # Define base display columns
        existing_static_cols = [col for col in static_cols if col in filtered_df.columns]
        if 'Security Name' in filtered_df.columns and 'Security Name' not in existing_static_cols:
            existing_static_cols.insert(0, 'Security Name') # Add Security Name near the start
        # Define column order, putting ISIN (the new id_col_name) first
        display_cols = [id_col_name] + existing_static_cols + [second_last_date_col, last_date_col, change_col_name]
        # Add contribution columns if they were calculated
        if contribution_calculated:
            display_cols.extend(new_contribution_cols)
        final_col_order = [col for col in display_cols if col in filtered_df.columns] # Ensure only existing columns are kept
        # Round numeric columns before converting to dict
        numeric_cols_in_final = filtered_df[final_col_order].select_dtypes(include=np.number).columns
        filtered_df[numeric_cols_in_final] = filtered_df[numeric_cols_in_final].round(4) # Use more precision for contribution?
        securities_data_list = filtered_df[final_col_order].to_dict(orient='records')
        # Handle potential NaN/NaT values for template rendering
        for row in securities_data_list:
             for key, value in row.items():
                 if pd.isna(value):
                     row[key] = None
        print(f"Final column order for display: {final_col_order}")
        return render_template('fund_duration_details.html',
                               fund_code=fund_code,
                               securities_data=securities_data_list,
                               column_order=final_col_order,
                               id_col_name=id_col_name,
                               message=None)
    except FileNotFoundError:
         return f"Error: Data file '{duration_filename}' not found.", 404
    except Exception as e:
        print(f"Error processing duration details for fund {fund_code}: {e}")
        traceback.print_exc()
        return f"An error occurred processing duration details for fund {fund_code}: {e}", 500
# --- New Route for Fund Detail Page ---
@fund_bp.route('/<fund_code>')
def fund_detail(fund_code):
    """
    Renders the fund detail page, displaying time-series charts for all available
    metrics associated with the given fund_code.
    Includes primary data and optionally secondary/SP data if corresponding files exist.
    """
    current_app.logger.info(f"--- Requesting Detail Page for Fund: {fund_code} ---")
    data_folder = current_app.config['DATA_FOLDER'] # Define data_folder using app config
    all_chart_data = []
    available_metrics = []
    processed_files = 0
    skipped_files = 0
    error_messages = [] # Collect specific errors
    # Helper function to convert NaN to None for JSON compatibility
    def nan_to_none(data_list):
        return [None if pd.isna(x) else x for x in data_list]
    try:
        # Find all primary time-series files
        ts_files_pattern = os.path.join(data_folder, 'ts_*.csv')
        ts_files = glob.glob(ts_files_pattern)
        # Exclude sp_ts files initially, handle them later
        ts_files = [f for f in ts_files if not os.path.basename(f).startswith('sp_ts_')]
        current_app.logger.info(f"Found {len(ts_files)} primary ts_ files: {[os.path.basename(f) for f in ts_files]}")
        if not ts_files:
            current_app.logger.warning("No primary ts_*.csv files found in Data folder.")
            return render_template('fund_detail_page.html',
                                   fund_code=fund_code,
                                   chart_data_json='[]',
                                   available_metrics=[],
                                   message="No time-series data files (ts_*.csv) found.")
        # Process each file
        for file_path in ts_files:
            filename = os.path.basename(file_path)
            # Extract metric name
            match = re.match(r'ts_(.+?)(?:_processed)?\.csv', filename, re.IGNORECASE)
            if not match:
                current_app.logger.warning(f"Could not extract metric name from filename: {filename}. Skipping.")
                skipped_files += 1
                continue
            metric_name_raw = match.group(1) # Keep raw name for SP file lookup
            metric_name_display = metric_name_raw.replace('_', ' ').title()
            current_app.logger.info(f"\\nProcessing {filename} for metric: {metric_name_display}")
            # --- Prepare SP file path ---
            sp_filename = f"sp_{filename}"
            sp_file_path = os.path.join(data_folder, sp_filename)
            sp_df = None
            sp_fund_col_name = None
            sp_load_error = None
            # --- Load primary data ---
            df = None
            fund_cols = None
            benchmark_col = None
            primary_load_error = None
            try:
                # Corrected keyword argument
                load_result = load_and_process_data(primary_filename=filename, data_folder_path=data_folder)
                df = load_result[0]
                fund_cols = load_result[1]
                benchmark_col = load_result[2]
                if df is None or df.empty:
                    current_app.logger.warning(f"No data loaded or DataFrame empty for {filename}. Skipping.")
                    skipped_files += 1
                    continue
                if 'Code' not in df.index.names:
                     current_app.logger.error(f"Index level 'Code' not found in DataFrame loaded from {filename}. Index: {df.index.names}. Skipping.")
                     error_messages.append(f"Failed to process {filename}: Missing 'Code' index level.")
                     skipped_files += 1
                     continue
            except Exception as e:
                current_app.logger.error(f"Error loading primary file {filename}: {e}", exc_info=False)
                primary_load_error = f"Error loading {filename}: {e}"
                error_messages.append(primary_load_error)
                skipped_files += 1
                # Continue processing other files, but skip this metric
                continue # Skip to next ts_file
            # --- Load SP data if primary load was successful and SP file exists ---
            if primary_load_error is None and os.path.exists(sp_file_path):
                current_app.logger.info(f"Found corresponding SP file: {sp_filename}. Attempting to load.")
                try:
                    # Corrected keyword argument
                    sp_load_result = load_and_process_data(primary_filename=sp_filename, data_folder_path=data_folder)
                    sp_df = sp_load_result[0]
                    sp_fund_cols = sp_load_result[1] # Assuming same structure for fund cols
                    # SP files typically don't have benchmarks in this context, ignore sp_load_result[2]
                    # --- Correction: Let's check for SP benchmark column too ---
                    sp_benchmark_col = sp_load_result[2] # Get potential SP benchmark col name
                    if sp_df is None or sp_df.empty:
                        current_app.logger.warning(f"No data loaded or DataFrame empty for SP file {sp_filename}.")
                        sp_df = None # Ensure sp_df is None if empty
                    elif 'Code' not in sp_df.index.names:
                         current_app.logger.error(f"Index level 'Code' not found in DataFrame loaded from SP file {sp_filename}. Index: {sp_df.index.names}.")
                         sp_df = None # Ensure sp_df is None if index is wrong
                         sp_load_error = f"SP file {sp_filename} missing 'Code' index."
                         error_messages.append(sp_load_error)
                    else:
                        # Find the fund column name in the SP data
                        sp_fund_col_name = next((col for col in sp_fund_cols if col in sp_df.columns), None)
                        if not sp_fund_col_name:
                             current_app.logger.warning(f"Could not find fund data column in SP file {sp_filename}.")
                             sp_df = None # Cannot use this SP data without fund column
                        # We keep sp_df if benchmark exists, even if fund doesn't
                except Exception as e:
                    current_app.logger.error(f"Error loading SP file {sp_filename}: {e}", exc_info=False)
                    sp_load_error = f"Error loading SP file {sp_filename}: {e}"
                    error_messages.append(sp_load_error)
                    sp_df = None # Ensure sp_df is None on error
            # --- Filter primary data for the fund ---
            fund_mask = df.index.get_level_values('Code') == fund_code
            fund_df = df[fund_mask]
            if fund_df.empty:
                current_app.logger.info(f"Fund code '{fund_code}' not found in primary data from {filename}. Skipping metric.")
                skipped_files += 1 # Processed file, but no data for this fund
                continue # Skip to next ts_file
            current_app.logger.info(f"Fund code '{fund_code}' found in primary data. Preparing chart data for '{metric_name_display}'...")
            available_metrics.append(metric_name_display) # Use display name
            # Drop the 'Code' level now we've filtered
            fund_df = fund_df.droplevel('Code')
            # --- Filter SP data for the fund (if loaded) ---
            sp_fund_df = None
            if sp_df is not None:
                sp_fund_mask = sp_df.index.get_level_values('Code') == fund_code
                sp_fund_df = sp_df[sp_fund_mask]
                if not sp_fund_df.empty:
                    sp_fund_df = sp_fund_df.droplevel('Code')
                    current_app.logger.info(f"Fund code '{fund_code}' found in SP data from {sp_filename}.")
                else:
                    current_app.logger.info(f"Fund code '{fund_code}' *not* found in SP data from {sp_filename}.")
                    sp_fund_df = None # Treat as if no SP data for this fund
            # --- Prepare chart data structure ---
            # Use the primary fund_df index as the master list of labels
            # Align SP data to this index later if needed
            chart_labels = fund_df.index.strftime('%Y-%m-%d').tolist()
            chart_data = {
                'metricName': metric_name_display, # Use display name
                'labels': chart_labels,
                'datasets': []
            }
            # Add primary fund dataset
            fund_col_name = next((col for col in fund_cols if col in fund_df.columns), None)
            if fund_col_name:
                # Reindex to ensure consistent length and alignment, fill missing with None
                fund_values = fund_df[fund_col_name].reindex(fund_df.index).where(pd.notna, None).tolist()
                chart_data['datasets'].append({
                    'label': f"{fund_code} {metric_name_display}",
                    'data': nan_to_none(fund_values),
                    'borderColor': current_app.config['COLOR_PALETTE'][0 % len(current_app.config['COLOR_PALETTE'])],
                    'tension': 0.1,
                    'pointRadius': 1,
                    'borderWidth': 1.5,
                    'isSpData': False # Explicitly mark as not SP
                })
            else:
                current_app.logger.warning(f"Warning: Could not find primary fund data column ({fund_cols}) in {filename} for fund {fund_code}")
            # Add benchmark dataset (from primary data)
            if benchmark_col and benchmark_col in fund_df.columns:
                # Reindex to ensure consistent length and alignment
                bench_values = fund_df[benchmark_col].reindex(fund_df.index).where(pd.notna, None).tolist()
                chart_data['datasets'].append({
                    'label': f"Benchmark ({benchmark_col})",
                    'data': nan_to_none(bench_values),
                    'borderColor': current_app.config['COLOR_PALETTE'][1 % len(current_app.config['COLOR_PALETTE'])],
                    'tension': 0.1,
                    'pointRadius': 1,
                    'borderDash': [5, 5],
                    'borderWidth': 1,
                    'isSpData': False # Explicitly mark as not SP
                })
            # --- Add SP fund dataset (if available) ---
            if sp_fund_df is not None:
                # Add SP Fund Data (if column exists)
                if sp_fund_col_name:
                    # Reindex SP data to the primary data's date index to ensure alignment for the chart
                    sp_fund_aligned = sp_fund_df[sp_fund_col_name].reindex(fund_df.index)
                    sp_values = sp_fund_aligned.where(pd.notna, None).tolist() # Replace NaN with None for JSON
                    chart_data['datasets'].append({
                        'label': f"{fund_code} {metric_name_display} (SP)",
                        'data': nan_to_none(sp_values),
                        'borderColor': current_app.config['COLOR_PALETTE'][2 % len(current_app.config['COLOR_PALETTE'])], # Use a different color
                        'tension': 0.1,
                        'pointRadius': 1,
                        'borderDash': [2, 2], # Different dash style
                        'borderWidth': 1.5,
                        'isSpData': True # Mark this dataset as SP data
                        # 'hidden': True # Optionally start hidden
                    })
                    current_app.logger.info(f"Added SP Fund dataset for metric '{metric_name_display}'.")
                else:
                    current_app.logger.warning(f"SP Fund column ('{sp_fund_cols}') not found in filtered SP data for {sp_filename}, fund {fund_code}.")
                # --- Add SP benchmark dataset (if available) ---
                if sp_benchmark_col and sp_benchmark_col in sp_fund_df.columns:
                    # Reindex SP benchmark data to the primary data's date index
                    sp_bench_aligned = sp_fund_df[sp_benchmark_col].reindex(fund_df.index)
                    sp_bench_values = sp_bench_aligned.where(pd.notna, None).tolist()
                    chart_data['datasets'].append({
                        'label': f"Benchmark ({sp_benchmark_col}) (SP)", # Label appropriately
                        'data': nan_to_none(sp_bench_values),
                        'borderColor': current_app.config['COLOR_PALETTE'][3 % len(current_app.config['COLOR_PALETTE'])], # Use another color
                        'tension': 0.1,
                        'pointRadius': 1,
                        'borderDash': [2, 2], # Use dash similar to SP fund
                        'borderWidth': 1,
                        'isSpData': True # Mark this dataset as SP data
                    })
                    current_app.logger.info(f"Added SP Benchmark dataset ('{sp_benchmark_col}') for metric '{metric_name_display}'.")
                elif sp_benchmark_col:
                    current_app.logger.warning(f"SP Benchmark column ('{sp_benchmark_col}') specified but not found in filtered SP data for {sp_filename}, fund {fund_code}.")
            # Only add chart if we have at least one non-empty dataset
            if any(d['data'] for d in chart_data['datasets']):
                all_chart_data.append(chart_data)
                processed_files += 1
            else:
                 current_app.logger.warning(f"No valid datasets generated for metric '{metric_name_display}' from {filename} (and potentially {sp_filename}). Skipping chart.")
                 skipped_files += 1 # Count as skipped if no dataset generated
            # Explicitly remove large dataframes from memory
            del df, fund_df, sp_df, sp_fund_df, load_result
            if 'sp_load_result' in locals(): del sp_load_result
            import gc
            gc.collect()
        # --- After processing all files ---
        current_app.logger.info(f"Finished processing files for fund {fund_code}. Generated charts for: {available_metrics}. Total Processed: {processed_files}, Skipped/No Data/Errors: {skipped_files}")
        if not all_chart_data:
             # Combine specific errors with the generic message if available
             final_message = f"No metrics found with data for fund '{fund_code}'."
             if error_messages:
                 final_message += " Errors encountered: " + "; ".join(error_messages)
             elif skipped_files > 0:
                 final_message += f" ({skipped_files} files skipped or had no data for this fund). Check logs for details."
             current_app.logger.warning(final_message) # Log the final message
             return render_template('fund_detail_page.html',
                                   fund_code=fund_code,
                                   chart_data_json='[]',
                                   available_metrics=[],
                                   message=final_message)
        # Convert chart data to JSON for the template
        chart_data_json = jsonify(all_chart_data).get_data(as_text=True)
        return render_template('fund_detail_page.html',
                               fund_code=fund_code,
                               chart_data_json=chart_data_json,
                               available_metrics=available_metrics,
                               message=None) # No message if data was found
    except Exception as e:
        current_app.logger.error(f"Unexpected error in fund_detail for {fund_code}: {e}", exc_info=True)
        traceback.print_exc()
        # Render the page with an error message
        return render_template('fund_detail_page.html',
                               fund_code=fund_code,
                               chart_data_json='[]',
                               available_metrics=[],
                               message=f"An unexpected error occurred: {e}")
</file>

<file path="views/issue_views.py">
# views/issue_views.py
# Purpose: Defines the Flask Blueprint and routes for the Data Issue Tracking feature.
from flask import Blueprint, render_template, request, redirect, url_for, flash
import issue_processing  # Use the new module
import pandas as pd
from datetime import datetime
issue_bp = Blueprint('issue_bp', __name__, template_folder='templates')
DATA_SOURCES = ["S&P", "Production", "Pi", "IVP", "Benchmark", "BANG Bug"] # Define allowed sources
@issue_bp.route('/issues', methods=['GET', 'POST'])
def manage_issues():
    """Displays existing issues and handles adding new issues."""
    message = None
    message_type = 'info'
    available_funds = issue_processing.load_fund_list() # Get fund list for dropdown
    if request.method == 'POST':
        # Process form for adding a new issue
        try:
            raised_by = request.form.get('raised_by')
            fund_impacted = request.form.get('fund_impacted')
            data_source = request.form.get('data_source')
            # Handle date input carefully
            issue_date_str = request.form.get('issue_date')
            issue_date = pd.to_datetime(issue_date_str).date() if issue_date_str else None
            description = request.form.get('description')
            # Basic Validation
            if not raised_by or not fund_impacted or not data_source or not description or not issue_date:
                 raise ValueError("Missing required fields.")
            if data_source not in DATA_SOURCES:
                 raise ValueError("Invalid data source selected.")
            issue_id = issue_processing.add_issue(
                raised_by=raised_by,
                fund_impacted=fund_impacted,
                data_source=data_source,
                issue_date=issue_date,
                description=description
            )
            message = f"Successfully added new issue (ID: {issue_id})."
            message_type = 'success'
            flash(message, message_type)
            return redirect(url_for('issue_bp.manage_issues')) # Redirect to clear form
        except ValueError as ve:
             message = f"Error adding issue: {ve}"
             message_type = 'danger'
             flash(message, message_type)
        except Exception as e:
            message = f"An unexpected error occurred: {e}"
            message_type = 'danger'
            flash(message, message_type)
        # If error, fall through to render template again with message
    # For GET request or if POST had an error
    issues_df = issue_processing.load_issues()
    open_issues = issues_df[issues_df['Status'] == 'Open'].sort_values(by='DateRaised', ascending=False).to_dict('records')
    closed_issues = issues_df[issues_df['Status'] == 'Closed'].sort_values(by='DateClosed', ascending=False).to_dict('records')
    return render_template('issues_page.html',
                           open_issues=open_issues,
                           closed_issues=closed_issues,
                           available_funds=available_funds,
                           data_sources=DATA_SOURCES,
                           )
@issue_bp.route('/issues/close', methods=['POST'])
def close_issue_route():
    """Handles closing an existing issue."""
    try:
        issue_id = request.form.get('issue_id')
        closed_by = request.form.get('closed_by')
        resolution_comment = request.form.get('resolution_comment')
        if not issue_id or not closed_by or not resolution_comment:
             raise ValueError("Missing required fields for closing the issue.")
        success = issue_processing.close_issue(issue_id, closed_by, resolution_comment)
        if success:
            flash(f"Issue {issue_id} marked as closed.", 'success')
        else:
             flash(f"Failed to close issue {issue_id}. It might not exist.", 'warning')
    except ValueError as ve:
         flash(f"Error closing issue: {ve}", 'danger')
    except Exception as e:
        flash(f"An unexpected error occurred while closing the issue: {e}", 'danger')
    return redirect(url_for('issue_bp.manage_issues'))
</file>

<file path="views/main_views.py">
# This file defines the routes related to the main, top-level views of the application.
# It primarily handles the dashboard or index page.
"""
Blueprint for main application routes, like the index page.
"""
from flask import Blueprint, render_template, current_app
import os
import pandas as pd
import traceback
# Import necessary functions/constants from other modules
# Removed: from config import DATA_FOLDER
from data_loader import load_and_process_data
from metric_calculator import calculate_latest_metrics
# Define the blueprint for main routes
main_bp = Blueprint('main', __name__)
@main_bp.route('/')
def index():
    """Renders the main dashboard page (`index.html`).
    This view performs the following steps:
    1. Scans the configured data directory for time-series metric files (prefixed with `ts_`).
    2. For each `ts_` file found:
        a. Loads and processes the data using `data_loader.load_and_process_data`,
           providing the configured data directory path.
        b. Calculates metrics (including Z-scores) using `metric_calculator.calculate_latest_metrics`.
        c. Extracts the 'Change Z-Score' columns for both the benchmark and any specific fund columns.
    3. Aggregates all extracted 'Change Z-score' columns from all files into a single pandas DataFrame (`summary_df`).
    4. Creates unique column names for the summary table by combining the original column name and the metric file name
       (e.g., 'Benchmark - Yield', 'FUND_A - Duration').
    5. Passes the list of available metric display names (filenames without `ts_`) and the aggregated Z-score
       DataFrame (`summary_df`) along with its corresponding column headers (`summary_metrics`) to the `index.html` template.
    This allows the dashboard to display a consolidated view of the most recent significant changes across all metrics.
    """
    # Retrieve the absolute data folder path from the app context
    data_folder = current_app.config['DATA_FOLDER']
    if not data_folder:
        current_app.logger.error("DATA_FOLDER is not configured in the application.")
        return "Internal Server Error: Data folder not configured", 500
    current_app.logger.info(f"Scanning data folder for dashboard: {data_folder}")
    # Find only files starting with ts_ and ending with .csv in the configured data folder
    try:
        files = [f for f in os.listdir(data_folder) if f.startswith('ts_') and f.endswith('.csv')]
    except FileNotFoundError:
        current_app.logger.error(f"Configured DATA_FOLDER does not exist: {data_folder}")
        files = []
    except Exception as e:
        current_app.logger.error(f"Error listing files in data folder {data_folder}: {e}")
        files = []
    # Create two lists: one for filenames (with ts_), one for display (without ts_)
    metric_filenames = sorted([os.path.splitext(f)[0] for f in files])
    metric_display_names = sorted([name[3:] for name in metric_filenames]) # Remove 'ts_' prefix
    all_z_scores_list = []
    # Store the unique combined column names for the summary table header
    processed_summary_columns = []
    print("Starting Change Z-score aggregation for dashboard (ts_ files only)...")
    # Iterate using the filenames with prefix
    for metric_filename in metric_filenames:
        filename = f"{metric_filename}.csv"
        # Get the corresponding display name for this file
        display_name = metric_filename[3:]
        try:
            print(f"Processing {filename}...")
            # Unpack all 6 values, but only use the primary ones for the dashboard summary
            # Pass the absolute data folder path to the loader
            df, fund_cols, benchmark_col, _sec_df, _sec_fund_cols, _sec_bench_col = load_and_process_data(
                primary_filename=filename,
                data_folder_path=data_folder # Pass the absolute path
            )
            # Check if data loading failed (df will be None)
            if df is None:
                 print(f"Warning: Failed to load data for {filename}. Skipping.")
                 continue # Skip this file if loading failed
            # Skip if no benchmark AND no fund columns identified
            if not benchmark_col and not fund_cols:
                 print(f"Warning: No benchmark or fund columns identified in {filename}. Skipping.")
                 continue
            # Calculate metrics using the current function
            latest_metrics = calculate_latest_metrics(df, fund_cols, benchmark_col)
            # --- Extract Change Z-score for ALL columns (benchmark + funds) --- 
            if not latest_metrics.empty:
                columns_to_check = []
                if benchmark_col:
                    columns_to_check.append(benchmark_col)
                if fund_cols:
                    columns_to_check.extend(fund_cols)
                if not columns_to_check:
                    print(f"Warning: No columns to check for Z-scores in {filename} despite loading data.")
                    continue
                print(f"Checking for Z-scores for columns: {columns_to_check} in metric {display_name}")
                found_z_for_metric = False
                for original_col_name in columns_to_check:
                    z_score_col_name = f'{original_col_name} Change Z-Score'
                    if z_score_col_name in latest_metrics.columns:
                        # Create a unique name for the summary table column
                        summary_col_name = f"{original_col_name} - {display_name}"
                        # Extract and rename
                        metric_z_scores = latest_metrics[[z_score_col_name]].rename(columns={z_score_col_name: summary_col_name})
                        all_z_scores_list.append(metric_z_scores)
                        # Add the unique column name to our list if not already present (preserves order of discovery)
                        if summary_col_name not in processed_summary_columns:
                             processed_summary_columns.append(summary_col_name)
                        found_z_for_metric = True
                        print(f"  -> Extracted: {summary_col_name}")
                    else:
                        print(f"  -> Z-score column '{z_score_col_name}' not found.")
                if not found_z_for_metric:
                    print(f"Warning: No Z-score columns found for any checked column in metric {display_name} (from {filename}).")
            else:
                 print(f"Warning: Could not calculate latest_metrics for {filename}. Skipping Z-score extraction.")
        except FileNotFoundError:
            print(f"Error: Data file '{filename}' not found.")
        except ValueError as ve:
            print(f"Value Error processing {metric_filename}: {ve}") # Log with filename
        except Exception as e:
            print(f"Error processing {metric_filename} during dashboard aggregation: {e}") # Log with filename
            traceback.print_exc()
    # Combine all Z-score Series/DataFrames into one
    summary_df = pd.DataFrame()
    if all_z_scores_list:
        summary_df = pd.concat(all_z_scores_list, axis=1)
        # Ensure the columns are in the order they were discovered
        if processed_summary_columns:
             # Handle potential missing columns if a file failed processing midway
             cols_available_in_summary = [col for col in processed_summary_columns if col in summary_df.columns]
             summary_df = summary_df[cols_available_in_summary]
             # Update the list of columns to only those actually present
             processed_summary_columns = cols_available_in_summary
        print("Successfully combined Change Z-scores.")
        print(f"Summary DF columns: {summary_df.columns.tolist()}")
    else:
        print("No Change Z-scores could be extracted for the summary.")
    return render_template('index.html',
                           metrics=metric_display_names, # Still used for top-level metric links
                           summary_data=summary_df,
                           summary_metrics=processed_summary_columns) # Pass the NEW list of combined column names
</file>

<file path="views/metric_views.py">
# This file defines the routes for displaying detailed views of specific time-series metrics.
# It handles requests where the user wants to see the data and charts for a single metric
# (like 'Yield' or 'Spread Duration') across all applicable funds.
# Updated to optionally load and display a secondary data source (prefixed with 'sp_').
"""
Blueprint for metric-specific routes (e.g., displaying individual metric charts).
"""
from flask import Blueprint, render_template, jsonify, current_app
import os
import pandas as pd
import numpy as np
import traceback
# Import necessary functions/constants from other modules
from config import COLOR_PALETTE
from data_loader import load_and_process_data, LoadResult # Import LoadResult type
from metric_calculator import calculate_latest_metrics
# Define the blueprint for metric routes, using '/metric' as the URL prefix
metric_bp = Blueprint('metric', __name__, url_prefix='/metric')
@metric_bp.route('/<string:metric_name>')
def metric_page(metric_name):
    """Renders the detailed page (`metric_page_js.html`) for a specific metric.
    Loads primary data (e.g., 'ts_Yield.csv') and optionally secondary data ('sp_ts_Yield.csv').
    Calculates metrics for both, prepares data for Chart.js, and passes it to the template.
    Includes a flag to indicate if secondary data is available.
    """
    primary_filename = f"ts_{metric_name}.csv"
    secondary_filename = f"sp_{primary_filename}"
    fund_code = 'N/A' # Default for logging fallback in case of early error
    latest_date_overall = pd.Timestamp.min # Initialize
    try:
        print(f"--- Processing metric: {metric_name} ---")
        print(f"Primary file: {primary_filename}, Secondary file: {secondary_filename}")
        # Load Data (Primary and Secondary)
        load_result: LoadResult = load_and_process_data(primary_filename, secondary_filename)
        primary_df, pri_fund_cols, pri_bench_col, secondary_df, sec_fund_cols, sec_bench_col = load_result
        # --- Validate Primary Data --- 
        if primary_df is None or primary_df.empty or pri_fund_cols is None:
            # Retrieve the configured absolute data folder path for error reporting
            data_folder_for_error = current_app.config['DATA_FOLDER']
            # Construct the full path using the absolute data_folder path
            primary_filepath = os.path.join(data_folder_for_error, primary_filename)
            if not os.path.exists(primary_filepath):
                 current_app.logger.error(f"Error: Primary data file not found: {primary_filepath}")
                 return f"Error: Data file for metric '{metric_name}' (expected: '{primary_filename}') not found.", 404
            else:
                 print(f"Error: Failed to process primary data file: {primary_filename}")
                 return f"Error: Could not process required data for metric '{metric_name}' (file: {primary_filename}). Check file format or logs.", 500
        # --- Determine Combined Metadata --- 
        all_dfs = [df for df in [primary_df, secondary_df] if df is not None and not df.empty]
        if not all_dfs:
             # Should be caught by primary check, but safeguard
            print(f"Error: No valid data loaded for {metric_name}")
            return f"Error: No data found for metric '{metric_name}'.", 404
        try:
            combined_index = pd.concat(all_dfs).index
            latest_date_overall = combined_index.get_level_values(0).max()
            latest_date_str = latest_date_overall.strftime('%Y-%m-%d')
        except Exception as idx_err:
            print(f"Error combining indices or getting latest date for {metric_name}: {idx_err}")
            # Fallback or re-raise? Let's try to proceed if possible, using primary date
            latest_date_overall = primary_df.index.get_level_values(0).max()
            latest_date_str = latest_date_overall.strftime('%Y-%m-%d')
            print(f"Warning: Using latest date from primary data only: {latest_date_str}")
        secondary_data_available = secondary_df is not None and not secondary_df.empty and sec_fund_cols is not None
        print(f"Secondary data available for {metric_name}: {secondary_data_available}")
        # --- Calculate Metrics --- 
        print(f"Calculating metrics for {metric_name}...")
        latest_metrics = calculate_latest_metrics(
            primary_df=primary_df,
            primary_fund_cols=pri_fund_cols,
            primary_benchmark_col=pri_bench_col,
            secondary_df=secondary_df if secondary_data_available else None,
            secondary_fund_cols=sec_fund_cols if secondary_data_available else None,
            secondary_benchmark_col=sec_bench_col if secondary_data_available else None,
            secondary_prefix="S&P "
        )
        # --- Handle Empty Metrics Result --- 
        if latest_metrics.empty:
            print(f"Warning: Metric calculation returned empty DataFrame for {metric_name}. Rendering page with no fund data.")
            missing_latest = pd.DataFrame()
            json_payload = {
                "metadata": {
                    "metric_name": metric_name,
                    "latest_date": latest_date_str,
                    "fund_col_names": pri_fund_cols or [],
                    "benchmark_col_name": pri_bench_col,
                    "secondary_fund_col_names": sec_fund_cols if secondary_data_available else [],
                    "secondary_benchmark_col_name": sec_bench_col if secondary_data_available else None,
                    "secondary_data_available": secondary_data_available
                },
                "funds": {}
            }
            return render_template('metric_page_js.html',
                           metric_name=metric_name,
                           charts_data_json=jsonify(json_payload).get_data(as_text=True),
                           latest_date=latest_date_overall.strftime('%d/%m/%Y'), 
                           missing_funds=missing_latest)
        # --- Identify Missing Funds (based on primary data) --- 
        print(f"Identifying potentially missing latest data for {metric_name}...")
        primary_cols_for_check = []
        if pri_bench_col:
            primary_cols_for_check.append(pri_bench_col)
        if pri_fund_cols:
            primary_cols_for_check.extend(pri_fund_cols)
        # Prefer checking Z-Score, fallback to Latest Value
        primary_z_score_cols = [f'{col} Change Z-Score' for col in primary_cols_for_check 
                                if f'{col} Change Z-Score' in latest_metrics.columns]
        primary_latest_val_cols = [f'{col} Latest Value' for col in primary_cols_for_check
                                   if f'{col} Latest Value' in latest_metrics.columns]
        check_cols_for_missing = primary_z_score_cols if primary_z_score_cols else primary_latest_val_cols
        if check_cols_for_missing:
            missing_latest = latest_metrics[latest_metrics[check_cols_for_missing].isna().any(axis=1)]
        else:
            print(f"Warning: No primary Z-Score or Latest Value columns found for {metric_name} to check for missing data.")
            missing_latest = pd.DataFrame(index=latest_metrics.index) # Assume none are missing if no check cols
        # --- Prepare Data Structure for JavaScript --- 
        print(f"Preparing chart and metric data for JavaScript for {metric_name}...")
        funds_data_for_js = {}
        fund_codes_in_metrics = latest_metrics.index
        primary_df_index = primary_df.index
        secondary_df_index = secondary_df.index if secondary_data_available and secondary_df is not None else None
        # Helper function to convert NaN to None for JSON compatibility
        def nan_to_none(data_list):
            return [None if pd.isna(x) else x for x in data_list]
        # Loop through funds present in the calculated metrics
        for fund_code in fund_codes_in_metrics:
            fund_latest_metrics_row = latest_metrics.loc[fund_code]
            is_missing_latest = fund_code in missing_latest.index
            fund_charts = [] # Initialize list to hold chart configs for this fund
            primary_labels = []
            primary_dt_index = None
            fund_hist_primary = None
            relative_primary_hist = None
            relative_secondary_hist = None # Initialize
            # --- Get Primary Historical Data ---
            if fund_code in primary_df_index.get_level_values(1):
                fund_hist_primary = primary_df.xs(fund_code, level=1).sort_index()
                if isinstance(fund_hist_primary.index, pd.DatetimeIndex):
                    primary_dt_index = fund_hist_primary.index # Store before filtering
                    # Filter out weekends (assuming data is daily/business daily)
                    fund_hist_primary = fund_hist_primary[primary_dt_index.dayofweek < 5]
                    primary_dt_index = fund_hist_primary.index # Update after filtering
                    primary_labels = primary_dt_index.strftime('%Y-%m-%d').tolist()
                else:
                    primary_labels = fund_hist_primary.index.astype(str).tolist()
                    print(f"Warning: Primary index for {fund_code} is not DatetimeIndex.")
            # --- Get Secondary Historical Data ---
            fund_hist_secondary = None
            if secondary_data_available and secondary_df_index is not None and fund_code in secondary_df_index.get_level_values(1):
                fund_hist_secondary_raw = secondary_df.xs(fund_code, level=1).sort_index()
                if isinstance(fund_hist_secondary_raw.index, pd.DatetimeIndex):
                    fund_hist_secondary_raw = fund_hist_secondary_raw[fund_hist_secondary_raw.index.dayofweek < 5]
                    # Reindex to primary date index if possible
                    if primary_dt_index is not None and not primary_dt_index.empty:
                         try:
                             if isinstance(fund_hist_secondary_raw.index, pd.DatetimeIndex):
                                 fund_hist_secondary = fund_hist_secondary_raw.reindex(primary_dt_index)
                                 print(f"Successfully reindexed secondary data for {fund_code}.")
                             else:
                                 print(f"Warning: Cannot reindex - Secondary index for {fund_code} is not DatetimeIndex after filtering.")
                         except Exception as reindex_err:
                             print(f"Warning: Reindexing secondary data for {fund_code} failed: {reindex_err}. Chart may be misaligned.")
                             fund_hist_secondary = fund_hist_secondary_raw # Use unaligned as fallback
                    else:
                         print(f"Warning: Cannot reindex secondary for {fund_code} - Primary DatetimeIndex unavailable.")
                         fund_hist_secondary = fund_hist_secondary_raw # Use unaligned as fallback
                else:
                    print(f"Warning: Secondary index for {fund_code} is not DatetimeIndex.")
                    fund_hist_secondary = fund_hist_secondary_raw # Use raw if not datetime
            # --- Prepare Main Chart Datasets (Primary Data) ---
            main_datasets = []
            if fund_hist_primary is not None:
                # Add primary fund column(s)
                if pri_fund_cols:
                    for i, col in enumerate(pri_fund_cols):
                        if col in fund_hist_primary.columns:
                            main_datasets.append({
                                "label": col,
                                "data": nan_to_none(fund_hist_primary[col].tolist()), # Convert NaN to None
                                "borderColor": COLOR_PALETTE[i % len(COLOR_PALETTE)],
                                "backgroundColor": f"{COLOR_PALETTE[i % len(COLOR_PALETTE)]}40", # Add alpha
                                "tension": 0.1,
                                "source": "primary",
                                "isSpData": False
                            })
                # Add primary benchmark column
                if pri_bench_col and pri_bench_col in fund_hist_primary.columns:
                    main_datasets.append({
                        "label": "Benchmark",
                        "data": nan_to_none(fund_hist_primary[pri_bench_col].tolist()), # Convert NaN to None
                        "borderColor": "black",
                        "backgroundColor": "grey",
                        "borderDash": [5, 5],
                        "tension": 0.1,
                        "source": "primary",
                        "isSpData": False
                    })
            # --- Add Secondary Data to Main Chart Datasets ---
            if secondary_data_available and fund_hist_secondary is not None:
                 # Add secondary fund column(s) - Use same color but different style
                if sec_fund_cols:
                    for i, col in enumerate(sec_fund_cols):
                        if col in fund_hist_secondary.columns:
                             main_datasets.append({
                                "label": f"S&P {col}", # Prefix with S&P
                                "data": nan_to_none(fund_hist_secondary[col].tolist()), # Convert NaN to None
                                "borderColor": COLOR_PALETTE[i % len(COLOR_PALETTE)], # Same base color
                                "backgroundColor": f"{COLOR_PALETTE[i % len(COLOR_PALETTE)]}20", # Lighter alpha
                                "borderDash": [2, 2], # Different dash style
                                "tension": 0.1,
                                "source": "secondary",
                                "isSpData": True # Mark as SP data
                            })
                # Add secondary benchmark column
                if sec_bench_col and sec_bench_col in fund_hist_secondary.columns:
                    main_datasets.append({
                        "label": "S&P Benchmark",
                        "data": nan_to_none(fund_hist_secondary[sec_bench_col].tolist()), # Convert NaN to None
                        "borderColor": "#FFA500", # Orange for SP Benchmark
                        "backgroundColor": "#FFDAB9", # Light Orange
                        "borderDash": [2, 2],
                        "tension": 0.1,
                        "source": "secondary",
                        "isSpData": True # Mark as SP data
                    })
            # --- Prepare Relative Chart Data (if possible) ---
            relative_datasets = []
            relative_chart_config = None
            relative_metrics_for_js = {}
            # 1. Calculate Primary Relative Series
            pri_fund_col_used = None
            if fund_hist_primary is not None and pri_fund_cols:
                for f_col in pri_fund_cols:
                    if f_col in fund_hist_primary.columns:
                        pri_fund_col_used = f_col
                        break
            if pri_fund_col_used and pri_bench_col and pri_bench_col in fund_hist_primary.columns:
                port_col_hist = fund_hist_primary[pri_fund_col_used]
                bench_col_hist = fund_hist_primary[pri_bench_col]
                if not port_col_hist.dropna().empty and not bench_col_hist.dropna().empty:
                    relative_primary_hist = (port_col_hist - bench_col_hist).round(3).replace([np.inf, -np.inf], np.nan).where(pd.notnull, None)
                    relative_datasets.append({
                        'label': 'Relative (Port - Bench)',
                        'data': nan_to_none(relative_primary_hist.tolist()),
                        'borderColor': '#1f77b4', # Specific color for primary relative
                        'backgroundColor': '#aec7e8',
                        'tension': 0.1,
                        'source': 'primary_relative',
                        'isSpData': False
                    })
                    # Extract primary relative metrics
                    for col in fund_latest_metrics_row.index:
                        if col.startswith('Relative '):
                             relative_metrics_for_js[col] = fund_latest_metrics_row[col] if pd.notna(fund_latest_metrics_row[col]) else None
            # 2. Calculate Secondary Relative Series (if applicable)
            sec_fund_col_used = None
            if fund_hist_secondary is not None and sec_fund_cols:
                 for f_col in sec_fund_cols:
                    if f_col in fund_hist_secondary.columns:
                        sec_fund_col_used = f_col
                        break
            if sec_fund_col_used and sec_bench_col and sec_bench_col in fund_hist_secondary.columns:
                port_col_hist_sec = fund_hist_secondary[sec_fund_col_used]
                bench_col_hist_sec = fund_hist_secondary[sec_bench_col]
                # Check if S&P Relative metrics exist, indicating calculation happened
                if 'S&P Relative Change Z-Score' in fund_latest_metrics_row.index and pd.notna(fund_latest_metrics_row['S&P Relative Change Z-Score']):
                    if not port_col_hist_sec.dropna().empty and not bench_col_hist_sec.dropna().empty:
                        relative_secondary_hist = (port_col_hist_sec - bench_col_hist_sec).round(3).replace([np.inf, -np.inf], np.nan).where(pd.notnull, None)
                        relative_datasets.append({
                            'label': 'S&P Relative (Port - Bench)',
                            'data': nan_to_none(relative_secondary_hist.tolist()),
                            'borderColor': '#ff7f0e', # Specific color for secondary relative
                            'backgroundColor': '#ffbb78',
                            'borderDash': [2, 2],
                            'tension': 0.1,
                            'source': 'secondary_relative',
                            'isSpData': True,
                            'hidden': True # Initially hidden
                        })
                         # Extract secondary relative metrics
                        for col in fund_latest_metrics_row.index:
                            if col.startswith('S&P Relative '):
                                relative_metrics_for_js[col] = fund_latest_metrics_row[col] if pd.notna(fund_latest_metrics_row[col]) else None
            # 3. Create Relative Chart Config if primary relative data exists
            if relative_primary_hist is not None:
                relative_chart_config = {
                    'chart_type': 'relative',
                    'title': f'{fund_code} - Relative ({metric_name})',
                    'labels': primary_labels,
                    'datasets': relative_datasets,
                    'latest_metrics': relative_metrics_for_js
                }
                # We will add this later, after the main chart
                # fund_charts.append(relative_chart_config) # Add relative chart first
            # --- Prepare Main Chart Config
            main_chart_config = None
            if main_datasets: # Only create if there's actual data
                main_chart_config = {
                    'chart_type': 'main',
                    'title': f'{fund_code} - {metric_name}',
                    'labels': primary_labels,
                    'datasets': main_datasets,
                    'latest_metrics': fund_latest_metrics_row.to_dict()
                }
                # Add main chart FIRST
                fund_charts.append(main_chart_config)
            # Now add the relative chart config if it exists
            if relative_chart_config:
                fund_charts.append(relative_chart_config)
            # --- Store Fund Data ---
            funds_data_for_js[fund_code] = {
                'charts': fund_charts,
                'is_missing_latest': is_missing_latest
            }
        # --- Final JSON Payload ---
        json_payload = {
            "metadata": {
                "metric_name": metric_name,
                "latest_date": latest_date_str,
                 # Keep original column names for potential reference, though chart uses specific labels now
                "fund_col_names": pri_fund_cols or [],
                "benchmark_col_name": pri_bench_col,
                "secondary_fund_col_names": sec_fund_cols if secondary_data_available else [],
                "secondary_benchmark_col_name": sec_bench_col if secondary_data_available else None,
                "secondary_data_available": secondary_data_available
            },
            "funds": funds_data_for_js # Use the new structure
        }
        print(f"--- Completed processing metric: {metric_name} ---")
        return render_template('metric_page_js.html',
                               metric_name=metric_name,
                               charts_data_json=jsonify(json_payload).get_data(as_text=True),
                               latest_date=latest_date_overall.strftime('%d/%m/%Y'),
                               missing_funds=missing_latest,
                               error_message=None) # Explicitly set error to None on success
    except FileNotFoundError as e:
        print(f"Error: File not found during processing for {metric_name}. Details: {e}")
        traceback.print_exc()
        error_msg = f"Error: Required data file not found for metric '{metric_name}'. {e}"
        return render_template('metric_page_js.html', metric_name=metric_name, charts_data_json='{}', latest_date='N/A', missing_funds=pd.DataFrame(), error_message=error_msg), 404
    except Exception as e:
        print(f"Error processing metric page for {metric_name} (Fund: {fund_code}): {e}")
        traceback.print_exc() # Log the full traceback to console/log file
        error_msg = f"An error occurred while processing metric '{metric_name}'. Please check the server logs for details. Error: {e}"
        # Attempt to render template with error message
        return render_template('metric_page_js.html', metric_name=metric_name, charts_data_json='{}', latest_date='N/A', missing_funds=pd.DataFrame(), error_message=error_msg), 500
</file>

<file path="views/security_views.py">
"""
Blueprint for security-related routes (e.g., summary page and individual details).
"""
from flask import Blueprint, render_template, jsonify, send_from_directory, url_for, current_app
import os
import pandas as pd
import numpy as np
import traceback
from urllib.parse import unquote
from datetime import datetime
from flask import request # Import request
import math
import json
# Import necessary functions/constants from other modules
from config import COLOR_PALETTE # Keep palette
from security_processing import load_and_process_security_data, calculate_security_latest_metrics
# Import the exclusion loading function
from views.exclusion_views import load_exclusions # Only import load_exclusions
# Define the blueprint
security_bp = Blueprint('security', __name__, url_prefix='/security')
PER_PAGE = 50 # Define how many items per page
def get_active_exclusions(data_folder_path: str):
    """Loads exclusions and returns a set of SecurityIDs that are currently active."""
    # Pass the data folder path to the load_exclusions function
    exclusions = load_exclusions(data_folder_path)
    active_exclusions = set()
    today = datetime.now().date()
    for ex in exclusions:
        try:
            add_date = ex['AddDate'].date() if pd.notna(ex['AddDate']) else None
            end_date = ex['EndDate'].date() if pd.notna(ex['EndDate']) else None
            security_id = str(ex['SecurityID']) # Ensure it's string for comparison
            if add_date and add_date <= today:
                if end_date is None or end_date >= today:
                    active_exclusions.add(security_id)
        except Exception as e:
            print(f"Error processing exclusion record {ex}: {e}") # Use logging in production
    print(f"Found {len(active_exclusions)} active exclusions: {active_exclusions}")
    return active_exclusions
@security_bp.route('/summary')
def securities_page():
    """Renders a page summarizing potential issues in security-level data, with server-side pagination, filtering, and sorting."""
    print("\n--- Starting Security Data Processing (Paginated) ---")
    # Retrieve the configured absolute data folder path
    data_folder = current_app.config['DATA_FOLDER']
    if not data_folder:
        current_app.logger.error("DATA_FOLDER is not configured in the application.")
        return "Internal Server Error: Data folder not configured", 500
    # --- Get Request Parameters ---
    page = request.args.get('page', 1, type=int)
    search_term = request.args.get('search_term', '', type=str).strip()
    sort_by = request.args.get('sort_by', None, type=str)
    # Default sort: Abs Change Z-Score Descending
    sort_order = request.args.get('sort_order', 'desc', type=str).lower() 
    # Ensure sort_order is either 'asc' or 'desc'
    if sort_order not in ['asc', 'desc']:
        sort_order = 'desc'
    # Collect active filters from request args (e.g., ?filter_Country=USA&filter_Sector=Tech)
    active_filters = {
        key.replace('filter_', ''): value 
        for key, value in request.args.items() 
        if key.startswith('filter_') and value # Ensure value is not empty
    }
    print(f"Request Params: Page={page}, Search='{search_term}', SortBy='{sort_by}', SortOrder='{sort_order}', Filters={active_filters}")
    # --- Load Base Data ---
    spread_filename = "sec_Spread.csv"
    # Construct absolute path
    data_filepath = os.path.join(data_folder, spread_filename)
    filter_options = {} # To store all possible options for filter dropdowns
    if not os.path.exists(data_filepath):
        print(f"Error: The required file '{spread_filename}' not found.")
        return render_template('securities_page.html', message=f"Error: Required data file '{spread_filename}' not found.", securities_data=[], pagination=None)
    try:
        print(f"Loading and processing file: {spread_filename}")
        # Pass the absolute data folder path
        df_long, static_cols = load_and_process_security_data(spread_filename, data_folder)
        if df_long is None or df_long.empty:
            print(f"Skipping {spread_filename} due to load/process errors or empty data.")
            return render_template('securities_page.html', message=f"Error loading or processing '{spread_filename}'.", securities_data=[], pagination=None)
        print("Calculating latest metrics...")
        combined_metrics_df = calculate_security_latest_metrics(df_long, static_cols)
        if combined_metrics_df.empty:
            print(f"No metrics calculated for {spread_filename}.")
            return render_template('securities_page.html', message=f"Could not calculate metrics from '{spread_filename}'.", securities_data=[], pagination=None)
        # Define ID column name
        id_col_name = 'ISIN' # <<< Use ISIN as the identifier
        # Check if the chosen ID column exists in the index or columns
        if id_col_name in combined_metrics_df.index.names:
            combined_metrics_df.index.name = id_col_name # Ensure index name is set if using index
            combined_metrics_df.reset_index(inplace=True)
        elif id_col_name in combined_metrics_df.columns:
            pass # ID is already a column
        else:
            # Fallback or error if ISIN isn't found
            old_id_col = combined_metrics_df.index.name or 'Security ID'
            print(f"Warning: ID column '{id_col_name}' not found. Falling back to '{old_id_col}'.")
            if old_id_col in combined_metrics_df.index.names:
                 combined_metrics_df.index.name = old_id_col
                 combined_metrics_df.reset_index(inplace=True)
                 id_col_name = old_id_col # Use the fallback name
            elif old_id_col in combined_metrics_df.columns:
                 id_col_name = old_id_col
            else:
                 print(f"Error: Cannot find a usable ID column ('{id_col_name}' or fallback '{old_id_col}') in {spread_filename}.")
                 return render_template('securities_page.html', message=f"Error: Cannot identify securities in {spread_filename}.", securities_data=[], pagination=None)
        # Store the original unfiltered dataframe's columns 
        original_columns = combined_metrics_df.columns.tolist()
        # combined_metrics_df.reset_index(inplace=True) # Reset index to make ID a regular column - ALREADY DONE OR ID IS A COLUMN
        # --- Collect Filter Options (from the full dataset BEFORE filtering) ---
        print("Collecting filter options...")
        # Ensure ID column is not treated as a filterable static column
        current_static_in_df = [col for col in static_cols if col in combined_metrics_df.columns and col != id_col_name]
        for col in current_static_in_df:
            unique_vals = combined_metrics_df[col].unique().tolist()
            unique_vals = [item.item() if isinstance(item, np.generic) else item for item in unique_vals]
            unique_vals = sorted([val for val in unique_vals if pd.notna(val) and val != '']) # Remove NaN/empty and sort
            if unique_vals: # Only add if there are valid options
                 filter_options[col] = unique_vals
        # Sort filter options dictionary by key for consistent display order
        final_filter_options = dict(sorted(filter_options.items()))
        # --- Apply Filtering Steps Sequentially ---
        print("Applying filters...")
        # 1. Search Term Filter (on ID column - now ISIN)
        if search_term:
            combined_metrics_df = combined_metrics_df[combined_metrics_df[id_col_name].astype(str).str.contains(search_term, case=False, na=False)]
            print(f"Applied search term '{search_term}'. Rows remaining: {len(combined_metrics_df)}")
        # 2. Active Exclusions Filter (should still work if exclusions use SecurityID/Name, adapt if needed)
        try:
            # Pass the absolute data folder path to get active exclusions
            active_exclusion_ids = get_active_exclusions(data_folder)
            # Assuming exclusions use Security Name/ID for now. If they use ISIN, this is correct.
            # If they use Security Name, we need to filter on that column instead.
            exclusion_col_to_check = id_col_name # Assumes exclusions use ISIN
            # If exclusions.csv uses Security Name, use this instead:
            # exclusion_col_to_check = 'Security Name' if 'Security Name' in combined_metrics_df.columns else id_col_name 
            if active_exclusion_ids:
                 combined_metrics_df = combined_metrics_df[~combined_metrics_df[exclusion_col_to_check].astype(str).isin(active_exclusion_ids)]
                 print(f"Applied {len(active_exclusion_ids)} exclusions based on '{exclusion_col_to_check}'. Rows remaining: {len(combined_metrics_df)}")
        except Exception as e:
            print(f"Warning: Error loading or applying exclusions: {e}")
        # 3. Dynamic Filters (from request args)
        if active_filters:
            for col, value in active_filters.items():
                if col in combined_metrics_df.columns:
                    # Ensure consistent type for comparison, handle NaNs
                    combined_metrics_df = combined_metrics_df[combined_metrics_df[col].astype(str) == str(value)]
                    print(f"Applied filter '{col}={value}'. Rows remaining: {len(combined_metrics_df)}")
                else:
                     print(f"Warning: Filter column '{col}' not found in DataFrame.")
        # --- Handle Empty DataFrame After Filtering ---
        if combined_metrics_df.empty:
            print("No data matches the specified filters.")
            message = "No securities found matching the current criteria."
            if search_term:
                message += f" Search term: '{search_term}'."
            if active_filters:
                 message += f" Active filters: {active_filters}."
            return render_template('securities_page.html',
                                   message=message,
                                   securities_data=[],
                                   filter_options=final_filter_options,
                                   column_order=[],
                                   id_col_name=id_col_name,
                                   search_term=search_term,
                                   active_filters=active_filters,
                                   pagination=None,
                                   current_sort_by=sort_by,
                                   current_sort_order=sort_order)
        # --- Apply Sorting ---
        print(f"Applying sort: By='{sort_by}', Order='{sort_order}'")
        # Default sort column if not provided or invalid
        effective_sort_by = sort_by
        is_default_sort = False
        if sort_by not in combined_metrics_df.columns:
             # Default to sorting by absolute Z-score if 'sort_by' is invalid or not provided
             if 'Change Z-Score' in combined_metrics_df.columns:
                 print(f"'{sort_by}' not valid or not provided. Defaulting sort to 'Abs Change Z-Score' {sort_order}")
                 # Calculate Abs Z-Score temporarily for sorting
                 combined_metrics_df['_abs_z_score_'] = combined_metrics_df['Change Z-Score'].fillna(0).abs()
                 effective_sort_by = '_abs_z_score_'
                 # Default Z-score sort is always descending unless explicitly requested otherwise for Z-score itself
                 if sort_by != 'Change Z-Score': 
                      sort_order = 'desc' 
                 is_default_sort = True
             else:
                  print("Warning: Cannot apply default sort, 'Change Z-Score' missing.")
                  effective_sort_by = id_col_name # Fallback sort
                  sort_order = 'asc'
        ascending_order = (sort_order == 'asc')
        try:
            # Use na_position='last' to handle NaNs consistently
            combined_metrics_df.sort_values(by=effective_sort_by, ascending=ascending_order, inplace=True, na_position='last', key=lambda col: col.astype(str).str.lower() if col.dtype == 'object' else col)
            print(f"Sorted by '{effective_sort_by}', {sort_order}.")
        except Exception as e:
             print(f"Error during sorting by {effective_sort_by}: {e}. Falling back to sorting by ID.")
             combined_metrics_df.sort_values(by=id_col_name, ascending=True, inplace=True, na_position='last')
             sort_by = id_col_name # Update sort_by to reflect fallback
             sort_order = 'asc'
        # Remove temporary sort column if added
        if is_default_sort and '_abs_z_score_' in combined_metrics_df.columns:
            combined_metrics_df.drop(columns=['_abs_z_score_'], inplace=True)
            # Set sort_by for template correctly if default was used
            sort_by = 'Change Z-Score' # Reflect the conceptual sort column
        # --- Pagination ---
        total_items = len(combined_metrics_df)
        # Ensure PER_PAGE is positive to avoid division by zero or negative pages
        safe_per_page = max(1, PER_PAGE)
        total_pages = math.ceil(total_items / safe_per_page)
        total_pages = max(1, total_pages) # Ensure at least 1 page, even if total_items is 0
        page = max(1, min(page, total_pages)) # Ensure page is within valid range [1, total_pages]
        start_index = (page - 1) * safe_per_page
        end_index = start_index + safe_per_page
        print(f"Pagination: Total items={total_items}, Total pages={total_pages}, Current page={page}, Per page={safe_per_page}")
        # Calculate page numbers to display in pagination controls (e.g., show 2 pages before and after current)
        page_window = 2 # Number of pages to show before/after current page
        start_page_display = max(1, page - page_window)
        end_page_display = min(total_pages, page + page_window)
        paginated_df = combined_metrics_df.iloc[start_index:end_index]
        # --- Prepare Data for Template ---
        securities_data_list = paginated_df.round(3).to_dict(orient='records')
        # Replace NaN with None for JSON compatibility / template rendering
        for row in securities_data_list:
            for key, value in row.items():
                if pd.isna(value):
                    row[key] = None
        # Define column order (ID first, then Static, then Metrics)
        # Ensure ISIN (id_col_name) is not in ordered_static_cols
        ordered_static_cols = sorted([col for col in static_cols if col in paginated_df.columns and col != id_col_name])
        metric_cols_ordered = ['Latest Value', 'Change', 'Change Z-Score', 'Mean', 'Max', 'Min']
        # Ensure only existing columns are included and ID col is first
        final_col_order = [id_col_name] + \
                          [col for col in ordered_static_cols if col in paginated_df.columns] + \
                          [col for col in metric_cols_ordered if col in paginated_df.columns]
        # Ensure all original columns are considered if they aren't static or metric
        # Make sure not to add id_col_name again
        other_cols = [col for col in paginated_df.columns if col not in final_col_order]
        final_col_order.extend(other_cols) # Add any remaining columns
        print(f"Final column order for display: {final_col_order}")
        # Create pagination context for the template
        pagination_context = {
            'page': page,
            'per_page': safe_per_page,
            'total_pages': total_pages,
            'total_items': total_items,
            'has_prev': page > 1,
            'has_next': page < total_pages,
            'prev_num': page - 1,
            'next_num': page + 1,
            'start_page_display': start_page_display, # Pass calculated start page
            'end_page_display': end_page_display,     # Pass calculated end page
            # Function to generate URLs for pagination links, preserving state
            'url_for_page': lambda p: url_for('security.securities_page', 
                                              page=p, 
                                              search_term=search_term, 
                                              sort_by=sort_by, 
                                              sort_order=sort_order, 
                                              **{f'filter_{k}': v for k, v in active_filters.items()}) 
        }
    except Exception as e:
        print(f"!!! Unexpected error during security page processing: {e}")
        traceback.print_exc()
        return render_template('securities_page.html', 
                               message=f"An unexpected error occurred: {e}", 
                               securities_data=[], 
                               pagination=None,
                               filter_options=final_filter_options if 'final_filter_options' in locals() else {},
                               active_filters=active_filters)
    # --- Render Template ---
    return render_template('securities_page.html',
                           securities_data=securities_data_list,
                           filter_options=final_filter_options,
                           column_order=final_col_order,
                           id_col_name=id_col_name,
                           search_term=search_term,
                           active_filters=active_filters, # Pass active filters for form state
                           pagination=pagination_context, # Pass pagination object
                           current_sort_by=sort_by,
                           current_sort_order=sort_order,
                           message=None) # Clear any previous error message if successful
# --- Helper Function to Replace NaN --- 
def replace_nan_with_none(obj):
    """Recursively replaces np.nan with None in a nested structure (dicts, lists)."""
    if isinstance(obj, dict):
        return {k: replace_nan_with_none(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [replace_nan_with_none(elem) for elem in obj]
    # Check specifically for pandas/numpy NaN values
    elif pd.isna(obj) and isinstance(obj, (float, np.floating)):
        return None
    else:
        return obj
@security_bp.route('/security/details/<metric_name>/<path:security_id>')
def security_details(metric_name, security_id):
    """
    Renders a detail page for a specific security, showing historical charts
    for the specified metric overlaid with Price, plus separate charts for
    Duration, Spread Duration, and Spread (each potentially overlaid with SP data).
    Handles security IDs that may contain special characters like slashes.
    Uses 'ISIN' as the primary identifier when fetching data.
    """
    # <<< ADDED: Decode the security_id to handle potential URL encoding (e.g., %2F for /)
    decoded_security_id = unquote(security_id)
    print(f"--- Requesting Security Details: Metric='{metric_name}', Decoded ID='{decoded_security_id}' ---")
    data_folder = current_app.config['DATA_FOLDER']
    all_dates = set()
    chart_data = {} # Dictionary to hold data for JSON output
    static_info = {} # To store static info for the security
    # --- Helper function to load, filter, and extract data ---
    def load_filter_and_extract(filename, security_id_to_filter, id_column_name='ISIN'): # <<< CHANGED default ID column
        """Loads a sec_*.csv file, filters by security ID, and returns the long-format series."""
        print(f"Loading file: {filename}")
        # Construct absolute path
        filepath = os.path.join(data_folder, filename)
        if not os.path.exists(filepath):
            print(f"Warning: File not found - {filename}")
            return None, set(), {} # Return None for data, empty set for dates, empty dict for static
        try:
            # Load data, specifying the data folder
            df_long, static_cols = load_and_process_security_data(filename, data_folder)
            if df_long is None or df_long.empty:
                print(f"Warning: No data loaded from {filename}")
                return None, set(), {}
            # Ensure the ID column is available for filtering
            # security_processing should have already set the index to 'ISIN' or made it a column
            if id_column_name not in df_long.index.names and id_column_name not in df_long.columns:
                 print(f"Error: ID column '{id_column_name}' not found in processed data from {filename}.")
                 # Attempt to find the index name if it wasn't 'ISIN'
                 fallback_id_col = df_long.index.name
                 if fallback_id_col and fallback_id_col in df_long.index.names:
                     print(f"Attempting filter using index name: '{fallback_id_col}'")
                     id_column_name = fallback_id_col # Use the actual index name
                 else:
                     return None, set(), {} # Cannot filter
            # Filter by the decoded security ID
            # Check if filtering on index or column
            if id_column_name in df_long.index.names:
                # If the ID is in the index (MultiIndex scenario: Date, ISIN)
                if isinstance(df_long.index, pd.MultiIndex):
                    # Need to select based on the level corresponding to the ID column
                    try:
                         level_index = df_long.index.names.index(id_column_name)
                         filtered_df = df_long[df_long.index.get_level_values(level_index) == security_id_to_filter]
                    except ValueError:
                         print(f"Error: Level '{id_column_name}' not found in MultiIndex.")
                         return None, set(), {}
                    except KeyError: # Handle case where the specific ID doesn't exist in the index level
                        print(f"Warning: Security ID '{security_id_to_filter}' not found in index level '{id_column_name}' of {filename}.")
                        filtered_df = pd.DataFrame() # Empty DataFrame
                else: # Single index (shouldn't happen if processed correctly, but handle defensively)
                     filtered_df = df_long.loc[[security_id_to_filter]]
            elif id_column_name in df_long.columns:
                # If the ID is a regular column
                 filtered_df = df_long[df_long[id_column_name] == security_id_to_filter]
            else:
                 # This case should have been caught earlier, but included for safety
                 print(f"Error: Cannot filter - ID column '{id_column_name}' not found.")
                 return None, set(), {}
            # Check if filtering yielded results
            if filtered_df.empty:
                 print(f"Warning: No data found for {id_column_name}='{security_id_to_filter}' in {filename}")
                 # Try alternative common ID column 'Security Name' if ISIN failed and it exists
                 alt_id_col = 'Security Name'
                 if id_column_name == 'ISIN' and alt_id_col in df_long.columns:
                    print(f"--> Retrying filter with '{alt_id_col}'...")
                    filtered_df = df_long[df_long[alt_id_col] == security_id_to_filter]
                    if filtered_df.empty:
                         print(f"Warning: No data found for {alt_id_col}='{security_id_to_filter}' either.")
                         return None, set(), {}
                    else:
                        print(f"Found data using '{alt_id_col}'.")
                        id_column_name = alt_id_col # Update the effective ID column used
                 else:
                     # Still empty after initial filter, and no/failed retry
                     return None, set(), {}
            # Extract the relevant data series (Date index, Value column)
            # The value column is typically the first column after resetting index, or 'Value'
            value_col_name = 'Value' # Default assumption from melt
            if value_col_name not in filtered_df.columns:
                # Find the first non-ID, non-static column if 'Value' isn't present
                 potential_value_cols = [col for col in filtered_df.columns if col not in static_cols and col != id_column_name]
                 if potential_value_cols:
                     value_col_name = potential_value_cols[0]
                     print(f"Using '{value_col_name}' as value column.")
                 else:
                     print(f"Error: Could not determine the value column in {filename}.")
                     return None, set(), {}
            # Ensure 'Date' is the index
            if 'Date' in filtered_df.columns:
                 filtered_df = filtered_df.set_index('Date')
            elif not isinstance(filtered_df.index, pd.DatetimeIndex):
                 # If Date is part of a MultiIndex, extract it
                 if 'Date' in filtered_df.index.names:
                     # Reset the index, set 'Date' as the main index
                     filtered_df = filtered_df.reset_index().set_index('Date')
                 else:
                     print(f"Error: Cannot find 'Date' index or column in {filename}.")
                     return None, set(), {}
            # Extract the series and dates
            data_series = filtered_df[value_col_name].sort_index()
            dates = set(data_series.index)
            # Extract static info from the first row (they should be constant per security)
            local_static_info = {}
            # Make sure we use the *effective* id_column_name used for filtering
            relevant_static_cols = [col for col in static_cols if col in filtered_df.columns and col != id_column_name]
            if not filtered_df.empty and relevant_static_cols:
                first_row = filtered_df.iloc[0]
                local_static_info = {col: first_row[col] for col in relevant_static_cols if pd.notna(first_row[col])}
                #print(f"Static info found in {filename}: {local_static_info}")
            return data_series, dates, local_static_info
        except KeyError as e:
             print(f"Warning: KeyError accessing data for {id_column_name}='{security_id_to_filter}' in {filename}. Likely missing ID. Error: {e}")
             return None, set(), {}
        except Exception as e:
            print(f"Error processing file {filename} for {id_column_name}='{security_id_to_filter}': {e}")
            import traceback
            traceback.print_exc() # Print full traceback for debugging
            return None, set(), {}
    # --- Load Data for Each Chart Section ---
    # 1. Primary Metric (passed in URL) + Price
    metric_filename = f"sec_{metric_name}.csv"
    price_filename = "sec_Price.csv"
    # Use the decoded ID for filtering
    metric_series, metric_dates, metric_static = load_filter_and_extract(metric_filename, decoded_security_id)
    price_series, price_dates, price_static = load_filter_and_extract(price_filename, decoded_security_id)
    all_dates.update(metric_dates)
    all_dates.update(price_dates)
    static_info.update(metric_static) # Prioritize static info from metric file
    static_info.update(price_static)  # Add/overwrite with price file info
    # 2. Duration + SP Duration
    duration_filename = "sec_Duration.csv"
    sp_duration_filename = "sec_DurationSP.csv" # Optional SP file
    duration_series, duration_dates, duration_static = load_filter_and_extract(duration_filename, decoded_security_id)
    sp_duration_series, sp_duration_dates, sp_duration_static = load_filter_and_extract(sp_duration_filename, decoded_security_id)
    all_dates.update(duration_dates)
    all_dates.update(sp_duration_dates)
    static_info.update(duration_static)
    static_info.update(sp_duration_static)
    # 3. Spread Duration + SP Spread Duration
    spread_dur_filename = "sec_Spread duration.csv"
    sp_spread_dur_filename = "sec_Spread durationSP.csv" # Optional SP file
    spread_dur_series, spread_dur_dates, spread_dur_static = load_filter_and_extract(spread_dur_filename, decoded_security_id)
    sp_spread_dur_series, sp_spread_dur_dates, sp_spread_dur_static = load_filter_and_extract(sp_spread_dur_filename, decoded_security_id)
    all_dates.update(spread_dur_dates)
    all_dates.update(sp_spread_dur_dates)
    static_info.update(spread_dur_static)
    static_info.update(sp_spread_dur_static)
    # 4. Spread + SP Spread
    spread_filename = "sec_Spread.csv" # May reload if metric_name wasn't Spread
    sp_spread_filename = "sec_SpreadSP.csv" # Optional SP file
    # Only load spread again if the primary metric wasn't spread
    if metric_name.lower() != 'spread':
        spread_series, spread_dates, spread_static = load_filter_and_extract(spread_filename, decoded_security_id)
        all_dates.update(spread_dates)
        static_info.update(spread_static)
    else:
        spread_series = metric_series # Reuse already loaded data
        spread_dates = metric_dates
        # Static info already handled
    sp_spread_series, sp_spread_dates, sp_spread_static = load_filter_and_extract(sp_spread_filename, decoded_security_id)
    all_dates.update(sp_spread_dates)
    static_info.update(sp_spread_static)
    # --- Prepare Data for Chart.js ---
    if not all_dates:
        print("No dates found across any datasets. Cannot generate chart labels.")
        # Render template with error message or indication of no data
        return render_template('security_details_page.html',
                               security_id=decoded_security_id,
                               metric_name=metric_name,
                               chart_data_json='{}', # Empty JSON
                               latest_date="N/A",
                               static_info=static_info, # Show static info if any was found
                               message="No historical data found for this security.")
    # Sort dates and format as strings for labels
    sorted_dates = sorted(list(all_dates))
    # Use .strftime for consistent formatting
    chart_data['labels'] = [d.strftime('%Y-%m-%d') for d in sorted_dates]
    latest_date_str = chart_data['labels'][-1] if chart_data['labels'] else "N/A"
    # Helper to prepare dataset structure for Chart.js
    def prepare_dataset(df_series, label, color, y_axis_id='y'):
        if df_series is None or df_series.empty:
            print(f"Cannot prepare dataset for '{label}': DataFrame is None or empty.")
             # Return structure with null data matching the length of labels
            return {
                'label': label,
                'data': [None] * len(chart_data['labels']), # Use None for missing points
                'borderColor': color,
                'backgroundColor': color + '80', # Optional: add transparency
                'fill': False,
                'tension': 0.1,
                'pointRadius': 2,
                'pointHoverRadius': 5,
                'yAxisID': y_axis_id,
                'spanGaps': True # Let Chart.js connect lines over nulls
            }
        # Ensure value column is numeric, coercing errors
        df_series = pd.to_numeric(df_series, errors='coerce')
        # Merge with the full date range, using pd.NA for missing numeric values
        merged_df = pd.merge(pd.DataFrame({'Date': sorted_dates}), df_series.reset_index(), on='Date', how='left')
        # <<< ADDED: Identify the value column name after the merge
        # It will be the column that is NOT 'Date'
        value_col_name_in_merged = [col for col in merged_df.columns if col != 'Date'][0]
        # Replace pandas NA/NaN with None for JSON compatibility
        # data_values = merged_df[value_col_name].replace({pd.NA: None, np.nan: None}).tolist()
        # Replace only NaN with None, keep numeric types where possible
        # Ensure the column is float first to handle potential integers mixed with NaN
        # <<< CHANGED: Use the identified column name
        data_values = merged_df[value_col_name_in_merged].astype(float).replace({np.nan: None}).tolist()
        return {
            'label': label,
            'data': data_values,
            'borderColor': color,
            'backgroundColor': color + '80',
            'fill': False,
            'tension': 0.1,
            'pointRadius': 2,
            'pointHoverRadius': 5,
            'yAxisID': y_axis_id,
            'spanGaps': True
        }
    # Primary Chart Datasets (Metric + Price)
    chart_data['primary_datasets'] = [] # <<< Initialize the list here
    print(f"DEBUG: COLOR_PALETTE contents just before use: {COLOR_PALETTE}") # <<< ADDED DEBUG PRINT
    if metric_series is not None:
        chart_data['primary_datasets'].append(
            prepare_dataset(metric_series, metric_name, COLOR_PALETTE[0], 'y')
        )
    else: # Add placeholder if metric data failed to load
        chart_data['primary_datasets'].append(
            prepare_dataset(None, metric_name, COLOR_PALETTE[0], 'y')
        )
    if price_series is not None:
        chart_data['primary_datasets'].append(
            prepare_dataset(price_series, 'Price', COLOR_PALETTE[1], 'y1') # Use secondary axis
        )
    else: # Add placeholder if price data failed to load
         chart_data['primary_datasets'].append(
            prepare_dataset(None, 'Price', COLOR_PALETTE[1], 'y1')
        )
    # Duration Chart Datasets
    chart_data['duration_dataset'] = prepare_dataset(duration_series, 'Duration', COLOR_PALETTE[2])
    chart_data['sp_duration_dataset'] = prepare_dataset(sp_duration_series, 'SP Duration', COLOR_PALETTE[3])
    # Spread Duration Chart Datasets
    chart_data['spread_duration_dataset'] = prepare_dataset(spread_dur_series, 'Spread Duration', COLOR_PALETTE[4])
    chart_data['sp_spread_duration_dataset'] = prepare_dataset(sp_spread_dur_series, 'SP Spread Duration', COLOR_PALETTE[5])
    # Spread Chart Datasets
    chart_data['spread_dataset'] = prepare_dataset(spread_series, 'Spread', COLOR_PALETTE[6])
    chart_data['sp_spread_dataset'] = prepare_dataset(sp_spread_series, 'SP Spread', COLOR_PALETTE[7])
    # Convert the entire chart_data dictionary to JSON safely
    chart_data_json = json.dumps(chart_data, default=replace_nan_with_none, indent=4) # Use helper for NaN->null
    print(f"Rendering security details page for {decoded_security_id}")
    # Pass the decoded ID to the template
    return render_template('security_details_page.html',
                           security_id=decoded_security_id, 
                           metric_name=metric_name,
                           chart_data_json=chart_data_json,
                           latest_date=latest_date_str,
                           static_info=static_info)
@security_bp.route('/static/<path:filename>')
def static_files(filename):
    return send_from_directory(os.path.join(security_bp.root_path, '..', 'static'), filename)
</file>

<file path="views/spread_duration_comparison_views.py">
# views/spread_duration_comparison_views.py
# This module defines the Flask Blueprint for comparing two security spread duration datasets.
# It includes routes for a summary view listing securities with comparison metrics
# and a detail view showing overlayed time-series charts and statistics for a single security.
from flask import Blueprint, render_template, request, current_app, jsonify, url_for
import pandas as pd
import os
import logging
import math # Add math for pagination calculation
from urllib.parse import unquote # Import unquote
from pathlib import Path
import re
from datetime import datetime
# Assuming security_processing and utils are in the parent directory or configured in PYTHONPATH
try:
    from security_processing import load_and_process_security_data # May need adjustments
    from utils import parse_fund_list # Example utility
    from config import COLOR_PALETTE # Still need colors
except ImportError:
    # Handle potential import errors if the structure is different
    logging.error("Could not import required modules from parent directory.")
    # Add fallback imports or path adjustments if necessary
    # Example: sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
    from ..security_processing import load_and_process_security_data
    from ..utils import parse_fund_list
    from ..config import COLOR_PALETTE
# Import utils module properly
try: 
    from utils import _is_date_like # Try direct import
except ImportError:
    # Define our own function if import fails
    def _is_date_like(column_name):
        """Check if a column name looks like a date.
        Returns True for formats like: YYYY-MM-DD, DD/MM/YYYY, DD-MM-YYYY, etc."""
        if not isinstance(column_name, str):
            return False
        # Match common date formats in column names
        date_patterns = [
            r'\d{2}/\d{2}/\d{4}',  # DD/MM/YYYY
            r'\d{2}-\d{2}-\d{4}',  # DD-MM-YYYY
            r'\d{4}/\d{2}/\d{2}',  # YYYY/MM/DD
            r'\d{4}-\d{2}-\d{2}',  # YYYY-MM-DD
            r'\d{1,2}/\d{1,2}/\d{2,4}',  # D/M/YY or D/M/YYYY
            r'\d{1,2}-\d{1,2}-\d{2,4}',  # D-M-YY or D-M-YYYY
        ]
        # Return True if any pattern matches
        return any(re.search(pattern, column_name) for pattern in date_patterns)
spread_duration_comparison_bp = Blueprint('spread_duration_comparison_bp', __name__,
                        template_folder='../templates',
                        static_folder='../static')
# Configure logging
# log = logging.getLogger(__name__)
PER_PAGE_COMPARISON = 50 # Items per page for comparison summary
# --- Data Loading and Processing ---
def load_weights_and_held_status(data_folder: str, weights_filename: str = 'w_secs.csv', id_col_override: str = 'ISIN') -> pd.Series:
    """
    Loads the weights file (e.g., w_secs.csv), identifies the latest date,
    and returns a boolean Series indicating which securities (indexed by ISIN)
    have a non-zero weight on that date (i.e., are currently held).
    Args:
        data_folder: The absolute path to the data directory.
        weights_filename: The name of the weights file.
        id_col_override: The specific column name in the weights file expected to contain the ISINs for joining.
    Returns:
        A pandas Series where the index is the Security ID (ISIN) and the value
        is True if the security is held on the latest date, False otherwise.
        Returns an empty Series if the file cannot be loaded or processed.
    """
    current_app.logger.info(f"--- Entering load_weights_and_held_status for {weights_filename} ---")
    weights_filepath = Path(data_folder) / weights_filename
    if not weights_filepath.exists():
        current_app.logger.warning(f"Weights file not found: {weights_filepath}")
        return pd.Series(dtype=bool)
    try:
        current_app.logger.info(f"Loading weights data from: {weights_filepath}")
        # Load without setting index initially to easily find date/ID columns
        weights_df = pd.read_csv(weights_filepath, low_memory=False)
        weights_df.columns = weights_df.columns.str.strip() # Clean column names
        # --- Identify Date and ID columns ---
        date_col = next((col for col in weights_df.columns if 'date' in col.lower()), None)
        # Prioritize the explicitly provided id_col_override, then look for ISIN/SecurityID
        id_col_in_file = id_col_override if id_col_override in weights_df.columns else \
                         next((col for col in weights_df.columns if col.lower() in ['isin', 'securityid']), None)
        # Check for ID column - required
        if not id_col_in_file:
            current_app.logger.error(f"Could not automatically identify ID column in {weights_filepath}. Columns found: {weights_df.columns.tolist()}")
            return pd.Series(dtype=bool)
        current_app.logger.info(f"Weights file ID column identified: '{id_col_in_file}'")
        # --- Identify and Melt Date Columns ---
        date_columns = [col for col in weights_df.columns if _is_date_like(col)]
        if not date_columns:
            # If no date-like columns were found and no explicit date column exists
            if not date_col:
                current_app.logger.error(f"No date column or date-like columns found in {weights_filepath}")
                return pd.Series(dtype=bool)
            # Try to use the explicitly found date column
            current_app.logger.info(f"No date-like columns found. Attempting to use explicit date column: '{date_col}'")
            try:
                weights_df[date_col] = pd.to_datetime(weights_df[date_col], errors='coerce')
                if weights_df[date_col].isnull().all():
                    raise ValueError("Date column parsing failed.")
                # If successful, proceed as if it was a long-format file from the start
                value_col = next((col for col in weights_df.columns if col.lower() == 'value'), 'Value') # Assume a 'Value' column exists
                if value_col not in weights_df.columns:
                    # If no obvious value column, assume last column is value
                    value_col = weights_df.columns[-1]
                    current_app.logger.warning(f"No 'Value' column found in long-format weights file, assuming last column '{value_col}' holds weights.")
                # Rename for consistency
                weights_df = weights_df.rename(columns={date_col: 'Date', id_col_in_file: 'ISIN', value_col: 'Value'})
                weights_df['Value'] = pd.to_numeric(weights_df['Value'], errors='coerce')
            except Exception as e:
                current_app.logger.error(f"Failed to process weights file {weights_filepath} as long format after date column detection failed: {e}")
                return pd.Series(dtype=bool)
        else:
            current_app.logger.info(f"Found {len(date_columns)} date-like columns in {weights_filename}: {date_columns[:5]}{'...' if len(date_columns) > 5 else ''}")
            # Wide format: Melt the DataFrame
            id_vars = [col for col in weights_df.columns if col not in date_columns]
            melted_weights = weights_df.melt(id_vars=id_vars, value_vars=date_columns, var_name='Date', value_name='Value')
            # Attempt to convert 'Date' column to datetime objects after melting
            melted_weights['Date'] = pd.to_datetime(melted_weights['Date'], errors='coerce')
            melted_weights['Value'] = pd.to_numeric(melted_weights['Value'], errors='coerce')
            # Rename the identified ID column to 'ISIN' for consistency
            melted_weights = melted_weights.rename(columns={id_col_in_file: 'ISIN'})
            weights_df = melted_weights # Use the melted df going forward
        # Find the latest date in the entire dataset
        latest_date = weights_df['Date'].max()
        if pd.isna(latest_date):
            current_app.logger.warning(f"Could not determine the latest date in {weights_filepath}.")
            return pd.Series(dtype=bool)
        current_app.logger.info(f"Latest date in weights file '{weights_filepath}': {latest_date}")
        # Filter for the latest date and where Value is not NaN and > 0
        latest_weights = weights_df[(weights_df['Date'] == latest_date) & (weights_df['Value'].notna()) & (weights_df['Value'] > 0)].copy()
        # --- Determine Held Status using the correct ID column ---
        # We now use the *renamed* 'ISIN' column, which originally came from id_col_in_file (our target)
        held_status_col = 'ISIN'
        if held_status_col not in latest_weights.columns:
             current_app.logger.error(f"The target ID column '{held_status_col}' (derived from '{id_col_in_file}') not found after processing {weights_filepath}. Columns: {latest_weights.columns.tolist()}")
             return pd.Series(dtype=bool)
        current_app.logger.info(f"Using '{held_status_col}' column from {weights_filename} for held_status index.")
        # Create the boolean Series: index is the ISIN, value is True
        # Drop duplicates in case a security appears multiple times on the same date (e.g., different funds)
        held_ids = latest_weights.drop_duplicates(subset=[held_status_col])[held_status_col]
        held_status = pd.Series(True, index=held_ids)
        held_status.index.name = 'ISIN' # Ensure the index name is 'ISIN' for merging
        # --- Logging for Debugging ---
        current_app.logger.debug(f"Weights data before setting index (first 5 rows using '{held_status_col}'):\n{latest_weights[[held_status_col, 'Value']].head().to_string()}")
        sample_values = latest_weights[held_status_col].unique()[:5]
        current_app.logger.debug(f"Sample values from '{held_status_col}' column to be used for index: {sample_values}")
        # This renaming logic is now less critical as we set the index correctly, but keep for consistency check
        if held_status.index.name != id_col_override:
             current_app.logger.warning(f"Renaming held_status index from '{held_status.index.name}' to '{id_col_override}' to ensure merge compatibility.")
             held_status.index.name = id_col_override # Explicitly set to ISIN for clarity
        current_app.logger.debug(f"Resulting held_status Series index name after potential rename: '{held_status.index.name}'")
        current_app.logger.debug(f"Held status index preview (first 5 values): {held_status.index[:5].tolist()}")
        current_app.logger.debug(f"Held status values preview (first 5): {held_status.head().to_dict()}")
        current_app.logger.info(f"Determined held status for {len(held_status)} IDs based on weights on {latest_date}.")
        return held_status
    except Exception as e:
        current_app.logger.error(f"Error loading or processing weights file {weights_filepath}: {e}", exc_info=True)
        return pd.Series(dtype=bool)
def load_spread_duration_comparison_data(data_folder_path: str, file1='sec_Spread duration.csv', file2='sec_Spread durationSP.csv'):
    """Loads, processes, merges data from two security spread duration files, and gets held status.
    Args:
        data_folder_path (str): The absolute path to the data folder.
        file1 (str, optional): Filename for the first dataset. Defaults to 'sec_Spread duration.csv'.
        file2 (str, optional): Filename for the second dataset. Defaults to 'sec_Spread durationSP.csv'.
    Returns:
        tuple: (merged_df, static_data, common_static_cols, id_col_name, held_status)
               Returns (pd.DataFrame(), pd.DataFrame(), [], None, pd.Series(dtype=bool)) on error.
    """
    log = current_app.logger # Use app logger
    log.info(f"Loading spread duration comparison data: {file1} and {file2} from {data_folder_path}")
    if not data_folder_path:
        log.error("No data_folder_path provided to load_spread_duration_comparison_data.")
        return pd.DataFrame(), pd.DataFrame(), [], None, pd.Series(dtype=bool)
    # Load held status first
    held_status = load_weights_and_held_status(data_folder_path)
    # Load main data files
    df1, static_cols1 = load_and_process_security_data(file1, data_folder_path)
    df2, static_cols2 = load_and_process_security_data(file2, data_folder_path)
    if df1.empty or df2.empty:
        log.warning(f"One or both spread duration dataframes are empty after loading. File1 empty: {df1.empty}, File2 empty: {df2.empty}")
        return pd.DataFrame(), pd.DataFrame(), [], None, held_status
    # --- Verify Index and Get Actual Names ---
    if df1.index.nlevels != 2 or df2.index.nlevels != 2:
        log.error("One or both spread duration dataframes do not have the expected 2 index levels after loading.")
        return pd.DataFrame(), pd.DataFrame(), [], None, held_status
    date_level_name, id_level_name = df1.index.names
    log.info(f"Spread duration data index levels identified: Date='{date_level_name}', ID='{id_level_name}'")
    # --- Reset Index ---
    df1 = df1.reset_index()
    df2 = df2.reset_index()
    log.debug(f"Spread duration df1 columns after reset: {df1.columns.tolist()}")
    log.debug(f"Spread duration df2 columns after reset: {df2.columns.tolist()}")
    # --- Check Required Columns (Post-Reset) ---
    required_cols_df1 = [id_level_name, date_level_name, 'Value']
    required_cols_df2 = [id_level_name, date_level_name, 'Value']
    missing_cols_df1 = [col for col in required_cols_df1 if col not in df1.columns]
    missing_cols_df2 = [col for col in required_cols_df2 if col not in df2.columns]
    if missing_cols_df1 or missing_cols_df2:
        log.error(f"Missing required columns after index reset in spread duration. Df1 missing: {missing_cols_df1}, Df2 missing: {missing_cols_df2}")
        return pd.DataFrame(), pd.DataFrame(), [], None, held_status
    # --- Prepare for Merge ---
    common_static_cols = list(set(static_cols1) & set(static_cols2))
    if id_level_name in common_static_cols:
        common_static_cols.remove(id_level_name)
    if 'Value' in common_static_cols:
        common_static_cols.remove('Value')
    try:
        df1_merge = df1[[id_level_name, date_level_name, 'Value'] + common_static_cols].rename(columns={'Value': 'Value_Orig'})
        df2_merge = df2[[id_level_name, date_level_name, 'Value']].rename(columns={'Value': 'Value_New'})
    except KeyError as e:
        log.error(f"KeyError during spread duration merge prep using names '{id_level_name}', '{date_level_name}': {e}")
        return pd.DataFrame(), pd.DataFrame(), [], None, held_status
    # --- Perform Merge and Calculate Changes ---
    merged_df = pd.merge(df1_merge, df2_merge, on=[id_level_name, date_level_name], how='outer')
    merged_df = merged_df.sort_values(by=[id_level_name, date_level_name])
    merged_df['Change_Orig'] = merged_df.groupby(id_level_name)['Value_Orig'].diff()
    merged_df['Change_New'] = merged_df.groupby(id_level_name)['Value_New'].diff()
    # --- Extract Static Data ---
    static_data = merged_df.groupby(id_level_name)[common_static_cols].last().reset_index()
    log.info(f"Successfully merged spread duration data. Shape: {merged_df.shape}")
    return merged_df, static_data, common_static_cols, id_level_name, held_status
def calculate_comparison_stats(merged_df, static_data, id_col):
    """Calculates comparison statistics for each security's spread duration.
    Args:
        merged_df (pd.DataFrame): The merged dataframe of original and new spread duration values.
        static_data (pd.DataFrame): DataFrame with static info per security.
        id_col (str): The name of the column containing the Security ID/Name.
    """
    log = current_app.logger # Use app logger
    if merged_df.empty:
        return pd.DataFrame()
    if id_col not in merged_df.columns:
        log.error(f"Specified id_col '{id_col}' not found in merged_df columns: {merged_df.columns.tolist()}")
        return pd.DataFrame() # Cannot group without the ID column
    log.info(f"Calculating spread duration comparison statistics using ID column: {id_col}...")
    stats_list = []
    # Use the passed id_col here
    for sec_id, group in merged_df.groupby(id_col):
        sec_stats = {id_col: sec_id} # Use actual id_col name
        # Filter out rows where both values are NaN for overall analysis period
        group_valid_overall = group.dropna(subset=['Value_Orig', 'Value_New'], how='all')
        overall_min_date = group_valid_overall['Date'].min()
        overall_max_date = group_valid_overall['Date'].max()
        # Filter out rows where EITHER value is NaN for correlation/diff calculations
        valid_comparison = group.dropna(subset=['Value_Orig', 'Value_New'])
        # 1. Correlation of Levels
        if len(valid_comparison) >= 2: # Need at least 2 points for correlation
            # Use the NaN-dropped dataframe for correlation
            level_corr = valid_comparison['Value_Orig'].corr(valid_comparison['Value_New'])
            sec_stats['Level_Correlation'] = level_corr if pd.notna(level_corr) else None
        else:
             sec_stats['Level_Correlation'] = None
        # 2. Max / Min (use original group to get true max/min including non-overlapping points)
        sec_stats['Max_Orig'] = group['Value_Orig'].max()
        sec_stats['Min_Orig'] = group['Value_Orig'].min()
        sec_stats['Max_New'] = group['Value_New'].max()
        sec_stats['Min_New'] = group['Value_New'].min()
        # 3. Date Range Comparison - Refined Logic
        # Find min/max dates within the MERGED data where each series is individually valid
        min_date_orig_idx = group['Value_Orig'].first_valid_index()
        max_date_orig_idx = group['Value_Orig'].last_valid_index()
        min_date_new_idx = group['Value_New'].first_valid_index()
        max_date_new_idx = group['Value_New'].last_valid_index()
        sec_stats['Start_Date_Orig'] = group.loc[min_date_orig_idx, 'Date'] if min_date_orig_idx is not None else None
        sec_stats['End_Date_Orig'] = group.loc[max_date_orig_idx, 'Date'] if max_date_orig_idx is not None else None
        sec_stats['Start_Date_New'] = group.loc[min_date_new_idx, 'Date'] if min_date_new_idx is not None else None
        sec_stats['End_Date_New'] = group.loc[max_date_new_idx, 'Date'] if max_date_new_idx is not None else None
        # Check if the start and end dates MATCH for the valid periods of EACH series
        same_start = pd.Timestamp(sec_stats['Start_Date_Orig']) == pd.Timestamp(sec_stats['Start_Date_New']) if sec_stats['Start_Date_Orig'] and sec_stats['Start_Date_New'] else False
        same_end = pd.Timestamp(sec_stats['End_Date_Orig']) == pd.Timestamp(sec_stats['End_Date_New']) if sec_stats['End_Date_Orig'] and sec_stats['End_Date_New'] else False
        sec_stats['Same_Date_Range'] = same_start and same_end
        # Add overall date range for info
        sec_stats['Overall_Start_Date'] = overall_min_date
        sec_stats['Overall_End_Date'] = overall_max_date
        # 4. Correlation of Daily Changes (Volatility Alignment)
        # Use the dataframe where BOTH values are non-NaN to calculate changes for correlation
        valid_comparison = valid_comparison.copy() # Avoid SettingWithCopyWarning
        valid_comparison['Change_Orig_Corr'] = valid_comparison['Value_Orig'].diff()
        valid_comparison['Change_New_Corr'] = valid_comparison['Value_New'].diff()
        # Drop NaNs created by the diff() itself (first row)
        valid_changes = valid_comparison.dropna(subset=['Change_Orig_Corr', 'Change_New_Corr'])
        if len(valid_changes) >= 2:
            change_corr = valid_changes['Change_Orig_Corr'].corr(valid_changes['Change_New_Corr'])
            sec_stats['Change_Correlation'] = change_corr if pd.notna(change_corr) else None
        else:
            sec_stats['Change_Correlation'] = None
            log.debug(f"Cannot calculate Spread Duration Change_Correlation for {sec_id}. Need >= 2 valid change pairs, found {len(valid_changes)}.")
        # 5. Difference Statistics (use the valid_comparison df where both values exist)
        valid_comparison['Abs_Diff'] = (valid_comparison['Value_Orig'] - valid_comparison['Value_New']).abs()
        sec_stats['Mean_Abs_Diff'] = valid_comparison['Abs_Diff'].mean() # Mean diff where both values exist
        sec_stats['Max_Abs_Diff'] = valid_comparison['Abs_Diff'].max() # Max diff where both values exist
        # Count NaNs - use original group
        sec_stats['NaN_Count_Orig'] = group['Value_Orig'].isna().sum()
        sec_stats['NaN_Count_New'] = group['Value_New'].isna().sum()
        sec_stats['Total_Points'] = len(group)
        stats_list.append(sec_stats)
    summary_df = pd.DataFrame(stats_list)
    # Merge static data back
    if not static_data.empty and id_col in static_data.columns and id_col in summary_df.columns:
        summary_df = pd.merge(summary_df, static_data, on=id_col, how='left')
    elif not static_data.empty:
         log.warning(f"Could not merge static data back for spread duration comparison. ID column '{id_col}' missing from static_data ({id_col in static_data.columns}) or summary_df ({id_col in summary_df.columns}).")
    log.info(f"Finished calculating spread duration stats. Summary shape: {summary_df.shape}")
    return summary_df
# --- Routes ---
@spread_duration_comparison_bp.route('/spread_duration_comparison/summary') # Updated route
def summary():
    """Displays the spread duration comparison summary page with server-side filtering, sorting, and pagination."""
    current_app.logger.info("--- Starting Spread Duration Comparison Summary Request ---")
    # Retrieve the configured absolute data folder path
    data_folder = current_app.config['DATA_FOLDER']
    if not data_folder:
        current_app.logger.error("DATA_FOLDER is not configured in the application.")
        return "Internal Server Error: Data folder not configured", 500
    try:
        # --- Get Request Parameters ---
        page = request.args.get('page', 1, type=int)
        sort_by = request.args.get('sort_by', 'Change_Correlation') # Default sort
        sort_order = request.args.get('sort_order', 'desc').lower()
        if sort_order not in ['asc', 'desc']:
            sort_order = 'desc'
        ascending = sort_order == 'asc'
        # NEW: Get holding status filter
        show_sold = request.args.get('show_sold', 'false').lower() == 'true'
        # Get active filters (ensuring keys are correct)
        active_filters = {k.replace('filter_', ''): v
                          for k, v in request.args.items()
                          if k.startswith('filter_') and v}
        current_app.logger.info(f"Request Params: Page={page}, SortBy={sort_by}, Order={sort_order}, Filters={active_filters}, ShowSold={show_sold}")
        # --- Load and Prepare Data ---
        merged_data, static_data, static_cols, actual_id_col, held_status = load_spread_duration_comparison_data(data_folder)
        if actual_id_col is None:
            current_app.logger.error("Failed to get ID column name during spread duration data loading.")
            return "Error loading spread duration comparison data: Could not determine ID column.", 500
        else:
            current_app.logger.info(f"Actual ID column identified for spread duration comparison data: '{actual_id_col}'")
        summary_stats = calculate_comparison_stats(merged_data, static_data, id_col=actual_id_col)
        if summary_stats.empty:
             current_app.logger.info("No spread duration summary statistics could be calculated.")
             return render_template('spread_duration_comparison_page.html', # Updated template
                                    table_data=[],
                                    columns_to_display=[],
                                    id_column_name=actual_id_col,
                                    filter_options={},
                                    active_filters={},
                                    current_sort_by=sort_by,
                                    current_sort_order=sort_order,
                                    pagination=None,
                                    show_sold=show_sold, # Pass filter status
                                    message="No spread duration comparison data available.")
        # --- Merge Held Status ---
        if not held_status.empty and actual_id_col in summary_stats.columns:
            # --- Convert merge keys to string BEFORE merging --- 
            try:
                original_dtype_stats = summary_stats[actual_id_col].dtype
                original_dtype_held = held_status.index.dtype
                summary_stats[actual_id_col] = summary_stats[actual_id_col].astype(str)
                held_status.index = held_status.index.astype(str)
                current_app.logger.info(f"Converted merge keys to string for spread duration. Original dtypes: summary_stats['{actual_id_col}']: {original_dtype_stats}, held_status.index: {original_dtype_held}")
            except Exception as e:
                current_app.logger.error(f"Error converting spread duration merge keys to string: {e}. Merge might fail.")
            # --- Add PRE-MERGE logging ---
            current_app.logger.info(f"PRE-MERGE CHECK: summary_stats ID column ('{actual_id_col}') dtype: {summary_stats[actual_id_col].dtype}, held_status index ('{held_status.index.name}') dtype: {held_status.index.dtype}")
            current_app.logger.info(f"PRE-MERGE CHECK: summary_stats ID column name: '{actual_id_col}', held_status index name: '{held_status.index.name}'")
            if actual_id_col != held_status.index.name:
                 current_app.logger.error(f"CRITICAL: Mismatch between spread duration summary_stats merge column ('{actual_id_col}') and held_status index name ('{held_status.index.name}'). Merge will likely fail or produce incorrect results.")
            # --- End PRE-MERGE logging ---
            current_app.logger.info(f"Attempting to merge held_status (index name: '{held_status.index.name}') with spread duration summary_stats on column '{actual_id_col}'")
            debug_cols_before = summary_stats.columns.tolist()
            current_app.logger.debug(f"summary_stats columns before merge: {debug_cols_before}")
            current_app.logger.debug(f"held_status index preview before merge: {held_status.index[:5].tolist()}")
            current_app.logger.debug(f"summary_stats ID column ('{actual_id_col}') preview before merge: {summary_stats[actual_id_col].unique()[:5].tolist()}")
            # Rename the Series when merging
            summary_stats = pd.merge(summary_stats, held_status.rename('is_held'), 
                                    left_on=actual_id_col, right_index=True, how='left')
            debug_cols_after = summary_stats.columns.tolist()
            current_app.logger.debug(f"Columns after merge attempt: {debug_cols_after}")
            held_count = summary_stats['is_held'].notna().sum()
            current_app.logger.info(f"Merged held status. Spread duration stats shape: {summary_stats.shape}. 'is_held' column has {held_count} non-NA values.")
            if held_count > 0:
                current_app.logger.debug(f"Preview of 'is_held' after merge (first 5 non-NA): {summary_stats[summary_stats['is_held'].notna()]['is_held'].head().to_dict()}")
            else:
                current_app.logger.debug("Preview of 'is_held' after merge (first 5 non-NA): {}")
            # Fill NaN in 'is_held'
            if 'is_held' in summary_stats.columns:
                summary_stats['is_held'] = summary_stats['is_held'].fillna(False)
                current_app.logger.info("Filled NA in 'is_held' with False for spread duration summary.")
            else:
                current_app.logger.error("'is_held' column not found after spread duration merge!")
                summary_stats['is_held'] = False # Add the column as False if merge failed entirely
        else:
            current_app.logger.warning(f"Could not merge held status for spread duration data. held_status empty: {held_status.empty}, '{actual_id_col}' in summary_stats: {actual_id_col in summary_stats.columns}")
            summary_stats['is_held'] = False
        # --- Apply Holding Status Filter ---
        original_count = len(summary_stats)
        if not show_sold:
            summary_stats = summary_stats[summary_stats['is_held'] == True]
            current_app.logger.info(f"Applied 'Show Held Only' filter. Kept {len(summary_stats)} out of {original_count} securities.")
        else:
            current_app.logger.info("Skipping 'Show Held Only' filter (show_sold is True).")
        if summary_stats.empty:
             current_app.logger.info("No securities remaining after applying holding status filter.")
             return render_template('spread_duration_comparison_page.html', # Updated template
                                    table_data=[],
                                    columns_to_display=[actual_id_col] + static_cols, # Show basic cols
                                    id_column_name=actual_id_col,
                                    filter_options={},
                                    active_filters={},
                                    current_sort_by=sort_by,
                                    current_sort_order=sort_order,
                                    pagination=None,
                                    show_sold=show_sold, # Pass filter status
                                    message="No currently held securities found.")
        # --- Collect Filter Options (From Data *After* Holding Filter) --- 
        filter_options = {}
        potential_filter_cols = static_cols 
        for col in potential_filter_cols:
            if col in summary_stats.columns:
                unique_vals = summary_stats[col].dropna().unique().tolist()
                try:
                    sorted_vals = sorted(unique_vals, key=lambda x: (isinstance(x, (int, float)), x))
                except TypeError:
                    sorted_vals = sorted(unique_vals, key=str)
                filter_options[col] = sorted_vals
        final_filter_options = dict(sorted(filter_options.items())) # Sort filter dropdowns alphabetically
        current_app.logger.info(f"Filter options generated: {list(final_filter_options.keys())}") # Use final_filter_options
        # --- Apply Static Column Filters --- 
        filtered_data = summary_stats.copy()
        if active_filters:
            current_app.logger.info(f"Applying static column filters: {active_filters}")
            for col, value in active_filters.items():
                if col in filtered_data.columns and value:
                    try:
                        # Robust string comparison
                         filtered_data = filtered_data[filtered_data[col].astype(str).str.lower() == str(value).lower()]
                    except Exception as e:
                        current_app.logger.warning(f"Could not apply filter for column '{col}' with value '{value}'. Error: {e}. Skipping filter.")
                else:
                    current_app.logger.warning(f"Filter column '{col}' not found in data. Skipping filter.")
            current_app.logger.info(f"Data shape after static filtering: {filtered_data.shape}")
        else:
            current_app.logger.info("No active static column filters.")
        if filtered_data.empty:
             current_app.logger.info("No data remaining after applying static column filters.")
             return render_template('spread_duration_comparison_page.html', # Updated template
                                    table_data=[],
                                    columns_to_display=[actual_id_col] + static_cols, # Show basic cols
                                    id_column_name=actual_id_col,
                                    filter_options=final_filter_options, # Show filter options
                                    active_filters=active_filters,
                                    current_sort_by=sort_by,
                                    current_sort_order=sort_order,
                                    pagination=None,
                                    show_sold=show_sold, # Pass filter status
                                    message="No data matches the current filters.")
        # --- Apply Sorting ---
        if sort_by in filtered_data.columns:
            current_app.logger.info(f"Sorting by '{sort_by}' ({'Ascending' if ascending else 'Descending'})")
            na_position = 'last' 
            try:
                filtered_data = filtered_data.sort_values(by=sort_by, ascending=ascending, na_position=na_position)
            except Exception as e:
                current_app.logger.error(f"Error during sorting by '{sort_by}': {e}. Falling back to default sort.")
                sort_by = 'Change_Correlation' 
                ascending = False
                filtered_data = filtered_data.sort_values(by=sort_by, ascending=ascending, na_position=na_position)
        else:
            current_app.logger.warning(f"Sort column '{sort_by}' not found. Using default ID sort.")
            sort_by = actual_id_col 
            ascending = True
            filtered_data = filtered_data.sort_values(by=actual_id_col, ascending=ascending, na_position='last')
        # --- Pagination ---
        total_items = len(filtered_data)
        safe_per_page = max(1, PER_PAGE_COMPARISON)
        total_pages = math.ceil(total_items / safe_per_page)
        total_pages = max(1, total_pages)
        page = max(1, min(page, total_pages))
        start_index = (page - 1) * safe_per_page
        end_index = start_index + safe_per_page
        paginated_data = filtered_data.iloc[start_index:end_index]
        current_app.logger.info(f"Pagination: Total items={total_items}, Total pages={total_pages}, Current page={page}, Displaying items {start_index}-{end_index-1}")
        page_window = 2
        start_page_display = max(1, page - page_window)
        end_page_display = min(total_pages, page + page_window)
        # --- Prepare for Template ---
        base_cols = [
            'Level_Correlation', 'Change_Correlation',
            'Mean_Abs_Diff', 'Max_Abs_Diff',
            'NaN_Count_Orig', 'NaN_Count_New', 'Total_Points',
            'Same_Date_Range',
            # Add/remove columns as needed
        ]
        columns_to_display = [actual_id_col] + \
                             [col for col in static_cols if col != actual_id_col and col in paginated_data.columns] + \
                             [col for col in base_cols if col in paginated_data.columns]
        table_data = paginated_data.to_dict(orient='records')
        # Format specific columns 
        for row in table_data:
            for col in ['Level_Correlation', 'Change_Correlation']:
                 if col in row and pd.notna(row[col]):
                    row[col] = f"{row[col]:.4f}" 
            # Add date formatting if needed for stats cols
        # Create pagination object
        pagination_context = {
            'page': page,
            'per_page': safe_per_page,
            'total_items': total_items,
            'total_pages': total_pages,
            'has_prev': page > 1,
            'has_next': page < total_pages,
            'prev_num': page - 1,
            'next_num': page + 1,
            'start_page_display': start_page_display,
            'end_page_display': end_page_display,
            # Function to generate URLs for pagination links, preserving state
             'url_for_page': lambda p: url_for('spread_duration_comparison_bp.summary', 
                                              page=p, 
                                              sort_by=sort_by, 
                                              sort_order=sort_order, 
                                              show_sold=str(show_sold).lower(), # Pass holding status
                                              **{f'filter_{k}': v for k, v in active_filters.items()})
        }
        current_app.logger.info("--- Successfully Prepared Data for Spread Duration Comparison Template ---")
        return render_template('spread_duration_comparison_page.html', # Updated template
                               table_data=table_data,
                               columns_to_display=columns_to_display,
                               id_column_name=actual_id_col, # Pass the ID column name
                               filter_options=final_filter_options,
                               active_filters=active_filters,
                               current_sort_by=sort_by,
                               current_sort_order=sort_order,
                               pagination=pagination_context,
                               show_sold=show_sold, # Pass holding filter status
                               message=None) # No message if data is present
    except FileNotFoundError as e:
        current_app.logger.error(f"Spread duration comparison file not found: {e}")
        return f"Error: Required spread duration comparison file not found ({e.filename}). Check the Data folder.", 404
    except Exception as e:
        current_app.logger.exception("An unexpected error occurred in the spread duration comparison summary view.") # Log full traceback
        return render_template('spread_duration_comparison_page.html', 
                               message=f"An unexpected error occurred: {e}",
                               table_data=[], pagination=None, filter_options={}, 
                               active_filters={}, show_sold=show_sold, columns_to_display=[], 
                               id_column_name='Security') # Include show_sold in error template
@spread_duration_comparison_bp.route('/spread_duration_comparison/details/<path:security_id>')
def spread_duration_comparison_details(security_id):
    """Displays side-by-side historical spread duration charts for a specific security."""
    current_app.logger.info(f"--- Starting Spread Duration Comparison Detail Request for Security ID: {security_id} ---")
    # Retrieve the configured absolute data folder path
    data_folder = current_app.config['DATA_FOLDER']
    if not data_folder:
        current_app.logger.error("DATA_FOLDER is not configured in the application.")
        return "Internal Server Error: Data folder not configured", 500
    # Decode the security_id from URL encoding
    decoded_security_id = unquote(security_id)
    current_app.logger.info(f"Decoded Security ID: {decoded_security_id}")
    try:
        # Pass the absolute data folder path
        merged_data, static_data, common_static_cols, id_col_name, _ = load_spread_duration_comparison_data(data_folder)
        if id_col_name is None:
             current_app.logger.error(f"Failed to get ID column name for details view (Security: {decoded_security_id}).") # Use decoded ID
             return "Error loading spread duration comparison data: Could not determine ID column.", 500
        if merged_data.empty:
            current_app.logger.warning(f"Merged spread duration data is empty for details view (Security: {decoded_security_id}).") # Use decoded ID
            return f"No merged spread duration data found for Security ID: {decoded_security_id}", 404 # Use decoded ID
        # --- Debugging: Log ID column and sample IDs from DataFrame --- START
        current_app.logger.info(f"Identified ID column name: '{id_col_name}'")
        if id_col_name in merged_data.columns:
             sample_ids = merged_data[id_col_name].unique()[:5] # Get first 5 unique IDs
             current_app.logger.info(f"Sample IDs from DataFrame column '{id_col_name}': {sample_ids}")
             current_app.logger.info(f"Data type of column '{id_col_name}': {merged_data[id_col_name].dtype}")
        else:
            current_app.logger.warning(f"ID column '{id_col_name}' not found in merged_data columns for sampling.")
        # --- Debugging: Log ID column and sample IDs from DataFrame --- END
        # Filter data for the specific security using the DECODED ID and correct ID column name
        security_data = merged_data[merged_data[id_col_name] == decoded_security_id].copy() # Use decoded_security_id
        if security_data.empty:
            current_app.logger.warning(f"No spread duration data found after filtering for the specific Security ID: {decoded_security_id}") # Use decoded ID
            # Consider checking if the ID exists in the original files?
            return f"Spread Duration data not found for Security ID: {decoded_security_id}", 404 # Use decoded ID
        # Get static info for this security (handle potential multiple rows if ID isn't unique, take first)
        static_info = security_data[[id_col_name] + common_static_cols].iloc[0].to_dict() if not security_data.empty else {}
        # Sort by date for charting
        security_data = security_data.sort_values(by='Date')
        # Prepare data for Chart.js
        # Ensure 'Date' is in the correct string format for JSON/JS
        security_data['Date_Str'] = security_data['Date'].dt.strftime('%Y-%m-%d')
        chart_data = {
            'labels': security_data['Date_Str'].tolist(),
            'datasets': [
                {
                    'label': 'Original Spread Duration', # Updated Label
                    'data': security_data['Value_Orig'].where(pd.notna(security_data['Value_Orig']), None).tolist(), # Replace NaN with None for JSON
                    'borderColor': COLOR_PALETTE[0 % len(COLOR_PALETTE)],
                    'fill': False,
                    'tension': 0.1
                },
                {
                    'label': 'New Spread Duration', # Updated Label
                    'data': security_data['Value_New'].where(pd.notna(security_data['Value_New']), None).tolist(), # Replace NaN with None for JSON
                    'borderColor': COLOR_PALETTE[1 % len(COLOR_PALETTE)],
                    'fill': False,
                    'tension': 0.1
                }
            ]
        }
        # Calculate overall statistics for this security
        stats_summary = calculate_comparison_stats(security_data, pd.DataFrame([static_info]), id_col=id_col_name) # Pass single security data
        stats_dict = stats_summary.iloc[0].to_dict() if not stats_summary.empty else {}
         # Format dates and numbers in stats_dict before passing
        for key, value in stats_dict.items():
            if isinstance(value, pd.Timestamp):
                stats_dict[key] = value.strftime('%Y-%m-%d')
            elif isinstance(value, (int, float)):
                 if 'Correlation' in key and pd.notna(value):
                     stats_dict[key] = f"{value:.4f}"
                 elif 'Diff' in key and pd.notna(value):
                      stats_dict[key] = f"{value:.2f}" # Adjust formatting as needed
        current_app.logger.info(f"Successfully prepared data for spread duration details template (Security: {decoded_security_id})") # Use decoded ID
        return render_template('spread_duration_comparison_details_page.html', # Updated template
                               security_id=decoded_security_id, # Pass decoded ID to template
                               static_info=static_info, # Pass static info
                               chart_data=chart_data,
                               stats_summary=stats_dict) # Pass calculated stats
    except FileNotFoundError as e:
        current_app.logger.error(f"Spread duration comparison file not found for details view: {e} (Security: {decoded_security_id})") # Use decoded ID
        return f"Error: Required spread duration comparison file not found ({e.filename}). Check the Data folder.", 404
    except KeyError as e:
         current_app.logger.error(f"KeyError accessing data for security '{decoded_security_id}': {e}. ID column used: '{id_col_name}'") # Use decoded ID
         return f"Error accessing data for security '{decoded_security_id}'. It might be missing required columns or have unexpected formatting.", 500 # Use decoded ID
    except Exception as e:
        current_app.logger.exception(f"An unexpected error occurred in the spread duration comparison details view for security '{decoded_security_id}'.") # Use decoded ID
        return f"An internal error occurred while processing details for security '{decoded_security_id}': {e}", 500 # Use decoded ID
</file>

<file path="views/weight_views.py">
# views/weight_views.py
# Purpose: Handles routes related to weight checks (e.g., ensuring weights are 100%).
import os
import pandas as pd
import traceback
import logging
from flask import Blueprint, render_template, current_app
# Define the blueprint
weight_bp = Blueprint('weight', __name__, url_prefix='/weights')
def _parse_percentage(value):
    """Attempts to parse a string like '99.5%' into a float 99.5."""
    if pd.isna(value) or value == '':
        return None
    try:
        # Remove '%' and convert to float
        return float(str(value).replace('%', '').strip())
    except (ValueError, TypeError):
        logging.warning(f"Could not parse percentage value: {value}")
        return None # Indicate parsing failure
def _is_date_like_column(col_name):
    """Checks if a column name matches YYYY-MM-DD or YYYY-MM-DDTHH:MM:SS format."""
    try:
        # Attempt to parse using pandas with flexible inference first
        # errors='raise' will fail if it's not recognizable as a date/datetime
        pd.to_datetime(col_name, errors='raise')
        # Optionally, add more specific format checks if needed, but `to_datetime` is quite good
        # e.g., check if it matches common regex patterns if `to_datetime` is too broad
        return True
    except (ValueError, TypeError):
        return False
def load_and_process_weight_data(data_folder_path: str, filename: str):
    """Loads a wide weight file, converts decimal values to percentages, checks against 100%.
    Args:
        data_folder_path (str): The absolute path to the data folder.
        filename (str): The name of the weight file (e.g., 'w_Funds.csv').
    Returns:
        tuple: (dict | None, list[str])
               - Dictionary of processed data {fund_code: {date: {data}}} or None on error.
               - List of sorted date headers (YYYY-MM-DD) found in the file.
    """
    if not data_folder_path:
        logging.error(f"No data_folder_path provided for file {filename}")
        return None, []
    filepath = os.path.join(data_folder_path, filename)
    if not os.path.exists(filepath):
        logging.error(f"Weight file not found: {filepath}")
        return None, []
    try:
        df = pd.read_csv(filepath, encoding='utf-8')
        df.columns = df.columns.str.strip() # Ensure no leading/trailing spaces
        # Identify ID column (assuming 'Fund Code')
        id_col = 'Fund Code'
        if id_col not in df.columns:
            logging.error(f"Required column '{id_col}' not found in {filename}")
            return None, []
        # Identify date columns based on ISO format
        date_cols = [col for col in df.columns if _is_date_like_column(col)]
        if not date_cols:
            logging.warning(f"No date-like columns found in {filename}")
            return None, []
        # Sort date columns chronologically
        # Convert to datetime objects for reliable sorting
        datetime_objs = [pd.to_datetime(d) for d in date_cols]
        # Sort based on datetime objects
        datetime_objs.sort()
        # Convert back to display format (YYYY-MM-DD) after sorting
        date_headers_display = [dt.strftime('%Y-%m-%d') for dt in datetime_objs]
        # Get the original column names corresponding to the sorted display headers
        # We need this to access the correct columns in the DataFrame later
        # Create a map from display format back to original format
        display_to_original_map = {pd.to_datetime(d).strftime('%Y-%m-%d'): d for d in date_cols}
        original_date_cols_sorted = [display_to_original_map[d] for d in date_headers_display]
        processed_data = {}
        # Set index for easier access
        df.set_index(id_col, inplace=True)
        for fund_code in df.index:
            processed_data[fund_code] = {}
            # Iterate using the sorted original column names and the sorted display headers
            for original_date_col, display_date_header in zip(original_date_cols_sorted, date_headers_display):
                original_value_str = str(df.loc[fund_code, original_date_col])
                # --- Updated Logic for Decimal Input ---
                calculated_percentage = None
                try:
                    # Attempt to convert the raw value directly to float
                    decimal_value = float(original_value_str)
                    # Convert decimal to percentage
                    calculated_percentage = decimal_value * 100.0
                except (ValueError, TypeError):
                    # Handle cases where the value is not a valid number
                    logging.warning(f"Could not convert value to float for {fund_code} on {display_date_header} in {filename}: {original_value_str}")
                    calculated_percentage = None # Ensure it remains None
                # --- End Updated Logic ---
                is_100 = False
                formatted_value_str = 'N/A' # Default if conversion fails or value is NaN
                if calculated_percentage is not None:
                    # Check if the calculated percentage is within the tolerance range
                    tolerance = 0.01 # e.g., allows 99.99 to 100.01
                    is_100 = abs(calculated_percentage - 100.0) <= tolerance
                    # Format the calculated percentage as a string with 2 decimal places
                    formatted_value_str = f"{calculated_percentage:.2f}%"
                # Store the formatted percentage string and the boolean check result
                processed_data[fund_code][display_date_header] = {
                    'value_percent_str': formatted_value_str, 
                    'is_100': is_100,
                    # 'parsed_value': parsed_value_float # Keep for potential future use/debugging
                }
        return processed_data, date_headers_display
    except Exception as e:
        logging.error(f"Error processing weight file {filename}: {e}")
        traceback.print_exc()
        return None, []
@weight_bp.route('/check')
def weight_check():
    """Displays the weight check page."""
    # Retrieve the configured absolute data folder path
    data_folder = current_app.config['DATA_FOLDER']
    if not data_folder:
        current_app.logger.error("DATA_FOLDER is not configured in the application.")
        return "Internal Server Error: Data folder not configured", 500
    fund_filename = 'w_Funds.csv'
    bench_filename = 'w_Bench.csv'
    # Pass the absolute data folder path to the helper function
    fund_data, fund_date_headers = load_and_process_weight_data(data_folder, fund_filename)
    bench_data, bench_date_headers = load_and_process_weight_data(data_folder, bench_filename)
    # Use the longer list of dates as the canonical header list, assuming they might differ slightly
    all_date_headers = sorted(list(set(fund_date_headers + bench_date_headers)))
    return render_template('weight_check_page.html',
                           fund_data=fund_data,
                           bench_data=bench_data,
                           date_headers=all_date_headers, # Pass combined, sorted list
                           fund_filename=fund_filename,
                           bench_filename=bench_filename)
</file>

<file path="weight_processing.py">
# weight_processing.py
# This script provides functionality to process weight files (e.g., w_Funds.csv).
# It reads a weight file, identifies the relevant columns, and saves the processed data
# to a specified output path. It replaces duplicate headers with dates from Dates.csv.
import pandas as pd
import logging
import os
import io
import re
from collections import Counter
# Get the logger instance. Assumes Flask app has configured logging.
logger = logging.getLogger(__name__)
def clean_date_format(dates):
    """
    Clean up date format by removing time components and ensuring consistent YYYY-MM-DD format.
    Args:
        dates (list): List of date strings or values to clean
    Returns:
        list: List of cleaned date strings in YYYY-MM-DD format
    """
    cleaned_dates = []
    for date in dates:
        # Remove time component if it exists (T00:00:00 format)
        if isinstance(date, str) and 'T' in date:
            cleaned_date = date.split('T')[0]
            cleaned_dates.append(cleaned_date)
        else:
            # If date is in datetime format, convert to string in YYYY-MM-DD format
            try:
                if pd.notnull(date):
                    date_obj = pd.to_datetime(date)
                    cleaned_dates.append(date_obj.strftime('%Y-%m-%d'))
                else:
                    cleaned_dates.append(date)
            except:
                cleaned_dates.append(date)
    return cleaned_dates
def process_weight_file(input_path: str, output_path: str, dates_path: str = None):
    """
    Reads a weight CSV file, replaces duplicate headers with dates from Dates.csv,
    and saves it to the specified output path.
    Args:
        input_path (str): Absolute path to the input weight CSV file (e.g., w_Funds.csv).
        output_path (str): Absolute path where the processed weight file should be saved.
        dates_path (str): Optional path to the Dates.csv file. If None, will look in the same
                          directory as the input file.
    """
    if not os.path.exists(input_path):
        logger.error(f"Weight file not found: {input_path}. Skipping processing.")
        return
    logger.info(f"Processing weight file: {input_path} -> {output_path}")
    try:
        # Find the folder containing the input file to look for Dates.csv if not provided
        if dates_path is None:
            input_dir = os.path.dirname(input_path)
            dates_path = os.path.join(input_dir, 'Dates.csv')
        # Check if Dates.csv exists
        if not os.path.exists(dates_path):
            logger.error(f"Dates.csv not found at {dates_path}. Cannot replace headers.")
            return
        # Read dates
        try:
            dates_df = pd.read_csv(dates_path)
            # Ensure the date column is parsed correctly
            dates_df['Date'] = pd.to_datetime(dates_df['Date'], errors='coerce')
            # Drop any rows where date parsing failed
            dates_df = dates_df.dropna(subset=['Date'])
            # Sort dates chronologically (oldest first)
            dates_df = dates_df.sort_values(by='Date')
            # Get the sorted list of date objects
            sorted_dates = dates_df['Date'].tolist()
            logger.info(f"Loaded {len(sorted_dates)} dates from {dates_path}")
            # Clean up date formats to remove time components
            dates = clean_date_format(sorted_dates)
            logger.info(f"Cleaned up date formats to remove time components")
        except Exception as e:
            logger.error(f"Error loading dates from {dates_path}: {e}")
            return
        # Read the input CSV - add robustness
        input_basename = os.path.basename(input_path).lower()
        # Special handling for different file types based on filename
        if "w_funds" in input_basename or "w_bench" in input_basename:
            # For funds and bench files, we know they have a simple structure:
            # First column is fund code, all others are weight columns to be replaced with dates
            df = pd.read_csv(input_path, on_bad_lines='skip', encoding='utf-8', encoding_errors='replace')
            if df.empty:
                logger.warning(f"Weight file {input_path} is empty. Skipping.")
                return
            # Get original column names
            original_cols = df.columns.tolist()
            # First column remains the same (Fund Code)
            id_col = original_cols[0]
            # All other columns become dates
            data_cols = original_cols[1:]
            if len(data_cols) > len(dates):
                logger.warning(f"Not enough dates ({len(dates)}) for all data columns ({len(data_cols)}). Using available dates only.")
                dates = dates[:len(data_cols)]
            elif len(data_cols) < len(dates):
                logger.warning(f"More dates ({len(dates)}) than data columns ({len(data_cols)}). Using first {len(data_cols)} dates.")
                dates = dates[:len(data_cols)]
            # Create new column list
            new_columns = [id_col] + dates
            # Rename columns
            df.columns = new_columns
            logger.info(f"Replaced {len(data_cols)} weight columns with dates")
        elif "w_secs" in input_basename:
            # For securities file, we need to handle potential metadata columns
            # Read the file but skip header normalization
            df = pd.read_csv(input_path, on_bad_lines='skip', encoding='utf-8', encoding_errors='replace')
            if df.empty:
                logger.warning(f"Weight file {input_path} is empty. Skipping.")
                return
            # Get original column names
            original_cols = df.columns.tolist()
            # For securities, we assume:
            # - First column is always the ID column (ISIN)
            # - Fixed number of metadata columns (e.g., up to column X)
            # - The rest are date columns that need replacing
            # Identify metadata vs data columns
            # Determine if any columns look like repeated weight/value columns
            # For simplicity, we'll assume a pattern like "Weight", "Price", etc. repeats
            value_patterns = ["weight", "price", "value", "duration"]
            # Count columns that match common patterns
            pattern_counts = {}
            for col in original_cols:
                col_lower = col.lower()
                for pattern in value_patterns:
                    if pattern in col_lower:
                        if pattern not in pattern_counts:
                            pattern_counts[pattern] = 0
                        pattern_counts[pattern] += 1
            # Find the most common pattern
            most_common_pattern = None
            max_count = 0
            for pattern, count in pattern_counts.items():
                if count > max_count:
                    max_count = count
                    most_common_pattern = pattern
            if most_common_pattern and max_count > 1:
                logger.info(f"Found repeated pattern '{most_common_pattern}' {max_count} times in {input_path}")
                # Create list of columns to replace (those matching the pattern)
                replace_indices = []
                for i, col in enumerate(original_cols):
                    if most_common_pattern in col.lower():
                        replace_indices.append(i)
                # --- Refined Date Replacement Logic for Mismatch --- 
                num_dates = len(dates)
                num_replace_cols = len(replace_indices)
                if num_dates != num_replace_cols:
                    logger.warning(f"Date count ({num_dates}) differs from pattern column count ({num_replace_cols}) for {input_path}. Replacing headers using the minimum count: {min(num_dates, num_replace_cols)}.")
                else:
                    logger.info(f"Found {num_dates} dates and {num_replace_cols} pattern columns. Replacing headers.")
                # Determine how many headers we can actually replace
                num_to_replace = min(num_dates, num_replace_cols)
                # Replace the identified columns with dates, up to the minimum count
                new_columns = original_cols.copy()
                for i in range(num_to_replace):
                    idx_to_replace = replace_indices[i]
                    # Ensure we don't try to access dates beyond its bounds (already handled by num_to_replace)
                    new_columns[idx_to_replace] = dates[i] 
                # Columns in replace_indices beyond num_to_replace will keep their original names
                # --- End Refined Logic --- 
                # Rename columns in the DataFrame
                df.columns = new_columns
                logger.info(f"Replaced {num_to_replace} columns matching '{most_common_pattern}' with dates")
            else:
                # Fallback: use the simpler approach of taking first column as ID, rest as dates
                logger.warning(f"No clear pattern found in {input_path}. Using default approach (first column = ID, rest = dates).")
                id_col = original_cols[0]
                data_cols = original_cols[1:]
                if len(data_cols) > len(dates):
                    logger.warning(f"Not enough dates ({len(dates)}) for all data columns ({len(data_cols)}). Using available dates only.")
                    dates = dates[:len(data_cols)]
                elif len(data_cols) < len(dates):
                    logger.warning(f"More dates ({len(dates)}) than data columns ({len(data_cols)}). Using first {len(data_cols)} dates.")
                    dates = dates[:len(data_cols)]
                new_columns = [id_col] + dates
                df.columns = new_columns
                logger.info(f"Replaced {len(data_cols)} columns with dates")
        else:
            # Default handling for unknown files
            logger.warning(f"Unknown file type: {input_path}. Using default handling.")
            df = pd.read_csv(input_path, on_bad_lines='skip', encoding='utf-8', encoding_errors='replace')
            # Assume first column is ID, rest are dates
            original_cols = df.columns.tolist()
            id_col = original_cols[0]
            data_cols = original_cols[1:]
            if len(data_cols) > len(dates):
                logger.warning(f"Not enough dates ({len(dates)}) for all data columns ({len(data_cols)}). Using available dates only.")
                dates = dates[:len(data_cols)]
            elif len(data_cols) < len(dates):
                logger.warning(f"More dates ({len(dates)}) than data columns ({len(data_cols)}). Using first {len(data_cols)} dates.")
                dates = dates[:len(data_cols)]
            new_columns = [id_col] + dates
            df.columns = new_columns
            logger.info(f"Replaced all columns after the first with dates")
        # Save the processed DataFrame to the output path
        df.to_csv(output_path, index=False, encoding='utf-8')
        logger.info(f"Successfully processed and saved weight file to: {output_path}")
    except FileNotFoundError:
        # This case is handled by the initial check, but included for completeness
        logger.error(f"Error: Input weight file not found during processing - {input_path}")
    except pd.errors.EmptyDataError:
         logger.warning(f"Weight file is empty - {input_path}. Skipping save.")
    except pd.errors.ParserError as pe:
        logger.error(f"Error parsing CSV weight file {input_path}: {pe}. Check file format and integrity.", exc_info=True)
    except PermissionError as pe:
        logger.error(f"Permission error saving to {output_path}: {pe}. Ensure the file is not open in another program.", exc_info=True)
    except Exception as e:
        logger.error(f"An unexpected error occurred processing weight file {input_path} to {output_path}: {e}", exc_info=True)
# Example usage:
# if __name__ == "__main__":
#    # Paths are relative to the workspace
#    data_dir = "Data"
#    input_file = os.path.join(data_dir, 'pre_w_Funds.csv')
#    output_file = os.path.join(data_dir, 'w_Funds.csv')
#    dates_file = os.path.join(data_dir, 'Dates.csv')
#
#    if os.path.exists(input_file):
#        print(f"Processing weight file: {input_file} -> {output_file}")
#        process_weight_file(input_file, output_file, dates_file)
#    else:
#        print(f"Input file not found: {input_file}")
</file>

</files>
