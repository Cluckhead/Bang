This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-04-08T20:00:01.335Z

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
instance/
.gitignore
app.py
config.py
data_loader.py
data_validation.py
LICENSE
metric_calculator.py
process_data.py
README.md
requirements.txt
security_processing.py
static/css/style.css
static/js/main.js
static/js/modules/charts/timeSeriesChart.js
static/js/modules/ui/chartRenderer.js
static/js/modules/ui/securityTableFilter.js
static/js/modules/ui/tableSorter.js
static/js/modules/utils/helpers.js
templates/base.html
templates/comparison_details_page.html
templates/comparison_page.html
templates/delete_metric_page.html
templates/exclusions_page.html
templates/fund_duration_details.html
templates/get_data.html
templates/index.html
templates/metric_page_js.html
templates/securities_page.html
templates/security_details_page.html
utils.py
views/__init__.py
views/api_views.py
views/comparison_views.py
views/exclusion_views.py
views/fund_views.py
views/main_views.py
views/metric_views.py
views/security_views.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".gitignore">
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class
*.csv
# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
# Usually these files are written by a python script from a template
# before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
.hypothesis/
.pytest_cache/

# Translations
*.mo
*.pot
*.log

# Django stuff:
*.log
local_settings.py
db.sqlite3

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
.python-version

# celery beat schedule file
celerybeat-schedule

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/
</file>

<file path="app.py">
# This file defines the main entry point and structure for the Simple Data Checker Flask web application.
# It utilizes the Application Factory pattern (`create_app`) to initialize and configure the Flask app.
# Key responsibilities include:
# - Creating the Flask application instance.
# - Setting up basic configuration (like the secret key).
# - Ensuring necessary folders (like the instance folder) exist.
# - Registering Blueprints (`main_bp`, `metric_bp`, `security_bp`, `fund_bp`, `exclusion_bp`) from the `views`
#   directory, which contain the application's routes and view logic.
# - Providing a conditional block (`if __name__ == '__main__':`) to run the development server
#   when the script is executed directly.
# This modular structure using factories and blueprints makes the application more organized and scalable.
# This file contains the main Flask application factory.
from flask import Flask, render_template, Blueprint, jsonify
import os
import logging
# --- Add imports for the new route ---
import subprocess
import sys # To get python executable path
# --- End imports ---
# Import configurations and utilities (potentially needed by factory setup later)
from config import DATA_FOLDER, COLOR_PALETTE # Uncommented import
# from utils import _is_date_like, parse_fund_list # Not directly used in factory itself yet
def create_app():
    """Factory function to create and configure the Flask app."""
    app = Flask(__name__, instance_relative_config=True) # instance_relative_config=True allows for instance folder config
    # Basic configuration (can be expanded later, e.g., loading from config file)
    app.config.from_mapping(
        SECRET_KEY='dev', # Default secret key for development. CHANGE for production!
        # Add other default configurations if needed
    )
    # Load configuration from config.py
    app.config.from_object('config')
    # Ensure the instance folder exists (if using instance_relative_config)
    try:
        os.makedirs(app.instance_path)
    except OSError:
        pass # Already exists
    # Serve static files (for JS, CSS, etc.)
    # Note: static_url_path defaults to /static, static_folder defaults to 'static' in root
    # No need to set app.static_folder = 'static' explicitly unless changing the folder name/path
    # --- Register Blueprints --- 
    from views.main_views import main_bp
    from views.metric_views import metric_bp
    from views.security_views import security_bp
    from views.fund_views import fund_bp
    from views.api_views import api_bp
    from views.exclusion_views import exclusion_bp
    # --- Add import for the new comparison blueprint ---
    from views.comparison_views import comparison_bp
    # --- End import ---
    app.register_blueprint(main_bp)
    app.register_blueprint(metric_bp)
    app.register_blueprint(security_bp)
    app.register_blueprint(fund_bp)
    app.register_blueprint(api_bp)
    app.register_blueprint(exclusion_bp)
    # --- Register the new comparison blueprint ---
    app.register_blueprint(comparison_bp)
    # --- End registration ---
    print("Registered Blueprints:")
    print(f"- {main_bp.name} (prefix: {main_bp.url_prefix})")
    print(f"- {metric_bp.name} (prefix: {metric_bp.url_prefix})")
    print(f"- {security_bp.name} (prefix: {security_bp.url_prefix})")
    print(f"- {fund_bp.name} (prefix: {fund_bp.url_prefix})")
    print(f"- {api_bp.name} (prefix: {api_bp.url_prefix})")
    print(f"- {exclusion_bp.name} (prefix: {exclusion_bp.url_prefix})")
    # --- Print registration info for the new blueprint ---
    print(f"- {comparison_bp.name} (prefix: {comparison_bp.url_prefix})")
    # --- End print ---
    # Add a simple test route to confirm app creation (optional)
    @app.route('/hello')
    def hello():
        return 'Hello, World! App factory is working.'
    # --- Add the new cleanup route ---
    @app.route('/run-cleanup', methods=['POST'])
    def run_cleanup():
        """Endpoint to trigger the process_data.py script."""
        script_path = os.path.join(os.path.dirname(__file__), 'process_data.py')
        python_executable = sys.executable # Use the same python that runs flask
        if not os.path.exists(script_path):
            app.logger.error(f"Cleanup script not found at: {script_path}")
            return jsonify({'status': 'error', 'message': 'Cleanup script not found.'}), 500
        app.logger.info(f"Attempting to run cleanup script: {script_path}")
        try:
            # Run the script using the same Python interpreter that is running Flask
            # Capture stdout and stderr, decode as UTF-8, handle potential errors
            result = subprocess.run(
                [python_executable, script_path],
                capture_output=True,
                text=True,
                check=False, # Don't raise exception on non-zero exit code
                encoding='utf-8' # Explicitly set encoding
            )
            log_output = f"STDOUT:\n{result.stdout}\nSTDERR:\n{result.stderr}"
            if result.returncode == 0:
                app.logger.info(f"Cleanup script finished successfully. Output:\n{log_output}")
                return jsonify({'status': 'success', 'output': result.stdout or "No output", 'error': result.stderr}), 200
            else:
                app.logger.error(f"Cleanup script failed with return code {result.returncode}. Output:\n{log_output}")
                return jsonify({'status': 'error', 'message': 'Cleanup script failed.', 'output': result.stdout, 'error': result.stderr}), 500
        except Exception as e:
            app.logger.error(f"Exception occurred while running cleanup script: {e}", exc_info=True)
            return jsonify({'status': 'error', 'message': f'An exception occurred: {e}'}), 500
    # --- End new route ---
    return app
# --- Application Execution --- 
if __name__ == '__main__':
    app = create_app() # Create the app instance using the factory
    app.run(debug=True) # Run in debug mode for development
</file>

<file path="config.py">
# This file defines configuration variables for the Simple Data Checker application.
# It centralizes settings like file paths and visual parameters (e.g., chart colors)
# to make them easily adjustable without modifying the core application code.
"""
Configuration settings for the Flask application.
"""
DATA_FOLDER = 'Data'
# Define a list of distinct colors for chart lines
# Add more colors if you expect more fund columns
COLOR_PALETTE = [
    'blue', 'red', 'green', 'purple', '#FF7F50', # Coral
    '#6495ED', # CornflowerBlue
    '#DC143C', # Crimson
    '#00FFFF'  # Aqua
]
</file>

<file path="data_loader.py">
# This file is responsible for loading and preprocessing data from CSV files.
# It includes functions to dynamically identify essential columns (Date, Code, Benchmark)
# based on patterns, handle potential naming variations, parse dates, standardize column names,
# set appropriate data types, and prepare the data in a pandas DataFrame format
# suitable for further analysis and processing within the application.
# data_loader.py
# This file is responsible for loading and preprocessing data from time-series CSV files (typically prefixed with `ts_`).
# It includes functions to dynamically identify essential columns (Date, Code, Benchmark)
# based on patterns, handle potential naming variations, parse dates (handling 'YYYY-MM-DD' and 'DD/MM/YYYY'),
# standardize column names, set appropriate data types, and prepare the data in a pandas DataFrame format
# suitable for further analysis within the application. It includes robust error handling and logging.
import pandas as pd
import os
import logging
from typing import List, Tuple, Optional
import re # Import regex for pattern matching
# --- Logging Setup ---
LOG_FILENAME = 'data_processing_errors.log'
LOG_FORMAT = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
# Get the logger for the current module
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO) # Set minimum level for the logger
# Prevent adding handlers multiple times
if not logger.handlers:
    # Console Handler (INFO and above)
    ch = logging.StreamHandler()
    ch.setLevel(logging.INFO)
    ch_formatter = logging.Formatter(LOG_FORMAT)
    ch.setFormatter(ch_formatter)
    logger.addHandler(ch)
    # File Handler (WARNING and above)
    try:
        # Attempt to create log file in the parent directory (project root)
        log_filepath = os.path.join(os.path.dirname(__file__), '..', LOG_FILENAME)
        fh = logging.FileHandler(log_filepath, mode='a') # Append mode
        fh.setLevel(logging.WARNING)
        fh_formatter = logging.Formatter(LOG_FORMAT)
        fh.setFormatter(fh_formatter)
        logger.addHandler(fh)
    except Exception as e:
        logger.error(f"Failed to configure file logging to {log_filepath}: {e}")
# --- End Logging Setup ---
# Define constants
DATA_FOLDER = 'Data'
# Standard internal column names after renaming
STD_DATE_COL = 'Date'
STD_CODE_COL = 'Code'
STD_BENCHMARK_COL = 'Benchmark'
def _find_column(pattern: str, columns: List[str], filename: str, col_type: str) -> str:
    """Helper function to find a single column matching a pattern (case-insensitive)."""
    matches = [col for col in columns if re.search(pattern, col, re.IGNORECASE)]
    if len(matches) == 1:
        logger.info(f"Found {col_type} column in '{filename}': '{matches[0]}'")
        return matches[0]
    elif len(matches) > 1:
        # Log error before raising
        logger.error(f"Multiple possible {col_type} columns found in '{filename}' matching pattern '{pattern}': {matches}. Please ensure unique column names.")
        raise ValueError(f"Multiple possible {col_type} columns found in '{filename}' matching pattern '{pattern}': {matches}. Please ensure unique column names.")
    else:
         # Log error before raising
        logger.error(f"No {col_type} column found in '{filename}' matching pattern '{pattern}'. Found columns: {columns}")
        raise ValueError(f"No {col_type} column found in '{filename}' matching pattern '{pattern}'. Found columns: {columns}")
def load_and_process_data(
    filename: str,
    # Remove default args for specific names as they are now dynamically found
    # date_col: str = DEFAULT_DATE_COL,
    # code_col: str = DEFAULT_CODE_COL,
    # benchmark_col: str = DEFAULT_BENCHMARK_COL,
    # other_fund_cols: Optional[List[str]] = None, # Keep for potential future explicit override, but primary logic is dynamic
    data_folder: str = DATA_FOLDER
) -> Tuple[pd.DataFrame, List[str], Optional[str]]: # Changed return type hint for benchmark_col
    """Loads a CSV file, dynamically identifies date, code, and benchmark columns,
    renames them to standard names ('Date', 'Code', 'Benchmark'), parses dates (trying YYYY-MM-DD then DD/MM/YYYY),
    sets index, identifies original fund column names, and ensures numeric types for value columns.
    Args:
        filename (str): The name of the CSV file within the data folder.
        data_folder (str): The path to the folder containing the data files. Defaults to DATA_FOLDER.
    Returns:
        Tuple[pd.DataFrame, List[str], Optional[str]]:
               Processed DataFrame indexed by the standardized 'Date' and 'Code' columns,
               list of original fund column names found in the file,
               the standardized benchmark column name ('Benchmark') if present, otherwise None.
    Raises:
        ValueError: If required columns (Date, Code) cannot be uniquely identified,
                    if date parsing fails completely, or if no value columns are found (and no benchmark).
        FileNotFoundError: If the specified file does not exist.
    """
    filepath = os.path.join(data_folder, filename)
    if not os.path.exists(filepath):
        logger.error(f"File not found: {filepath}")
        raise FileNotFoundError(f"File not found: {filepath}")
    try:
        # Read only the header first to get column names accurately
        header_df = pd.read_csv(filepath, nrows=0, encoding='utf-8', encoding_errors='replace', on_bad_lines='skip')
        original_cols = [col.strip() for col in header_df.columns.tolist()] # Strip whitespace immediately
        logger.info(f"Original columns found in '{filename}': {original_cols}")
        # --- Dynamically find required columns using patterns ---
        # Use word boundaries (\\b) to avoid partial matches like \'Benchmarking\'
        # Updated pattern to match 'Position Date' or 'Date'
        date_pattern = r'\b(Position\s*)?Date\b' # Store pattern for logging
        logger.info(f"Attempting to find Date column in '{filename}' using pattern: '{date_pattern}'") # DEBUG
        actual_date_col = _find_column(date_pattern, original_cols, filename, 'Date') 
        logger.info(f"Found actual Date column: '{actual_date_col}'") # DEBUG
        code_pattern = r'\bCode\b' # Store pattern for logging
        logger.info(f"Attempting to find Code column in '{filename}' using pattern: '{code_pattern}'") # DEBUG
        actual_code_col = _find_column(code_pattern, original_cols, filename, 'Code')
        logger.info(f"Found actual Code column: '{actual_code_col}'") # DEBUG
        # Allow benchmark column to be optional - look for it, but don't fail if not found.
        try:
            benchmark_pattern = r'\bBenchmark\b' # Store pattern for logging
            logger.info(f"Attempting to find Benchmark column in '{filename}' using pattern: '{benchmark_pattern}'") # DEBUG
            actual_benchmark_col = _find_column(benchmark_pattern, original_cols, filename, 'Benchmark')
            benchmark_col_present = True
        except ValueError:
            logger.warning(f"No Benchmark column found in '{filename}' matching pattern '\\bBenchmark\\b'. Proceeding without benchmark.")
            actual_benchmark_col = None # Indicate benchmark is not present
            benchmark_col_present = False
        # --- Identify original fund value columns ---
        # Fund columns are everything EXCEPT the identified date and code columns.
        # Benchmark is also excluded IF it was found.
        excluded_cols_for_funds = {actual_date_col, actual_code_col}
        if benchmark_col_present and actual_benchmark_col: # Check actual_benchmark_col is not None
            excluded_cols_for_funds.add(actual_benchmark_col)
        # Identify columns that are not date, code, or benchmark (if present)
        original_fund_val_col_names = [col for col in original_cols if col not in excluded_cols_for_funds]
        if not original_fund_val_col_names and not benchmark_col_present:
             logger.error(f"No fund value columns and no benchmark column identified in '{filename}'. Cannot process.")
             raise ValueError(f"No fund value columns and no benchmark column identified in '{filename}'. Cannot process.")
        elif not original_fund_val_col_names:
             logger.warning(f"No specific fund value columns identified in '{filename}' besides the benchmark column.")
        else:
            logger.info(f"Identified Fund columns in '{filename}': {original_fund_val_col_names}")
        # --- Read the full CSV ---
        # Read WITHOUT parsing dates initially, handle it manually later for flexibility
        df = pd.read_csv(filepath, encoding='utf-8', encoding_errors='replace', on_bad_lines='skip', dtype={actual_date_col: str}) # Read date col as string
        df.columns = df.columns.str.strip() # Ensure columns are stripped again after full read
        # --- Rename columns to standard names BEFORE date parsing ---
        rename_map = {
            actual_date_col: STD_DATE_COL,
            actual_code_col: STD_CODE_COL
        }
        if benchmark_col_present and actual_benchmark_col:
            rename_map[actual_benchmark_col] = STD_BENCHMARK_COL
        df.rename(columns=rename_map, inplace=True)
        logger.info(f"Renamed columns in '{filename}' to standard names: {list(rename_map.keys())} -> {list(rename_map.values())}")
        # --- Robust Date Parsing ---
        logger.info(f"Starting date parsing for column '{STD_DATE_COL}' (original: '{actual_date_col}') in '{filename}'.") # DEBUG
        date_series = df[STD_DATE_COL]
        logger.debug(f"Original date series head:\\n{date_series.head()}") # Use DEBUG for potentially long series
        # Let pandas infer the format. errors='coerce' will turn unparseable strings into NaT (Not a Time).
        logger.info(f"Attempting date parsing using pandas format inference...") 
        parsed_dates = pd.to_datetime(date_series, errors='coerce')
        # Check if parsing failed for all entries 
        all_null_after_parsing = parsed_dates.isnull().all()
        logger.info(f"Result of isnull().all() after pandas format inference: {all_null_after_parsing}") # DEBUG
        if all_null_after_parsing:
            logger.error(f"Could not parse any dates in column '{STD_DATE_COL}' (original: '{actual_date_col}') using pandas format inference in file {filename}.")
            raise ValueError(f"Date parsing failed for file {filename}. Could not infer format.")
        else:
            # Log how many were successfully parsed vs NaT
            nat_count = parsed_dates.isnull().sum()
            total_count = len(parsed_dates)
            success_count = total_count - nat_count
            logger.info(f"Successfully parsed {success_count}/{total_count} dates using pandas inference in {filename}. ({nat_count} resulted in NaT).")
            if nat_count > 0:
                 logger.warning(f"{nat_count} date values in '{STD_DATE_COL}' (original: '{actual_date_col}') from {filename} could not be parsed and resulted in NaT.")
        # Log count of NaNs before dropping
        # nat_count_before_drop = parsed_dates.isnull().sum() # Already calculated above as nat_count
        logger.info(f"Number of NaT (unparseable) dates before dropping: {nat_count} out of {len(parsed_dates)}") # DEBUG
        # Assign the successfully parsed dates back to the DataFrame
        df[STD_DATE_COL] = parsed_dates
        # Drop rows where date parsing failed completely (became NaT)
        original_row_count = len(df)
        df.dropna(subset=[STD_DATE_COL], inplace=True)
        rows_dropped = original_row_count - len(df)
        if rows_dropped > 0:
            logger.warning(f"Dropped {rows_dropped} rows from {filename} due to failed date parsing.")
        # --- Set Index using standard names ---
        if df.empty:
            logger.warning(f"DataFrame became empty after dropping rows with unparseable dates in {filename}.")
            # Return an empty DataFrame matching the expected structure but log the issue
            final_benchmark_col_name = STD_BENCHMARK_COL if benchmark_col_present else None
            # Need to define columns if df is empty, based on expected output structure
            expected_cols = [STD_DATE_COL, STD_CODE_COL] + original_fund_val_col_names
            if final_benchmark_col_name:
                expected_cols.append(final_benchmark_col_name)
            # Create an empty df with the right index and columns
            empty_index = pd.MultiIndex(levels=[[], []], codes=[[], []], names=[STD_DATE_COL, STD_CODE_COL])
            return pd.DataFrame(index=empty_index, columns=[col for col in expected_cols if col not in [STD_DATE_COL, STD_CODE_COL]]), original_fund_val_col_names, final_benchmark_col_name
        else:
            df.set_index([STD_DATE_COL, STD_CODE_COL], inplace=True)
        # --- Convert value columns to numeric ---
        # Use original fund names and the standard benchmark name (if present)
        value_cols_to_convert = original_fund_val_col_names[:] # Make a copy
        if benchmark_col_present:
             # Use the RENAMED benchmark column name for conversion
            value_cols_to_convert.append(STD_BENCHMARK_COL)
        if not value_cols_to_convert:
            # This case implies only date/code columns were found, which should be caught earlier, but safeguard.
            logger.error(f"No valid fund or benchmark value columns found to convert in {filename} after processing.")
            raise ValueError(f"No valid fund or benchmark value columns found to convert in {filename} after processing.")
        # Ensure the columns actually exist in the DataFrame after renaming before converting
        valid_cols_for_conversion = [col for col in value_cols_to_convert if col in df.columns]
        if not valid_cols_for_conversion:
             logger.error(f"None of the identified value columns ({value_cols_to_convert}) exist in the DataFrame after renaming. Columns: {df.columns.tolist()}")
             raise ValueError(f"None of the identified value columns ({value_cols_to_convert}) exist in the DataFrame after renaming. Columns: {df.columns.tolist()}")
        # Use apply with pd.to_numeric for robust conversion (errors='coerce' is crucial)
        df[valid_cols_for_conversion] = df[valid_cols_for_conversion].apply(pd.to_numeric, errors='coerce')
        # Check for NaNs after conversion
        nan_check_cols = [col for col in valid_cols_for_conversion if col in df.columns] # Re-check existence just in case
        if nan_check_cols and df[nan_check_cols].isnull().all().all():
            logger.warning(f"All values in value columns {nan_check_cols} became NaN after conversion in file {filename}. Check data types.")
        # Return the DataFrame, the ORIGINAL fund column names, and the STANDARD benchmark name
        # Return STD_BENCHMARK_COL if benchmark was present, else None
        final_benchmark_col_name = STD_BENCHMARK_COL if benchmark_col_present else None
        logger.info(f"Successfully loaded and processed '{filename}'. Identified Original Funds: {original_fund_val_col_names}, Standard Benchmark Name Used: {final_benchmark_col_name}")
        return df, original_fund_val_col_names, final_benchmark_col_name
    except Exception as e:
        # Log the error with traceback information to file and console
        logger.error(f"Error processing file {filepath}: {e}", exc_info=True)
        # Re-raise the exception to be handled by the calling code (e.g., in app.py or script runner)
        # The calling code should decide whether to skip the file or halt execution.
        raise
</file>

<file path="data_validation.py">
'''
Placeholder module for validating data retrieved from the API.
This module will contain functions to check the structure, data types,
and potentially the content consistency of the DataFrames returned by the
Rex API before they are saved as CSV files.
'''
import pandas as pd
def validate_data(df: pd.DataFrame, filename: str):
    """
    Validates the structure and types of the DataFrame based on filename conventions.
    This is a placeholder function. Implement specific checks based on the
    expected format for different file types (e.g., 'ts_*.csv', 'sec_*.csv').
    Args:
        df (pd.DataFrame): The DataFrame returned by the API call.
        filename (str): The intended filename for the data (e.g., 'ts_Duration.csv').
    Returns:
        tuple[bool, list[str]]: A tuple containing:
            - bool: True if the data is valid, False otherwise.
            - list[str]: A list of validation error messages, empty if valid.
    """
    errors = []
    if df is None or not isinstance(df, pd.DataFrame):
        errors.append("Invalid input: DataFrame is None or not a pandas DataFrame.")
        return False, errors
    if df.empty:
        # It might be valid for some queries to return no data, but flag it for review.
        errors.append("Warning: DataFrame is empty.")
        # Decide if empty is truly invalid or just a warning.
        # For now, let's consider it potentially valid but issue a warning.
        # return False, errors # Uncomment if empty df is strictly invalid
    # Example checks based on filename conventions:
    if filename.startswith('ts_'):
        # Checks for time-series files
        required_cols = ['Date', 'Code'] # Assuming these are standard post-processing names
        if not all(col in df.columns for col in required_cols):
            errors.append(f"Missing required columns for time-series data: Expected {required_cols}, got {list(df.columns)}")
        # Check if 'Date' column is datetime type (or can be coerced)
        # try:
        #     pd.to_datetime(df['Date'])
        # except Exception as e:
        #     errors.append(f"'Date' column cannot be parsed as datetime: {e}")
        # Check if value columns (excluding Date, Code, Benchmark if exists) are numeric
        value_cols = [col for col in df.columns if col not in ['Date', 'Code', 'Benchmark']]
        for col in value_cols:
            if not pd.api.types.is_numeric_dtype(df[col]):
                 errors.append(f"Column '{col}' in time-series data is not numeric.")
    elif filename.startswith('sec_'):
        # Checks for security-level files
        # Example: Check for an ID column (e.g., 'Security ID', 'ISIN')
        # Example: Check if columns intended as dates are parseable
        # Example: Check if value columns are numeric
        pass # Add specific checks here
    elif filename == 'FundList.csv':
        # Example: Check required columns for FundList
        required_cols = ['Fund Code', 'Total Asset Value USD', 'Picked']
        if not all(col in df.columns for col in required_cols):
             errors.append(f"Missing required columns for FundList.csv: Expected {required_cols}, got {list(df.columns)}")
    # --- Add more specific validation rules as needed based on data specs --- 
    is_valid = len(errors) == 0
    return is_valid, errors
# Example Usage (can be run manually for testing):
if __name__ == '__main__':
    # Create dummy dataframes for testing validation logic
    print("Testing validation functions...")
    # Test case 1: Valid time-series data
    valid_ts_data = {
        'Date': pd.to_datetime(['2023-01-01', '2023-01-01', '2023-01-02', '2023-01-02']),
        'Code': ['FUNDA', 'FUNDB', 'FUNDA', 'FUNDB'],
        'Value': [10.1, 20.2, 10.5, 20.8],
        'Benchmark': [10.0, 20.0, 10.4, 20.7]
    }
    valid_ts_df = pd.DataFrame(valid_ts_data)
    is_valid, errors = validate_data(valid_ts_df, 'ts_ExampleMetric.csv')
    print(f"Valid TS Data Test: Valid={is_valid}, Errors={errors}")
    assert is_valid
    # Test case 2: Invalid time-series data (missing column)
    invalid_ts_data = {
        'Date': pd.to_datetime(['2023-01-01']),
        # 'Code': ['FUNDA'], # Missing Code column
        'Value': [10.1]
    }
    invalid_ts_df = pd.DataFrame(invalid_ts_data)
    is_valid, errors = validate_data(invalid_ts_df, 'ts_AnotherMetric.csv')
    print(f"Invalid TS Data Test (Missing Col): Valid={is_valid}, Errors={errors}")
    assert not is_valid
    assert "Missing required columns" in errors[0]
    # Test case 3: Invalid time-series data (non-numeric value)
    invalid_ts_data_type = {
        'Date': pd.to_datetime(['2023-01-01']),
        'Code': ['FUNDA'],
        'Value': ['abc'] # Non-numeric value
    }
    invalid_ts_df_type = pd.DataFrame(invalid_ts_data_type)
    is_valid, errors = validate_data(invalid_ts_df_type, 'ts_BadData.csv')
    print(f"Invalid TS Data Test (Bad Type): Valid={is_valid}, Errors={errors}")
    # Note: This specific check might depend on when type conversion happens.
    # If conversion happens *before* validation, this might pass if 'abc' becomes NaN.
    # The check here assumes the raw data from API might be non-numeric.
    assert not is_valid # Assuming the validation catches non-numeric directly
    assert "not numeric" in errors[0]
    # Test case 4: Empty DataFrame
    empty_df = pd.DataFrame()
    is_valid, errors = validate_data(empty_df, 'ts_EmptyData.csv')
    print(f"Empty DF Test: Valid={is_valid}, Errors={errors}")
    assert is_valid # Currently allows empty with warning
    assert "DataFrame is empty" in errors[0]
    # Test case 5: None DataFrame
    none_df = None
    is_valid, errors = validate_data(none_df, 'ts_NoneData.csv')
    print(f"None DF Test: Valid={is_valid}, Errors={errors}")
    assert not is_valid
    assert "DataFrame is None" in errors[0]
    print("Validation tests completed.")
</file>

<file path="LICENSE">
MIT License

Copyright (c) [2025] [Robert Clark]

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</file>

<file path="metric_calculator.py">
# This file provides functions for calculating various statistical metrics from the preprocessed data.
# Key functionalities include calculating historical statistics (mean, max, min), latest values,
# period-over-period changes, and Z-scores for changes for both benchmark and fund columns.
# It operates on a pandas DataFrame indexed by Date and Fund Code, producing a summary DataFrame
# containing these metrics for each fund, often sorted by the most significant recent changes (Z-scores).
# metric_calculator.py
# This file contains functions for calculating metrics from the processed data.
import pandas as pd
import numpy as np
import logging
import os # Needed for logging setup
from typing import List, Dict, Any, Tuple, Optional
# --- Logging Setup ---
# Use the same log file as data_loader
LOG_FILENAME = 'data_processing_errors.log'
LOG_FORMAT = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
# Get the logger for the current module
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
# Prevent adding handlers multiple times (especially if imported by other modules)
if not logger.handlers:
    # Console Handler (INFO and above)
    ch = logging.StreamHandler()
    ch.setLevel(logging.INFO)
    ch_formatter = logging.Formatter(LOG_FORMAT)
    ch.setFormatter(ch_formatter)
    logger.addHandler(ch)
    # File Handler (WARNING and above)
    try:
        # Create log file path relative to this file's location
        log_filepath = os.path.join(os.path.dirname(__file__), '..', LOG_FILENAME)
        fh = logging.FileHandler(log_filepath, mode='a')
        fh.setLevel(logging.WARNING)
        fh_formatter = logging.Formatter(LOG_FORMAT)
        fh.setFormatter(fh_formatter)
        logger.addHandler(fh)
    except Exception as e:
        # Log to stderr if file logging setup fails
        import sys
        print(f"Error setting up file logging for metric_calculator: {e}", file=sys.stderr)
# --- End Logging Setup ---
# Configure logging (can be configured globally elsewhere if part of a larger app)
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
def _calculate_column_stats(
    col_series: pd.Series,
    col_change_series: pd.Series,
    latest_date: pd.Timestamp,
    col_name: str
) -> Dict[str, Any]:
    """Helper function to calculate stats for a single column series.
    Calculates historical mean/max/min, latest value, latest change, and change z-score.
    Handles potential NaN values resulting from calculations or missing data gracefully.
    Args:
        col_series (pd.Series): The historical data for the column.
        col_change_series (pd.Series): The historical changes for the column.
        latest_date (pd.Timestamp): The overall latest date in the dataset.
        col_name (str): The name of the column being processed.
    Returns:
        Dict[str, Any]: A dictionary containing the calculated metrics for this column.
    """
    metrics = {}
    # Calculate base historical stats for the column level
    # Pandas functions like mean, max, min typically handle NaNs by skipping them.
    metrics[f'{col_name} Mean'] = col_series.mean()
    metrics[f'{col_name} Max'] = col_series.max()
    metrics[f'{col_name} Min'] = col_series.min()
    # Calculate stats for the column change
    change_mean = col_change_series.mean()
    change_std = col_change_series.std()
    # Get latest values if data exists for the latest date
    # Check if the latest_date exists in the specific series index
    if latest_date in col_series.index:
        latest_value = col_series.loc[latest_date]
        # Use .get() for change series to handle potential index mismatch (though unlikely if derived correctly)
        latest_change = col_change_series.get(latest_date, np.nan)
        metrics[f'{col_name} Latest Value'] = latest_value
        metrics[f'{col_name} Change'] = latest_change
        # Calculate Change Z-Score: (latest_change - change_mean) / change_std
        change_z_score = np.nan # Default to NaN
        if pd.notna(latest_change) and pd.notna(change_mean) and pd.notna(change_std) and change_std != 0:
            change_z_score = (latest_change - change_mean) / change_std
        elif change_std == 0 and pd.notna(latest_change) and pd.notna(change_mean):
             # Handle case where std dev is zero (e.g., constant series)
             if latest_change == change_mean:
                 change_z_score = 0.0 # No deviation
             else:
                 change_z_score = np.inf if latest_change > change_mean else -np.inf # Infinite deviation
             logger.debug(f"Standard deviation of change for '{col_name}' is zero. Z-score set to {change_z_score}.")
        else:
            # Log if Z-score calculation couldn't be performed due to NaNs
            if not (pd.notna(latest_change) and pd.notna(change_mean) and pd.notna(change_std)):
                 logger.debug(f"Cannot calculate Z-score for '{col_name}' due to NaN inputs (latest_change={latest_change}, change_mean={change_mean}, change_std={change_std})")
        metrics[f'{col_name} Change Z-Score'] = change_z_score
    else:
        # Data for the latest date is missing for this specific column/fund
        logger.debug(f"Latest date {latest_date} not found for column '{col_name}'. Setting latest metrics to NaN.")
        metrics[f'{col_name} Latest Value'] = np.nan
        metrics[f'{col_name} Change'] = np.nan
        metrics[f'{col_name} Change Z-Score'] = np.nan
    return metrics
def calculate_latest_metrics(
    df: pd.DataFrame,
    fund_cols: List[str],
    benchmark_col: Optional[str] # Allow benchmark_col to be None
) -> pd.DataFrame:
    """Calculates latest metrics for each individual column (benchmark and funds) per fund code.
    For each fund code and for each relevant column (benchmark and funds),
    it calculates: Latest Value, Change, Mean, Max, Min, and Change Z-Score.
    Args:
        df (pd.DataFrame): Processed DataFrame indexed by Date (level 0) and Fund Code (level 1).
                           Assumes date index is sorted ascendingly within each fund code.
        fund_cols (List[str]): List of fund value column names (original names from loader).
        benchmark_col (Optional[str]): Name of the *standardized* benchmark value column ('Benchmark'), or None if not present.
    Returns:
        pd.DataFrame: Flattened metrics indexed by Fund Code.
                      Columns are named like '{col_name} Latest Value', '{col_name} Change', etc.
                      (Note: Uses original fund column names and standardized benchmark name in output cols).
                      The DataFrame is sorted by the maximum absolute 'Change Z-Score' found
                      across all columns for each fund, in descending order.
                      Funds with no Z-scores (e.g., due to missing data or zero std dev)
                      are placed at the end.
    """
    if df is None or df.empty:
        logger.warning("Input DataFrame is None or empty. Cannot calculate metrics.")
        return pd.DataFrame()
    if df.index.nlevels != 2:
        logger.error("Input DataFrame must have a MultiIndex with 2 levels (Date, Fund Code).")
        # Consider raising ValueError or returning empty DataFrame? Let's return empty for resilience.
        return pd.DataFrame()
    # Ensure the date level is sorted for correct .diff() calculation
    try:
        df_sorted = df.sort_index(level=0)
        latest_date = df_sorted.index.get_level_values(0).max()
        fund_codes = df_sorted.index.get_level_values(1).unique()
    except Exception as e:
         logger.error(f"Error preparing DataFrame for metric calculation (sorting/indexing): {e}", exc_info=True)
         return pd.DataFrame()
    # Determine which columns to actually process based on presence in df
    cols_to_process = []
    output_col_name_map = {} # Map processed col name (potentially std) to output name (original/std)
    if benchmark_col and benchmark_col in df_sorted.columns:
        cols_to_process.append(benchmark_col) # Use the standardized name for processing
        output_col_name_map[benchmark_col] = benchmark_col # Output name is the same std name
    elif benchmark_col:
        logger.warning(f"Specified benchmark column '{benchmark_col}' not found in DataFrame columns: {df_sorted.columns.tolist()}")
    # Use the *original* fund column names provided by the loader
    for f_col in fund_cols:
        if f_col in df_sorted.columns:
            cols_to_process.append(f_col)
            output_col_name_map[f_col] = f_col # Output name is the original fund name
        else:
            logger.warning(f"Specified fund column '{f_col}' not found in DataFrame columns: {df_sorted.columns.tolist()}")
    if not cols_to_process:
        logger.error("No valid columns (benchmark or funds) found in the DataFrame to calculate metrics for.")
        return pd.DataFrame()
    logger.info(f"Calculating metrics for columns: {cols_to_process}")
    all_metrics_list = []
    # Store the maximum absolute change z-score *per fund* across all its columns for sorting purposes
    max_abs_change_z_scores: Dict[str, float] = {}
    for fund_code in fund_codes:
        try:
            # Extract historical data for the specific fund code
            # .xs drops the level, providing a DataFrame indexed by Date
            # Use .loc to avoid potential KeyError if fund_code doesn't exist
            if fund_code not in df_sorted.index.get_level_values(1):
                 logger.warning(f"Fund code '{fund_code}' not found in DataFrame index level 1. Skipping.")
                 continue
            # Select only columns we intend to process to avoid carrying unused data
            fund_data_hist = df_sorted.loc[(slice(None), fund_code), cols_to_process]
            # After .loc, the index might still be MultiIndex if only one fund exists, reset to Date
            fund_data_hist = fund_data_hist.reset_index(level=1, drop=True).sort_index()
        except Exception as e: # Catch broader errors during data extraction
            logger.error(f"Error extracting data for fund code '{fund_code}': {e}", exc_info=True)
            continue
        # Initialize metrics for this fund
        fund_metrics: Dict[str, Any] = {'Fund Code': fund_code}
        current_fund_max_abs_z: float = -1.0 # Use -1 to handle cases where all Zs are NaN or non-positive
        for col_name in cols_to_process:
            # This check should be redundant now due to filtering `cols_to_process` above, but keep as safeguard
            if col_name not in fund_data_hist.columns:
                logger.warning(f"Column '{col_name}' unexpectedly not found for fund '{fund_code}' after filtering. Skipping metrics.")
                continue
            # Get the specific column's historical data and calculate its difference
            col_hist = fund_data_hist[col_name]
            # Calculate diff only if series is not empty and has more than one non-NaN value
            col_change_hist = pd.Series(index=col_hist.index, dtype=np.float64) # Initialize with NaNs
            if not col_hist.dropna().empty and len(col_hist.dropna()) > 1:
                 col_change_hist = col_hist.diff()
            else:
                 logger.debug(f"Cannot calculate difference for column '{col_name}', fund '{fund_code}' due to insufficient data.")
            # Calculate stats for this specific column
            # Use the mapped output name for the metric dictionary keys
            output_name = output_col_name_map[col_name]
            col_stats = _calculate_column_stats(col_hist, col_change_hist, latest_date, output_name)
            fund_metrics.update(col_stats)
            # Update the fund's overall max absolute Z-score if this column's Z-score is valid and larger
            col_z_score = col_stats.get(f'{output_name} Change Z-Score', np.nan)
            # Replace inf/-inf with a large number for sorting comparison, but keep original in metrics
            compare_z_score = col_z_score
            if np.isinf(compare_z_score):
                compare_z_score = 1e9 * np.sign(compare_z_score) # Large number with correct sign
            if pd.notna(compare_z_score):
                current_fund_max_abs_z = max(current_fund_max_abs_z, abs(compare_z_score))
        # Store the calculated metrics dictionary for this fund
        all_metrics_list.append(fund_metrics)
        # Store the max abs change Z-score found for this fund across all its columns
        # Use the potentially modified value (inf replaced) for sorting
        max_abs_change_z_scores[fund_code] = current_fund_max_abs_z if current_fund_max_abs_z >= 0 else np.nan
    # --- Post-processing --- 
    if not all_metrics_list:
        logger.warning("No funds were processed successfully. Returning empty DataFrame.")
        return pd.DataFrame()
    # Create DataFrame from the list of metric dictionaries
    try:
        latest_metrics_df = pd.DataFrame(all_metrics_list).set_index('Fund Code')
    except Exception as e:
         logger.error(f"Error creating final metrics DataFrame: {e}", exc_info=True)
         return pd.DataFrame()
    # Sort the DataFrame based on the calculated max absolute Z-scores
    # Create a Series from the max_abs_change_z_scores dict, align its index with the DataFrame
    # then sort the DataFrame based on the values of this series.
    try:
        sort_series = pd.Series(max_abs_change_z_scores).reindex(latest_metrics_df.index)
        # Sort descending, NaNs go last
        latest_metrics_df_sorted = latest_metrics_df.reindex(sort_series.sort_values(ascending=False, na_position='last').index)
    except Exception as e:
        logger.error(f"Error sorting metrics DataFrame: {e}", exc_info=True)
        latest_metrics_df_sorted = latest_metrics_df # Return unsorted if sorting fails
    logger.info(f"Successfully calculated metrics for {len(latest_metrics_df_sorted)} funds.")
    return latest_metrics_df_sorted
</file>

<file path="process_data.py">
# This script serves as a pre-processing step for specific CSV files within the 'Data' directory.
# It targets files prefixed with 'pre_', reads them, and performs data aggregation and cleaning.
# The core logic involves grouping rows based on identical values across most columns, excluding 'Funds' and 'Security Name'.
# For rows sharing the same 'Security Name' but differing in other data points, the 'Security Name' is suffixed
# (e.g., _1, _2) to ensure uniqueness. The 'Funds' associated with identical data rows are aggregated
# into a single list-like string representation (e.g., '[FUND1,FUND2]').
# The processed data is then saved to a new CSV file prefixed with 'new_'.
# process_data.py
# This script processes CSV files in the 'Data' directory that start with 'pre_'.
# It merges rows based on identical values in all columns except 'Funds'.
# Duplicated 'Security Name' entries with differing data are suffixed (_1, _2, etc.).
# The aggregated 'Funds' are stored as a list in the output file.
import os
import pandas as pd
import logging
# Add datetime for date parsing and sorting
from datetime import datetime
# --- Logging Setup ---
# Use the same log file as other data processing scripts
LOG_FILENAME = 'data_processing_errors.log'
LOG_FORMAT = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
# Get the logger for the current module
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
# Prevent adding handlers multiple times
if not logger.handlers:
    # Console Handler (INFO and above)
    ch = logging.StreamHandler()
    ch.setLevel(logging.INFO)
    ch_formatter = logging.Formatter(LOG_FORMAT)
    ch.setFormatter(ch_formatter)
    logger.addHandler(ch)
    # File Handler (WARNING and above)
    try:
        # Create log file path relative to this file's location (assuming it's in the project root)
        log_filepath = os.path.join(os.path.dirname(__file__), LOG_FILENAME) # If script is in root
        # If script is in a sub-directory, adjust path:
        # log_filepath = os.path.join(os.path.dirname(__file__), '..', LOG_FILENAME)
        fh = logging.FileHandler(log_filepath, mode='a')
        fh.setLevel(logging.WARNING)
        fh_formatter = logging.Formatter(LOG_FORMAT)
        fh.setFormatter(fh_formatter)
        logger.addHandler(fh)
    except Exception as e:
        # Log to stderr if file logging setup fails
        import sys
        print(f"Error setting up file logging for process_data: {e}", file=sys.stderr)
# --- End Logging Setup ---
# Define a constant for the dates file path
DATES_FILE_PATH = os.path.join('Data', 'dates.csv') # Define path to dates file
def read_and_sort_dates(dates_file):
    """
    Reads dates from a CSV file, sorts them, and returns them as a list of strings.
    Args:
        dates_file (str): Path to the CSV file containing dates (expected single column).
    Returns:
        list[str] | None: A sorted list of date strings (YYYY-MM-DD) or None if an error occurs.
    """
    try:
        dates_df = pd.read_csv(dates_file, parse_dates=[0]) # Assume date is the first column
        # Handle potential parsing errors if the column isn't purely dates
        if dates_df.iloc[:, 0].isnull().any():
             logger.warning(f"Warning: Some values in {dates_file} could not be parsed as dates.")
             # Attempt to drop NaT values and proceed
             dates_df = dates_df.dropna(subset=[dates_df.columns[0]])
             if dates_df.empty:
                 logger.error(f"Error: No valid dates found in {dates_file} after handling parsing issues.")
                 return None
        # Sort dates chronologically
        sorted_dates = dates_df.iloc[:, 0].sort_values()
        # Format dates as 'YYYY-MM-DD' strings for column headers
        date_strings = sorted_dates.dt.strftime('%Y-%m-%d').tolist()
        logger.info(f"Successfully read and sorted {len(date_strings)} dates from {dates_file}.")
        # --- Deduplicate the date list while preserving order --- 
        unique_date_strings = []
        seen_dates = set()
        duplicates_found = False
        for date_str in date_strings:
            if date_str not in seen_dates:
                unique_date_strings.append(date_str)
                seen_dates.add(date_str)
            else:
                duplicates_found = True
        if duplicates_found:
            logger.warning(f"Duplicate dates found in {dates_file}. Using unique sorted dates: {len(unique_date_strings)} unique dates.")
        # --- End Deduplication ---
        return unique_date_strings # Return the deduplicated list
    except FileNotFoundError:
        logger.error(f"Error: Dates file not found at {dates_file}")
        return None
    except pd.errors.EmptyDataError:
        logger.error(f"Error: Dates file is empty - {dates_file}")
        return None
    except IndexError:
        logger.error(f"Error: Dates file {dates_file} seems to have no columns.")
        return None
    except Exception as e:
        logger.error(f"An unexpected error occurred reading dates from {dates_file}: {e}", exc_info=True)
        return None
# Modify process_csv_file signature to accept date_columns
def process_csv_file(input_path, output_path, date_columns):
    """
    Reads a 'pre_' CSV file, potentially replaces placeholder columns with dates,
    processes it according to the rules, and writes the result to a 'new_' CSV file.
    Args:
        input_path (str): Path to the input CSV file (e.g., 'Data/pre_sec_duration.csv').
        output_path (str): Path to the output CSV file (e.g., 'Data/new_sec_duration.csv').
        date_columns (list[str] | None): Sorted list of date strings for headers, or None if dates couldn't be read.
    """
    # If date_columns is None (due to error reading dates.csv), log and skip processing files needing date replacement.
    # We'll handle the actual replacement logic further down.
    if date_columns is None:
         logger.warning(f"Skipping {input_path} because date information is unavailable (check logs for errors reading dates.csv).")
         # A file might still be processable if its columns are *already* correct dates,
         # but the current logic requires date_columns for the check/replacement.
         # To proceed without dates.csv, the logic would need significant changes.
         return # Skip processing this file if dates aren't loaded.
    try:
        # Read the input CSV - add robustness
        # Skip bad lines, handle encoding errors
        df = pd.read_csv(input_path, on_bad_lines='skip', encoding='utf-8', encoding_errors='replace')
        logger.info(f"Processing file: {input_path}")
        if df.empty:
            logger.warning(f"Input file {input_path} is empty or contains only invalid lines. Skipping processing.")
            return
        # --- Column Header Replacement Logic ---
        original_cols = df.columns.tolist()
        required_cols = ['Funds', 'Security Name'] # Core columns that should always exist
        # Ensure required columns are actually in the dataframe before proceeding
        if not all(col in original_cols for col in required_cols):
             missing = [col for col in required_cols if col not in original_cols]
             logger.error(f"Skipping {input_path}: Missing required columns: {missing}. Found columns: {original_cols}")
             return
        # Find the index after the last required column to start searching for placeholders
        # This assumes required columns appear early and together, adjust if needed.
        last_required_idx = -1
        for req_col in required_cols:
            try:
                last_required_idx = max(last_required_idx, original_cols.index(req_col))
            except ValueError: # Should not happen due to check above, but safeguard
                 logger.error(f"Required column '{req_col}' unexpectedly not found after initial check in {input_path}. Skipping.")
                 return
        candidate_start_index = last_required_idx + 1
        candidate_cols = original_cols[candidate_start_index:]
        # Decide on the final columns to use for processing
        current_cols = original_cols # Default to original columns
        # --- Enhanced Placeholder Detection ---
        # Detect sequences like 'Col', 'Col.1', 'Col.2', ...
        potential_placeholder_base = None
        placeholder_start_index_in_candidates = -1
        detected_sequence = []
        if not candidate_cols:
            logger.warning(f"File {input_path} has no columns after required columns '{required_cols}'. Cannot check for date placeholders.")
        else:
            # Iterate through candidate columns to find the *start* of the sequence
            found_sequence = False
            for start_idx in range(len(candidate_cols)):
                # Potential base is the column at start_idx
                current_potential_base = candidate_cols[start_idx]
                # Check if it's a potential base name (no '.' suffix)
                if '.' not in current_potential_base:
                    logger.debug(f"Checking potential base '{current_potential_base}' at index {start_idx} in candidate columns.")
                    # Found a potential start, now check for the sequence
                    potential_placeholder_base = current_potential_base
                    placeholder_start_index_in_candidates = start_idx
                    detected_sequence = [potential_placeholder_base] # Start sequence with the base
                    # Check subsequent columns for the pattern 'base.1', 'base.2', etc.
                    for i in range(1, len(candidate_cols) - start_idx):
                        expected_col = f"{potential_placeholder_base}.{i}"
                        actual_col_index = start_idx + i
                        if candidate_cols[actual_col_index] == expected_col:
                            detected_sequence.append(candidate_cols[actual_col_index])
                        else:
                            # Sequence broken
                            logger.debug(f"Sequence broken at index {actual_col_index}. Expected '{expected_col}', found '{candidate_cols[actual_col_index]}'.")
                            break # Stop checking for this base
                    # Check if a sequence of at least length 2 (Base, Base.1) was found
                    if len(detected_sequence) > 1:
                        logger.info(f"Found sequence starting with '{potential_placeholder_base}' at candidate index {placeholder_start_index_in_candidates}.")
                        found_sequence = True
                        break # Exit the outer loop, we found our sequence
                    else:
                        # Only the base was found, or sequence broke immediately. Reset and continue searching.
                        logger.debug(f"Only base '{potential_placeholder_base}' found or sequence too short. Continuing search.")
                        potential_placeholder_base = None
                        placeholder_start_index_in_candidates = -1
                        detected_sequence = []
                        # Continue the outer loop to check the next column as a potential base
                else:
                     logger.debug(f"Column '{current_potential_base}' at index {start_idx} has '.' suffix, skipping as potential base.")
            if not found_sequence:
                 logger.info(f"No placeholder sequence like 'Base', 'Base.1', ... found anywhere in candidate columns of {input_path}.")
                 potential_placeholder_base = None # Ensure it's None if no sequence found
                 detected_sequence = []
        # --- Date Replacement Logic using Detected Sequence ---
        if date_columns is None:
            logger.warning(f"Date information from {DATES_FILE_PATH} is unavailable. Cannot check or replace headers in {input_path}. Processing with original headers: {original_cols}")
        elif potential_placeholder_base is not None and detected_sequence:
            # A sequence like 'Base', 'Base.1', ... was detected
            placeholder_count = len(detected_sequence)
            original_placeholder_start_index = candidate_start_index + placeholder_start_index_in_candidates
            logger.info(f"Detected placeholder sequence based on '{potential_placeholder_base}' with {placeholder_count} columns, starting at index {original_placeholder_start_index} in original columns.")
            # Compare count with loaded dates
            if len(date_columns) == placeholder_count:
                logger.info(f"Replacing {placeholder_count} placeholder columns starting with '{potential_placeholder_base}' with dates.")
                # Construct new columns: Keep columns before sequence + dates + columns after sequence
                cols_before = original_cols[:original_placeholder_start_index]
                # Important: Calculate cols_after based on the *original* position and *count* of placeholders
                cols_after = original_cols[original_placeholder_start_index + placeholder_count:]
                new_columns = cols_before + date_columns + cols_after
                if len(new_columns) != len(original_cols):
                    logger.error(f"Internal error: Column count mismatch after constructing new columns ({len(new_columns)} vs {len(original_cols)}). Columns before: {cols_before}, Dates: {date_columns}, Columns after: {cols_after}. Reverting to original headers.")
                    current_cols = original_cols # Revert to original
                else:
                    df.columns = new_columns
                    current_cols = new_columns # Use the new columns for further processing
                    logger.info(f"Columns after replacement: {current_cols}")
            else:
                # Counts mismatch - log warning and proceed with original headers
                logger.warning(f"Placeholder count mismatch in {input_path}: Found sequence based on '{potential_placeholder_base}' with {placeholder_count} columns, but expected {len(date_columns)} dates based on {DATES_FILE_PATH}. Skipping date replacement. Processing with original headers.")
                # current_cols remains original_cols
        else:
            # No 'Base', 'Base.1', ... sequence found. Check if the candidate columns *already* match the date_columns.
            logger.info(f"No placeholder sequence like 'Base', 'Base.1', ... detected in {input_path}. Checking if existing columns match dates.")
            if candidate_cols == date_columns:
                 logger.info(f"Columns in {input_path} (after required ones) already match the expected dates. No replacement needed.")
                 # current_cols is already original_cols, which are correct.
            else:
                 # Log the mismatch if they don't match dates either
                 logger.warning(f"Columns in {input_path} do not match expected dates and no 'Base', 'Base.1', ... sequence found. Processing with original headers: {original_cols}")
                 # current_cols remains original_cols
        # --- End Column Header Replacement Logic ---
        # Identify columns to check for identity (all except Funds and Security Name) using the CURRENT columns
        # These might be the original placeholders or the replaced dates.
        # Crucially, this now correctly includes any original static columns that were *not* replaced.
        id_cols = [col for col in current_cols if col not in required_cols]
        processed_rows = []
        # Convert 'Security Name' and 'Funds' to string first to handle potential non-string types causing issues later
        df['Security Name'] = df['Security Name'].astype(str)
        df['Funds'] = df['Funds'].astype(str)
        # Group by the primary identifier 'Security Name'
        # Convert 'Security Name' to string first to handle potential non-string types causing groupby issues
        df['Security Name'] = df['Security Name'].astype(str)
        # Ensure 'Funds' is also string for consistent processing later
        df['Funds'] = df['Funds'].astype(str)
        # Use the potentially renamed DataFrame for grouping
        grouped_by_sec = df.groupby('Security Name', sort=False, dropna=False)
        for sec_name, sec_group in grouped_by_sec:
            # Within each security group, further group by all other identifying columns (which might now be dates)
            # This separates rows where the same Security Name has different associated data
            distinct_versions = []
            if id_cols: # Only subgroup if there are other identifying columns
                try:
                    # dropna=False treats NaNs in id_cols as equal for grouping
                    sub_grouped = sec_group.groupby(id_cols, dropna=False, sort=False)
                    distinct_versions = [group for _, group in sub_grouped]
                except KeyError as e:
                    logger.error(f"KeyError during sub-grouping for Security Name '{sec_name}' in {input_path}. Column: {e}. Grouping columns: {id_cols}. Skipping this security.", exc_info=True)
                    continue # Skip this security name if subgrouping fails
                except Exception as e:
                     logger.error(f"Unexpected error during sub-grouping for Security Name '{sec_name}' in {input_path}: {e}. Grouping columns: {id_cols}. Skipping this security.", exc_info=True)
                     continue
            else:
                # If only Security Name and Funds exist (after potential date column issues), treat the whole sec_group as one version
                 distinct_versions = [sec_group]
            num_versions = len(distinct_versions)
            # Iterate through each distinct version found for the current Security Name
            for i, current_version_df in enumerate(distinct_versions):
                if current_version_df.empty:
                    continue # Should not happen, but safeguard
                # Aggregate the unique 'Funds' for this specific version
                # Handle potential NaN values in Funds column before aggregation
                unique_funds = current_version_df['Funds'].dropna().unique()
                # Convert funds to string before joining
                funds_list = sorted([str(f) for f in unique_funds])
                # Take the first row of this version as the template for the output row
                # Use .iloc[0] safely as we checked current_version_df is not empty
                new_row_series = current_version_df.iloc[0].copy()
                # Assign the aggregated funds as a string formatted like a list: "[FUND1,FUND2,...]"
                new_row_series['Funds'] = f"[{','.join(funds_list)}]"
                # If there was more than one distinct version for this Security Name, suffix the name
                if num_versions > 1:
                    # Ensure sec_name is a string before formatting
                    new_row_series['Security Name'] = f"{str(sec_name)}_{i+1}"
                # Else: keep the original Security Name (already stringified and set in new_row_series)
                # Append the processed row (as a dictionary) to our results list
                processed_rows.append(new_row_series.to_dict())
        if not processed_rows:
            logger.warning(f"No data rows processed for {input_path}. Output file will not be created.")
            # Changed behavior: Do not create an empty output file if no rows are processed.
            return
            # Create an empty DataFrame with original columns if no rows processed
            # output_df = pd.DataFrame(columns=original_cols)
        else:
             # Create the final DataFrame from the list of processed rows
            output_df = pd.DataFrame(processed_rows)
             # Ensure the column order reflects the potentially updated columns (current_cols)
             # Filter current_cols to only those present in output_df to avoid KeyErrors if a column was unexpectedly dropped
            final_cols = [col for col in current_cols if col in output_df.columns]
            output_df = output_df[final_cols]
        # Write the processed data to the new CSV file
        # The Funds column now contains comma-separated strings, which pandas will quote if necessary.
        output_df.to_csv(output_path, index=False, encoding='utf-8')
        logger.info(f"Successfully created: {output_path} with {len(output_df)} rows.")
    except FileNotFoundError:
        logger.error(f"Error: Input file not found - {input_path}")
    except pd.errors.EmptyDataError:
         logger.warning(f"Input file is empty or contains only header - {input_path}. Skipping.")
    except pd.errors.ParserError as pe:
        logger.error(f"Error parsing CSV file {input_path}: {pe}. Check file format and integrity.", exc_info=True)
    except Exception as e:
        logger.error(f"An unexpected error occurred processing {input_path}: {e}", exc_info=True)
def main():
    """
    Main function to find and process all 'pre_*.csv' files in the 'Data' directory.
    """
    # --- Read Dates ---
    # Define the path to the dates file
    dates_file_path = DATES_FILE_PATH # Use the constant defined at the top
    # Attempt to read and sort dates
    date_columns = read_and_sort_dates(dates_file_path)
    # If dates couldn't be read, date_columns will be None.
    # process_csv_file will handle this by skipping files.
    # Log message indicating status of date loading is handled within read_and_sort_dates.
    # --- End Read Dates ---
    input_dir = 'Data'
    input_prefix = 'pre_'
    # output_prefix = 'new_' #disabled
    output_prefix = ''
    if not os.path.isdir(input_dir):
        logger.error(f"Input directory '{input_dir}' not found.")
        return
    logger.info(f"Starting pre-processing scan in directory: '{input_dir}'")
    processed_count = 0
    skipped_count = 0
    # Iterate through all files in the specified directory
    for filename in os.listdir(input_dir):
        # Check if the file matches the pattern 'pre_*.csv'
        if filename.startswith(input_prefix) and filename.endswith('.csv'):
            input_file_path = os.path.join(input_dir, filename)
            # Construct the output filename by replacing 'pre_' with 'new_'
            output_filename = filename.replace(input_prefix, output_prefix, 1)
            output_file_path = os.path.join(input_dir, output_filename)
            # Process the individual CSV file
            try:
                # Pass the loaded date_columns to the processing function
                process_csv_file(input_file_path, output_file_path, date_columns)
                # Simple check if output exists might not be enough if process_csv_file skips creation
                # We rely on logs from process_csv_file to indicate success/failure/skip
                processed_count +=1 # Increment even if skipped internally, as we attempted it.
            except Exception as e:
                 # Catch any unexpected errors bubbling up from process_csv_file
                 logger.error(f"Unhandled exception processing {input_file_path} in main loop: {e}", exc_info=True)
                 skipped_count += 1
        else:
            # Optionally log files that don't match the pattern if needed for debugging
            # logger.debug(f"Skipping file '{filename}' as it does not match pattern 'pre_*.csv'")
            pass
    logger.info(f"Pre-processing scan finished. Attempted processing {processed_count} files. Encountered errors in {skipped_count} files during main loop (check logs for details).")
if __name__ == "__main__":
    # Ensure the script runs the main function when executed directly
    main()
</file>

<file path="README.md">
# Simple Data Checker

This application provides a web interface to load, process, and check financial data, primarily focusing on time-series metrics and security-level data. It helps identify potential data anomalies by calculating changes and Z-scores.

## Features

*   **Time-Series Metric Analysis:** Load `ts_*.csv` files, view latest changes, Z-scores, and historical data charts for various metrics per fund.
*   **Security-Level Analysis:** Load wide-format `sec_*.csv` files, view latest changes and Z-scores across securities, and drill down into historical charts (Value, Price, Duration) for individual securities.
*   **Fund-Specific Views:** Analyze data aggregated or filtered by specific funds (e.g., Fund Duration Details).
*   **Security Exclusions:** Maintain a list of securities to temporarily exclude from the main Security Summary page (`/security/summary`). Exclusions can have start/end dates and comments.

## File Structure Overview

```mermaid
graph TD
    A[Simple Data Checker] --> B(app.py);
    A --> C{Python Modules};
    A --> D{Views};
    A --> E{Templates};
    A --> F{Static Files};
    A --> G(Data);
    A --> H(Config/Utils);

    C --> C1(data_loader.py);
    C --> C2(metric_calculator.py);
    C --> C3(security_processing.py);
    C --> C4(process_data.py);

    D --> D1(main_views.py);
    D --> D2(metric_views.py);
    D --> D3(security_views.py);
    D --> D4(fund_views.py);
    D --> D5(exclusion_views.py);

    E --> E1(base.html);
    E --> E2(index.html);
    E --> E3(metric_page_js.html);
    E --> E4(securities_page.html);
    E --> E5(security_details_page.html);
    E --> E6(fund_duration_details.html);
    E --> E7(metric_page.html);
    E --> E8(exclusions_page.html);

    F --> F1(js);
    F1 --> F1a(main.js);
    F1 --> F1b(modules);
    F1b --> F1b1(ui);
    F1b1 --> F1b1a(chartRenderer.js);
    F1b1 --> F1b1b(securityTableFilter.js);
    F1b --> F1b2(utils);
    F1b2 --> F1b2a(helpers.js);
    F1b --> F1b3(charts);
    F1b3 --> F1b3a(timeSeriesChart.js);

    G --> G1(ts_*.csv);
    G --> G2(sec_*.csv);
    G --> G3(pre_*.csv);
    G --> G4(new_*.csv);
    G --> G5(exclusions.csv);

    H --> H1(config.py);
    H --> H2(utils.py);

    B --> D;
    D --> C;
    D --> H;
    D --> E;
    E --> F;
```

## Data Files (`Data/`)

*   `ts_*.csv`: Time-series data, indexed by Date and Code (Fund/Benchmark).
*   `sec_*.csv`: Security-level data, typically wide format with dates as columns.
*   `pre_*.csv`: Input files for the `process_data.py` script.
*   `new_*.csv`: Output files from the `process_data.py` script.
*   **`exclusions.csv`**: Stores the list of excluded securities. Contains columns: `SecurityID`, `AddDate`, `EndDate`, `Comment`.
*   Other files like `QueryMap.csv`, `FundList.csv`, `Dates.csv` may exist for specific configurations or helper data.

## Python Files

### `app.py`
*   **Purpose:** Defines the main entry point and structure for the Simple Data Checker Flask web application. It utilizes the Application Factory pattern (`create_app`) to initialize and configure the Flask app.
*   **Key Responsibilities:**
    *   Creating the Flask application instance.
    *   Setting up basic configuration (like the secret key).
    *   Ensuring necessary folders (like the instance folder) exist.
    *   Registering Blueprints (`main_bp`, `metric_bp`, `security_bp`, `fund_bp`, `exclusion_bp`) from the `views` directory, which contain the application's routes and view logic.
    *   Providing a conditional block (`if __name__ == '__main__':`) to run the development server when the script is executed directly.
*   **Functions:**
    *   `create_app()`: Factory function to create and configure the Flask app.
    *   `hello()`: Simple test route (can be removed).

### `config.py`
*   **Purpose:** Defines configuration variables for the Simple Data Checker application. It centralizes settings like file paths and visual parameters (e.g., chart colors) to make them easily adjustable without modifying the core application code.
*   **Variables:**
    *   `DATA_FOLDER`: Specifies the directory containing data files.
    *   `COLOR_PALETTE`: Defines a list of colors for chart lines.

### `data_loader.py`
*   **Purpose:** Responsible for loading and preprocessing data from time-series CSV files (typically prefixed with `ts_`). It includes functions to dynamically identify essential columns (Date, Code, Benchmark) based on patterns, handle potential naming variations, parse dates (expecting 'YYYY-MM-DD' format in the 'Date' column), standardize column names, set appropriate data types, and prepare the data in a pandas DataFrame format suitable for further analysis.
*   **Robustness Features:**
    *   Uses Python's `logging` module to report progress and errors. Warnings and errors are logged to `data_processing_errors.log` in the project root.
    *   Handles file not found errors gracefully.
    *   Uses `on_bad_lines='skip'` and `encoding_errors='replace'` when reading CSVs to handle malformed rows and encoding issues.
    *   Performs data type conversion for value columns using `pd.to_numeric` with `errors='coerce'`, converting unparseable values to `NaN`. Logs a warning if all values in a column become `NaN`. Replaces `NaN`/`Inf` with `None` for JSON compatibility where needed.
    *   Dynamically finds required columns ('Date', 'Code') and optionally 'Benchmark', raising errors if they cannot be uniquely identified.
    *   **Handles both `YYYY-MM-DD` and `DD/MM/YYYY` date formats when parsing the date column.**
*   **Functions:**
    *   `_find_column(pattern, columns, filename, col_type)`: Helper to find a single column matching a regex pattern (case-insensitive).
    *   `load_and_process_data(filename, data_folder)`: Loads a CSV, identifies key columns dynamically, renames them, parses dates (handling both common formats), sets a MultiIndex (Date, Code), identifies original fund column names, ensures numeric types, handles NaN/Inf, and logs issues encountered.

### `metric_calculator.py`
*   **Purpose:** Provides functions for calculating various statistical metrics from the preprocessed time-series data. Key functionalities include calculating historical statistics (mean, max, min), latest values, period-over-period changes, and Z-scores for changes for both benchmark and fund columns. It operates on a pandas DataFrame indexed by Date and Fund Code.
*   **Robustness Features:**
    *   Uses the shared `logging` module (`data_processing_errors.log`).
    *   Handles empty or incorrectly indexed input DataFrames gracefully.
    *   Safely calculates statistics (mean, max, min, std) by implicitly or explicitly handling `NaN` values.
    *   Handles potential `NaN` or `inf` results during Z-score calculation (e.g., due to missing data or zero standard deviation).
    *   Checks for the existence of expected columns (benchmark, funds) and skips calculations with a warning if columns are missing.
    *   Handles cases where a specific fund or the latest date might be missing for a given column.
*   **Functions:**
    *   `_calculate_column_stats(col_series, col_change_series, latest_date, col_name)`: Helper to calculate stats (mean, max, min, latest value, latest change, change Z-score) for a single column series, handling potential `NaN`s.
    *   `calculate_latest_metrics(df, fund_cols, benchmark_col)`: Calculates the latest metrics for each individual column (benchmark and funds) per fund code, returning a summary DataFrame sorted by the maximum absolute 'Change Z-Score' found across all columns for each fund.

### `process_data.py`
*   **Purpose:** Serves as a pre-processing step for specific CSV files within the `Data` directory, typically targeting files prefixed with `pre_`. It reads these files, aggregates rows based on identical values across most columns (excluding 'Funds' and 'Security Name'), handles duplicate 'Security Name' entries by suffixing them, aggregates associated 'Funds' into a list-like string, and saves the processed data to a new CSV file prefixed with `new_`. This is often used for preparing security attribute files.
*   **Robustness Features:**
    *   Uses the shared `logging` module (`data_processing_errors.log`).
    *   Reads input CSVs using `on_bad_lines='skip'` and encoding error handling.
    *   Checks for required columns ('Funds', 'Security Name') and skips files if missing.
    *   Handles empty or invalid input files.
    *   Uses `dropna=False` and explicit type conversions during grouping to handle potential `NaN` values and diverse data types in grouping keys ('Security Name', `id_cols`).
    *   Safely aggregates 'Funds', handling potential `NaN`s.
    *   Catches errors during file processing and grouping, logging issues and attempting to continue where possible.
    *   Logs a summary of files processed/skipped in the `main` function.
*   **Functions:**
    *   `process_csv_file(input_path, output_path)`: Reads a `pre_*.csv` file, performs the aggregation and renaming logic, handles errors, and writes to a `new_*.csv` file.
    *   `main()`: Finds and processes all `pre_*.csv` files in the `Data` directory when the script is run directly, logging a summary.

### `security_processing.py`
*   **Purpose:** Handles the loading, processing, and analysis of security-level data, assuming input CSV files are structured with one security per row and time-series data spread across columns where headers represent dates (e.g., YYYY-MM-DD).
*   **Robustness Features:**
    *   Uses the shared `logging` module (`data_processing_errors.log`).
    *   Reads input CSVs using `on_bad_lines='skip'` and encoding error handling.
    *   Dynamically identifies ID, static, and date columns; handles files missing date columns or ID columns by logging an error and returning an empty DataFrame.
    *   Uses `errors='coerce'` for date parsing (`pd.to_datetime`) and value conversion (`pd.to_numeric`), turning errors into `NaT`/`NaN`.
    *   Drops rows with conversion errors or missing IDs after melting, logging a warning about dropped rows.
    *   Handles empty DataFrames after processing steps.
    *   Metric calculation (`calculate_security_latest_metrics`) handles `NaN`s gracefully in stats (mean, max, min, std, diff) and Z-score calculations, including zero standard deviation cases.
    *   Checks for missing 'Value' column or incorrect index structure before metric calculation.
*   **Functions:**
    *   `_is_date_like(column_name)`: Checks if a column name resembles a date format (more flexible matching).
    *   `load_and_process_security_data(filename)`: Reads a wide-format security CSV, identifies ID, static, and date columns, 'melts' the data into a long format (Date, Security ID), converts types robustly, handles errors, and returns the processed DataFrame and static column names.
    *   `calculate_security_latest_metrics(df, static_cols)`: Takes the long-format security DataFrame and calculates metrics (Latest Value, Change, Mean, Max, Min, Change Z-Score) for each security's 'Value' over time, preserving static attributes and handling potential data issues.

### `utils.py`
*   **Purpose:** Contains utility functions used throughout the application, providing common helper functionalities like parsing specific string formats or validating data types.
*   **Functions:**
    *   `parse_fund_list(fund_string)`: Parses a string like `'[FUND1,FUND2]'` into a list of strings.

## View Modules (`views/`)

These modules contain the Flask Blueprints that define the application's routes and logic for handling web requests and rendering templates.

```mermaid
graph LR
    subgraph Flask App (app.py)
        direction LR
        FApp[Flask Instance]
    end

    subgraph Blueprints
        direction TB
        BP_Main[main_bp]
        BP_Metric[metric_bp]
        BP_Security[security_bp]
        BP_Fund[fund_bp]
        BP_Exclusion[exclusion_bp]
    end

    subgraph Routes
        direction TB
        R_Index[/] --> BP_Main;
        R_Metric[/metric/<metric_name>] --> BP_Metric;
        R_Securities[/security/summary] --> BP_Security;
        R_SecDetails[/security/details/<metric_name>/<security_id>] --> BP_Security;
        R_FundDetails[/fund_duration_details/<fund_code>] --> BP_Fund;
        R_Exclusions[/exclusions] --> BP_Exclusion;
        R_RemoveExclusion[/exclusions/remove] --> BP_Exclusion;
    end

    subgraph Templates
        direction TB
        T_Index[index.html]
        T_MetricJS[metric_page_js.html]
        T_Securities[securities_page.html]
        T_SecDetails[security_details_page.html]
        T_FundDetails[fund_duration_details.html]
        T_Exclusions[exclusions_page.html]
    end
    
    subgraph Data Processing
        direction TB
        DP_Loader[data_loader.py]
        DP_MetricCalc[metric_calculator.py]
        DP_SecProc[security_processing.py]
        DP_ExclView[exclusion_views.py]
        DP_Utils[utils.py]
        Data_Excl[exclusions.csv]
    end

    FApp -- registers --> BP_Main;
    FApp -- registers --> BP_Metric;
    FApp -- registers --> BP_Security;
    FApp -- registers --> BP_Fund;
    FApp -- registers --> BP_Exclusion;

    BP_Main -- uses --> DP_Loader;
    BP_Main -- uses --> DP_MetricCalc;
    BP_Main -- renders --> T_Index;

    BP_Metric -- uses --> DP_Loader;
    BP_Metric -- uses --> DP_MetricCalc;
    BP_Metric -- uses --> DP_Utils;
    BP_Metric -- renders --> T_MetricJS;
    
    BP_Security -- uses --> DP_SecProc;
    BP_Security -- uses --> DP_ExclView;
    BP_Security -- uses --> DP_Utils;
    BP_Security -- renders --> T_Securities;
    BP_Security -- renders --> T_SecDetails;

    BP_Fund -- uses --> DP_SecProc;
    BP_Fund -- uses --> DP_Utils;
    BP_Fund -- renders --> T_FundDetails;

    BP_Exclusion -- uses --> DP_ExclView;
    DP_ExclView -- reads/writes --> Data_Excl;
    BP_Exclusion -- renders --> T_Exclusions;

    T_Securities -- links to --> R_Exclusions;

```

### `views/main_views.py` (`main_bp`)
*   **Purpose:** Defines routes for the main, top-level views, primarily the dashboard/index page.
*   **Routes:**
    *   `/`: Renders `index.html`. Scans for `ts_*.csv` files, calculates Z-score summaries across all metrics and funds, and displays them in a table. Also provides links to individual metric pages.

### `views/metric_views.py` (`metric_bp`)
*   **Purpose:** Defines routes for displaying detailed views of specific time-series metrics (e.g., Yield, Duration).
*   **Routes:**
    *   `/metric/<metric_name>`: Renders `metric_page_js.html`. Takes a metric display name, loads the corresponding `ts_*.csv` file, calculates metrics for each fund, prepares data (including historical values) for charting, and passes it as a **structured JSON payload** to the template for JavaScript rendering. The JSON includes a `metadata` key (metric name, latest date, column names) and a `funds` key (fund-specific data: labels, datasets, calculated metrics). Replaces NaN/Inf with `None` in the JSON data.

### `views/security_views.py` (`security_bp`)
*   **Purpose:** Defines routes related to displaying security-level data checks.
*   **Routes:**
    *   `/security/summary`: Renders `securities_page.html`. 
        *   Accepts an optional `search_term` query parameter for filtering by security name (case-insensitive contains search).
        *   Loads data processed by `security_processing.py` (currently from `sec_Spread.csv`).
        *   Applies the `search_term` filter if provided.
        *   Loads the active exclusions from `Data/exclusions.csv` via `exclusion_views.py` and filters out excluded securities.
        *   Calculates latest metrics for each remaining security.
        *   Provides filter options based on static columns.
        *   Displays the results in a filterable table, sorted by absolute 'Change Z-Score'.
        *   Rows are highlighted based on 'Change Z-Score'.
        *   Security IDs link to the details page.
        *   A button links to the 'Manage Exclusions' page.
    *   `/security/details/<metric_name>/<security_id>`: Renders `security_details_page.html`. Loads historical data for a specific security and metric (and potentially related metrics like Price and Duration), prepares chart data, and passes it to the template for JavaScript rendering of time-series charts.

### `views/fund_views.py` (`fund_bp`)
*   **Purpose:** Defines routes for displaying fund-specific details, currently focused on duration changes.
*   **Routes:**
    *   `/fund_duration_details/<fund_code>`: Renders `fund_duration_details.html`. Loads security duration data (likely from `new_sec_duration.csv`), filters it for the specified `fund_code`, calculates recent duration changes for each security held by the fund, and displays the results sorted by the largest change. Security names link to their respective detail pages.

### `views/exclusion_views.py` (`exclusion_bp`)
*   **Purpose:** Defines routes for managing the security exclusion list.
*   **Routes:**
    *   `/exclusions` (GET): Renders `exclusions_page.html`. Loads the current list from `Data/exclusions.csv` and loads available securities (from `Data/sec_spread.csv`'s 'Security Name' column) for the add form dropdown.
    *   `/exclusions` (POST): Processes the form submission to add a new exclusion entry to `Data/exclusions.csv`. Requires Security Name and Comment; End Date is optional.
    *   `/exclusions/remove` (POST): Processes requests to remove an exclusion entry from `Data/exclusions.csv` based on SecurityID and AddDate.

## HTML Templates (`templates/`)

These files define the structure and presentation of the web pages using HTML and Jinja2 templating.

*   **`base.html`:** The main layout template. Includes the common structure (doctype, head, Bootstrap CSS/JS, navbar, main content block, script block). Other templates extend this base. Linked JS: `static/js/main.js`.
*   **`index.html`:** The dashboard page. Extends `base.html`. Displays links to metric detail pages and a summary table of the latest 'Change Z-Scores' across all funds and time-series metrics. Includes links to the Securities Check page and the Get Data API simulation page.
*   **`metric_page_js.html`:** The detail page for a specific time-series metric. Extends `base.html`. Displays the metric name, latest date, and warnings for missing data. Contains a `<script id="chartData">` tag where Flask embeds JSON data. An empty `<div id="chartsArea">` serves as the container where `main.js` (via `chartRenderer.js`) dynamically renders tables and charts for each fund code. Requires `Chart.js`.
*   **`securities_page.html`:** Displays the table of security-level checks. Extends `base.html`. 
    *   Includes a search bar to filter securities by name (submits via GET).
    *   Includes filter dropdowns for static columns (powered by `securityTableFilter.js`).
    *   Contains the main table (`<table id="securities-table">`) populated with data from the `security_bp` view.
    *   Rows are styled based on Z-scores, and security IDs link to the details page.
*   **`security_details_page.html`:** Shows details for a single security. Extends `base.html`. Displays static info and provides canvas elements (`<canvas id="primarySecurityChart">`, `<canvas id="durationSecurityChart">`) and embedded JSON data (`<script id="chartJsonData">`) for JavaScript to render time-series charts (metric value, price, duration). Uses Jinja2's `|tojson` filter to safely embed Flask variables (like `metric_name` and the main `chart_data_json`) into the JavaScript/JSON context, preventing errors from special characters. Requires `Chart.js`.
*   **`fund_duration_details.html`:** Shows the security duration changes for a specific fund. Extends `base.html`. Displays a table (`<table id="fund-duration-table">`) listing securities held by the fund, sorted by their 1-day duration change. Security names link to their detail pages.
*   **`metric_page.html`:** (Potentially older/alternative version) Similar to `metric_page_js.html` but seems designed for server-side rendering of charts (e.g., using a library like `mpld3` or passing chart HTML directly) rather than client-side rendering with JavaScript.
*   **`exclusions_page.html`**: The UI for managing security exclusions. Extends `base.html`. Displays the current list of exclusions with their details (SecurityID, AddDate, EndDate, Comment) and a 'Remove' button for each. Also includes a form to add new exclusions, featuring a filterable dropdown for selecting securities and inputs for End Date (optional) and Comment (required).
*   **`get_data.html`:** The page for simulating API data retrieval. 
    *   Displays a **"Current Data File Status" table** at the top, showing each file listed in `QueryMap.csv`, its last modified time, the *latest* date found within the file (checking common date columns like 'Date', 'Position Date' and formats like YYYY-MM-DD, DD/MM/YYYY), and a list of the fund codes found (checking columns like 'Code', 'Fund Code').
    *   Provides a form to select funds and a date range for simulating API calls.
    *   Includes an area to display the status and results of the simulation.
*   **`comparison_summary_page.html`:** Displays a summary table comparing original and new data (e.g., sec_spread vs. sec_spreadSP), showing differences and allowing filtering.
*   **`comparison_details_page.html`:** Shows a detailed side-by-side chart comparing original and new time-series data for a single security.

## JavaScript Files (`static/js/`)

These files handle client-side interactivity, primarily chart rendering and table filtering.

```mermaid
graph TD
    subgraph Browser Page
        HTML[HTML Document]
        JS_Main[main.js]
    end

    subgraph Modules
        direction TB
        Mod_UI[ui/]
        Mod_Utils[utils/]
        Mod_Charts[charts/]
    end
    
    subgraph UI Module
        direction TB
        UI_ChartRender[chartRenderer.js]
        UI_TableFilter[securityTableFilter.js]
    end

    subgraph Utils Module
        direction TB
        Util_Helpers[helpers.js]
    end

    subgraph Charts Module
        direction TB
        Chart_TimeSeries[timeSeriesChart.js]
    end

    HTML -- loads --> JS_Main;
    JS_Main -- imports --> UI_ChartRender;
    JS_Main -- imports --> UI_TableFilter;
    
    Mod_UI --> UI_ChartRender;
    Mod_UI --> UI_TableFilter;
    
    Mod_Utils --> Util_Helpers;
    
    Mod_Charts --> Chart_TimeSeries;

    UI_ChartRender -- imports --> Chart_TimeSeries;
    UI_ChartRender -- imports --> Util_Helpers;

    UI_TableFilter -- uses DOM --> HTML;

    JS_Main -- detects element & calls --> UI_ChartRender;
    JS_Main -- detects element & calls --> UI_TableFilter;
    JS_Main -- detects element & calls --> UI_ChartRender; # For single chart
    
    UI_ChartRender -- uses Chart.js --> HTML; # Renders charts
    UI_TableFilter -- manipulates DOM --> HTML; # Filters table rows
```

*   **`main.js`:** The main entry point. Runs on DOMContentLoaded.
    *   Imports functions from modules.
    *   Checks for specific element IDs (`chartData`, `securities-table`, `primarySecurityChart`) to determine which page context it's in.
    *   If `#chartData` (on `metric_page_js.html`) exists, **parses the embedded structured JSON (metadata and funds data)** and calls `renderChartsAndTables` from `chartRenderer.js` **with all necessary arguments**.
    *   If `#securities-table` (on `securities_page.html`) exists, calls `initSecurityTableFilter` from `securityTableFilter.js`.
    *   If `#primarySecurityChart` and `#chartJsonData` (on `security_details_page.html`) exist, parses JSON and calls `renderSingleSecurityChart` from `chartRenderer.js`.
    *   Linked from: `base.html`.

*   **`modules/ui/chartRenderer.js`:** Handles creating the DOM elements (tables, chart canvases) and rendering charts for metric pages and single security pages.
    *   `renderChartsAndTables(...)`: **Accepts metadata (metric name, date, columns) and fund data as arguments.** Iterates through fund data, creates wrapper divs, canvas elements, metric tables (using `createMetricsTable`), and calls `createTimeSeriesChart` for each fund. Applies highlighting based on Z-scores. Handles the link to `fund_duration_details.html` if the metric is 'Duration' **(link path corrected)**.
    *   `createMetricsTable(...)`: Helper function to generate the HTML table structure for displaying calculated metrics below each chart on the metric page. **Uses passed-in column names.**
    *   `renderSingleSecurityChart(...)`: Creates a single time-series chart on the security details page using Chart.js and the provided data.
    *   Imports: `createTimeSeriesChart`, `formatNumber`.

*   **`modules/ui/securityTableFilter.js`:** Implements client-side filtering for the table on `securities_page.html`.
    *   `initSecurityTableFilter()`: Adds event listeners to filter dropdowns (`.security-filter-select`). When a filter changes, it reads all filter values, iterates through the original table rows (stored in memory), checks if a row matches the current filters, and updates the table body (`#securities-table-body`) to show only matching rows.

*   **`modules/utils/helpers.js`:** Contains general utility functions.
    *   `formatNumber(value, digits)`: Formats a number to a fixed number of decimal places, returning 'N/A' for null/undefined/NaN values.

*   **`modules/charts/timeSeriesChart.js`:** Contains the specific logic for creating and configuring time-series line charts using Chart.js.
    *   `createTimeSeriesChart(...)`: Takes canvas ID, data (labels, datasets), names, Z-score info, and creates a Chart.js line chart instance with appropriate titles, axes, legends, tooltips, and styling (including time-series X-axis). Handles updating/destroying existing charts on the canvas.

```
</file>

<file path="requirements.txt">
Flask
pandas
plotly
</file>

<file path="security_processing.py">
# This file handles the loading, processing, and analysis of security-level data.
# It assumes input CSV files are structured with one security per row and time series data
# spread across columns where headers represent dates (e.g., YYYY-MM-DD).
# Key functions:
# - `load_and_process_security_data`: Reads a wide-format CSV, identifies the security ID column,
#   static attribute columns, and date columns. It then 'melts' the data into a long format,
#   converting date strings to datetime objects and setting a MultiIndex (Date, Security ID).
# - `calculate_security_latest_metrics`: Takes the processed long-format DataFrame and calculates
#   various metrics for each security's 'Value' over time, including latest value, change,
#   historical stats (mean, max, min), and change Z-score. It also preserves the static attributes.
import pandas as pd
import os
import numpy as np
import re # For checking date-like column headers
import logging
import traceback
# --- Logging Setup ---
# Use the same log file as data_loader and metric_calculator
LOG_FILENAME = 'data_processing_errors.log'
LOG_FORMAT = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
# Get the logger for the current module
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
# Prevent adding handlers multiple times
if not logger.handlers:
    # Console Handler (INFO and above)
    ch = logging.StreamHandler()
    ch.setLevel(logging.INFO)
    ch_formatter = logging.Formatter(LOG_FORMAT)
    ch.setFormatter(ch_formatter)
    logger.addHandler(ch)
    # File Handler (WARNING and above)
    try:
        # Create log file path relative to this file's location
        log_filepath = os.path.join(os.path.dirname(__file__), '..', LOG_FILENAME)
        fh = logging.FileHandler(log_filepath, mode='a')
        fh.setLevel(logging.WARNING)
        fh_formatter = logging.Formatter(LOG_FORMAT)
        fh.setFormatter(fh_formatter)
        logger.addHandler(fh)
    except Exception as e:
        # Log to stderr if file logging setup fails
        import sys
        print(f"Error setting up file logging for security_processing: {e}", file=sys.stderr)
# --- End Logging Setup ---
DATA_FOLDER = 'Data'
def _is_date_like(column_name):
    """Check if a column name looks like a common date format.
    Recognizes formats like YYYY-MM-DD, YYYY/MM/DD, MM/DD/YYYY, M/D/YYYY, YYYYMMDD.
    """
    col_str = str(column_name)
    # Regex to match common date patterns
    # - YYYY[-/]MM[-/]DD
    # - MM[-/]DD[-/]YYYY (allows 1-2 digits for M, D and 2 or 4 for Y)
    # - YYYYMMDD
    pattern = r'^(\d{4}[-/]\d{1,2}[-/]\d{1,2}|\d{1,2}[-/]\d{1,2}[-/](\d{4}|\d{2})|\d{8})$'
    return bool(re.match(pattern, col_str))
def load_and_process_security_data(filename):
    """Loads security data, identifies static/date columns, and melts to long format.
    Args:
        filename (str): The name of the CSV file (e.g., 'sec_Spread.csv').
    Returns:
        tuple: (pandas.DataFrame, list[str])
               - Processed DataFrame in long format with MultiIndex (Date, Security ID).
               - List of identified static column names (excluding Security ID).
        Returns (pd.DataFrame(), []) if a critical error occurs during loading or processing.
    """
    filepath = os.path.join(DATA_FOLDER, filename)
    logger.info(f"Attempting to load security data from: {filepath}")
    try:
        # Read just the header to identify column types
        # Use on_bad_lines='skip' for robustness
        header_df = pd.read_csv(filepath, nrows=0, on_bad_lines='skip', encoding='utf-8', encoding_errors='replace')
        all_cols = [str(col).strip() for col in header_df.columns.tolist()] # Ensure string type and strip
        if not all_cols:
            logger.error(f"CSV file '{filename}' appears to be empty or header is missing.")
            raise ValueError(f"CSV file '{filename}' appears to be empty or header is missing.")
        # Assume first column is the Security ID
        id_col = all_cols[0]
        # Identify static and date columns dynamically using the updated _is_date_like
        static_cols = []
        date_cols = []
        for col in all_cols[1:]: # Skip the ID column
            if _is_date_like(col):
                date_cols.append(col)
            else:
                static_cols.append(col) # Already stripped
        if not date_cols:
            logger.error(f"No date-like columns found in '{filename}' using flexible patterns. Cannot process as security time series.")
            raise ValueError("No date-like columns found using flexible patterns.")
        if not id_col:
             # This case should technically not be reachable if all_cols is not empty
             logger.error(f"Could not identify the Security ID column (expected first column) in '{filename}'.")
             raise ValueError("Could not identify the Security ID column (expected first column).")
        logger.info(f"Identified ID Col: {id_col}")
        logger.info(f"Identified Static Cols: {static_cols}")
        # logger.info(f"Identified Date Cols: {date_cols[:5]}...") # Avoid excessive logging
        # Read the full data
        # Use on_bad_lines='skip' again for robustness
        df_wide = pd.read_csv(filepath, encoding='utf-8', on_bad_lines='skip', encoding_errors='replace')
        df_wide.columns = df_wide.columns.map(lambda x: str(x).strip()) # Ensure string type and strip
        # Ensure ID column name is also stripped if needed for melting (already done)
        # id_col = id_col.strip() 
        # Ensure static columns used for id_vars exist after stripping
        # The id_vars must match column names *exactly* after stripping
        id_vars = [id_col] + [col for col in static_cols if col in df_wide.columns]
        value_vars = [col for col in date_cols if col in df_wide.columns] # Ensure date columns exist
        if not value_vars:
             logger.error(f"Date columns identified in header of '{filename}' not found in data frame after loading. Columns available: {df_wide.columns.tolist()}")
             raise ValueError("Date columns identified in header not found in data frame after loading.")
        # Melt the DataFrame
        df_long = pd.melt(df_wide,
                          id_vars=id_vars,
                          value_vars=value_vars,
                          var_name='Date_Str',
                          value_name='Value')
        # Process Date and Value columns
        # Coerce errors: invalid date formats will become NaT
        df_long['Date'] = pd.to_datetime(df_long['Date_Str'], errors='coerce')
        # Check how many dates failed to parse after trying inference
        failed_date_parse_count = df_long['Date'].isna().sum()
        original_date_str_count = len(df_long['Date_Str'])
        if failed_date_parse_count > 0:
             logger.warning(f"Could not parse {failed_date_parse_count} out of {original_date_str_count} date strings in '{filename}' using pandas format inference.")
             # Keep only successfully parsed dates before proceeding
             df_long = df_long.dropna(subset=['Date']) 
             if df_long.empty:
                  logger.error(f"No dates could be parsed successfully in '{filename}'. Aborting processing for this file.")
                  return pd.DataFrame(), [] # Return empty as no valid date data
        # Coerce errors: non-numeric values will become NaN
        df_long['Value'] = pd.to_numeric(df_long['Value'], errors='coerce')
        # Drop rows where value is missing after conversion or ID is missing
        # Date NaNs were handled above
        initial_rows = len(df_long)
        df_long.dropna(subset=['Value', id_col], inplace=True)
        rows_dropped = initial_rows - len(df_long)
        if rows_dropped > 0:
             logger.warning(f"Dropped {rows_dropped} rows from '{filename}' due to missing Values or Security IDs after melting/conversion (excluding date parse failures handled separately).")
        if df_long.empty:
             logger.warning(f"DataFrame for '{filename}' is empty after melting, value conversion, and NaN drop. No data to process.")
             # Return empty df, list as per function spec
             return pd.DataFrame(), static_cols
        # Set MultiIndex
        # Use the identified (and potentially stripped) id_col name
        df_long.set_index(['Date', id_col], inplace=True)
        df_long.sort_index(inplace=True)
        # Drop the original string date column
        df_long.drop(columns=['Date_Str'], inplace=True)
        logger.info(f"Successfully loaded and processed '{filename}'. Shape: {df_long.shape}")
        # Return only the static columns that were actually found and used as id_vars (excluding the ID col itself)
        final_static_cols = [col for col in id_vars if col != id_col]
        return df_long, final_static_cols
    except FileNotFoundError:
        logger.error(f"Error: File not found at {filepath}")
        return pd.DataFrame(), [] # Return empty dataframe and list
    except ValueError as ve:
        logger.error(f"Error processing header or columns in {filename}: {ve}")
        return pd.DataFrame(), []
    except KeyError as ke:
        logger.error(f"Error melting DataFrame for {filename}, likely due to missing column used as id_var or value_var: {ke}")
        return pd.DataFrame(), []
    except Exception as e:
        logger.error(f"An unexpected error occurred loading/processing {filename}: {e}", exc_info=True)
        # traceback.print_exc() # Logger handles traceback now
        return pd.DataFrame(), []
def calculate_security_latest_metrics(df, static_cols):
    """Calculates latest metrics for each security based on its 'Value' column.
    Args:
        df (pd.DataFrame): Processed long-format DataFrame with MultiIndex (Date, Security ID).
                           Must contain a 'Value' column.
        static_cols (list[str]): List of static column names present in the DataFrame's columns (not index).
    Returns:
        pandas.DataFrame: DataFrame indexed by Security ID, including static columns and
                          calculated metrics (Latest Value, Change, Mean, Max, Min, Change Z-Score).
                          Returns an empty DataFrame if input is empty or processing fails.
    """
    if df is None or df.empty:
        logger.warning("Input DataFrame is None or empty. Cannot calculate security metrics.")
        return pd.DataFrame()
    if 'Value' not in df.columns:
        logger.error("Input DataFrame for security metrics calculation must contain a 'Value' column.")
        return pd.DataFrame()
    # Ensure index has two levels and get their names dynamically
    if df.index.nlevels != 2:
        logger.error("Input DataFrame for security metrics must have 2 index levels (Date, Security ID).")
        return pd.DataFrame()
    date_level_name, id_level_name = df.index.names
    try:
        latest_date = df.index.get_level_values(date_level_name).max()
        security_ids = df.index.get_level_values(id_level_name).unique()
        all_metrics_list = []
        for sec_id in security_ids:
            try:
                # Extract data for the current security ID
                # Use .loc for potentially cleaner selection and ensure sorting
                sec_data_hist = df.loc[(slice(None), sec_id), :].reset_index(level=id_level_name, drop=True).sort_index()
                if sec_data_hist.empty:
                     logger.debug(f"No data found for security '{sec_id}' after extraction. Skipping.")
                     continue
                sec_metrics = {} # Dictionary to hold metrics for this security
                # Add static columns first
                # Take the first available row's values, assuming they are constant per security
                # Need to handle potential multi-index if static_cols contains index names by mistake
                valid_static_cols = [col for col in static_cols if col in sec_data_hist.columns]
                if not sec_data_hist.empty:
                    static_data_row = sec_data_hist.iloc[0]
                    for static_col in valid_static_cols:
                        sec_metrics[static_col] = static_data_row.get(static_col, np.nan)
                else: # Should not happen due to check above, but safeguard
                    for static_col in valid_static_cols:
                         sec_metrics[static_col] = np.nan 
                # Ensure all expected static cols are present in the dict, even if missing from data
                for static_col in static_cols:
                     if static_col not in sec_metrics:
                          logger.warning(f"Static column '{static_col}' not found in data for security '{sec_id}', adding as NaN.")
                          sec_metrics[static_col] = np.nan
                # Calculate metrics for the 'Value' column
                value_hist = sec_data_hist['Value']
                # Calculate diff only if series has enough data
                value_change_hist = pd.Series(index=value_hist.index, dtype=np.float64)
                if not value_hist.dropna().empty and len(value_hist.dropna()) > 1:
                    value_change_hist = value_hist.diff()
                else:
                    logger.debug(f"Cannot calculate difference for 'Value' column, security '{sec_id}' due to insufficient data.")
                # Base historical stats (level) - handle potential all-NaN series
                sec_metrics['Mean'] = value_hist.mean() if value_hist.notna().any() else np.nan
                sec_metrics['Max'] = value_hist.max() if value_hist.notna().any() else np.nan
                sec_metrics['Min'] = value_hist.min() if value_hist.notna().any() else np.nan
                # Stats for change
                change_mean = value_change_hist.mean() if value_change_hist.notna().any() else np.nan
                change_std = value_change_hist.std() if value_change_hist.notna().any() else np.nan
                # Latest values
                # Check if latest_date exists in this security's specific history
                if latest_date in sec_data_hist.index:
                    latest_value = sec_data_hist.loc[latest_date, 'Value']
                    latest_change = value_change_hist.get(latest_date, np.nan)
                    sec_metrics['Latest Value'] = latest_value
                    sec_metrics['Change'] = latest_change
                    # Calculate Change Z-Score
                    change_z_score = np.nan
                    if pd.notna(latest_change) and pd.notna(change_mean) and pd.notna(change_std) and change_std != 0:
                        change_z_score = (latest_change - change_mean) / change_std
                    elif change_std == 0 and pd.notna(latest_change) and pd.notna(change_mean):
                         # Handle zero standard deviation
                         if latest_change == change_mean:
                              change_z_score = 0.0
                         else:
                             change_z_score = np.inf if latest_change > change_mean else -np.inf
                         logger.debug(f"Std dev of change for security '{sec_id}' is zero. Z-score set to {change_z_score}.")
                    else:
                         # Log if Z-score calculation failed due to NaNs
                        if not (pd.notna(latest_change) and pd.notna(change_mean) and pd.notna(change_std)):
                             logger.debug(f"Cannot calculate Z-score for security '{sec_id}' due to NaN inputs (latest_change={latest_change}, change_mean={change_mean}, change_std={change_std})")
                    sec_metrics['Change Z-Score'] = change_z_score
                else:
                    # Security missing the overall latest date
                    logger.debug(f"Security '{sec_id}' missing data for latest date {latest_date}. Setting latest metrics to NaN.")
                    sec_metrics['Latest Value'] = np.nan
                    sec_metrics['Change'] = np.nan
                    sec_metrics['Change Z-Score'] = np.nan
                # Add the security ID itself for setting the index later
                sec_metrics[id_level_name] = sec_id 
                all_metrics_list.append(sec_metrics)
            except Exception as inner_e:
                logger.error(f"Error calculating metrics for security '{sec_id}': {inner_e}", exc_info=True)
                # Optionally add a placeholder row with NaNs? Or just skip. Let's skip.
                continue
        if not all_metrics_list:
            logger.warning("No security metrics were successfully calculated. Returning empty DataFrame.")
            return pd.DataFrame()
        # Create DataFrame and set index
        latest_metrics_df = pd.DataFrame(all_metrics_list)
        # id_col_name = df.index.names[1] # Get the actual ID column name used
        if id_level_name in latest_metrics_df.columns:
             latest_metrics_df.set_index(id_level_name, inplace=True)
        else:
             logger.error(f"Security ID column '{id_level_name}' not found in the created metrics list for setting index. Columns: {latest_metrics_df.columns.tolist()}")
             # Fallback or error? Let's return as is for now, index might be RangeIndex.
        # Reorder columns to have static columns first, then calculated metrics
        metric_cols = ['Latest Value', 'Change', 'Mean', 'Max', 'Min', 'Change Z-Score']
        # Get static cols that are actually present in the final df columns (excluding the ID index)
        present_static_cols = [col for col in static_cols if col in latest_metrics_df.columns]
        final_col_order = present_static_cols + [m_col for m_col in metric_cols if m_col in latest_metrics_df.columns]
        try:
            latest_metrics_df = latest_metrics_df[final_col_order]
        except KeyError as ke:
            logger.error(f"Error reordering columns, likely a metric column is missing: {ke}. Columns available: {latest_metrics_df.columns.tolist()}")
            # Proceed with potentially incorrect order
        # Sorting (e.g., by Z-score) should be done in the view function where it's displayed
        logger.info(f"Successfully calculated metrics for {len(latest_metrics_df)} securities.")
        return latest_metrics_df
    except Exception as e:
        logger.error(f"An unexpected error occurred during security metric calculation: {e}", exc_info=True)
        # traceback.print_exc() # Logger handles traceback
        return pd.DataFrame()
</file>

<file path="static/css/style.css">
/* Basic styling for sortable table headers */
th.sortable {
    cursor: pointer;
    position: relative; /* Needed for absolute positioning of indicator */
}
/* Hide default indicator span content */
.sort-indicator {
    display: inline-block;
    width: 1em;
    height: 1em;
    margin-left: 5px;
    vertical-align: middle;
    content: "";
}
/* Style for ascending sort indicator */
th.sortable.sort-asc .sort-indicator::before {
    content: "\25B2"; /* Up arrow  */
    font-size: 0.8em;
}
/* Style for descending sort indicator */
th.sortable.sort-desc .sort-indicator::before {
    content: "\25BC"; /* Down arrow  */
    font-size: 0.8em;
}
/* Hover effect for sortable headers */
th.sortable:hover {
    background-color: #e9ecef; /* Light grey background on hover */
}
</file>

<file path="static/js/main.js">
// This file acts as the main entry point for the application's JavaScript.
// It runs after the DOM is fully loaded and performs several key initializations:
// 1. Imports necessary functions from UI modules (chart rendering, table filtering).
// 2. Checks for the presence of specific elements on the page to determine the context
//    (e.g., metric details page, securities list page, single security detail page).
// 3. If on a metric details page (`metric_page_js.html`):
//    - Finds the embedded JSON data (`<script id="chartData">`).
//    - Parses the JSON data containing historical values and calculated metrics for all funds.
//    - Calls `renderChartsAndTables` from `chartRenderer.js` to dynamically create
//      the metric tables and time-series charts for each fund code.
// 4. If on a securities list page (`securities_page.html`):
//    - Finds the main securities table (`<table id="securities-table">`).
//    - Calls `initSecurityTableFilter` from `securityTableFilter.js` to add
//      interactive filtering capabilities to the table header.
// 5. If on a single security detail page (`security_details_page.html`):
//    - Finds the chart canvas (`<canvas id="securityChart">`) and its associated JSON data (`<script id="chartJsonData">`).
//    - Parses the JSON data containing the time-series for that specific security.
//    - Calls `renderSingleSecurityChart` from `chartRenderer.js` to display the chart.
// This modular approach ensures that initialization code only runs when the corresponding HTML elements are present.
// static/js/main.js
// Purpose: Main entry point for client-side JavaScript. Initializes modules based on page content.
import { renderChartsAndTables, renderSingleSecurityChart } from './modules/ui/chartRenderer.js';
import { initSecurityTableFilter } from './modules/ui/securityTableFilter.js';
import { initTableSorter } from './modules/ui/tableSorter.js';
document.addEventListener('DOMContentLoaded', () => {
    console.log("DOM fully loaded and parsed");
    // --- Metric Page (Multiple Charts) ---    
    const chartDataElement = document.getElementById('chartData');
    const chartsArea = document.getElementById('chartsArea');
    if (chartDataElement && chartsArea) {
        console.log("Metric page detected. Initializing charts.");
        try {
            const chartDataJson = chartDataElement.textContent;
            const fullChartData = JSON.parse(chartDataJson);
            console.log('Parsed full chart data from JSON:', JSON.parse(JSON.stringify(fullChartData)));
            if (fullChartData && fullChartData.metadata && fullChartData.funds && Object.keys(fullChartData.funds).length > 0) {
                const metadata = fullChartData.metadata;
                const fundsData = fullChartData.funds; 
                console.log("Extracted Metadata:", metadata);
                console.log("Extracted Funds Data:", fundsData);
                renderChartsAndTables(
                    chartsArea,
                    fundsData,
                    metadata.metric_name,
                    metadata.latest_date,
                    metadata.fund_col_names,
                    metadata.benchmark_col_name
                );
            } else {
                console.error('Parsed chart data is missing expected structure (metadata/funds) or funds object is empty:', fullChartData);
                chartsArea.innerHTML = '<div class="alert alert-danger">Error: Invalid data structure received from backend. Check console.</div>';
            }
        } catch (e) {
            console.error('Error parsing chart data JSON or calling renderer:', e);
            chartsArea.innerHTML = '<div class="alert alert-danger">Error loading chart data. Please check console for details.</div>';
        }
    } else {
        // console.log("Chart data element or charts area not found, skipping multi-chart rendering.");
    }
    // --- Securities Summary Page (Filterable & Sortable Table) ---
    const securitiesTable = document.getElementById('securities-table');
    if (securitiesTable) {
        console.log("Securities page table detected. Initializing filters and sorter.");
        initSecurityTableFilter('securities-table');
        initTableSorter('securities-table');
    } else {
        // console.log("Securities table not found, skipping table features initialization.");
    }
    // --- Comparison Summary Page (Filterable & Sortable Table) ---
    const comparisonTable = document.getElementById('comparison-table');
    if (comparisonTable) {
        console.log("Comparison page table detected. Initializing sorter.");
        // Note: Filters are handled server-side via form submission for this table
        initTableSorter('comparison-table'); // Enable client-side sorting
    }
    // --- Security Details Page (Single Chart) ---
    const securityChartCanvas = document.getElementById('primarySecurityChart');
    const securityJsonDataElement = document.getElementById('chartJsonData');
    if (securityChartCanvas && securityJsonDataElement) {
        console.log("Security details page detected. Initializing single chart.");
        try {
            const securityChartData = JSON.parse(securityJsonDataElement.textContent);
            if (securityChartData && securityChartData.primary && securityChartData.primary.labels && securityChartData.primary.datasets) {
                renderSingleSecurityChart(
                    securityChartCanvas.id,
                    securityChartData.primary.labels,
                    securityChartData.primary.datasets,
                    securityChartData.security_id + ' - ' + securityChartData.metric_name
                );
                const durationChartCanvas = document.getElementById('durationSecurityChart');
                if(durationChartCanvas && securityChartData.duration && securityChartData.duration.labels && securityChartData.duration.datasets) {
                    renderSingleSecurityChart(
                        durationChartCanvas.id,
                        securityChartData.duration.labels,
                        securityChartData.duration.datasets,
                        securityChartData.security_id + ' - Duration'
                    );
                }
            } else {
                console.warn('Security chart JSON data is incomplete or invalid.', securityChartData);
            }
        } catch (error) {
            console.error('Error parsing security chart data or rendering chart:', error);
        }
    } else {
       // console.log("Security chart canvas or JSON data element not found, skipping single chart rendering.");
    }
    // Add any other global initializations here
});
</file>

<file path="static/js/modules/charts/timeSeriesChart.js">
// This file contains the specific logic for creating and configuring time-series line charts
// using the Chart.js library. It's designed to be reusable for generating consistent charts
// across different metrics and funds.
// static/js/modules/charts/timeSeriesChart.js
// Encapsulates Chart.js configuration and rendering for multiple time series datasets
/**
 * Creates and renders a time series chart using Chart.js.
 * @param {string} canvasId - The ID of the canvas element.
 * @param {object} chartData - Data object containing labels, multiple datasets, metrics.
 * @param {string} metricName - Name of the overall metric (e.g., Duration).
 * @param {string} fundCode - Code of the specific fund.
 * @param {number | null} maxZScore - The maximum absolute Z-score for this fund (used in title).
 * @param {boolean} isMissingLatest - Flag indicating if the latest point is missing for any spread.
 */
export function createTimeSeriesChart(canvasId, chartData, metricName, fundCode, maxZScore, isMissingLatest) {
    const ctx = document.getElementById(canvasId).getContext('2d');
    if (!ctx) {
        console.error(`Failed to get 2D context for canvas ID: ${canvasId}`);
        return; // Exit if canvas context is not available
    }
    // --- Prepare Chart Title --- 
    let titleSuffix = maxZScore !== null ? `(Max Spread Z: ${maxZScore.toFixed(2)})` : '(Z-Score N/A)';
    if (isMissingLatest) {
        titleSuffix = "(MISSING LATEST DATA)";
    }
    const chartTitle = `${metricName} for ${fundCode} ${titleSuffix}`;
    // --- Prepare Chart Data & Styling --- 
    const datasets = chartData.datasets.map((ds, index) => {
        const isBenchmark = ds.label.includes('Benchmark'); // Basic check, refine if needed
        const isLastDataset = index === chartData.datasets.length - 1; // Check if it's the benchmark dataset based on order from app.py
        return {
            ...ds,
            // Style points - highlight last point for non-benchmark lines
            pointRadius: (context) => {
                const isLastPoint = context.dataIndex === (ds.data.length - 1);
                // Only show large radius for last point of non-benchmark datasets
                return isLastPoint && !isLastDataset ? 6 : 0;
            },
            pointHoverRadius: (context) => {
                const isLastPoint = context.dataIndex === (ds.data.length - 1);
                return isLastPoint && !isLastDataset ? 8 : 5;
            },
            pointBackgroundColor: isLastDataset ? 'darkgrey' : ds.borderColor, // Use border color for fund points, grey for benchmark
            borderWidth: isLastDataset ? 2 : 1.5, // Slightly thicker benchmark line
        };
    });
    // --- Chart Configuration --- 
    const config = {
        type: 'line',
        data: {
            labels: chartData.labels, // Dates as strings
            datasets: datasets // Now includes multiple fund series + benchmark
        },
        options: {
            responsive: true,
            maintainAspectRatio: false, 
            plugins: {
                title: { display: true, text: chartTitle, font: { size: 16 } },
                legend: { position: 'top' },
                tooltip: { 
                    mode: 'index', 
                    intersect: false, 
                    // Optional: Customize tooltip further if needed
                }
            },
            hover: { mode: 'nearest', intersect: true },
            scales: {
                x: {
                    type: 'time',
                    time: {
                        unit: 'day',
                        tooltipFormat: 'MMM dd, yyyy',
                        displayFormats: { day: 'MMM dd', week: 'MMM dd yyyy', month: 'MMM yyyy' }
                    },
                    title: { display: true, text: 'Date' }
                },
                y: {
                    display: true,
                    title: { display: true, text: metricName },
                    // Dynamic scaling based on *all* datasets
                    suggestedMin: Math.min(...datasets.flatMap(ds => ds.data.filter(d => d !== null && !isNaN(d)))),
                    suggestedMax: Math.max(...datasets.flatMap(ds => ds.data.filter(d => d !== null && !isNaN(d))))
                }
            }
        }
    };
    // --- Create Chart Instance --- 
    // Check if a chart instance already exists on the canvas and destroy it
    let existingChart = Chart.getChart(canvasId);
    if (existingChart) {
        existingChart.destroy();
    }
    new Chart(ctx, config);
    console.log(`Chart created for ${fundCode} on ${canvasId}`);
}
</file>

<file path="static/js/modules/ui/chartRenderer.js">
// This file is responsible for dynamically creating and rendering the user interface elements
// related to charts and associated metric tables within the application.
// It separates the logic for generating the visual components from the main application flow.
// static/js/modules/ui/chartRenderer.js
// Handles creating DOM elements for charts and tables
import { createTimeSeriesChart } from '../charts/timeSeriesChart.js';
import { formatNumber } from '../utils/helpers.js';
/**
 * Renders charts and metric tables into the specified container.
 * @param {HTMLElement} container - The parent element to render into.
 * @param {object} chartsData - The chart data object from Flask.
 * @param {string} metricName - The name of the metric being displayed.
 * @param {string} latestDate - The latest date string.
 * @param {string[]} fundColNames - List of fund value column names.
 * @param {string} benchmarkColName - Name of the benchmark value column.
 */
export function renderChartsAndTables(container, chartsData, metricName, latestDate, fundColNames, benchmarkColName) {
    console.log("[chartRenderer] Rendering charts and tables for metric:", metricName, "Latest Date:", latestDate);
    console.log("[chartRenderer] Received Data:", JSON.parse(JSON.stringify(chartsData))); // Deep copy for logging
    console.log("[chartRenderer] Fund Column Names:", fundColNames);
    console.log("[chartRenderer] Benchmark Column Name:", benchmarkColName);
    container.innerHTML = ''; // Clear previous content
    if (!chartsData || Object.keys(chartsData).length === 0) {
        console.warn("[chartRenderer] No data available for metric:", metricName);
        container.innerHTML = '<p>No data available for this metric.</p>';
        return;
    }
    // Iterate through each fund's data (which is already sorted by max Change Z-score)
    for (const [fundCode, data] of Object.entries(chartsData)) {
        console.log(`[chartRenderer] Processing fund: ${fundCode}`);
        const metrics = data.metrics; // This is now the flattened metrics object
        // --- Use passed-in names, not names derived from potentially incomplete 'data' object ---
        const fundColumns = fundColNames; 
        const benchmarkColumn = benchmarkColName;
        console.log(`[chartRenderer] Fund ${fundCode} - Using Fund Columns:`, fundColumns);
        console.log(`[chartRenderer] Fund ${fundCode} - Using Benchmark Column:`, benchmarkColumn);
        console.log(`[chartRenderer] Fund ${fundCode} - Metrics Object:`, metrics);
        console.log(`[chartRenderer] Fund ${fundCode} - Full Data Object:`, JSON.parse(JSON.stringify(data))); // Deep copy
        // Find the maximum absolute *Change Z-Score* across all original columns for this fund
        let maxAbsZScore = 0;
        let zScoreForTitle = null; // Use the Z-score corresponding to the max absolute value
        if (metrics) {
            // Combine benchmark (if exists) and fund columns for checking Z-scores
            const colsToCheck = [];
            if (benchmarkColumn) colsToCheck.push(benchmarkColumn);
            if (fundColumns && Array.isArray(fundColumns)) colsToCheck.push(...fundColumns);
            console.log(`[chartRenderer] Fund ${fundCode} - Columns to check for Z-Score:`, colsToCheck);
            colsToCheck.forEach(colName => {
                if (!colName) return; // Skip null/empty column names
                const zScoreKey = `${colName} Change Z-Score`; 
                const zScore = metrics[zScoreKey];
                // console.log(`[chartRenderer] Fund ${fundCode} - Checking Z-Score for column '${colName}' (key: '${zScoreKey}'):`, zScore);
                 if (zScore !== null && typeof zScore !== 'undefined' && !isNaN(zScore)) {
                     const absZ = Math.abs(zScore);
                     if (absZ > maxAbsZScore) {
                         maxAbsZScore = absZ;
                         zScoreForTitle = zScore; // Store this specific Z-score
                     }
                 }
            });
            console.log(`[chartRenderer] Fund ${fundCode} - Max Abs Z-Score found: ${maxAbsZScore}, Specific Z-Score for title: ${zScoreForTitle}`);
        } else {
            console.warn(`[chartRenderer] Fund ${fundCode} - Metrics object is missing or null.`);
        }
        // Determine CSS class based on the maximum Z-score for highlighting the whole section
        let zClass = '';
        if (maxAbsZScore > 3) { 
            zClass = 'very-high-z';
        } else if (maxAbsZScore > 2) {
            zClass = 'high-z';
        }
        console.log(`[chartRenderer] Fund ${fundCode} - Assigned Z-Class: '${zClass}'`);
        // Create wrapper div
        const wrapper = document.createElement('div');
        wrapper.className = `chart-container-wrapper ${zClass}`;
        wrapper.id = `chart-wrapper-${fundCode}`;
        // Add Duration Details Link (if applicable)
        if (metricName === 'Duration') {
            console.log(`[chartRenderer] Fund ${fundCode} - Adding Duration details link.`);
            const linkDiv = document.createElement('div');
            linkDiv.className = 'mb-2 text-right'; // Add some margin below
            const link = document.createElement('a');
            // Use template literal correctly for URL generation
            link.href = `/fund_duration_details/${fundCode}`; // Corrected URL path
            link.className = 'btn btn-info btn-sm';
            link.textContent = `View Security Duration Changes for ${fundCode} `;
            linkDiv.appendChild(link);
            wrapper.appendChild(linkDiv); // Add link *before* chart
        }
        // Create Chart Canvas
        const canvas = document.createElement('canvas');
        canvas.id = `chart-${fundCode}`;
        canvas.className = 'chart-canvas';
        wrapper.appendChild(canvas);
        console.log(`[chartRenderer] Fund ${fundCode} - Created canvas with id: ${canvas.id}`);
        // Create Metrics Table using the *rewritten* function
        // Pass the specific fund/benchmark names from the *passed-in arguments*
        console.log(`[chartRenderer] Fund ${fundCode} - Calling createMetricsTable with:`, metrics, latestDate, fundColumns, benchmarkColumn, zClass);
        const table = createMetricsTable(metrics, latestDate, fundColumns, benchmarkColumn, zClass);
        wrapper.appendChild(table);
        console.log(`[chartRenderer] Fund ${fundCode} - Appended metrics table.`);
        container.appendChild(wrapper);
        console.log(`[chartRenderer] Fund ${fundCode} - Appended wrapper to container.`);
        // Render Chart
        // Use setTimeout to ensure the canvas is in the DOM and sized before drawing
        setTimeout(() => {
            console.log(`[chartRenderer] Fund ${fundCode} - Preparing to render chart in setTimeout.`);
             if (canvas.getContext('2d')) {
                 console.log(`[chartRenderer] Fund ${fundCode} - Canvas context obtained. Calling createTimeSeriesChart with:`, {
                    canvasId: canvas.id,
                    data: JSON.parse(JSON.stringify(data)), // Log deep copy
                    metricName: metricName,
                    fundCode: fundCode,
                    zScoreForTitle: zScoreForTitle,
                    is_missing_latest: data.is_missing_latest
                 });
                 // Pass zScoreForTitle (which is the max abs Change Z-Score across columns)
                 createTimeSeriesChart(canvas.id, data, metricName, fundCode, zScoreForTitle, data.is_missing_latest);
                 console.log(`[chartRenderer] Fund ${fundCode} - createTimeSeriesChart call finished.`);
            } else {
                console.error(`[chartRenderer] Fund ${fundCode} - Could not get 2D context for canvas ${canvas.id}`);
                const errorP = document.createElement('p');
                errorP.textContent = 'Error rendering chart.';
                errorP.className = 'text-danger';
                canvas.parentNode.replaceChild(errorP, canvas); // Replace canvas with error message
            }
        }, 0); 
    }
    console.log("[chartRenderer] Finished rendering all charts and tables.");
}
/**
 * Creates the *simplified* HTML table element displaying metrics for each original column.
 * @param {object | null} metrics - Flattened metrics object from Flask for a specific fund code.
 * @param {string} latestDate - The latest date string.
 * @param {string[]} fundColNames - List of fund value column names for this metric.
 * @param {string | null} benchmarkColName - Name of the benchmark value column for this metric (can be null).
 * @param {string} zClass - CSS class based on max Z-score (used for table highlight).
 * @returns {HTMLTableElement} The created table element.
 */
function createMetricsTable(metrics, latestDate, fundColNames, benchmarkColName, zClass) {
    console.log("[createMetricsTable] Creating table. Metrics:", metrics, "Latest Date:", latestDate, "Funds:", fundColNames, "Bench:", benchmarkColName, "zClass:", zClass);
    const table = document.createElement('table');
    // Apply overall highlight based on max Z across columns
    table.className = `table table-sm table-bordered metrics-table ${zClass}`;
    const thead = table.createTHead();
    const headerRow = thead.insertRow();
    // Define the new, simpler headers
    headerRow.innerHTML = `
        <th>Column</th>
        <th>Latest Value (${latestDate})</th>
        <th>Change</th>
        <th>Mean</th> 
        <th>Max</th> 
        <th>Min</th> 
        <th>Change Z-Score</th> 
    `;
    const tbody = table.createTBody();
    if (!metrics) {
        console.warn("[createMetricsTable] Metrics object is null or undefined.");
        const row = tbody.insertRow();
        const cell = row.insertCell();
        cell.colSpan = 7; // Match new header count
        cell.textContent = 'Metrics not available.';
        return table;
    }
    // Combine benchmark and fund columns for iteration
    const allColumns = [];
    if (benchmarkColName) allColumns.push(benchmarkColName);
    if (fundColNames && Array.isArray(fundColNames)) allColumns.push(...fundColNames);
    console.log("[createMetricsTable] Columns to create rows for:", allColumns);
    // Create one row per original column
    allColumns.forEach(colName => {
        if (!colName) {
            console.warn("[createMetricsTable] Skipping null/empty column name.");
            return; // Skip if column name is somehow empty
        }
        console.log(`[createMetricsTable] Creating row for column: ${colName}`);
        // Define the keys to access the flattened metrics object
        const latestValKey = `${colName} Latest Value`;
        const changeKey = `${colName} Change`;
        const meanKey = `${colName} Mean`;
        const maxKey = `${colName} Max`;
        const minKey = `${colName} Min`;
        const zScoreKey = `${colName} Change Z-Score`;
        const row = tbody.insertRow();
        // Determine cell class for Z-score highlighting on this specific row
        const zScoreValue = metrics[zScoreKey];
        let zScoreClass = '';
        // console.log(`[createMetricsTable] Column ${colName} - Z-Score Value: ${zScoreValue}`);
        if (zScoreValue !== null && typeof zScoreValue !== 'undefined' && !isNaN(zScoreValue)) {
             const absZ = Math.abs(zScoreValue);
             if (absZ > 3) { zScoreClass = 'very-high-z'; }
             else if (absZ > 2) { zScoreClass = 'high-z'; }
        }
        // Populate the row cells
        row.innerHTML = `
            <td>${colName}</td>
            <td>${formatNumber(metrics[latestValKey])}</td>
            <td>${formatNumber(metrics[changeKey])}</td>
            <td>${formatNumber(metrics[meanKey])}</td>
            <td>${formatNumber(metrics[maxKey])}</td>
            <td>${formatNumber(metrics[minKey])}</td>
            <td class="${zScoreClass}">${formatNumber(metrics[zScoreKey])}</td> 
        `; // Apply Z-score class only to the Z-score cell
        // console.log(`[createMetricsTable] Row HTML for ${colName}:`, row.innerHTML);
    });
    console.log("[createMetricsTable] Finished creating table.");
    return table;
} 
/**
 * Renders a single time series chart for a specific security.
 * @param {string} canvasId - The ID of the canvas element.
 * @param {object} chartData - The chart data (labels, datasets) from Flask.
 * @param {string} securityId - The ID of the security.
 * @param {string} metricName - The name of the metric.
 */
export function renderSingleSecurityChart(canvasId, chartData, securityId, metricName) {
    const ctx = document.getElementById(canvasId);
    if (!ctx) {
        console.error(`Canvas element with ID '${canvasId}' not found.`);
        return;
    }
    if (!chartData || !chartData.labels || !chartData.datasets) {
        console.error('Invalid or incomplete chart data provided.');
        ctx.parentElement.innerHTML = '<p class="text-danger">Error: Invalid chart data.</p>';
        return;
    }
    try {
        new Chart(ctx, {
            type: 'line',
            data: {
                labels: chartData.labels,
                datasets: chartData.datasets
            },
            options: {
                responsive: true,
                maintainAspectRatio: false,
                plugins: {
                    title: {
                        display: true,
                        text: `${securityId} - ${metricName} Time Series`,
                        font: { size: 16 }
                    },
                    legend: {
                        position: 'top',
                    }
                },
                scales: {
                    x: {
                        title: {
                            display: true,
                            text: 'Date'
                        }
                    },
                    y: {
                        type: 'linear',
                        display: true,
                        position: 'left',
                        title: {
                            display: true,
                            text: `${metricName} Value`
                        },
                        beginAtZero: false,
                        ticks: {
                            maxTicksLimit: 8
                        }
                    },
                    y1: {
                        type: 'linear',
                        display: true,
                        position: 'right',
                        title: {
                            display: true,
                            text: 'Price'
                        },
                        grid: {
                            drawOnChartArea: false,
                        },
                        beginAtZero: false
                    }
                },
                interaction: {
                    intersect: false,
                    mode: 'index',
                },
            }
        });
        console.log(`Chart rendered for ${securityId} - ${metricName}`);
    } catch (error) {
        console.error(`Error creating chart for ${securityId} - ${metricName}:`, error);
        ctx.parentElement.innerHTML = '<p class="text-danger">Error rendering chart.</p>';
    }
}
</file>

<file path="static/js/modules/ui/securityTableFilter.js">
// This file implements client-side filtering for the HTML table displaying security-level metrics.
// It enhances the user experience by allowing interactive filtering based on the values
// in specific static columns (e.g., Sector, Rating) without requiring a page reload.
// static/js/modules/ui/securityTableFilter.js
// This module handles client-side filtering for the securities table.
/**
 * Initializes the filtering functionality for the securities table.
 */
export function initSecurityTableFilter() {
    const filterSelects = document.querySelectorAll('.security-filter-select');
    const tableBody = document.getElementById('securities-table-body');
    if (!tableBody || filterSelects.length === 0) {
        console.log("Security table body or filter selects not found. Filtering disabled.");
        return; // Exit if necessary elements aren't present
    }
    // Store all original rows. Use querySelectorAll for robustness.
    const originalRows = Array.from(tableBody.querySelectorAll('tr'));
    if (originalRows.length === 0) {
        console.log("No rows found in the table body.");
        return; // Exit if no data rows
    }
    // Function to get current filter values
    const getCurrentFilters = () => {
        const filters = {};
        filterSelects.forEach(select => {
            if (select.value) { // Only add if a filter is selected (not 'All')
                filters[select.dataset.column] = select.value;
            }
        });
        return filters;
    };
    // Function to perform filtering and update the table
    const applyFilters = () => {
        const currentFilters = getCurrentFilters();
        const filterKeys = Object.keys(currentFilters);
        // Clear current table body content efficiently
        tableBody.innerHTML = ''; 
        originalRows.forEach(row => {
            let matches = true;
            // Get all cells in the current row
            const cells = row.querySelectorAll('td');
            // Assuming the order of cells matches the order of `column_order` from Python
            // We need a way to map filter column names to cell indices
            // Let's get the header names to map column names to indices
            const headerCells = document.querySelectorAll('#securities-table th');
            const columnNameToIndexMap = {};
            headerCells.forEach((th, index) => {
                columnNameToIndexMap[th.textContent.trim()] = index;
            });
            for (const column of filterKeys) {
                const columnIndex = columnNameToIndexMap[column];
                if (columnIndex !== undefined) {
                    const cellValue = cells[columnIndex]?.textContent.trim(); // Use optional chaining
                    // Strict comparison - ensure types match if needed, or use == for type coercion
                    if (cellValue !== currentFilters[column]) {
                        matches = false;
                        break; // No need to check other filters for this row
                    }
                }
                 else {
                      console.warn(`Column "${column}" not found in table header for filtering.`);
                      // Decide how to handle: skip filter, always fail match? Let's skip filter for robustness.
                 }
            }
            if (matches) {
                // Append the row if it matches all active filters
                tableBody.appendChild(row.cloneNode(true)); // Append a clone to avoid issues
            }
        });
        // Display a message if no rows match
        if (tableBody.children.length === 0) {
             const noMatchRow = tableBody.insertRow();
             const cell = noMatchRow.insertCell();
             cell.colSpan = headerCells.length; // Span across all columns
             cell.textContent = 'No securities match the current filter criteria.';
             cell.style.textAlign = 'center';
             cell.style.fontStyle = 'italic';
        }
    };
    // Add event listeners to all filter dropdowns
    filterSelects.forEach(select => {
        select.addEventListener('change', applyFilters);
    });
    console.log("Security table filtering initialized.");
}
</file>

<file path="static/js/modules/ui/tableSorter.js">
// static/js/modules/ui/tableSorter.js
// Purpose: Handles client-side sorting for HTML tables.
/**
 * Initializes sorting functionality for a specified table.
 * @param {string} tableId The ID of the table element to make sortable.
 */
export function initTableSorter(tableId) {
    const table = document.getElementById(tableId);
    if (!table) {
        console.warn(`Table sorter: Table with ID '${tableId}' not found.`);
        return;
    }
    const headers = table.querySelectorAll('thead th.sortable');
    const tbody = table.querySelector('tbody');
    if (!tbody) {
        console.warn(`Table sorter: Table with ID '${tableId}' does not have a tbody.`);
        return;
    }
    headers.forEach(header => {
        header.addEventListener('click', () => {
            // Get column name from data attribute
            const columnName = header.dataset.columnName;
            const currentIsAscending = header.classList.contains('sort-asc');
            const direction = currentIsAscending ? -1 : 1; // -1 for desc, 1 for asc
            // Find the index of the clicked column
            const columnIndex = Array.from(header.parentNode.children).indexOf(header);
            // Remove sorting indicators from other columns
            headers.forEach(h => {
                if (h !== header) {
                  h.classList.remove('sort-asc', 'sort-desc');
                }
            });
            // Set sorting indicator for the current column
            header.classList.toggle('sort-asc', !currentIsAscending);
            header.classList.toggle('sort-desc', currentIsAscending);
            // Sort the rows, passing the column name
            sortRows(tbody, columnIndex, direction, columnName);
        });
    });
}
/**
 * Sorts the rows within a table body.
 * @param {HTMLElement} tbody The table body element containing the rows.
 * @param {number} columnIndex The index of the column to sort by.
 * @param {number} direction 1 for ascending, -1 for descending.
 * @param {string} columnName The name of the column being sorted.
 */
function sortRows(tbody, columnIndex, direction, columnName) {
    const rows = Array.from(tbody.querySelectorAll('tr'));
    // Get the correct comparison function, passing the column name
    const compareFunction = getCompareFunction(rows, columnIndex, columnName);
    // Sort the rows
    rows.sort((rowA, rowB) => {
        const cellA = rowA.children[columnIndex];
        const cellB = rowB.children[columnIndex];
        // Use data-value attribute primarily, fall back to textContent
        const valueA = cellA?.dataset.value ?? cellA?.textContent?.trim() ?? '';
        const valueB = cellB?.dataset.value ?? cellB?.textContent?.trim() ?? '';
        return compareFunction(valueA, valueB) * direction;
    });
    // Re-append sorted rows
    tbody.append(...rows); // More efficient way to re-append
}
/**
 * Determines the appropriate comparison function (numeric or text) based on column content.
 * @param {Array<HTMLElement>} rows Array of table row elements.
 * @param {number} columnIndex The index of the column to check.
 * @param {string} columnName The name of the column being sorted.
 * @returns {function(string, string): number} The comparison function.
 */
function getCompareFunction(rows, columnIndex, columnName) {
    // Check the first few rows (up to 5 data rows) to guess the data type
    let isNumeric = true;
    for (let i = 0; i < Math.min(rows.length, 5); i++) {
        const cell = rows[i].children[columnIndex];
        // Use data-value attribute primarily for checking type
        const value = cell?.dataset.value ?? cell?.textContent?.trim() ?? '';
        // Allow empty strings in numeric columns, but if we find something non-numeric (and not empty), switch to text sort
        if (value !== '' && isNaN(Number(value.replace(/,/g, '')))) {
            isNumeric = false;
            break;
        }
    }
    if (isNumeric) {
        // Check if it's the special column 'Change Z-Score'
        if (columnName === 'Change Z-Score') {
             // Use absolute value for comparison
            return (a, b) => {
                const numA = Math.abs(parseNumber(a));
                const numB = Math.abs(parseNumber(b));
                return numA - numB;
            };
        } else {
            // Standard numeric comparison for other numeric columns
            return (a, b) => {
                const numA = parseNumber(a);
                const numB = parseNumber(b);
                return numA - numB;
            };
        }
    } else {
        // Case-insensitive text comparison
        return (a, b) => a.toLowerCase().localeCompare(b.toLowerCase());
    }
}
/**
 * Helper to parse number, handling empty strings and NaN.
 * Returns -Infinity for values that cannot be parsed as numbers or are empty,
 * ensuring they sort consistently.
 * @param {string} val The string value to parse.
 * @returns {number}
 */
function parseNumber(val) {
    if (val === null || val === undefined || val.trim() === '') {
        return -Infinity; // Treat empty/null/undefined as very small
    }
    const num = Number(val.replace(/,/g, ''));
    // Treat non-numeric as very small. Math.abs(-Infinity) is Infinity, which might be desired
    // when sorting absolute values (non-numbers/empty go to the end when ascending by abs value).
    return isNaN(num) ? -Infinity : num;
}
</file>

<file path="static/js/modules/utils/helpers.js">
// This file contains general JavaScript utility functions that can be reused across different modules.
// It helps keep common tasks, like formatting numbers for display, consistent and DRY (Don't Repeat Yourself).
// static/js/modules/utils/helpers.js
// Utility functions
/**
 * Formats a number for display, handling null/undefined.
 * @param {number | null | undefined} value - The number to format.
 * @param {number} [digits=2] - Number of decimal places.
 * @returns {string} Formatted number or 'N/A'.
 */
export function formatNumber(value, digits = 2) {
    if (value === null || typeof value === 'undefined' || isNaN(value)) {
        return 'N/A';
    }
    return Number(value).toFixed(digits);
}
</file>

<file path="templates/base.html">
<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>{% block title %}Data Checker{% endblock %}</title>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/css/bootstrap.min.css" rel="stylesheet">
    <style>
        /* Basic styles - can be expanded */
        body { padding-top: 5rem; }
        .sticky-top {
            top: 56px; /* Adjust based on navbar height */
        }
        /* Add any custom global styles here */
        .table-danger {
            background-color: #f8d7da !important; /* Red for high Z */
        }
        .table-warning {
            background-color: #fff3cd !important; /* Yellow for medium Z */
        }
    </style>
</head>
<body>
    <nav class="navbar navbar-expand-md navbar-dark bg-dark fixed-top">
        <div class="container-fluid">
            <a class="navbar-brand" href="{{ url_for('main.index') }}">Data Checker</a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarCollapse">
                <ul class="navbar-nav me-auto mb-2 mb-md-0">
                    <li class="nav-item">
                        <a class="nav-link" href="{{ url_for('main.index') }}">Time Series Dashboard</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="{{ url_for('security.securities_page') }}">Securities Check</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="{{ url_for('comparison_bp.summary') }}">Spread Comparison</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>
    <main role="main" class="container">
        {% block content %}
        {# Page specific content will go here #}
        {% endblock %}
    </main>
    <!-- Bootstrap Bundle with Popper -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/js/bootstrap.bundle.min.js"></script>
    <!-- Load Main Application JS -->
    <script type="module" src="{{ url_for('static', filename='js/main.js') }}"></script>
    {% block scripts %}
    {# Page specific scripts can go here #}
    {% endblock %}
</body>
</html>
</file>

<file path="templates/comparison_details_page.html">
{% extends "base.html" %}
{% block title %}Spread Comparison Details: {{ security_name }}{% endblock %}
{% block content %}
<div class="container mt-4">
    <nav aria-label="breadcrumb">
        <ol class="breadcrumb">
            <li class="breadcrumb-item"><a href="{{ url_for('comparison_bp.summary') }}">Comparison Summary</a></li>
            <li class="breadcrumb-item active" aria-current="page">{{ security_name }} ({{ security_id }})</li>
        </ol>
    </nav>
    <h1>Spread Comparison Details: {{ security_name }}</h1>
    <h5 class="text-muted">Security ID: {{ security_id }}</h5>
    <div class="row mt-4 mb-4">
        <div class="col-md-6">
            <h2>Comparison Statistics</h2>
            <ul class="list-group">
                <li class="list-group-item d-flex justify-content-between align-items-center">
                    Level Correlation
                    <span class="badge bg-primary rounded-pill">{{ "%.4f"|format(stats.Level_Correlation) if stats.Level_Correlation is not none else 'N/A' }}</span>
                </li>
                <li class="list-group-item d-flex justify-content-between align-items-center">
                    Change Correlation
                    <span class="badge bg-primary rounded-pill">{{ "%.4f"|format(stats.Change_Correlation) if stats.Change_Correlation is not none else 'N/A' }}</span>
                </li>
                <li class="list-group-item d-flex justify-content-between align-items-center">
                    Mean Absolute Difference
                    <span class="badge bg-secondary rounded-pill">{{ "%.2f"|format(stats.Mean_Abs_Diff) if stats.Mean_Abs_Diff is not none else 'N/A' }}</span>
                </li>
                <li class="list-group-item d-flex justify-content-between align-items-center">
                    Max Absolute Difference
                    <span class="badge bg-secondary rounded-pill">{{ "%.2f"|format(stats.Max_Abs_Diff) if stats.Max_Abs_Diff is not none else 'N/A' }}</span>
                </li>
                 <li class="list-group-item d-flex justify-content-between align-items-center">
                    Data Points (Original)
                    <span class="badge bg-info rounded-pill">{{ stats.Total_Points - stats.NaN_Count_Orig }} / {{ stats.Total_Points }}</span>
                </li>
                 <li class="list-group-item d-flex justify-content-between align-items-center">
                    Data Points (New)
                    <span class="badge bg-info rounded-pill">{{ stats.Total_Points - stats.NaN_Count_New }} / {{ stats.Total_Points }}</span>
                </li>
                 <li class="list-group-item d-flex justify-content-between align-items-center">
                    Same Date Range?
                    <span class="badge {{ 'bg-success' if stats.Same_Date_Range else 'bg-warning' }} rounded-pill">{{ 'Yes' if stats.Same_Date_Range else 'No' }}</span>
                </li>
            </ul>
        </div>
        {# Placeholder for additional stats or info if needed #}
        {# <div class="col-md-6">
             <h2>Other Info</h2>
        </div> #}
    </div>
    <h2>Time Series Comparison</h2>
    <p class="text-muted">Overlayed credit spreads from Original (sec_spread) and New (sec_spreadSP) datasets.</p>
    <div>
        <canvas id="comparisonChart"></canvas>
    </div>
    {# Embed chart data as JSON for JavaScript #}
    <script type="application/json" id="comparisonChartData">
        {{ chart_data | tojson | safe }}
    </script>
</div>
{% endblock %}
{% block scripts %}
{{ super() }}
{# We need Chart.js - ensure it's included in base.html or here #}
<script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
{# We also need the JS to render this specific chart #}
<script>
    document.addEventListener('DOMContentLoaded', function() {
        const chartDataElement = document.getElementById('comparisonChartData');
        const comparisonChartCanvas = document.getElementById('comparisonChart');
        if (chartDataElement && comparisonChartCanvas) {
            try {
                const chartData = JSON.parse(chartDataElement.textContent);
                const ctx = comparisonChartCanvas.getContext('2d');
                // --- REVERTING TO SIMPLE CONFIG --- 
                // Remove scriptable options for now
                /*
                // Define base colors (assuming these are the intended original colors)
                const baseColors = [
                    'rgba(13, 110, 253, 1)', // Bootstrap Blue (or COLOR_PALETTE[0])
                    'rgba(220, 53, 69, 1)'  // Bootstrap Red (or COLOR_PALETTE[1])
                ];
                const gapColor = 'rgba(150, 150, 150, 0.7)'; // Semi-transparent gray for gaps
                // Prepare datasets with scriptable borderColor
                console.log("Preparing datasets with scriptable colors...");
                const datasetsWithScriptableColors = chartData.datasets.map((dataset, index) => {
                     console.log(`Mapping dataset index: ${index}`);
                     return {
                        ...dataset, // Keep original label, data, tension, spanGaps etc.
                        borderColor: context => {
                            // Check if drawing a line segment and context is valid
                            if (context.type === 'segment' && context.p0 && context.p1) {
                                // --- NEW APPROACH: Check the 'skip' property of the points (safely) --- 
                                // Ensure p0 and p1 exist before accessing skip
                                const p0skip = context.p0 ? context.p0.skip : false;
                                const p1skip = context.p1 ? context.p1.skip : false;
                                if (p0skip || p1skip) {
                                     // --- DEBUG LOGGING START ---
                                    // Log when gap is detected using the 'skip' property
                                    console.log(`>>> Gap DETECTED (via skip): p0.skip=${p0skip}, p1.skip=${p1skip}, datasetIndex=${context.datasetIndex}. Applying gapColor.`);
                                    // --- DEBUG LOGGING END ---
                                    return gapColor;
                                }
                            }
                            // Default color for non-gap segments, points, legend
                            return baseColors[context.datasetIndex % baseColors.length];
                        },
                        // Ensure point colors match the line start/end unless hovered
                        pointBorderColor: context => baseColors[context.datasetIndex % baseColors.length],
                        pointBackgroundColor: context => baseColors[context.datasetIndex % baseColors.length],
                    }
                });
                console.log("Data prepared. Initializing Chart...");
                */
                // --- END REVERT --- 
                console.log("Initializing Chart with original data..."); // Log before init
                new Chart(ctx, {
                    type: 'line',
                    // Use the ORIGINAL datasets from Python/JSON
                    data: chartData, 
                    options: {
                        responsive: true,
                        maintainAspectRatio: true, // Adjust as needed
                        plugins: {
                            legend: {
                                position: 'top',
                            },
                            title: {
                                display: true,
                                text: 'Spread Comparison: {{ security_name|tojson }}'
                            }
                        },
                        scales: {
                            x: {
                                // Assuming labels are date strings, configure time scale if needed
                                // type: 'time',
                                // time: {
                                //     unit: 'day' // or week, month, etc.
                                // },
                                title: {
                                    display: true,
                                    text: 'Date'
                                }
                            },
                            y: {
                                title: {
                                    display: true,
                                    text: 'Spread'
                                }
                            }
                        },
                        interaction: {
                             intersect: false,
                             mode: 'index',
                        },
                        // Add other options from existing charts for consistency
                    }
                });
            } catch (error) {
                console.error("Error parsing chart data or rendering chart:", error);
                comparisonChartCanvas.parentElement.innerHTML = '<p class="text-danger">Error rendering chart.</p>';
            }
        } else {
             console.warn("Chart data or canvas element not found for comparison chart.");
        }
    });
</script>
{% endblock %}
</file>

<file path="templates/comparison_page.html">
{% extends "base.html" %}
{% block title %}Spread Comparison Summary{% endblock %}
{% block content %}
<div class="container mt-4">
    <h1>Spread Comparison: Original (sec_spread) vs. New (sec_spreadSP)</h1>
    <p class="text-muted">Comparing credit spreads between the two datasets. Click on a Security ID/Name to see details. Use filters or click column headers to sort.</p>
    {# --- Filter Form --- #}
    {% if filter_options %}
    <form method="GET" action="{{ url_for('comparison_bp.summary') }}" class="mb-3 p-3 border rounded bg-light" id="filter-form">
        <h5>Filters</h5>
        <div class="row">
            {% for column, options in filter_options.items() %}
            <div class="col-md-3 mb-2">
                <label for="filter-{{ column }}" class="form-label">{{ column }}</label>
                <select id="filter-{{ column }}" name="filter_{{ column }}" class="form-select form-select-sm comparison-filter-select" data-column="{{ column }}"> 
                    <option value="">All</option>
                    {% for option in options %}
                    <option value="{{ option }}" {% if active_filters.get(column) == option %}selected{% endif %}>{{ option }}</option>
                    {% endfor %}
                </select>
            </div>
            {% endfor %}
        </div>
        <div class="row">
            <div class="col-12 mt-2">
                <button type="submit" class="btn btn-primary btn-sm">Apply Filters</button>
                {# Add a clear button only if filters are active #}
                {% if active_filters %}
                <a href="{{ url_for('comparison_bp.summary') }}" class="btn btn-secondary btn-sm">Clear Filters</a>
                {% endif %}
            </div>
        </div>
        {# Hidden fields to preserve current sort order when applying filters #}
        <input type="hidden" name="sort_by" value="{{ current_sort_by }}">
        <input type="hidden" name="sort_order" value="{{ current_sort_order }}">
    </form>
    {% endif %}
    {# --- Data Table --- #}
    <div class="table-responsive">
        <table class="table table-striped table-hover table-sm" id="comparison-table">
            <thead>
                <tr>
                    {% set base_url = url_for('comparison_bp.summary', **request.args) %}
                    {# Loop through the columns passed from the view #}
                    {% for col_name in columns_to_display %}
                        {% set is_sort_col = (col_name == current_sort_by) %}
                        {% set next_sort_order = 'asc' if is_sort_col and current_sort_order == 'desc' else 'desc' %}
                        {# Build filter query string part #}
                        {% set filter_params = {} %}
                        {% for f_key, f_val in active_filters.items() %}
                             {% set _ = filter_params.update({'filter_' ~ f_key: f_val}) %}
                        {% endfor %}
                        {# Generate URL for this header, including current filters and new sort parameters #}
                        {% set sort_url = url_for('comparison_bp.summary', sort_by=col_name, sort_order=next_sort_order, **filter_params) %}
                        {# Add classes for styling and JS #}
                        <th class="sortable {{ 'sorted-' + current_sort_order if is_sort_col else '' }}" 
                            data-column-name="{{ col_name }}">
                            <a href="{{ sort_url }}">
                                {# Map internal names to user-friendly names if needed #}
                                {{ col_name.replace('_', ' ') | title }} 
                                {% if is_sort_col %}
                                    <span class="sort-indicator">{{ '' if current_sort_order == 'asc' else '' }}</span>
                                {% endif %}
                            </a>
                        </th>
                    {% endfor %}
                </tr>
            </thead>
            <tbody id="comparison-table-body">
                {% set id_col = id_column_name %}
                {% for row in table_data %}
                <tr>
                    {# Loop through the same columns to ensure order matches header #}
                    {% for col_name in columns_to_display %}
                        <td>
                            {% if col_name == id_col %}
                                <a href="{{ url_for('comparison_bp.details', security_id=row[id_col]) }}">{{ row[id_col] }}</a>
                            {% elif col_name in ['Level_Correlation', 'Change_Correlation'] and row[col_name] is not none %}
                                {{ "%.3f"|format(row[col_name]) }}
                            {% elif col_name in ['Mean_Abs_Diff', 'Max_Abs_Diff'] and row[col_name] is not none %}
                                {{ "%.2f"|format(row[col_name]) }}
                            {% elif col_name == 'Same_Date_Range' %}
                                {{ 'Yes' if row[col_name] else 'No' }}
                            {% elif row[col_name] is number %}
                                {{ row[col_name] }} {# Display other numbers directly or format as needed #}
                            {% else %}
                                {{ row[col_name] if row[col_name] is not none else 'N/A' }} {# Display strings or N/A #}
                            {% endif %}
                        </td>
                    {% endfor %}
                </tr>
                {% else %}
                <tr>
                    <td colspan="{{ columns_to_display|length }}" class="text-center">No comparison data available matching the current filters.</td>
                </tr>
                {% endfor %}
            </tbody>
        </table>
    </div>
</div>
{% endblock %}
{% block scripts %}
{{ super() }}
{# Reference the shared table sorter if it handles this pattern #}
{# Or add specific JS for this page if needed #}
{# Ensure tableSorter.js is loaded via base.html or explicitly here #}
{# <script type="module" src="{{ url_for('static', filename='js/modules/ui/tableSorter.js') }}"></script> #} 
{# No specific JS needed for server-side filtering/sorting initially, but can add client-side enhancement #}
{# Example: Add client-side sorting after initial load (optional) #}
{# <script type="module">
    import { initTableSorter } from '{{ url_for("static", filename="js/modules/ui/tableSorter.js") }}';
    document.addEventListener('DOMContentLoaded', () => {
        initTableSorter('comparison-table'); // Enable client-side sorting on the table
    });
</script> #}
{% endblock %}
</file>

<file path="templates/delete_metric_page.html">
<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <title>{{ metric_name }} Check</title>
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
    <style>
        body { padding-top: 5rem; }
        .chart-container { margin-bottom: 15px; }
        .metrics-table { margin-top: 5px; margin-bottom: 25px; font-size: 0.9em; }
        .metrics-table th, .metrics-table td { padding: 4px 8px; border: 1px solid #dee2e6; }
        .missing-warning { color: red; font-weight: bold; }
        .high-z { background-color: #fff3cd; }
        .very-high-z { background-color: #f8d7da; font-weight: bold; }
    </style>
</head>
<body>
    <nav class="navbar navbar-expand-md navbar-dark bg-dark fixed-top">
        <a class="navbar-brand" href="/">Data Verification</a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarsExampleDefault" aria-controls="navbarsExampleDefault" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="navbarsExampleDefault">
            <ul class="navbar-nav mr-auto">
                <li class="nav-item">
                    <a class="nav-link" href="/">Dashboard</a>
                </li>
            </ul>
        </div>
    </nav>
    <main role="main" class="container">
        <h1>{{ metric_name }} Check</h1>
        <p>Latest Data Date: <strong>{{ latest_date }}</strong></p>
        <p>Charts sorted by the absolute Z-score of the latest <strong>Fund - Benchmark Spread</strong> (most deviation first).</p>
        {% if not missing_funds.empty %}
            <div class="alert alert-warning" role="alert">
                <strong>Warning:</strong> The following funds are missing data for the latest date ({{ latest_date }}):
                {{ missing_funds.index.tolist() | join(', ') }}
            </div>
        {% endif %}
        {% for fund_code, data in charts_data.items() %}
            {% set metrics = data.metrics %}
            {% set z_score = metrics['Spread Z-Score'] %}
            {% set z_class = 'high-z' if z_score and z_score|abs > 2 else ('very-high-z' if z_score and z_score|abs > 3 else '') %}
            <div class="chart-container {{ z_class }}">
                {{ data.chart_html|safe }}
            </div>
            <table class="table table-sm table-bordered metrics-table {{ z_class }}">
                <thead>
                    <tr>
                        <th>Metric</th>
                        <th>Latest Value ({{ latest_date }})</th>
                        <th>Change from Previous</th>
                        <th>Historical Spread</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Fund Value</td>
                        <td>{{ metrics['Latest Fund Value']|round(2) if metrics['Latest Fund Value'] is not none else 'N/A' }}</td>
                        <td>{{ metrics['Fund Value Change']|round(2) if metrics['Fund Value Change'] is not none else 'N/A' }}</td>
                        <td rowspan="2">Mean: {{ metrics['Historical Spread Mean']|round(2) if metrics['Historical Spread Mean'] is not none else 'N/A' }}</td>
                    </tr>
                    <tr>
                        <td>Benchmark Value</td>
                        <td>{{ metrics['Latest Benchmark Value']|round(2) if metrics['Latest Benchmark Value'] is not none else 'N/A' }}</td>
                        <td>N/A</td> {# Change not calculated for benchmark #}
                    </tr>
                    <tr>
                        <td>Fund - Benchmark Spread</td>
                        <td>{{ metrics['Latest Spread']|round(2) if metrics['Latest Spread'] is not none else 'N/A' }}</td>
                        <td>{{ metrics['Spread Change']|round(2) if metrics['Spread Change'] is not none else 'N/A' }}</td>
                        <td>Std Dev: {{ metrics['Historical Spread Std Dev']|round(2) if metrics['Historical Spread Std Dev'] is not none else 'N/A' }}</td>
                    </tr>
                     <tr>
                        <td><strong>Spread Z-Score</strong></td>
                        <td colspan="3"><strong>{{ z_score|round(2) if z_score is not none else 'N/A' }}</strong></td>
                    </tr>
                </tbody>
            </table>
            {# Conditionally add link to fund duration details page #}
            {% if metric_name == 'Duration' %}
                <div class="mb-4 text-right">
                     <a href="{{ url_for('fund_duration_details', fund_code=fund_code) }}" class="btn btn-info btn-sm">View Security Duration Changes for {{ fund_code }} &rarr;</a>
                </div>
            {% endif %}
        {% else %}
            <p>No data processed for this metric.</p>
        {% endfor %}
    </main>
    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.5.4/dist/umd/popper.min.js"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"></script>
</body>
</html>
</file>

<file path="templates/exclusions_page.html">
{% extends 'base.html' %}
{% block title %}Manage Security Exclusions{% endblock %}
{% block content %}
<div class="container mt-4">
    <h2>Manage Security Exclusions</h2>
    <hr>
    {# Display messages if any #}
    {% if message %}
        <div class="alert alert-{{ message_type }} alert-dismissible fade show" role="alert">
            {{ message }}
            <button type="button" class="btn-close" data-bs-dismiss="alert" aria-label="Close"></button>
        </div>
    {% endif %}
    <div class="row">
        {# Left Column: Display Current Exclusions #}
        <div class="col-md-7">
            <h4>Current Exclusions</h4>
            {% if exclusions %}
                <table class="table table-striped table-sm">
                    <thead>
                        <tr>
                            <th>Security ID</th>
                            <th>Date Added</th>
                            <th>End Date</th>
                            <th>Comment</th>
                            <th>Action</th>
                        </tr>
                    </thead>
                    <tbody>
                        {% for exclusion in exclusions %}
                            <tr>
                                <td>{{ exclusion.SecurityID }}</td>
                                <td>{{ exclusion.AddDate.strftime('%Y-%m-%d') if exclusion.AddDate else 'N/A' }}</td>
                                <td>{{ exclusion.EndDate.strftime('%Y-%m-%d') if exclusion.EndDate else '' }}</td>
                                <td>{{ exclusion.Comment }}</td>
                                <td>
                                    <form method="POST" action="{{ url_for('exclusion_bp.remove_exclusion_route') }}" style="display: inline;">
                                        <input type="hidden" name="security_id" value="{{ exclusion.SecurityID }}">
                                        <input type="hidden" name="add_date" value="{{ exclusion.AddDate.strftime('%Y-%m-%d') if exclusion.AddDate else '' }}">
                                        <button type="submit" class="btn btn-danger btn-sm" onclick="return confirm('Are you sure you want to remove this exclusion?');">Remove</button>
                                    </form>
                                </td>
                            </tr>
                        {% endfor %}
                    </tbody>
                </table>
            {% else %}
                <p>No securities are currently excluded.</p>
            {% endif %}
        </div>
        {# Right Column: Add New Exclusion Form #}
        <div class="col-md-5">
            <h4>Add New Exclusion</h4>
            <form method="POST" action="{{ url_for('exclusion_bp.manage_exclusions') }}">
                <div class="mb-3">
                    <label for="security-search-input" class="form-label">Search & Select Security ID:</label>
                    <input type="text" id="security-search-input" class="form-control mb-2" placeholder="Type to filter securities...">
                    <select class="form-select" id="security-select" name="security_id" required>
                        <option value="" disabled selected>Select a Security ID</option>
                        {% for sec_id in available_securities %}
                            <option value="{{ sec_id }}">{{ sec_id }}</option>
                        {% endfor %}
                    </select>
                </div>
                <div class="mb-3">
                    <label for="end_date" class="form-label">End Date (Optional):</label>
                    <input type="date" class="form-control" id="end_date" name="end_date">
                </div>
                <div class="mb-3">
                    <label for="comment" class="form-label">Comment (Required):</label>
                    <textarea class="form-control" id="comment" name="comment" rows="3" required></textarea>
                </div>
                <button type="submit" class="btn btn-primary">Add Exclusion</button>
            </form>
        </div>
    </div>
</div>
{% endblock %}
{% block scripts %}
{{ super() }} {# Include scripts from base.html #}
{# We will add specific JS for the dynamic dropdown here later #}
<script>
    // Basic dynamic filtering for the dropdown
    document.getElementById('security-search-input').addEventListener('input', function() {
        let filter = this.value.toLowerCase();
        let select = document.getElementById('security-select');
        let options = select.options;
        let firstVisibleOption = null;
        for (let i = 0; i < options.length; i++) {
            let option = options[i];
            // Skip the placeholder option
            if (option.value === "") {
                option.style.display = ""; // Always show placeholder if input is empty, hide otherwise
                option.style.display = filter ? "none" : "";
                continue;
            }
            let txtValue = option.textContent || option.innerText;
            if (txtValue.toLowerCase().indexOf(filter) > -1) {
                option.style.display = "";
                if (!firstVisibleOption) {
                     firstVisibleOption = option; // Keep track of the first match
                }
            } else {
                option.style.display = "none";
            }
        }
         // Optionally, select the first visible option if the user hasn't selected one manually
        // This part can be enhanced, maybe select only if input length > N or on specific event
        // if (filter && firstVisibleOption && select.selectedIndex <= 0) {
            // select.value = firstVisibleOption.value;
        // }
    });
    // Reset filter when dropdown is clicked (to show all options again initially)
    document.getElementById('security-select').addEventListener('mousedown', function(){
       // Optional: Uncomment below to clear search on dropdown click
       // document.getElementById('security-search-input').value = '';
       // let event = new Event('input');
       // document.getElementById('security-search-input').dispatchEvent(event);
    });
</script>
{% endblock %}
</file>

<file path="templates/fund_duration_details.html">
{% extends 'base.html' %}
{% block title %}Duration Change Details for {{ fund_code }}{% endblock %}
{% block content %}
<div class="container mt-4">
    <h2>Duration Change Details for Fund: {{ fund_code }}</h2>
    <p>Showing securities from <code>sec_duration.csv</code> held by <strong>{{ fund_code }}</strong>, sorted by the latest 1-day change in duration (largest change first).</p>
    {% if message %}
    <div class="alert alert-warning" role="alert">
        {{ message }}
    </div>
    {% endif %}
    {# Data Table Section #}
    {% if securities_data %}
    <div class="table-responsive">
        <table class="table table-striped table-hover table-sm small" id="fund-duration-table">
            <thead class="table-light">
                <tr>
                    {# Use the column_order provided by the backend #}
                    {% for col_name in column_order %}
                        <th>{{ col_name }}</th>
                    {% endfor %}
                </tr>
            </thead>
            <tbody id="fund-duration-table-body">
                {% for row in securities_data %}
                     {# Optionally add row highlighting based on the change magnitude if needed #}
                     {% set change_value = row['1 Day Duration Change'] %}
                     {% set row_class = '' %} {# Add logic here if desired e.g., based on change_value sign or magnitude #}
                     {# Example highlighting:
                     {% if change_value is not none %}
                        {% if change_value > 0.5 %}
                            {% set row_class = 'table-warning' %}
                        {% elif change_value < -0.5 %}
                             {% set row_class = 'table-info' %}
                        {% endif %}
                     {% endif %}
                     #}
                    <tr class="{{ row_class }}">
                        {% for col_name in column_order %}
                            <td>
                                {# Special formatting for the change column or others if needed #}
                                {% if col_name == id_col_name %}
                                     {# Make the Security Name a link #}
                                     <a href="{{ url_for('security.security_details_page', metric_name='Duration', security_id=row[col_name]|urlencode) }}">{{ row[col_name] }}</a>
                                {% elif row[col_name] is number %}
                                    {# Format numeric columns, maybe specific format for change #}
                                     {{ "%.3f"|format(row[col_name]) }}
                                {% else %}
                                     {{ row[col_name] if row[col_name] is not none else '' }}
                                {% endif %}
                            </td>
                        {% endfor %}
                    </tr>
                {% endfor %}
            </tbody>
        </table>
    </div>
    {% elif not message %}
     <div class="alert alert-info" role="alert">
        No securities data to display for fund {{ fund_code }}.
    </div>
    {% endif %}
    <div class="mt-3">
         <a href="{{ url_for('metric.metric_page', metric_name='Duration') }}" class="btn btn-secondary btn-sm">&larr; Back to Duration Metric Page</a>
    </div>
</div>
{% endblock %}
</file>

<file path="templates/get_data.html">
{% extends "base.html" %}
{% block title %}Get Data via API{% endblock %}
{% block content %}
<div class="container mt-4">
    {# --- Display Data File Statuses --- #}
    <div class="card mb-4">
        <div class="card-header">
            Current Data File Status
        </div>
        <div class="card-body">
            {% if data_file_statuses %}
            <table class="table table-sm table-striped table-bordered">
                <thead>
                    <tr>
                        <th>File Name</th>
                        <th>Latest Data Date (in file)</th>
                        <th>File Last Modified</th>
                        <th>Funds Included</th>
                    </tr>
                </thead>
                <tbody>
                    {% for status in data_file_statuses %}
                    <tr>
                        <td>{{ status.filename }}</td>
                        <td>
                            {% if status.exists %}
                                {{ status.latest_data_date }}
                            {% else %}
                                <span class="text-muted">File Not Found</span>
                            {% endif %}
                        </td>
                        <td>
                             {% if status.exists %}
                                {{ status.last_modified }}
                            {% else %}
                                <span class="text-muted">N/A</span>
                            {% endif %}
                        </td>
                        <td>
                             {% if status.exists %}
                                {{ status.funds_included }}
                            {% else %}
                                <span class="text-muted">N/A</span>
                            {% endif %}
                        </td>
                    </tr>
                    {% endfor %}
                </tbody>
            </table>
            {% else %}
            <p class="text-muted">Could not retrieve data file statuses. Check QueryMap.csv or server logs.</p>
            {% endif %}
        </div>
    </div>
    {# --- End Display Data File Statuses --- #}
    <h2>Get Data via Simulated API (Rex)</h2>
    <p>Select funds and date range to simulate retrieving data using the Rex API.</p>
    <p>The simulated API calls will be printed in the terminal where the Flask app is running.</p>
    <form id="get-data-form">
        <div class="row mb-3">
            <div class="col-md-4">
                <label for="daysBack" class="form-label">Days Back:</label>
                <input type="number" class="form-control" id="daysBack" name="days_back" value="30" required>
                <div class="form-text">Number of days of history to retrieve ending on the End Date.</div>
            </div>
            <div class="col-md-4">
                <label for="endDate" class="form-label">End Date:</label>
                <input type="date" class="form-control" id="endDate" name="end_date" value="{{ default_end_date }}" required>
                <div class="form-text">Defaults to the previous business day.</div>
            </div>
        </div>
        <div class="mb-3">
            <label class="form-label">Select Funds:</label>
             <button type="button" class="btn btn-sm btn-outline-secondary ms-2" id="select-all-funds">Select All</button>
             <button type="button" class="btn btn-sm btn-outline-secondary ms-1" id="deselect-all-funds">Deselect All</button>
            <div id="fund-list" class="border p-3" style="max-height: 300px; overflow-y: auto;">
                {% for fund in funds %}
                <div class="form-check">
                    <input class="form-check-input fund-checkbox" type="checkbox" value="{{ fund['Fund Code'] }}" id="fund-{{ fund['Fund Code'] }}" name="funds"
                           {% if fund['Picked'] %}checked{% endif %}>
                    <label class="form-check-label" for="fund-{{ fund['Fund Code'] }}">
                        {{ fund['Fund Code'] }} (AUM: {{ fund['Total Asset Value USD']|int }})
                    </label>
                </div>
                {% else %}
                <p class="text-danger">No funds found or FundList.csv could not be loaded correctly.</p>
                {% endfor %}
            </div>
             <div class="form-text text-danger d-none" id="fund-selection-error">Please select at least one fund.</div>
        </div>
        <button type="submit" class="btn btn-primary">Simulate API Calls</button>
        <button type="button" id="run-overwrite-button" class="btn btn-warning ms-2">Run and Overwrite Data</button>
        <button type="button" id="run-cleanup-button" class="btn btn-secondary ms-2" style="display: none;">Run Data Cleanup</button>
    </form>
    <div id="status-area" class="mt-4" style="display: none;">
        <h4>Processing Status</h4>
        <div class="progress mb-2" style="height: 20px;">
            <div id="progress-bar" class="progress-bar progress-bar-striped progress-bar-animated" role="progressbar" style="width: 0%;" aria-valuenow="0" aria-valuemin="0" aria-valuemax="100">0%</div>
        </div>
        <p id="status-message"></p>
        <div id="results-summary" class="mt-3">
            <h5>Results Summary</h5>
            <table class="table table-sm table-striped">
                <thead>
                    <tr>
                        <th>Query ID</th>
                        <th>File Name</th>
                        <th>Simulated Rows Returned</th>
                        <th>Simulated File Lines</th>
                        <th>Status</th>
                        <th>Actions</th>
                    </tr>
                </thead>
                <tbody id="results-table-body">
                    <!-- Results will be populated here -->
                </tbody>
            </table>
        </div>
         <div id="error-message" class="alert alert-danger mt-3" style="display: none;">
             <!-- Errors shown here -->
         </div>
    </div>
</div>
{% endblock %}
{% block scripts %}
{{ super() }} {# Include scripts from base.html #}
<script>
document.addEventListener('DOMContentLoaded', function() {
    const form = document.getElementById('get-data-form');
    const statusArea = document.getElementById('status-area');
    const statusMessage = document.getElementById('status-message');
    const progressBar = document.getElementById('progress-bar');
    const resultsTableBody = document.getElementById('results-table-body');
    const errorMessageDiv = document.getElementById('error-message');
    const fundSelectionError = document.getElementById('fund-selection-error');
    const cleanupButton = document.getElementById('run-cleanup-button');
    const runOverwriteButton = document.getElementById('run-overwrite-button');
    const cleanupStatus = document.createElement('div');
    cleanupStatus.id = 'cleanup-status';
    cleanupStatus.className = 'mt-2';
    cleanupButton.parentNode.insertBefore(cleanupStatus, cleanupButton.nextSibling);
    const fundCheckboxes = document.querySelectorAll('.fund-checkbox');
    const selectAllButton = document.getElementById('select-all-funds');
    const deselectAllButton = document.getElementById('deselect-all-funds');
    // Select/Deselect All Funds buttons
    selectAllButton.addEventListener('click', () => {
        fundCheckboxes.forEach(checkbox => checkbox.checked = true);
    });
    deselectAllButton.addEventListener('click', () => {
        fundCheckboxes.forEach(checkbox => checkbox.checked = false);
    });
    // --- Function to handle the API call logic ---
    async function handleApiCall(overwriteMode = false) {
        // Clear previous results and errors
        statusArea.style.display = 'none';
        resultsTableBody.innerHTML = '';
        errorMessageDiv.style.display = 'none';
        errorMessageDiv.textContent = '';
        cleanupButton.style.display = 'none';
        cleanupStatus.textContent = '';
        cleanupStatus.className = 'mt-2';
        statusMessage.textContent = '';
        progressBar.style.width = '0%';
        progressBar.textContent = '0%';
        progressBar.classList.remove('bg-success', 'bg-danger');
        fundSelectionError.classList.add('d-none');
        // Get selected funds
        const selectedFunds = Array.from(document.querySelectorAll('input[name="funds"]:checked'))
                                 .map(cb => cb.value);
        // Basic client-side validation
        if (selectedFunds.length === 0) {
            fundSelectionError.classList.remove('d-none');
            return; // Stop submission
        }
        const daysBack = document.getElementById('daysBack').value;
        const endDate = document.getElementById('endDate').value;
        if (!endDate) {
             errorMessageDiv.textContent = 'Please select an End Date.';
             errorMessageDiv.style.display = 'block';
            return; // Stop submission
        }
        // Show status area and indicate processing
        statusArea.style.display = 'block';
        statusMessage.textContent = `Starting ${overwriteMode ? 'overwrite' : 'simulation/merge'}...`;
        progressBar.classList.add('progress-bar-animated');
        progressBar.classList.remove('bg-success', 'bg-danger');
        progressBar.style.width = '5%'; // Initial small progress
        progressBar.textContent = '5%';
        try {
            // Prepare request body, including the overwrite_mode flag
            const requestBody = {
                days_back: parseInt(daysBack, 10),
                end_date: endDate,
                funds: selectedFunds,
                overwrite_mode: overwriteMode // Add the flag here
            };
            const response = await fetch('{{ url_for("api_bp.run_api_calls") }}', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                },
                body: JSON.stringify(requestBody) // Send the body with the flag
            });
            const result = await response.json();
            // Stop animation
             progressBar.classList.remove('progress-bar-animated');
            if (response.ok && (result.status === 'completed' || result.status === 'completed_with_errors')) {
                statusMessage.textContent = result.message;
                progressBar.style.width = '100%';
                progressBar.textContent = '100%';
                progressBar.classList.add(result.status === 'completed_with_errors' ? 'bg-warning' : 'bg-success');
                // Populate results table
                resultsTableBody.innerHTML = ''; // Clear any potential previous entries
                if (result.summary && result.summary.length > 0) {
                    result.summary.forEach(item => {
                        // Determine row content based on real vs simulated
                        let rowsCellContent = 'N/A';
                        let linesCellContent = 'N/A';
                        if (item.actual_rows !== undefined && item.actual_rows !== null) { // Real API mode was used
                            rowsCellContent = item.actual_rows;
                            linesCellContent = item.actual_lines !== undefined ? item.actual_lines : (rowsCellContent > 0 ? rowsCellContent + 1 : 0);
                        } else if (item.simulated_rows !== undefined && item.simulated_rows !== null) { // Simulated mode was used
                            rowsCellContent = item.simulated_rows;
                            linesCellContent = item.simulated_lines !== undefined ? item.simulated_lines : (rowsCellContent > 0 ? rowsCellContent + 1 : 0);
                        }
                        // Define fundCode. Prefer 'fund_code' if present, else derive from 'query_id' if possible
                        let fundCodeForRerun = item.fund_code || null;
                        // Basic attempt to extract from query_id if needed (adjust regex/logic if format differs)
                        if (!fundCodeForRerun && item.query_id && typeof item.query_id === 'string') {
                            const match = item.query_id.match(/some_pattern_to_extract_fund_code/); // Replace with actual pattern if applicable
                            if (match && match[1]) {
                                fundCodeForRerun = match[1];
                            }
                        }
                        const rerunButtonHtml = fundCodeForRerun
                            ? `<button class="btn btn-sm btn-outline-primary rerun-button" data-fund-code="${fundCodeForRerun}">Rerun</button>`
                            : `<span class="text-muted">Rerun N/A</span>`; // No rerun if fund code unknown
                        const row = `<tr data-query-id="${item.query_id}">
                                        <td>${item.query_id}</td>
                                        <td>${item.file_name}</td>
                                        <td>${rowsCellContent}</td>
                                        <td>${linesCellContent}</td>
                                        <td><span class="badge ${item.status.includes('OK') || item.status.includes('Saved') || item.status.includes('Simulated') ? 'bg-success' : (item.status.includes('Warning') ? 'bg-warning' : 'bg-danger')}">${item.status}</span></td>
                                        <td>${rerunButtonHtml}</td>
                                     </tr>`;
                        resultsTableBody.innerHTML += row;
                    });
                } else {
                    resultsTableBody.innerHTML = '<tr><td colspan="6">No summary data returned.</td></tr>'; // colspan is 6 now
                }
                 errorMessageDiv.style.display = 'none'; // Hide error div if successful
                 cleanupButton.style.display = 'inline-block';
            } else {
                // Handle errors reported by the server (e.g., validation errors, file not found)
                statusMessage.textContent = 'Processing failed.';
                progressBar.style.width = '100%';
                progressBar.textContent = 'Error';
                progressBar.classList.add('bg-danger');
                errorMessageDiv.textContent = `Error: ${result.message || 'Unknown error'}`;
                errorMessageDiv.style.display = 'block';
            }
        } catch (error) {
            // Handle network errors or issues with the fetch itself
            console.error("Fetch error:", error);
            progressBar.classList.remove('progress-bar-animated');
            progressBar.style.width = '100%';
            progressBar.textContent = 'Error';
            progressBar.classList.add('bg-danger');
            statusMessage.textContent = 'An error occurred during the request.';
            errorMessageDiv.textContent = 'Network error or server unreachable. Check console for details.';
            errorMessageDiv.style.display = 'block';
        }
    }
    // --- End of API call handler function ---
    // --- Event Listener for the original "Simulate API Calls" button (which is type="submit") ---
    form.addEventListener('submit', function(event) {
        event.preventDefault(); // Prevent traditional form submission
        handleApiCall(false); // Call the handler function with overwriteMode = false
    });
    // --- Event Listener for the new "Run and Overwrite Data" button ---
    runOverwriteButton.addEventListener('click', function() {
        // No need for event.preventDefault() as it's not a submit button
        handleApiCall(true); // Call the handler function with overwriteMode = true
    });
    // Add event listener for the Rerun buttons using event delegation
    resultsTableBody.addEventListener('click', async function(event) {
        if (event.target.classList.contains('rerun-button')) {
            const button = event.target;
            const row = button.closest('tr');
            const queryId = row.dataset.queryId;
            const fundCode = button.dataset.fundCode;
            const daysBack = document.getElementById('daysBack').value;
            const endDate = document.getElementById('endDate').value;
            const cells = row.cells; // Define cells here so it's available in all blocks
            if (!fundCode || !queryId) {
                console.error('Missing fund code or query ID for rerun');
                // Optionally display an error to the user near the button/row
                return;
            }
            // Provide visual feedback
            button.disabled = true;
            button.textContent = 'Running...';
            // You could also add a temporary status cell or highlight the row
            try {
                const response = await fetch('/rerun-api-call', { // New endpoint needed
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                    },
                    body: JSON.stringify({
                        query_id: queryId,
                        // Send fundCode as a list in the 'funds' key
                        funds: [fundCode],
                        days_back: parseInt(daysBack, 10),
                        end_date: endDate
                    })
                });
                const result = await response.json();
                if (response.ok && result.status.includes('OK')) {
                    // Update the specific row in the table
                    cells[2].textContent = result.simulated_rows !== undefined ? result.simulated_rows : 'N/A'; // Simulated Rows
                    cells[3].textContent = result.simulated_lines !== undefined ? result.simulated_lines : 'N/A'; // Simulated Lines
                    cells[4].innerHTML = `<span class="badge bg-success">${result.status}</span>`; // Status
                    // Optional: Add a temporary success indicator
                    button.textContent = 'Rerun Success';
                    setTimeout(() => { button.textContent = 'Rerun'; }, 2000); // Reset after 2s
                } else {
                    // Handle error - update status cell, show message
                    cells[4].innerHTML = `<span class="badge bg-danger">Error</span>`;
                    console.error("Rerun failed:", result.message || 'Unknown error');
                    // Optionally display error details near the row or in the main error area
                     button.textContent = 'Rerun Failed';
                     setTimeout(() => { button.textContent = 'Rerun'; }, 3000); // Reset after 3s
                }
            } catch (error) {
                console.error("Rerun fetch error:", error);
                 cells[4].innerHTML = `<span class="badge bg-danger">Network Error</span>`;
                 button.textContent = 'Rerun Error';
                 setTimeout(() => { button.textContent = 'Rerun'; }, 3000); // Reset after 3s
                // Optionally display error details
            } finally {
                 button.disabled = false; // Re-enable button
                 // Remove any temporary status indicators if needed
            }
        }
    });
    // Add event listener for the new Cleanup button
    cleanupButton.addEventListener('click', async function() {
        cleanupStatus.textContent = 'Starting cleanup process...';
        cleanupStatus.className = 'mt-2 alert alert-info'; // Show feedback
        cleanupButton.disabled = true; // Disable button while running
        try {
            const response = await fetch('/run-cleanup', { // Call the new endpoint
                method: 'POST',
                 headers: {
                    'Content-Type': 'application/json', // Optional: Send empty JSON or adjust endpoint
                },
                // body: JSON.stringify({}) // Optional: Send empty JSON or adjust endpoint
            });
            const result = await response.json();
            if (response.ok && result.status === 'success') {
                cleanupStatus.textContent = `Cleanup process finished successfully. Output:\n${result.output}`;
                cleanupStatus.className = 'mt-2 alert alert-success';
            } else {
                 cleanupStatus.textContent = `Cleanup process failed. Error:\n${result.error || result.message || 'Unknown error'}`;
                 cleanupStatus.className = 'mt-2 alert alert-danger';
            }
        } catch (error) {
            console.error("Cleanup fetch error:", error);
            cleanupStatus.textContent = 'Failed to trigger cleanup process. Network error or server unreachable.';
            cleanupStatus.className = 'mt-2 alert alert-danger';
        } finally {
             cleanupButton.disabled = false; // Re-enable button
        }
    });
});
</script>
{% endblock %}
</file>

<file path="templates/index.html">
<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <title>Data Verification Dashboard</title>
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
    <style>
        body { padding-top: 5rem; }
        .metric-link { margin: 5px; }
        /* Add some basic table styling */
        table { margin-top: 20px; }
        th, td { text-align: center; vertical-align: middle; }
        .table thead th { vertical-align: middle; }
        /* Style for NaN values */
        .nan-value { color: #ccc; font-style: italic; }
         /* Style for Z-scores outside +/- 2 */
        .z-high { background-color: #ffcccc; } /* Light red */
        .z-low { background-color: #ccccff; } /* Light blue */
    </style>
</head>
<body>
    <nav class="navbar navbar-expand-md navbar-dark bg-dark fixed-top">
        <a class="navbar-brand" href="/">Data Verification</a>
    </nav>
    <main role="main" class="container">
        <div class="jumbotron">
            <h1>Dashboard</h1>
            <p class="lead">Select a metric below to view the detailed checks, or see the latest Z-Score summary below.</p>
            <div class="row row-cols-1 row-cols-md-2 row-cols-lg-3 g-4">
                {% for metric in metrics %}
                <div class="col">
                    <div class="card h-100 metric-card">
                        <div class="card-body d-flex flex-column">
                            <h5 class="card-title">{{ metric }}</h5>
                            <p class="card-text flex-grow-1">View details for {{ metric }}.</p>
                            <a href="{{ url_for('metric.metric_page', metric_name=metric) }}" class="btn btn-primary metric-link">View Details</a>
                        </div>
                    </div>
                </div>
                {% endfor %}
            </div>
        </div>
        <!-- Z-Score Summary Table -->
        {% if not summary_data.empty %}
        <h2>Latest Change Z-Score Summary</h2>
        <div class="table-responsive"> <!-- Make table scrollable on small screens -->
            <table class="table table-striped table-bordered table-hover table-sm">
                <thead class="thead-light">
                    <tr>
                        <th>Fund Code</th>
                        {# Use the new summary_metrics list which contains combined names #}
                        {% for full_metric_name in summary_metrics %}
                        <th>{{ full_metric_name }}</th>
                        {% endfor %}
                    </tr>
                </thead>
                <tbody>
                    {% for fund_code, row in summary_data.iterrows() %}
                    <tr>
                        <td>{{ fund_code }}</td>
                        {# Iterate through the same new list for data access #}
                        {% for full_metric_name in summary_metrics %}
                            {% set z_score = row[full_metric_name] %}
                            {% if z_score is none or z_score != z_score %}
                                {# Handle NaN/None #}
                                <td class="nan-value">N/A</td>
                            {% else %}
                                {# Apply conditional styling based on Z-score value #}
                                {% set z_abs = z_score|abs %}
                                {% set cell_class = '' %}
                                {% if z_abs > 2.0 %}
                                    {# Decide between high or low based on sign #}
                                    {% if z_score > 0 %}
                                        {% set cell_class = 'z-high' %}
                                    {% else %}
                                        {% set cell_class = 'z-low' %}
                                    {% endif %}
                                {% endif %}
                                <td class="{{ cell_class }}">{{ "%.2f"|format(z_score) }}</td>
                            {% endif %}
                        {% endfor %}
                    </tr>
                    {% endfor %}
                </tbody>
            </table>
        </div>
        {% else %}
        <div class="alert alert-warning" role="alert">
            No Z-score data could be generated for the summary table. Check the console logs for errors.
        </div>
        {% endif %}
        <!-- Add a link to the new API Data Retrieval page -->
        <div class="mt-4 mb-4 p-3 border rounded bg-light">
            <h5>Get Data via API (Simulated)</h5>
            <p>Select funds and dates to simulate retrieving data from the Rex API.</p>
            <a href="{{ url_for('api_bp.get_data_page') }}" class="btn btn-success btn-sm">Go to Get Data Page</a>
        </div>
        <!-- Add a link to the new Securities page -->
        <div class="mt-4 p-3 border rounded bg-light">
            <h5>Securities Data Check</h5>
            <p>View checks for individual securities based on latest daily changes.</p>
            <a href="{{ url_for('security.securities_page') }}" class="btn btn-info btn-sm">View Securities Check</a>
        </div>
    </main>
    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.5.4/dist/umd/popper.min.js"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"></script>
</body>
</html>
</file>

<file path="templates/metric_page_js.html">
<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <title>{{ metric_name }} Check (JS)</title>
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
    <!-- Include Chart.js and the date adapter -->
    <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-adapter-date-fns/dist/chartjs-adapter-date-fns.bundle.min.js"></script>
    <style>
        body { padding-top: 5rem; }
        .chart-container-wrapper { margin-bottom: 15px; padding: 10px; border: 1px solid #eee; }
        .chart-canvas { max-height: 400px; }
        .metrics-table { margin-top: 15px; margin-bottom: 25px; font-size: 0.9em; }
        .metrics-table th, .metrics-table td { padding: 4px 8px; border: 1px solid #dee2e6; }
        .missing-warning { color: red; font-weight: bold; }
        .high-z { background-color: #fff3cd; }
        .very-high-z { background-color: #f8d7da; font-weight: bold; }
    </style>
</head>
<body>
    <nav class="navbar navbar-expand-md navbar-dark bg-dark fixed-top">
        <a class="navbar-brand" href="/">Data Verification</a>
        <!-- Navbar content -->
    </nav>
    <main role="main" class="container">
        <h1>{{ metric_name }} Check</h1>
        <p>Latest Data Date: <strong>{{ latest_date }}</strong></p>
        <p>Charts sorted by the maximum absolute <strong>Change Z-Score</strong> across all columns for the fund (most deviation first).</p>
        {% if not missing_funds.empty %}
            <div class="alert alert-warning" role="alert">
                <strong>Warning:</strong> The following funds are missing data for the latest date ({{ latest_date }}):
                {{ missing_funds.index.tolist() | join(', ') }}
            </div>
        {% endif %}
        {% if error_message %}
        <div class="alert alert-danger" role="alert">
          {{ error_message }}
        </div>
        {% endif %}
        <!-- Data passed from Flask, embedded as JSON -->
        <script type="application/json" id="chartData">
            {{ charts_data_json | safe }}
        </script>
        <div id="chartsArea">
            <!-- Charts will be rendered here by JavaScript -->
        </div>
        <!-- END: Metrics Table -->
    </main>
    <!-- Link to the external JavaScript module -->
    <script type="module" src="{{ url_for('static', filename='js/main.js') }}"></script>
    <!-- Bootstrap JS (optional) -->
    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.5.4/dist/umd/popper.min.js"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"></script>
</body>
</html>
</file>

<file path="templates/securities_page.html">
lockdown-install.js:1 Removing unpermitted intrinsics
Alpha002:181 Initializing Chart with original data...
main.js:29 DOM fully loaded and parsed
{% extends 'base.html' %}
{% block title %}Security Data Check{% endblock %}
{% block head_extra %}
  {# Link the new CSS file #}
  <link rel="stylesheet" href="{{ url_for('static', filename='css/style.css') }}">
{% endblock %}
{% block content %}
<div class="container mt-4">
    <div class="d-flex justify-content-between align-items-center mb-3">
        <h2>Security Data Check</h2>
        {# Add button to navigate to the exclusions page #}
        <a href="{{ url_for('exclusion_bp.manage_exclusions') }}" class="btn btn-outline-secondary btn-sm">Manage Exclusions</a>
    </div>
    <p>Potential data issues based on the latest daily change Z-score for each security and metric.</p>
    {% if message %}
    <div class="alert alert-warning" role="alert">
        {{ message }}
    </div>
    {% endif %}
    {# Search Form #}
    <form method="GET" action="{{ url_for('security.securities_page') }}" class="mb-3">
        <div class="input-group">
            <input type="text" name="search_term" class="form-control" placeholder="Search by Security Name..." value="{{ search_term or '' }}">
            <button class="btn btn-outline-secondary" type="submit">Search</button>
            {# Optional: Add a clear button #}
            {% if search_term %}
                <a href="{{ url_for('security.securities_page') }}" class="btn btn-outline-danger">Clear</a>
            {% endif %}
        </div>
    </form>
    {# Filter Section - Basic implementation without JS interactivity for now #}
    {% if filter_options %}
    <div class="mb-3 p-3 border rounded bg-light">
        <h5>Filters</h5>
        <div class="row">
            {% for column, options in filter_options.items() %}
            <div class="col-md-3 mb-2">
                <label for="filter-{{ column|replace(' ', '_') }}" class="form-label">{{ column }}</label>
                <select id="filter-{{ column|replace(' ', '_') }}" class="form-select form-select-sm security-filter-select" data-column="{{ column }}"> 
                    <option value="">All</option>
                    {% for option in options %}
                    <option value="{{ option }}">{{ option }}</option>
                    {% endfor %}
                </select>
            </div>
            {% endfor %}
        </div>
    </div>
    {% endif %}
    {# Data Table Section #}
    {% if securities_data %}
    <div class="table-responsive">
        <table class="table table-striped table-hover table-sm small" id="securities-table">
            <thead class="table-light">
                <tr>
                    {# Add sortable class and data-column attribute to each header #}
                    {% for col_name in column_order %}
                        <th class="sortable" data-column-name="{{ col_name }}">
                            {{ col_name }} <span class="sort-indicator"></span> {# Placeholder for sort arrow #}
                        </th>
                    {% endfor %}
                </tr>
            </thead>
            <tbody id="securities-table-body">
                {% for row in securities_data %}
                    {% set z_score = row['Change Z-Score'] %}
                    {% set abs_z_score = z_score|abs if z_score is not none else 0 %}
                    {% set row_class = 'table-danger' if abs_z_score >= 3 else ('table-warning' if abs_z_score >= 2 else '') %}
                    <tr class="{{ row_class }}">
                        {% for col_name in column_order %}
                            {# Add data-value attribute for easier sorting, especially for formatted numbers #}
                            <td data-value="{{ row[col_name] if row[col_name] is not none else '' }}">
                                {# Check if this is the ID column #}
                                {% if col_name == id_col_name %}
                                    {# Construct the link - Requires a new route #}
                                    {# Since this page only shows Spread data now, hardcode metric_name #}
                                    <a href="{{ url_for('security.security_details_page', metric_name='Spread', security_id=row[col_name]|urlencode) }}">
                                        {{ row[col_name] }}
                                    </a>
                                {% elif col_name == 'Change Z-Score' and row[col_name] is not none %}
                                    {{ "%.2f"|format(row[col_name]) }}
                                {% elif row[col_name] is number %}
                                    {{ "%.3f"|format(row[col_name]) }}
                                {% else %}
                                    {{ row[col_name] if row[col_name] is not none else '' }}
                                {% endif %}
                            </td>
                        {% endfor %}
                    </tr>
                {% endfor %}
            </tbody>
        </table>
    </div>
    {% elif not message %}
     <div class="alert alert-info" role="alert">
        No security metrics data to display.
    </div>
    {% endif %}
</div>
{% endblock %}
</file>

<file path="templates/security_details_page.html">
{% extends 'base.html' %}
{% block title %}Security Details: {{ security_id }} - {{ metric_name }}{% endblock %}
{% block content %}
<div class="container mt-4">
    <h1>{{ security_id }} - {{ metric_name }}</h1>
    <p>Latest data as of: <strong>{{ latest_date }}</strong></p>
    {# Display Static Info #}
    {% if static_info %}
    <div class="mb-3 p-3 border rounded bg-light small">
        <strong>Static Details:</strong> 
        {% for key, value in static_info.items() %}
            <span class="me-3">{{ key }}: {{ value }}</span>
        {% endfor %}
    </div>
    {% endif %}
    {# Primary Chart Area (Metric + Price Overlay) #}
    <h2>{{ metric_name }} and Price Time Series</h2>
    <div id="primary-chart-container" class="mb-4" style="height: 450px; position: relative;">
        <canvas id="primarySecurityChart"></canvas>
    </div>
    {# Duration Chart Area #}
    <h2>Duration Time Series</h2>
    <div id="duration-chart-container" class="mb-4" style="height: 450px; position: relative;">
        <canvas id="durationSecurityChart"></canvas>
    </div>
    {# Hidden script tag for chart data #}
    <script id="chartJsonData" type="application/json">
        {{ chart_data_json|safe }}
    </script>
</div>
{% endblock %}
{% block scripts %}
{# Include Chart.js library (Likely inherited from base.html, but included here for safety/explicitness if needed) #}
{# If base.html already includes it, this line can be removed #}
<script src="https://cdn.jsdelivr.net/npm/chart.js"></script> 
<script>
    document.addEventListener('DOMContentLoaded', function() {
        // Get the chart data passed from Flask
        const chartDataElement = document.getElementById('chartJsonData');
        const chartData = JSON.parse(chartDataElement.textContent);
        // --- Render Primary Chart (Metric + Price) ---
        const primaryCtx = document.getElementById('primarySecurityChart').getContext('2d');
        if (chartData.primary_datasets && chartData.primary_datasets.length > 0) {
            new Chart(primaryCtx, {
                type: 'line',
                data: {
                    labels: chartData.labels,
                    datasets: chartData.primary_datasets // Use the primary datasets array
                },
                options: {
                    responsive: true,
                    maintainAspectRatio: false,
                    scales: {
                        x: {
                            title: {
                                display: true,
                                text: 'Date'
                            }
                        },
                        y: { // Primary Y-axis (for the main metric)
                            position: 'left',
                            title: {
                                display: true,
                                text: {{ metric_name|tojson }} + ' Value' // Use the metric name dynamically with tojson filter
                            }
                        },
                        y1: { // Secondary Y-axis (for Price)
                            position: 'right',
                            title: {
                                display: true,
                                text: 'Price'
                            },
                            grid: {
                                drawOnChartArea: false, // only draw grid lines for the first Y axis
                            },
                        }
                    },
                    plugins: {
                        legend: {
                            position: 'top',
                        },
                        tooltip: {
                            mode: 'index',
                            intersect: false,
                        }
                    }
                }
            });
        } else {
            console.error('No primary datasets found for the primary chart.');
            document.getElementById('primary-chart-container').innerHTML = '<p class="text-danger">Could not render primary chart: No data available.</p>';
        }
        // --- Render Duration Chart ---
        const durationCtx = document.getElementById('durationSecurityChart').getContext('2d');
        if (chartData.duration_dataset) {
             // Check if there's actual data in the duration dataset
            const hasDurationData = chartData.duration_dataset.data && chartData.duration_dataset.data.some(value => value !== null);
            if (hasDurationData) {
                new Chart(durationCtx, {
                    type: 'line',
                    data: {
                        labels: chartData.labels,
                        datasets: [chartData.duration_dataset] // Pass the single duration dataset in an array
                    },
                    options: {
                        responsive: true,
                        maintainAspectRatio: false,
                        scales: {
                            x: {
                                title: {
                                    display: true,
                                    text: 'Date'
                                }
                            },
                            y: { // Single Y-axis for duration
                                title: {
                                    display: true,
                                    text: 'Duration Value'
                                }
                            }
                        },
                        plugins: {
                            legend: {
                                position: 'top',
                            },
                            tooltip: {
                                mode: 'index',
                                intersect: false,
                            }
                        }
                    }
                });
            } else {
                 console.log('No non-null data points found for the duration chart.');
                 document.getElementById('duration-chart-container').innerHTML = '<p class="text-info">No duration data available for this security over the selected period.</p>';
            }
        } else {
            console.log('Duration dataset not provided or empty.');
            document.getElementById('duration-chart-container').innerHTML = '<p class="text-info">Duration data not available for this security.</p>';
        }
    });
</script>
{% endblock %}
</file>

<file path="utils.py">
# This file contains utility functions used throughout the Simple Data Checker application.
# These functions provide common helper functionalities like parsing specific string formats
# or validating data types, helping to keep the main application logic cleaner.
"""
Utility functions for the Flask application.
"""
import re
import pandas as pd
def _is_date_like(column_name):
    """Check if a column name looks like a date (e.g., YYYY-MM-DD).
    Simple regex, adjust if date formats vary significantly.
    Ensures the pattern matches the entire string.
    """
    return bool(re.match(r'^\d{4}-\d{2}-\d{2}$', str(column_name)))
def parse_fund_list(fund_string):
    """Safely parses the fund list string like '[FUND1,FUND2]' or '[FUND1]' into a list.
       Handles potential errors and variations in spacing.
    """
    if not isinstance(fund_string, str) or not fund_string.startswith('[') or not fund_string.endswith(']'):
        return [] # Return empty list if format is unexpected
    try:
        # Remove brackets and split by comma
        content = fund_string[1:-1]
        # Split by comma, strip whitespace from each element
        funds = [f.strip() for f in content.split(',') if f.strip()]
        return funds
    except Exception as e:
        print(f"Error parsing fund string '{fund_string}': {e}")
        return []
</file>

<file path="views/__init__.py">
"""
This file makes the 'views' directory a Python package.
"""
# You can leave this file empty or use it to import blueprints
# for easier registration in the app factory, though explicit imports
# in the factory as done currently are also perfectly fine.
</file>

<file path="views/api_views.py">
'''
Defines the Flask Blueprint for handling API data retrieval requests, including simulation,
fetching real data, saving data (with options for merging or overwriting),
and rerunning specific API calls.
'''
import os
import pandas as pd
from flask import Blueprint, render_template, request, current_app, jsonify
import datetime
from pandas.tseries.offsets import BDay
import time # Import the time module
#from tqs import tqs_query as tqs
# # Import the placeholder validation function
from data_validation import validate_data
# --- Feature Switch --- 
# Set to True to attempt real API calls, validation, and saving.
# Set to False to only simulate the API call (print to console).
USE_REAL_TQS_API = False
# ----------------------
# Blueprint Configuration
api_bp = Blueprint(
    'api_bp', __name__,
    template_folder='../templates',
    static_folder='../static'
)
def _simulate_and_print_tqs_call(QueryID, FundCodeList, StartDate, EndDate):
    '''Simulates calling the TQS API by printing the call signature.
    This function is used when USE_REAL_TQS_API is False.
    It does NOT interact with any external API.
    Returns:
        int: A simulated number of rows for status reporting.
    '''
    # Format the call signature exactly as requested: tqs(QueryID,[FundList],StartDate,EndDate)
    call_signature = f"tqs({QueryID}, {FundCodeList}, {StartDate}, {EndDate})"
    print(f"--- SIMULATING TQS API CALL (USE_REAL_TQS_API = False) ---")
    print(call_signature)
    print(f"--------------------------------------------------------")
    # Return a simulated row count for the summary table
    simulated_row_count = len(FundCodeList) * 10 if FundCodeList else 0 # Dummy calculation
    return simulated_row_count
def _fetch_real_tqs_data(QueryID, FundCodeList, StartDate, EndDate):
    '''Fetches real data from the TQS API.
    This function is called when USE_REAL_TQS_API is True.
    Replace the placeholder logic with the actual API interaction code.
    Args:
        QueryID: The query identifier.
        FundCodeList: List of fund codes.
        StartDate: Start date string (YYYY-MM-DD).
        EndDate: End date string (YYYY-MM-DD).
    Returns:
        pd.DataFrame or None: The DataFrame containing the fetched data,
                              or None if the API call fails or returns no data.
    '''
    current_app.logger.info(f"Attempting real TQS API call for QueryID: {QueryID}")
    print(f"--- EXECUTING REAL TQS API CALL (USE_REAL_TQS_API = True) --- ")
    print(f"tqs({QueryID}, {FundCodeList}, {StartDate}, {EndDate})")
    print(f"--------------------------------------------------------")
    dataframe = None # Initialize dataframe to None
    try:
        # --- !!! Replace this comment and the line below with the actual API call !!! ---
        # Ensure the `tqs` function/library is imported (commented out at the top)
        # dataframe = tqs.get_data(QueryID, FundCodeList, StartDate, EndDate) # Example real call
        print(dataframe.head()) if dataframe is not None else print("No data to display")
        pass # Remove this pass when uncommenting the line above
        # --- End of section to replace --- 
        # Check if the API returned valid data (e.g., a DataFrame)
        if dataframe is not None and isinstance(dataframe, pd.DataFrame):
            current_app.logger.info(f"Real TQS API call successful for QueryID: {QueryID}, Rows: {len(dataframe)}")
            return dataframe
        elif dataframe is None:
             # Explicitly handle the case where the API call itself returned None (e.g., planned failure or empty result coded as None)
             current_app.logger.warning(f"Real TQS API call for QueryID: {QueryID} returned None.")
             return None
        else:
            # Handle cases where the API returned something unexpected (not a DataFrame)
            current_app.logger.warning(f"Real TQS API call for QueryID: {QueryID} returned an unexpected data type: {type(dataframe)}.")
            return None # Treat unexpected types as failure
    except NameError as ne:
         # Specific handling if the tqs function isn't defined (import is commented out)
         current_app.logger.error(f"Real TQS API call failed for QueryID: {QueryID}. TQS function not imported/defined. Error: {ne}")
         print(f"    ERROR: TQS function not available. Ensure 'from tqs import tqs_query as tqs' is uncommented and the library is installed.")
         return None
    except Exception as e:
        # Handle API call errors (timeout, connection issues, authentication, etc.)
        current_app.logger.error(f"Real TQS API call failed for QueryID: {QueryID}. Error: {e}", exc_info=True)
        print(f"    ERROR during real API call: {e}")
        return None
# --- Helper Function to Get File Statuses ---
def get_data_file_statuses(data_folder):
    """
    Scans the data folder based on QueryMap.csv and returns status for each file.
    """
    statuses = []
    query_map_path = os.path.join(data_folder, 'QueryMap.csv')
    if not os.path.exists(query_map_path):
        current_app.logger.warning(f"QueryMap.csv not found at {query_map_path} for status check.")
        return statuses # Return empty list if map is missing
    try:
        query_map_df = pd.read_csv(query_map_path)
        if 'FileName' not in query_map_df.columns:
             current_app.logger.warning(f"QueryMap.csv at {query_map_path} is missing 'FileName' column.")
             return statuses
        date_column_candidates = ['Date', 'date', 'AsOfDate', 'ASOFDATE', 'Effective Date', 'Trade Date', 'Position Date'] # Add more candidates if needed
        for index, row in query_map_df.iterrows():
            filename = row['FileName']
            file_path = os.path.join(data_folder, filename)
            status_info = {
                'filename': filename,
                'exists': False,
                'last_modified': 'N/A',
                'latest_data_date': 'N/A',
                'funds_included': 'N/A' # Initialize new key
            }
            if os.path.exists(file_path):
                status_info['exists'] = True
                try:
                    # Get file modification time
                    mod_timestamp = os.path.getmtime(file_path)
                    status_info['last_modified'] = datetime.datetime.fromtimestamp(mod_timestamp).strftime('%Y-%m-%d %H:%M:%S')
                    # Try to read the CSV and find the latest date
                    try:
                        df = pd.read_csv(file_path, low_memory=False) # low_memory=False can help with mixed types
                        df_head = df.head()
                        # Determine the actual date column name
                        date_col = None
                        # --- Update Date Column Candidates --- 
                        date_column_candidates = ['Date', 'date', 'AsOfDate', 'ASOFDATE', 'Effective Date', 'Trade Date', 'Position Date'] 
                        found_cols = df_head.columns.str.strip()
                        current_app.logger.info(f"[{filename}] Checking for date columns: {date_column_candidates} in columns {found_cols.tolist()}") 
                        for candidate in date_column_candidates:
                            # Case-insensitive check
                            matching_cols = [col for col in found_cols if col.lower() == candidate.lower()]
                            if matching_cols:
                                date_col = matching_cols[0] # Use the actual name found
                                current_app.logger.info(f"[{filename}] Found date column: '{date_col}'") 
                                break # Found the first match
                        if date_col:
                            try:
                                # --- FIX: Use the full DataFrame's date column --- 
                                if date_col not in df.columns:
                                    # Handle case where column name from head differs slightly after full read (e.g., whitespace)
                                    # Find it again in the full df columns, case-insensitively
                                    corrected_date_col = None
                                    for col in df.columns:
                                        if col.strip().lower() == date_col.lower():
                                            corrected_date_col = col
                                            break
                                    if not corrected_date_col:
                                         raise ValueError(f"Date column '{date_col}' found in header but not in full DataFrame columns: {df.columns.tolist()}")
                                    date_col = corrected_date_col # Use the name from the full df
                                date_series = df[date_col] # Use the full series from the complete DataFrame
                                # --------------------------------------------------
                                current_app.logger.info(f"[{filename}] Attempting to parse full date column '{date_col}' (length: {len(date_series)}). Top 5 values: {date_series.head().to_list()}") 
                                # Try standard YYYY-MM-DD first
                                parsed_dates = pd.to_datetime(date_series, format='%Y-%m-%d', errors='coerce')
                                # If all are NaT, try DD/MM/YYYY
                                if parsed_dates.isnull().all():
                                    current_app.logger.info(f"[{filename}] Format YYYY-MM-DD failed, trying DD/MM/YYYY...") 
                                    parsed_dates = pd.to_datetime(date_series, format='%d/%m/%Y', errors='coerce')
                                # If still all NaT, try inferring (less reliable but fallback)
                                if parsed_dates.isnull().all():
                                     current_app.logger.warning(f"[{filename}] Both specific formats failed, trying to infer date format...") 
                                     parsed_dates = pd.to_datetime(date_series, errors='coerce', infer_datetime_format=True)
                                # Check if any dates were successfully parsed
                                if not parsed_dates.isnull().all():
                                    latest_date = parsed_dates.max()
                                    if pd.notna(latest_date):
                                        status_info['latest_data_date'] = latest_date.strftime('%Y-%m-%d')
                                        current_app.logger.info(f"[{filename}] Successfully found latest date: {status_info['latest_data_date']}") 
                                    else:
                                        status_info['latest_data_date'] = 'No Valid Dates Found'
                                        current_app.logger.warning(f"[{filename}] Parsed dates but found no valid max date (all NaT?).") 
                                else:
                                    status_info['latest_data_date'] = 'Date Parsing Failed'
                                    current_app.logger.warning(f"[{filename}] All parsing attempts failed for date column '{date_col}'.") 
                            except Exception as date_err:
                                current_app.logger.error(f"Error parsing date column '{date_col}' in {file_path}: {date_err}", exc_info=True)
                                status_info['latest_data_date'] = f'Error Parsing Date ({date_col})'
                        else:
                            status_info['latest_data_date'] = 'No Date Column Found/Parsed'
                            current_app.logger.warning(f"[{filename}] Could not find a suitable date column.") 
                        # --- Add Fund Code Extraction --- 
                        code_col = None
                        # FIX: Search for 'code' OR 'fund code' (case-insensitive)
                        code_candidates = ['code', 'fund code'] 
                        found_code_col_name = None
                        for candidate in code_candidates:
                             matches = [c for c in df.columns if c.strip().lower() == candidate]
                             if matches:
                                 found_code_col_name = matches[0] # Use the actual column name found
                                 break # Stop searching once found
                        if found_code_col_name:
                            code_col = found_code_col_name # Assign the found name to code_col
                            current_app.logger.info(f"[{filename}] Found Code column: '{code_col}'")
                            if not df.empty and code_col in df:
                                try:
                                    unique_funds = sorted([str(f) for f in df[code_col].unique() if pd.notna(f)])
                                    if unique_funds:
                                        if len(unique_funds) <= 5:
                                            status_info['funds_included'] = ', '.join(unique_funds)
                                        else:
                                            status_info['funds_included'] = ', '.join(unique_funds[:5]) + f' ... ({len(unique_funds)} total)'
                                        current_app.logger.info(f"[{filename}] Found funds: {status_info['funds_included']}")
                                    else:
                                        status_info['funds_included'] = 'No Codes Found'
                                except Exception as fund_err:
                                     current_app.logger.error(f"[{filename}] Error extracting funds from column '{code_col}': {fund_err}")
                                     status_info['funds_included'] = 'Error Extracting Funds'
                            else:
                                 status_info['funds_included'] = 'Code Column Empty?' # Should be covered by EmptyDataError usually
                        else:
                            status_info['funds_included'] = 'Code Column Missing'
                            current_app.logger.warning(f"[{filename}] Code column ('Code' or 'Fund Code') not found.")
                        # --- End Fund Code Extraction ---
                    except pd.errors.EmptyDataError:
                         status_info['latest_data_date'] = 'File is Empty'
                         status_info['funds_included'] = 'File is Empty' # Also set for funds
                         current_app.logger.warning(f"CSV file is empty: {file_path}")
                    except Exception as read_err:
                        status_info['latest_data_date'] = 'Read Error'
                        current_app.logger.error(f"Error reading CSV {file_path} for status check: {read_err}", exc_info=True)
                except Exception as file_err:
                     current_app.logger.error(f"Error accessing file properties for {file_path}: {file_err}", exc_info=True)
                     status_info['last_modified'] = 'Error Accessing File'
            statuses.append(status_info)
    except Exception as e:
        current_app.logger.error(f"Failed to process QueryMap.csv for file statuses: {e}", exc_info=True)
        # Optionally return a status indicating the map couldn't be processed
        return [{'filename': 'QueryMap Error', 'exists': False, 'last_modified': str(e), 'latest_data_date': '', 'funds_included': ''}]
    return statuses
# --- End Helper Function ---
@api_bp.route('/get_data')
def get_data_page():
    '''Renders the page for users to select parameters for API data retrieval.'''
    try:
        # Construct the path to FundList.csv relative to the app's instance path or root
        # Assuming DATA_FOLDER is configured relative to the app root
        data_folder = current_app.config.get('DATA_FOLDER', 'Data')
        fund_list_path = os.path.join(data_folder, 'FundList.csv')
        if not os.path.exists(fund_list_path):
            current_app.logger.error(f"FundList.csv not found at {fund_list_path}")
            return "Error: FundList.csv not found.", 500
        fund_df = pd.read_csv(fund_list_path)
        # Ensure required columns exist
        if not {'Fund Code', 'Total Asset Value USD', 'Picked'}.issubset(fund_df.columns):
             current_app.logger.error(f"FundList.csv is missing required columns.")
             return "Error: FundList.csv is missing required columns (Fund Code, Total Asset Value USD, Picked).", 500
        # Convert Total Asset Value to numeric, coercing errors
        fund_df['Total Asset Value USD'] = pd.to_numeric(fund_df['Total Asset Value USD'], errors='coerce')
        fund_df.dropna(subset=['Total Asset Value USD'], inplace=True) # Remove rows where conversion failed
        # Sort by Total Asset Value USD descending
        fund_df = fund_df.sort_values(by='Total Asset Value USD', ascending=False)
        # Convert Picked to boolean
        fund_df['Picked'] = fund_df['Picked'].astype(bool)
        # Prepare fund data for the template
        funds = fund_df.to_dict('records')
        # Calculate default end date (previous business day)
        default_end_date = (datetime.datetime.today() - BDay(1)).strftime('%Y-%m-%d')
        # --- Get Data File Statuses ---
        data_file_statuses = get_data_file_statuses(data_folder)
        # --- End Get Data File Statuses ---
    except Exception as e:
        current_app.logger.error(f"Error preparing get_data page: {e}", exc_info=True)
        # Provide a user-friendly error message, specific details are logged
        return f"An error occurred while preparing the data retrieval page: {e}", 500
    return render_template('get_data.html', funds=funds, default_end_date=default_end_date, data_file_statuses=data_file_statuses)
# --- Helper function to find key columns ---
def _find_key_columns(df, filename):
    """Attempts to find the date and fund/identifier columns."""
    date_col = None
    fund_col = None
    # Date column candidates (add more if needed)
    date_candidates = ['Date', 'date', 'AsOfDate', 'ASOFDATE', 'Effective Date', 'Trade Date', 'Position Date']
    # Fund/ID column candidates
    fund_candidates = ['Code', 'Fund Code', 'Fundcode', 'security id', 'SecurityID', 'Security Name'] # Broadened list
    found_cols = df.columns.str.strip().str.lower()
    for candidate in date_candidates:
        if candidate.lower() in found_cols:
            # Find the original casing
            original_cols = [col for col in df.columns if col.strip().lower() == candidate.lower()]
            if original_cols:
                date_col = original_cols[0]
                current_app.logger.info(f"[{filename}] Found date column: '{date_col}'")
                break
    for candidate in fund_candidates:
        if candidate.lower() in found_cols:
            # Find the original casing
            original_cols = [col for col in df.columns if col.strip().lower() == candidate.lower()]
            if original_cols:
                fund_col = original_cols[0]
                current_app.logger.info(f"[{filename}] Found fund/ID column: '{fund_col}'")
                break
    if not date_col:
        current_app.logger.warning(f"[{filename}] Could not reliably identify a date column from candidates: {date_candidates}")
    if not fund_col:
        current_app.logger.warning(f"[{filename}] Could not reliably identify a fund/ID column from candidates: {fund_candidates}")
    return date_col, fund_col
@api_bp.route('/run_api_calls', methods=['POST'])
def run_api_calls():
    '''Handles the form submission to trigger API calls (real or simulated).'''
    try:
        # Get data from form
        data = request.get_json()
        days_back = int(data.get('days_back', 30)) # Default to 30 days if not provided
        end_date_str = data.get('end_date')
        selected_funds = data.get('funds', [])
        overwrite_mode = data.get('overwrite_mode', False) # Get the new overwrite flag
        if not end_date_str:
            # Should have been validated client-side, but handle defensively
            return jsonify({"status": "error", "message": "End date is required."}), 400
        if not selected_funds:
             return jsonify({"status": "error", "message": "At least one fund must be selected."}), 400
        # Calculate dates
        end_date = pd.to_datetime(end_date_str)
        start_date = end_date - pd.Timedelta(days=days_back)
        # Format dates as YYYY-MM-DD for the TQS call
        start_date_tqs_str = start_date.strftime('%Y-%m-%d')
        end_date_tqs_str = end_date.strftime('%Y-%m-%d')
        # --- Get Query Map ---
        data_folder = current_app.config.get('DATA_FOLDER', 'Data')
        query_map_path = os.path.join(data_folder, 'QueryMap.csv')
        if not os.path.exists(query_map_path):
            return jsonify({"status": "error", "message": f"QueryMap.csv not found at {query_map_path}"}), 500
        query_map_df = pd.read_csv(query_map_path)
        if not {'QueryID', 'FileName'}.issubset(query_map_df.columns):
            return jsonify({"status": "error", "message": "QueryMap.csv missing required columns (QueryID, FileName)."}), 500
        # Sort queries: ts_*, pre_*, others
        def sort_key(query):
            filename = query.get('FileName', '').lower()
            if filename.startswith('ts_'):
                return 0
            elif filename.startswith('pre_'):
                return 1
            else:
                # Keep original order for non-ts/pre files relative to each other
                # Or assign a consistent rank if needed (e.g., based on original index)
                return 2 # All others get rank 2 for now
        # Add original index to preserve relative order for non-ts/pre files
        queries_with_indices = list(enumerate(query_map_df.to_dict('records')))
        def sort_key_with_index(item):
            index, query = item
            filename = query.get('FileName', '').lower()
            if filename.startswith('ts_'):
                return (0, index) # Sort by ts_ first, then original index
            elif filename.startswith('pre_'):
                return (1, index) # Sort by pre_ next, then original index
            else:
                return (2, index) # Others last, sorted by original index
        queries_with_indices.sort(key=sort_key_with_index)
        # Extract the sorted queries list
        queries = [item[1] for item in queries_with_indices]
        current_app.logger.info(f"Processing order after sorting: {[q.get('FileName', 'N/A') for q in queries]}")
        results_summary = []
        total_queries = len(queries)
        completed_queries = 0
        all_ts_files_succeeded = True # Flag to track success of ts_ files
        # Determine mode for logging/messaging
        current_mode_desc = "SIMULATED mode" if not USE_REAL_TQS_API else ("REAL API mode (Overwrite Enabled)" if overwrite_mode else "REAL API mode (Merge/Append)")
        current_app.logger.info(f"--- Starting /run_api_calls in {current_mode_desc} ---")
        # Loop through sorted queries
        for query_info in queries:
            # Extract query details safely
            query_id = query_info.get('QueryID')
            file_name = query_info.get('FileName')
            # Make sure QueryID and FileName exist
            if not query_id or not file_name:
                 current_app.logger.warning(f"Skipping entry due to missing QueryID or FileName: {query_info}")
                 # Add a summary entry indicating the skip?
                 summary = {
                    "query_id": query_id or "N/A", "file_name": file_name or "N/A",
                    "status": "Skipped (Missing QueryID/FileName)",
                    "simulated_rows": None, "simulated_lines": None,
                    "actual_rows": None, "actual_lines": None,
                    "save_action": "N/A", "validation_status": "Not Run"
                 }
                 results_summary.append(summary)
                 # Don't increment completed_queries if it fundamentally couldn't run
                 continue 
            output_path = os.path.join(data_folder, file_name)
            # Initialize summary for this query (moved after basic validation)
            summary = {
                "query_id": query_id,
                "file_name": file_name,
                "status": "Pending", # Initial status
                "simulated_rows": None, # Initialize keys
                "simulated_lines": None,
                "actual_rows": None,
                "actual_lines": None,
                "save_action": "N/A",
                "validation_status": "Not Run"
            }
            # Determine file type
            file_type = 'other'
            if file_name.lower().startswith('ts_'):
                file_type = 'ts'
            elif file_name.lower().startswith('pre_'):
                file_type = 'pre'
            current_app.logger.info(f"--- Starting Process for QueryID: {query_id}, File: {file_name} (Type: {file_type}) ---")
            # Skip pre_ files if any ts_ file failed
            if file_type == 'pre' and not all_ts_files_succeeded:
                current_app.logger.warning(f"[{file_name}] Skipping pre_ file because a previous ts_ file failed processing.")
                summary['status'] = 'Skipped (Previous TS Failure)'
                summary['validation_status'] = 'Not Run'
                summary['save_action'] = 'Skipped'
                results_summary.append(summary)
                completed_queries += 1 # It was processed (by skipping)
                continue # Move to the next query
            # --- Existing try block for processing a single query ---
            try:
                if USE_REAL_TQS_API:
                    # --- Real API Call, Validation, and Save Logic ---
                    df_new = None
                    df_to_save = None # Will hold the final DF to be saved
                    force_overwrite = overwrite_mode # Use the flag passed from frontend
                    try:
                        # 1. Fetch Real Data (Common step)
                        df_new = _fetch_real_tqs_data(query_id, selected_funds, start_date_tqs_str, end_date_tqs_str)
                        # --- Handle fetch result ---
                        if df_new is None:
                            current_app.logger.warning(f"[{file_name}] No data returned from API call for QueryID {query_id}.")
                            summary['status'] = 'Warning - No data returned from API'
                            summary['validation_status'] = 'Skipped (API Returned None)'
                            if file_type == 'ts': all_ts_files_succeeded = False # Mark failure for ts_ files
                        elif df_new.empty:
                            current_app.logger.warning(f"[{file_name}] Empty DataFrame returned from API call for QueryID {query_id}.")
                            summary['status'] = 'Warning - Empty data returned from API'
                            # Don't skip validation if empty, allow saving empty file
                            summary['validation_status'] = 'OK (Empty Data)'
                            # An empty dataframe is still data, proceed to save/overwrite logic below
                            df_to_save = df_new # Allow overwriting with empty data if needed
                            summary['actual_rows'] = 0 # Explicitly set 0 rows
                        else: # Data fetched successfully (and not empty)
                            current_app.logger.info(f"[{file_name}] Fetched {len(df_new)} new rows.")
                            summary['actual_rows'] = len(df_new)
                            df_to_save = df_new # Prepare to save this new data (might be modified below)
                        # --- Type-Specific Processing (only if df_new is not None) ---
                        if df_new is not None: # Includes empty DataFrame case
                            # == TS File Processing ==
                            if file_type == 'ts':
                                current_app.logger.info(f"[{file_name}] Processing as ts_ file (Overwrite Mode: {force_overwrite}).")
                                try:
                                    # 2. Identify Key Columns in New Data (TS specific)
                                    if not df_new.empty: # Only check non-empty DFs
                                        date_col_new, fund_col_new = _find_key_columns(df_new, f"{file_name} (New TS Data)")
                                        if not date_col_new or not fund_col_new:
                                            err_msg = f"Could not find essential date/fund columns in fetched ts_ data for {file_name}. Cannot proceed."
                                            current_app.logger.error(f"[{file_name}] {err_msg}")
                                            raise ValueError(err_msg) # Caught below
                                    else:
                                        date_col_new, fund_col_new = None, None # Cannot find cols in empty df
                                        current_app.logger.info(f"[{file_name}] Skipping key column check for empty TS data.")
                                    # 3. Handle Existing File (TS specific - merge/append logic)
                                    if force_overwrite:
                                        current_app.logger.info(f"[{file_name}] Overwrite Mode enabled. Skipping check for existing file.")
                                        if os.path.exists(output_path):
                                            summary['save_action'] = 'Overwritten (User Request)'
                                        else:
                                            summary['save_action'] = 'Created (Overwrite Mode)'
                                        # df_to_save is already df_new
                                    elif os.path.exists(output_path):
                                        current_app.logger.info(f"[{file_name}] TS file exists. Reading existing data for merge/append.")
                                        try:
                                            df_existing = pd.read_csv(output_path, low_memory=False)
                                            if not df_existing.empty:
                                                # --- Existing TS merge/append logic ---
                                                if date_col_new and fund_col_new: # Requires new data cols to be found
                                                    date_col_existing, fund_col_existing = _find_key_columns(df_existing, f"{file_name} (Existing TS)")
                                                    if date_col_existing == date_col_new and fund_col_existing == fund_col_new:
                                                        date_col = date_col_new # Use consistent names
                                                        fund_col = fund_col_new
                                                        # Date range comparison (optional check)
                                                        # ... (keep existing date comparison logic if desired) ...
                                                        # Combine Data (Append/Overwrite) logic
                                                        current_app.logger.info(f"[{file_name}] Combining new data for funds/IDs {df_new[fund_col].unique()} with existing data.")
                                                        funds_in_new_data = df_new[fund_col].unique()
                                                        # Ensure fund columns have compatible types (e.g., strings)
                                                        try:
                                                            df_existing[fund_col] = df_existing[fund_col].astype(str)
                                                            df_new[fund_col] = df_new[fund_col].astype(str) # Ensure new is also str
                                                            funds_in_new_data = [str(f) for f in funds_in_new_data]
                                                        except Exception as type_err:
                                                             current_app.logger.warning(f"[{file_name}] Potential type mismatch in fund column \'{fund_col}\' during filtering: {type_err}. Filtering might be incomplete.")
                                                        df_existing_filtered = df_existing[~df_existing[fund_col].isin(funds_in_new_data)]
                                                        current_app.logger.info(f"[{file_name}] Kept {len(df_existing_filtered)} rows from existing file (other funds).")
                                                        # Concatenate
                                                        df_combined = pd.concat([df_existing_filtered, df_new], ignore_index=True)
                                                        # Optional Sort
                                                        try:
                                                            df_combined = df_combined.sort_values(by=[date_col, fund_col])
                                                        except Exception as sort_err:
                                                             current_app.logger.warning(f"[{file_name}] Could not sort combined data: {sort_err}")
                                                        df_to_save = df_combined # Update the df to save
                                                        summary['save_action'] = 'Combined (Append/Overwrite)'
                                                        current_app.logger.info(f"[{file_name}] Prepared combined data ({len(df_to_save)} rows).")
                                                    else: # Key columns mismatch
                                                        current_app.logger.warning(f"[{file_name}] Key columns mismatch between existing ({date_col_existing}, {fund_col_existing}) and new ({date_col_new}, {fund_col_new}). Overwriting entire file.")
                                                        # df_to_save is already df_new
                                                        summary['save_action'] = 'Overwritten (Column Mismatch)'
                                                else: # Cannot proceed with merge if new cols weren't found (e.g., new data was empty)
                                                    current_app.logger.warning(f"[{file_name}] Cannot merge TS data as key columns were not identified in new data. Overwriting.")
                                                    # df_to_save is already df_new
                                                    summary['save_action'] = 'Overwritten (Merge Skipped)'
                                            else: # Existing file is empty
                                                current_app.logger.warning(f"[{file_name}] Existing TS file is empty. Overwriting.")
                                                # df_to_save is already df_new
                                                summary['save_action'] = 'Overwritten (Existing Empty)'
                                        except pd.errors.EmptyDataError:
                                            current_app.logger.warning(f"[{file_name}] Existing TS file is empty (EmptyDataError). Overwriting.")
                                            # df_to_save is already df_new
                                            summary['save_action'] = 'Overwritten (Existing Empty)'
                                        except Exception as read_err:
                                            current_app.logger.error(f"[{file_name}] Error reading existing TS file: {read_err}. Overwriting.", exc_info=True)
                                            # df_to_save is already df_new
                                            summary['save_action'] = 'Overwritten (Read Error)'
                                            all_ts_files_succeeded = False # Failed to read existing TS file properly
                                    else: # No existing file (and not forcing overwrite)
                                        current_app.logger.info(f"[{file_name}] TS file does not exist (or overwrite mode is on and file was absent). Creating new file.")
                                        # df_to_save is already df_new
                                        summary['save_action'] = 'Created'
                                except ValueError as ve: # Catch _find_key_columns error
                                    current_app.logger.error(f"[{file_name}] TS validation failed: {ve}")
                                    summary['status'] = f'Error - TS Validation Failed: {ve}'
                                    summary['validation_status'] = 'Failed (Missing Columns)'
                                    all_ts_files_succeeded = False # TS validation failed
                                    df_to_save = None # Don't save if validation fails
                            # == PRE File Processing ==
                            elif file_type == 'pre':
                                current_app.logger.info(f"[{file_name}] Processing as pre_ file (checking column count).")
                                # df_to_save is already df_new (or empty df) - Always overwritten
                                if os.path.exists(output_path):
                                    try:
                                        # Check column count consistency (only if new data is not empty)
                                        if not df_new.empty:
                                            existing_header_df = pd.read_csv(output_path, nrows=0, low_memory=False) # Read only header
                                            existing_cols = existing_header_df.columns.tolist()
                                            new_cols = df_new.columns.tolist()
                                            if force_overwrite or len(existing_cols) != len(new_cols) or set(existing_cols) != set(new_cols):
                                                if force_overwrite:
                                                    current_app.logger.info(f"[{file_name}] Overwriting pre_ file as requested by user.")
                                                    summary['save_action'] = 'Overwritten (User Request)'
                                                else:
                                                    current_app.logger.warning(f"[{file_name}] Column count/names mismatch between existing pre_ file ({len(existing_cols)} cols: {existing_cols}) and new data ({len(new_cols)} cols: {new_cols}). Overwriting.")
                                                    summary['save_action'] = 'Overwritten (Column Mismatch)'
                                            else:
                                                current_app.logger.info(f"[{file_name}] Existing pre_ file found with matching columns. Overwriting.")
                                                summary['save_action'] = 'Overwritten'
                                        else: # New data is empty, just overwrite
                                            current_app.logger.info(f"[{file_name}] New data for pre_ file is empty. Overwriting existing file.")
                                            summary['save_action'] = 'Overwritten (New Data Empty)'
                                    except pd.errors.EmptyDataError:
                                         current_app.logger.warning(f"[{file_name}] Existing pre_ file is empty (EmptyDataError). Overwriting.")
                                         summary['save_action'] = 'Overwritten (Existing Empty)'
                                    except Exception as read_err:
                                         current_app.logger.error(f"[{file_name}] Error reading existing pre_ file header: {read_err}. Overwriting.", exc_info=True)
                                         summary['save_action'] = 'Overwritten (Read Error)'
                                else: # No existing pre_ file
                                    current_app.logger.info(f"[{file_name}] Pre_ file does not exist. Creating new file.")
                                    summary['save_action'] = 'Created'
                                # Note: df_to_save remains df_new for pre_ files.
                            # == Other File Processing ==
                            else: # Handle other files (e.g., sec_*)
                                # For now, treat 'other' files like 'pre_' files (overwrite, check column counts)
                                # This avoids the date/fund column check which might fail for sec_* files too
                                current_app.logger.info(f"[{file_name}] Processing as 'other' file type (using column count check).")
                                # df_to_save is already df_new (or empty df)
                                if os.path.exists(output_path):
                                    try:
                                        if not df_new.empty:
                                            existing_header_df = pd.read_csv(output_path, nrows=0, low_memory=False)
                                            existing_cols = existing_header_df.columns.tolist()
                                            new_cols = df_new.columns.tolist()
                                            if force_overwrite or len(existing_cols) != len(new_cols) or set(existing_cols) != set(new_cols):
                                                if force_overwrite:
                                                    current_app.logger.info(f"[{file_name}] Overwriting 'other' file as requested by user.")
                                                    summary['save_action'] = 'Overwritten (User Request)'
                                                else:
                                                    current_app.logger.warning(f"[{file_name}] Column count/names mismatch for 'other' file. Overwriting.")
                                                    summary['save_action'] = 'Overwritten (Column Mismatch)'
                                            else:
                                                current_app.logger.info(f"[{file_name}] Existing 'other' file found with matching columns. Overwriting.")
                                                summary['save_action'] = 'Overwritten'
                                        else:
                                            current_app.logger.info(f"[{file_name}] New data for 'other' file is empty. Overwriting existing file.")
                                            summary['save_action'] = 'Overwritten (New Data Empty)'
                                    except pd.errors.EmptyDataError:
                                         current_app.logger.warning(f"[{file_name}] Existing 'other' file is empty (EmptyDataError). Overwriting.")
                                         summary['save_action'] = 'Overwritten (Existing Empty)'
                                    except Exception as read_err:
                                         current_app.logger.error(f"[{file_name}] Error reading existing 'other' file header: {read_err}. Overwriting.", exc_info=True)
                                         summary['save_action'] = 'Overwritten (Read Error)'
                                else:
                                    current_app.logger.info(f"[{file_name}] 'Other' file does not exist. Creating new file.")
                                    summary['save_action'] = 'Created'
                            # 4. Save the Final DataFrame (Common step, if df_to_save is valid)
                            if df_to_save is not None: # Allow saving empty dataframe to overwrite/create
                                current_app.logger.info(f"[{file_name}] Attempting to save {len(df_to_save)} rows to {output_path} (Action: {summary['save_action']})")
                                # ... existing warning log ...
                                try:
                                    df_to_save.to_csv(output_path, index=False, header=True)
                                    current_app.logger.info(f"[{file_name}] Successfully saved data to {output_path}")
                                    summary['status'] = 'OK - Data Saved'
                                    # Update lines_in_file count after successful save
                                    try:
                                         with open(output_path, 'r', encoding='utf-8') as f:
                                              # Store in actual_lines as this is real API mode
                                              summary['actual_lines'] = sum(1 for line in f)
                                    except Exception:
                                         summary['actual_lines'] = 'N/A' # Or len(df_to_save)+1?
                                    # Validation step (consider if validate_data needs adjustment for pre_/other files)
                                    summary['validation_status'] = validate_data(df_to_save, file_name)
                                    current_app.logger.info(f"[{file_name}] Validation status: {summary['validation_status']})")
                                    # If validation fails for a TS file, should it mark all_ts_files_succeeded = False? Maybe.
                                    # if file_type == 'ts' and 'Error' in summary['validation_status']:
                                    #    all_ts_files_succeeded = False
                                    #    current_app.logger.warning(f"[{file_name}] TS file validation failed, marking overall TS process as failed.")
                                except Exception as write_err:
                                    current_app.logger.error(f"[{file_name}] Error writing final data to {output_path}: {write_err}", exc_info=True)
                                    summary['status'] = f'Error - Failed to save file: {write_err}'
                                    summary['validation_status'] = 'Failed (Save Error)'
                                    if file_type == 'ts': all_ts_files_succeeded = False # Save failed for ts_ file
                            # This case handles if df_new was None initially, or if df_to_save was set to None due to TS validation error
                            elif df_new is None: 
                                pass # Status already set when df_new was None
                            elif df_to_save is None and file_type == 'ts':
                                pass # Status already set from TS validation error
                            else: # Should not happen? Log if it does.
                                 current_app.logger.error(f"[{file_name}] Reached unexpected state where df_to_save is None but no prior error logged.")
                                 summary['status'] = 'Error - Internal Logic Error (df_to_save is None)'
                    except Exception as proc_err: # Catch errors during the fetch/process stage for one file
                        current_app.logger.error(f"Error processing real data for QueryID {query_id}, File {file_name}: {proc_err}", exc_info=True)
                        summary['status'] = f'Error - Processing failed: {proc_err}'
                        summary['validation_status'] = 'Failed (Processing Error)'
                        if file_type == 'ts': all_ts_files_succeeded = False # Processing failed for ts_ file
                else: # Simulate API Call (keep existing)
                    # ... existing simulation logic ...
                    # status = "Simulated OK" # Update summary status if needed
                    simulated_rows = _simulate_and_print_tqs_call(query_id, selected_funds, start_date_tqs_str, end_date_tqs_str)
                    summary['status'] = "Simulated OK"
                    summary['simulated_rows'] = simulated_rows
                    summary['simulated_lines'] = simulated_rows + 1 if simulated_rows > 0 else 0
                    summary['actual_rows'] = None # Ensure actual keys are None in sim mode
                    summary['actual_lines'] = None
            except Exception as outer_err: # Catch unexpected errors during the processing of a single query's try block
                 current_app.logger.error(f"Unexpected outer error processing QueryID {query_id} ({file_name}): {outer_err}", exc_info=True)
                 # Ensure status reflects the outer error if not already set
                 if summary['status'] == 'Pending' or summary['status'].startswith('Warning'):
                      summary['status'] = f"Outer Processing Error: {outer_err}"
                 # Mark TS as failed if outer error occurs
                 if file_type == 'ts': all_ts_files_succeeded = False 
            # Append results for this query
            results_summary.append(summary)
            completed_queries += 1
            # Pause between real API calls (keep existing)
            if USE_REAL_TQS_API and completed_queries < total_queries: # Avoid pause after last call
                print(f"Pausing for 3 seconds before next real API call ({completed_queries}/{total_queries})...") # Optional status message
                time.sleep(3) 
        # After loop (keep existing)
        mode_message = "SIMULATED mode" if not USE_REAL_TQS_API else ("REAL API mode (Overwrite Enabled)" if overwrite_mode else "REAL API mode (Merge/Append)")
        final_status = "completed"
        if USE_REAL_TQS_API and not all_ts_files_succeeded:
             completion_message = f"Processed {completed_queries}/{total_queries} API calls ({mode_message}). WARNING: One or more ts_ files failed processing or validation."
             final_status = "completed_with_errors"
        else:
             completion_message = f"Processed {completed_queries}/{total_queries} API calls ({mode_message})."
        return jsonify({
            "status": final_status, # Provide more info on completion status
            "message": completion_message,
            "summary": results_summary
        })
    except ValueError as ve:
        # Handle potential errors like invalid integer conversion for days_back
        current_app.logger.error(f"Value error in /run_api_calls: {ve}", exc_info=True)
        return jsonify({"status": "error", "message": f"Invalid input value: {ve}"}), 400
    except FileNotFoundError as fnf:
        # Specific handling for file not found during setup (e.g., QueryMap)
        current_app.logger.error(f"File not found error in /run_api_calls: {fnf}", exc_info=True)
        return jsonify({"status": "error", "message": f"Required file not found: {fnf}"}), 500
    except Exception as e:
        # Catch-all for other unexpected errors during the process
        current_app.logger.error(f"Unexpected error in /run_api_calls: {e}", exc_info=True)
        return jsonify({"status": "error", "message": f"An unexpected error occurred: {e}"}), 500
# === NEW RERUN ROUTE ===
@api_bp.route('/rerun-api-call', methods=['POST'])
def rerun_api_call():
    '''Handles the request to rerun a single API call (real or simulated).'''
    try:
        data = request.get_json()
        query_id = data.get('query_id')
        days_back = int(data.get('days_back', 30))
        end_date_str = data.get('end_date')
        selected_funds = data.get('funds', []) # Get the list of funds
        overwrite_mode = data.get('overwrite_mode', False) # Get the new overwrite flag
        # --- Basic Input Validation ---
        if not query_id:
            return jsonify({"status": "error", "message": "Query ID is required."}), 400
        if not end_date_str:
            return jsonify({"status": "error", "message": "End date is required."}), 400
        if not selected_funds:
             # Allow rerunning even if no funds are selected? Decide based on API behavior.
             # For now, let's require funds similar to the initial run.
             return jsonify({"status": "error", "message": "At least one fund must be selected."}), 400
        # --- Calculate Dates ---
        end_date = pd.to_datetime(end_date_str)
        start_date = end_date - pd.Timedelta(days=days_back)
        start_date_tqs_str = start_date.strftime('%Y-%m-%d')
        end_date_tqs_str = end_date.strftime('%Y-%m-%d')
        # --- Find FileName from QueryMap ---
        data_folder = current_app.config.get('DATA_FOLDER', 'Data')
        query_map_path = os.path.join(data_folder, 'QueryMap.csv')
        if not os.path.exists(query_map_path):
            return jsonify({"status": "error", "message": f"QueryMap.csv not found at {query_map_path}"}), 500
        query_map_df = pd.read_csv(query_map_path)
        # Ensure comparison is string vs string
        query_map_df['QueryID'] = query_map_df['QueryID'].astype(str)
        if 'QueryID' not in query_map_df.columns or 'FileName' not in query_map_df.columns:
             return jsonify({"status": "error", "message": "QueryMap.csv missing required columns (QueryID, FileName)."}), 500
        # Compare string query_id from request with string QueryID column
        query_row = query_map_df[query_map_df['QueryID'] == query_id]
        if query_row.empty:
            # Log the types for debugging if it still fails
            current_app.logger.warning(f"QueryID '{query_id}' (type: {type(query_id)}) not found in QueryMap QueryIDs (types: {query_map_df['QueryID'].apply(type).unique()}).")
            return jsonify({"status": "error", "message": f"QueryID '{query_id}' not found in QueryMap.csv."}), 404
        file_name = query_row.iloc[0]['FileName']
        output_path = os.path.join(data_folder, file_name)
        # --- Execute Single API Call (Simulated or Real) ---
        status = "Rerun Error: Unknown"
        rows_returned = 0
        lines_in_file = 0
        actual_df = None
        simulated_rows = None # Initialize simulation keys too
        simulated_lines = None
        try:
            if USE_REAL_TQS_API:
                # --- Real API Call, Validation, and Save ---
                actual_df = _fetch_real_tqs_data(query_id, selected_funds, start_date_tqs_str, end_date_tqs_str)
                if actual_df is not None and isinstance(actual_df, pd.DataFrame):
                    rows_returned = len(actual_df)
                    if actual_df.empty:
                        current_app.logger.info(f"(Rerun) API returned empty DataFrame for {query_id} ({file_name}). Saving empty file.")
                        status = "Saved OK (Empty)"
                    else:
                        is_valid, validation_errors = validate_data(actual_df, file_name)
                        if not is_valid:
                            current_app.logger.warning(f"(Rerun) Data validation failed for {file_name}: {validation_errors}")
                            status = f"Validation Failed: {'; '.join(validation_errors)}"
                            lines_in_file = 0
                        # else: Validation passed
                    if not status.startswith("Validation Failed"):
                        try:
                            os.makedirs(os.path.dirname(output_path), exist_ok=True)
                            actual_df.to_csv(output_path, index=False)
                            current_app.logger.info(f"(Rerun) Successfully saved data to {output_path}")
                            lines_in_file = rows_returned + 1
                            if status != "Saved OK (Empty)":
                                status = "Saved OK"
                        except Exception as e:
                            current_app.logger.error(f"(Rerun) Error saving DataFrame to {output_path}: {e}", exc_info=True)
                            status = f"Save Error: {e}"
                            lines_in_file = 0
                elif actual_df is None:
                    current_app.logger.warning(f"(Rerun) Real API call/fetch for {query_id} ({file_name}) returned None.")
                    status = "No Data / API Error / TQS Missing"
                    rows_returned = 0
                    lines_in_file = 0
                else:
                    current_app.logger.error(f"(Rerun) Real API fetch for {query_id} ({file_name}) returned unexpected type: {type(actual_df)}.")
                    status = "API Returned Invalid Type"
                    rows_returned = 0
                    lines_in_file = 0
            else:
                # --- Simulate API Call ---
                simulated_rows = _simulate_and_print_tqs_call(query_id, selected_funds, start_date_tqs_str, end_date_tqs_str)
                rows_returned = simulated_rows
                lines_in_file = simulated_rows + 1 if simulated_rows > 0 else 0
                status = "Simulated OK"
        except Exception as e:
            current_app.logger.error(f"Error during single rerun for query {query_id} ({file_name}): {e}", exc_info=True)
            status = f"Processing Error: {e}"
            rows_returned = 0
            lines_in_file = 0
        # --- Return Result for the Single Query ---
        result_data = {
            "status": status,
             # Provide consistent keys for the frontend to update the table
            "simulated_rows": simulated_rows, # Value if simulated, None otherwise
            "actual_rows": rows_returned if USE_REAL_TQS_API else None, # Value if real, None otherwise
            "simulated_lines": simulated_lines, # Value if simulated, None otherwise
            "actual_lines": lines_in_file if USE_REAL_TQS_API else None # Value if real, None otherwise
        }
        return jsonify(result_data)
    except ValueError as ve:
        current_app.logger.error(f"Value error in /rerun-api-call: {ve}", exc_info=True)
        return jsonify({"status": "error", "message": f"Invalid input value: {ve}"}), 400
    except FileNotFoundError as fnf:
        current_app.logger.error(f"File not found error in /rerun-api-call: {fnf}", exc_info=True)
        return jsonify({"status": "error", "message": f"Required file not found: {fnf}"}), 500
    except Exception as e:
        current_app.logger.error(f"Unexpected error in /rerun-api-call: {e}", exc_info=True)
        return jsonify({"status": "error", "message": f"An unexpected error occurred: {e}"}), 500
# Ensure no code remains after this point in the file for this function.
</file>

<file path="views/comparison_views.py">
# views/comparison_views.py
# This module defines the Flask Blueprint for comparing two security spread datasets.
# It includes routes for a summary view listing securities with comparison metrics
# and a detail view showing overlayed time-series charts and statistics for a single security.
from flask import Blueprint, render_template, request, current_app, jsonify
import pandas as pd
import os
import logging
# Assuming security_processing and utils are in the parent directory or configured in PYTHONPATH
try:
    from security_processing import load_and_process_security_data, calculate_security_latest_metrics # May need adjustments
    from utils import parse_fund_list # Example utility
    from config import DATA_FOLDER, COLOR_PALETTE
except ImportError:
    # Handle potential import errors if the structure is different
    logging.error("Could not import required modules from parent directory.")
    # Add fallback imports or path adjustments if necessary
    # Example: sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
    from ..security_processing import load_and_process_security_data, calculate_security_latest_metrics
    from ..utils import parse_fund_list
    from ..config import DATA_FOLDER, COLOR_PALETTE
comparison_bp = Blueprint('comparison_bp', __name__,
                        template_folder='../templates',
                        static_folder='../static')
# Configure logging
log = logging.getLogger(__name__)
# --- Data Loading and Processing ---
def load_comparison_data(file1='sec_spread.csv', file2='sec_spreadSP.csv'):
    """Loads, processes, and merges data from two security spread files.
    Returns:
        tuple: (merged_df, static_data, common_static_cols, id_col_name)
               Returns (pd.DataFrame(), pd.DataFrame(), [], None) on error.
    """
    log.info(f"Loading comparison data: {file1} and {file2}")
    # Pass only the filename, as load_and_process_security_data prepends DATA_FOLDER internally
    df1, static_cols1 = load_and_process_security_data(file1)
    df2, static_cols2 = load_and_process_security_data(file2)
    if df1.empty or df2.empty:
        log.warning(f"One or both dataframes are empty. File1 empty: {df1.empty}, File2 empty: {df2.empty}")
        return pd.DataFrame(), pd.DataFrame(), [], None # Return None for id_col_name
    # Identify common static columns (excluding the ID column used for merging)
    common_static_cols = list(set(static_cols1) & set(static_cols2))
    # Get the actual ID column name (should be the same for both, use df1)
    if df1.index.nlevels == 2:
        id_col_name = df1.index.names[1] # Assuming 'Security ID'/Name is the second level
        log.info(f"Identified ID column from index: {id_col_name}")
    else:
        log.error("Processed DataFrame df1 does not have the expected 2-level MultiIndex.")
        return pd.DataFrame(), pd.DataFrame(), [], None # Return None for id_col_name
    # Prepare for merge - keep only necessary columns and rename Value columns
    df1_merge = df1.reset_index()[[id_col_name, 'Date', 'Value'] + common_static_cols].rename(columns={'Value': 'Value_Orig'})
    df2_merge = df2.reset_index()[[id_col_name, 'Date', 'Value']].rename(columns={'Value': 'Value_New'}) # Don't need static cols twice
    # Perform an outer merge to keep all dates and securities from both files
    merged_df = pd.merge(df1_merge, df2_merge, on=[id_col_name, 'Date'], how='outer')
    # Calculate daily changes
    merged_df = merged_df.sort_values(by=[id_col_name, 'Date'])
    merged_df['Change_Orig'] = merged_df.groupby(id_col_name)['Value_Orig'].diff()
    merged_df['Change_New'] = merged_df.groupby(id_col_name)['Value_New'].diff()
    # Store static data separately - get the latest version per security
    static_data = merged_df.groupby(id_col_name)[common_static_cols].last().reset_index()
    log.info(f"Successfully merged data. Shape: {merged_df.shape}")
    return merged_df, static_data, common_static_cols, id_col_name # Return the identified ID column name
def calculate_comparison_stats(merged_df, static_data, id_col):
    """Calculates comparison statistics for each security.
    Args:
        merged_df (pd.DataFrame): The merged dataframe of original and new values.
        static_data (pd.DataFrame): DataFrame with static info per security.
        id_col (str): The name of the column containing the Security ID/Name.
    """
    if merged_df.empty:
        return pd.DataFrame()
    if id_col not in merged_df.columns:
        log.error(f"Specified id_col '{id_col}' not found in merged_df columns: {merged_df.columns.tolist()}")
        return pd.DataFrame() # Cannot group without the ID column
    log.info(f"Calculating comparison statistics using ID column: {id_col}...")
    stats_list = []
    # Use the passed id_col here
    for sec_id, group in merged_df.groupby(id_col):
        sec_stats = {id_col: sec_id} # Use actual id_col name
        # Filter out rows where both values are NaN for overall analysis period
        group_valid_overall = group.dropna(subset=['Value_Orig', 'Value_New'], how='all')
        overall_min_date = group_valid_overall['Date'].min()
        overall_max_date = group_valid_overall['Date'].max()
        # Filter out rows where EITHER value is NaN for correlation/diff calculations
        valid_comparison = group.dropna(subset=['Value_Orig', 'Value_New'])
        # 1. Correlation of Levels
        if len(valid_comparison) >= 2: # Need at least 2 points for correlation
            # Use the NaN-dropped dataframe for correlation
            level_corr = valid_comparison['Value_Orig'].corr(valid_comparison['Value_New'])
            sec_stats['Level_Correlation'] = level_corr if pd.notna(level_corr) else None
        else:
             sec_stats['Level_Correlation'] = None
        # 2. Max / Min (use original group to get true max/min including non-overlapping points)
        sec_stats['Max_Orig'] = group['Value_Orig'].max()
        sec_stats['Min_Orig'] = group['Value_Orig'].min()
        sec_stats['Max_New'] = group['Value_New'].max()
        sec_stats['Min_New'] = group['Value_New'].min()
        # 3. Date Range Comparison - Refined Logic
        # Find min/max dates within the MERGED data where each series is individually valid
        min_date_orig_idx = group['Value_Orig'].first_valid_index()
        max_date_orig_idx = group['Value_Orig'].last_valid_index()
        min_date_new_idx = group['Value_New'].first_valid_index()
        max_date_new_idx = group['Value_New'].last_valid_index()
        sec_stats['Start_Date_Orig'] = group.loc[min_date_orig_idx, 'Date'] if min_date_orig_idx is not None else None
        sec_stats['End_Date_Orig'] = group.loc[max_date_orig_idx, 'Date'] if max_date_orig_idx is not None else None
        sec_stats['Start_Date_New'] = group.loc[min_date_new_idx, 'Date'] if min_date_new_idx is not None else None
        sec_stats['End_Date_New'] = group.loc[max_date_new_idx, 'Date'] if max_date_new_idx is not None else None
        # Check if the start and end dates MATCH for the valid periods of EACH series
        same_start = pd.Timestamp(sec_stats['Start_Date_Orig']) == pd.Timestamp(sec_stats['Start_Date_New']) if sec_stats['Start_Date_Orig'] and sec_stats['Start_Date_New'] else False
        same_end = pd.Timestamp(sec_stats['End_Date_Orig']) == pd.Timestamp(sec_stats['End_Date_New']) if sec_stats['End_Date_Orig'] and sec_stats['End_Date_New'] else False
        sec_stats['Same_Date_Range'] = same_start and same_end
        # Add overall date range for info
        sec_stats['Overall_Start_Date'] = overall_min_date
        sec_stats['Overall_End_Date'] = overall_max_date
        # 4. Correlation of Daily Changes (Volatility Alignment)
        # Use the dataframe where BOTH values are non-NaN to calculate changes for correlation
        valid_comparison = valid_comparison.copy() # Avoid SettingWithCopyWarning
        valid_comparison['Change_Orig_Corr'] = valid_comparison['Value_Orig'].diff()
        valid_comparison['Change_New_Corr'] = valid_comparison['Value_New'].diff()
        # Drop NaNs created by the diff() itself (first row)
        valid_changes = valid_comparison.dropna(subset=['Change_Orig_Corr', 'Change_New_Corr'])
        # --- Debug Logging Start ---
        # if sec_id == 'Alpha001': # Log only for a specific security to avoid flooding
        #     log.debug(f"Debug {sec_id} - valid_changes DataFrame (first 5 rows):\n{valid_changes.head()}")
        #     log.debug(f"Debug {sec_id} - valid_changes count: {len(valid_changes)}")
        # --- Debug Logging End ---
        if len(valid_changes) >= 2:
            change_corr = valid_changes['Change_Orig_Corr'].corr(valid_changes['Change_New_Corr'])
            sec_stats['Change_Correlation'] = change_corr if pd.notna(change_corr) else None
        else:
            sec_stats['Change_Correlation'] = None
            # Log why correlation is None
            log.debug(f"Cannot calculate Change_Correlation for {sec_id}. Need >= 2 valid change pairs, found {len(valid_changes)}.")
        # 5. Difference Statistics (use the valid_comparison df where both values exist)
        valid_comparison['Abs_Diff'] = (valid_comparison['Value_Orig'] - valid_comparison['Value_New']).abs()
        sec_stats['Mean_Abs_Diff'] = valid_comparison['Abs_Diff'].mean() # Mean diff where both values exist
        sec_stats['Max_Abs_Diff'] = valid_comparison['Abs_Diff'].max() # Max diff where both values exist
        # Count NaNs - use original group
        sec_stats['NaN_Count_Orig'] = group['Value_Orig'].isna().sum()
        sec_stats['NaN_Count_New'] = group['Value_New'].isna().sum()
        sec_stats['Total_Points'] = len(group)
        stats_list.append(sec_stats)
    summary_df = pd.DataFrame(stats_list)
    # Merge static data back
    if not static_data.empty and id_col in static_data.columns and id_col in summary_df.columns:
        summary_df = pd.merge(summary_df, static_data, on=id_col, how='left')
    elif not static_data.empty:
         log.warning(f"Could not merge static data back. ID column '{id_col}' missing from static_data ({id_col in static_data.columns}) or summary_df ({id_col in summary_df.columns}).")
    log.info(f"Finished calculating stats. Summary shape: {summary_df.shape}")
    return summary_df
# --- Routes ---
@comparison_bp.route('/comparison/summary')
def summary():
    """Displays the comparison summary page with filtering and sorting."""
    try:
        # Capture the actual ID column name returned by the load function
        merged_data, static_data, static_cols, actual_id_col = load_comparison_data()
        if actual_id_col is None:
            log.error("Failed to get ID column name during data loading. Cannot proceed.")
            return "Error loading comparison data: Could not determine ID column.", 500
        # Pass the actual ID column name to the stats calculation function
        summary_stats = calculate_comparison_stats(merged_data, static_data, id_col=actual_id_col)
        if summary_stats.empty and not merged_data.empty:
             log.warning("Calculation resulted in empty stats DataFrame, but merged data was present.")
        # --- Filtering ---
        # Get active filters from query parameters (e.g., ?filter_Metric=Govt&filter_Currency=USD)
        active_filters = {k.replace('filter_', ''): v 
                          for k, v in request.args.items() 
                          if k.startswith('filter_') and v} # Only keep non-empty filters
        # Apply filters if any are active
        filtered_stats = summary_stats.copy()
        if active_filters:
            log.info(f"Applying filters: {active_filters}")
            for col, value in active_filters.items():
                if col in filtered_stats.columns:
                    # Ensure we handle potential type mismatches (e.g., filtering numeric columns)
                    try:
                        # Attempt to convert filter value to column type if numeric, otherwise use string comparison
                        if pd.api.types.is_numeric_dtype(filtered_stats[col]):
                             # Handle potential conversion errors if value is not numeric
                            try:
                                value_converted = pd.to_numeric(value)
                                filtered_stats = filtered_stats[filtered_stats[col] == value_converted]
                            except ValueError:
                                log.warning(f"Could not convert filter value '{value}' to numeric for column '{col}'. Skipping filter.")
                                # Optionally keep all rows if conversion fails, or filter for string match?
                                # For now, we skip this specific filter if conversion fails.
                        else:
                             # String comparison (case-insensitive)
                             filtered_stats = filtered_stats[filtered_stats[col].astype(str).str.contains(value, case=False, na=False)]
                    except Exception as filter_err:
                        log.error(f"Error applying filter for column '{col}' with value '{value}': {filter_err}")
                else:
                    log.warning(f"Filter column '{col}' not found in summary statistics.")
            log.info(f"Stats shape after filtering: {filtered_stats.shape}")
        # --- Sorting ---
        # Get sort parameters from query (default to Change_Correlation descending)
        sort_by = request.args.get('sort_by', 'Change_Correlation')
        sort_order = request.args.get('sort_order', 'desc')
        ascending = sort_order == 'asc'
        # Validate sort_by column
        if sort_by not in filtered_stats.columns:
            log.warning(f"Invalid sort column '{sort_by}'. Defaulting to 'Change_Correlation'.")
            sort_by = 'Change_Correlation'
            # Ensure default column exists, otherwise use ID col
            if sort_by not in filtered_stats.columns and actual_id_col in filtered_stats.columns:
                 sort_by = actual_id_col
        # Apply sorting if the column exists
        if sort_by in filtered_stats.columns:
             log.info(f"Sorting by '{sort_by}' ({'ascending' if ascending else 'descending'})")
             # Handle NaNs: put them last regardless of sort order
             na_position = 'last' 
             sorted_stats = filtered_stats.sort_values(by=sort_by, ascending=ascending, na_position=na_position)
        else:
             log.warning(f"Sort column '{sort_by}' not found after filtering. Skipping sort.")
             sorted_stats = filtered_stats # Keep the filtered but unsorted data
        # --- Prepare Data for Template ---
        # Generate filter options FROM THE ORIGINAL UNFILTERED DATA static columns
        filter_options = {}
        if not summary_stats.empty:
             # Use static_cols identified during loading for filter options
             for col in static_cols:
                 if col in summary_stats.columns and col != actual_id_col: # Exclude ID col from filters typically
                     # Get unique, non-null, sorted values
                     options = sorted([str(v) for v in summary_stats[col].dropna().unique()])
                     if options: # Only add filter if there are options
                         filter_options[col] = options
        # Convert NaNs to None for template rendering
        sorted_stats = sorted_stats.where(pd.notnull(sorted_stats), None)
        # Prepare data for template
        table_data = sorted_stats.to_dict(orient='records')
        # Determine visible columns (adjust as needed)
        # Start with ID, Name (if different), core stats, then static cols
        visible_columns = [actual_id_col]
        if 'Security Name' in sorted_stats.columns and actual_id_col != 'Security Name':
             visible_columns.append('Security Name')
        visible_columns.extend([
            'Level_Correlation', 'Change_Correlation', 'Mean_Abs_Diff', 
            'Max_Abs_Diff', 'Same_Date_Range', 'NaN_Count_Orig', 
            'NaN_Count_New', 'Total_Points'
        ])
        # Add static columns that are NOT the ID or Name (if shown separately)
        for col in static_cols:
             if col not in visible_columns:
                  visible_columns.append(col)
        # Filter visible_columns to only include those actually present in the final df
        visible_columns = [col for col in visible_columns if col in sorted_stats.columns]
        log.info(f"Rendering comparison summary with {len(table_data)} rows.")
        return render_template('comparison_page.html',
                               table_data=table_data,
                               # Pass the ordered list of columns to display
                               columns_to_display=visible_columns, 
                               filter_options=filter_options,
                               active_filters=active_filters, # Pass current filters back to template
                               id_column_name=actual_id_col, # Pass actual ID col name
                               current_sort_by=sort_by,
                               current_sort_order=sort_order)
    except Exception as e:
        log.exception("Error generating comparison summary page.")
        # Render an error page or return an error message
        return f"An error occurred: {e}", 500
@comparison_bp.route('/comparison/details/<path:security_id>')
def details(security_id):
    """Displays the comparison details page for a single security."""
    try:
        # Reload or filter the merged data for the specific security
        # This might be inefficient - consider caching or passing data if possible
        merged_data, static_data, _, actual_id_col = load_comparison_data()
        if actual_id_col is None:
            log.error("Failed to get ID column name during data loading for details page.")
            return "Error loading comparison data: Could not determine ID column.", 500
        # Filter using the actual ID column name
        security_data = merged_data[merged_data[actual_id_col] == security_id].copy()
        if security_data.empty:
            return "Security ID not found", 404
        # Get the static data for this specific security
        sec_static_data = static_data[static_data[actual_id_col] == security_id]
        # Recalculate detailed stats for this security, passing the correct ID column
        stats_df = calculate_comparison_stats(security_data.copy(), sec_static_data, id_col=actual_id_col)
        security_stats = stats_df.iloc[0].where(pd.notnull(stats_df.iloc[0]), None).to_dict() if not stats_df.empty else {}
        # Prepare chart data
        security_data['Date_Str'] = security_data['Date'].dt.strftime('%Y-%m-%d')
        # Convert NaN to None using list comprehension after .tolist()
        data_orig = security_data['Value_Orig'].tolist()
        data_orig_processed = [None if pd.isna(x) else x for x in data_orig]
        data_new = security_data['Value_New'].tolist()
        data_new_processed = [None if pd.isna(x) else x for x in data_new]
        chart_data = {
            'labels': security_data['Date_Str'].tolist(),
            'datasets': [
                {
                    'label': 'Original Spread (Sec_spread)',
                    'data': data_orig_processed, # Use processed list
                    'borderColor': COLOR_PALETTE[0 % len(COLOR_PALETTE)],
                    'tension': 0.1
                },
                {
                    'label': 'New Spread (Sec_spreadSP)',
                    'data': data_new_processed, # Use processed list
                    'borderColor': COLOR_PALETTE[1 % len(COLOR_PALETTE)],
                    'tension': 0.1
                }
            ]
        }
        # Get static attributes for display (use actual_id_col if it's 'Security Name')
        # Best to get from security_stats which should now include merged static data
        security_name_display = security_stats.get('Security Name', security_id) if actual_id_col == 'Security Name' else security_id
        # If 'Security Name' is not the ID, try to get it from stats
        if actual_id_col != 'Security Name' and 'Security Name' in security_stats:
             security_name_display = security_stats.get('Security Name', security_id)
        return render_template('comparison_details_page.html',
                               security_id=security_id,
                               security_name=security_name_display,
                               chart_data=chart_data, # Pass as JSONifiable dict
                               stats=security_stats, # Pass comparison stats
                               id_column_name=actual_id_col) # Pass actual ID col name
    except Exception as e:
        log.exception(f"Error generating comparison details page for {security_id}.")
        return f"An error occurred: {e}", 500
</file>

<file path="views/exclusion_views.py">
"""
This module defines the Flask Blueprint for handling security exclusion management.
It provides routes to view the current exclusion list and add new securities
to the list.
"""
import os
import pandas as pd
from flask import Blueprint, render_template, request, redirect, url_for, current_app
from datetime import datetime
import logging
# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
# Define the Blueprint
exclusion_bp = Blueprint('exclusion_bp', __name__, template_folder='../templates')
EXCLUSIONS_FILE = 'exclusions.csv'
# Assuming sec_spread.csv contains the list of all possible securities
# We need to determine the correct file and column name for security IDs
# Let's tentatively use sec_spread.csv and 'Security ID'
SECURITIES_SOURCE_FILE = 'sec_spread.csv' # Adjust if needed
SECURITY_ID_COLUMN = 'Security Name' # Corrected column name
def get_data_path(filename):
    """Constructs the full path to a data file within the DATA_FOLDER."""
    return os.path.join(current_app.config['DATA_FOLDER'], filename)
def load_exclusions():
    """Loads the current list of exclusions from the CSV file."""
    exclusions_path = get_data_path(EXCLUSIONS_FILE)
    try:
        if os.path.exists(exclusions_path) and os.path.getsize(exclusions_path) > 0:
            df = pd.read_csv(exclusions_path, parse_dates=['AddDate', 'EndDate'], dayfirst=False) # Specify date format if needed
            # Ensure correct types after loading
            df['AddDate'] = pd.to_datetime(df['AddDate'], errors='coerce')
            df['EndDate'] = pd.to_datetime(df['EndDate'], errors='coerce')
            df['SecurityID'] = df['SecurityID'].astype(str)
            df['Comment'] = df['Comment'].astype(str)
            df = df.sort_values(by='AddDate', ascending=False)
            return df.to_dict('records')
        else:
            logging.info(f"'{EXCLUSIONS_FILE}' is empty or does not exist. Returning empty list.")
            return []
    except Exception as e:
        logging.error(f"Error loading exclusions from {exclusions_path}: {e}")
        return [] # Return empty list on error
def load_available_securities():
    """Loads the list of available security IDs from the source file."""
    securities_file_path = get_data_path(SECURITIES_SOURCE_FILE)
    try:
        if os.path.exists(securities_file_path):
            # Load only the necessary column
            # Use security_processing logic if more complex loading is needed
            df_securities = pd.read_csv(securities_file_path, usecols=[SECURITY_ID_COLUMN], encoding_errors='replace', on_bad_lines='skip')
            df_securities.dropna(subset=[SECURITY_ID_COLUMN], inplace=True)
            security_ids = df_securities[SECURITY_ID_COLUMN].astype(str).unique().tolist()
            security_ids.sort() # Sort for dropdown consistency
            return security_ids
        else:
            logging.warning(f"Securities source file '{SECURITIES_SOURCE_FILE}' not found at {securities_file_path}.")
            return []
    except KeyError:
        logging.error(f"Column '{SECURITY_ID_COLUMN}' not found in '{SECURITIES_SOURCE_FILE}'. Cannot load available securities.")
        return []
    except Exception as e:
        logging.error(f"Error loading available securities from {securities_file_path}: {e}")
        return []
def add_exclusion(security_id, end_date_str, comment):
    """Adds a new exclusion to the CSV file."""
    exclusions_path = get_data_path(EXCLUSIONS_FILE)
    try:
        # Basic validation
        if not security_id or not comment:
            logging.warning("Attempted to add exclusion with missing SecurityID or Comment.")
            return False, "Security ID and Comment are required."
        add_date = datetime.now().strftime('%Y-%m-%d')
        # Parse end_date, allow it to be empty
        end_date = pd.to_datetime(end_date_str, errors='coerce').strftime('%Y-%m-%d') if end_date_str else ''
        new_exclusion = pd.DataFrame({
            'SecurityID': [str(security_id)],
            'AddDate': [add_date],
            'EndDate': [end_date],
            'Comment': [str(comment)]
        })
        # Append to CSV, create header if file doesn't exist or is empty
        file_exists = os.path.exists(exclusions_path)
        is_empty = file_exists and os.path.getsize(exclusions_path) == 0
        write_header = not file_exists or is_empty
        new_exclusion.to_csv(exclusions_path, mode='a', header=write_header, index=False)
        logging.info(f"Added exclusion for SecurityID: {security_id}")
        return True, "Exclusion added successfully."
    except Exception as e:
        logging.error(f"Error adding exclusion to {exclusions_path}: {e}")
        return False, "An error occurred while saving the exclusion."
def remove_exclusion(security_id_to_remove, add_date_str_to_remove):
    """Removes a specific exclusion entry from the CSV file based on SecurityID and AddDate."""
    exclusions_path = get_data_path(EXCLUSIONS_FILE)
    try:
        if not os.path.exists(exclusions_path) or os.path.getsize(exclusions_path) == 0:
            logging.warning(f"Attempted to remove exclusion, but '{EXCLUSIONS_FILE}' is empty or does not exist.")
            return False, "Exclusion file is empty or missing."
        df = pd.read_csv(exclusions_path)
        # Ensure columns used for matching are strings
        df['SecurityID'] = df['SecurityID'].astype(str)
        # Keep AddDate as string for direct comparison with the string from the form
        df['AddDate'] = df['AddDate'].astype(str)
        security_id_to_remove = str(security_id_to_remove)
        original_count = len(df)
        # Filter out the row(s) to remove
        # Match both SecurityID and the AddDate string
        df_filtered = df[~((df['SecurityID'] == security_id_to_remove) & (df['AddDate'] == add_date_str_to_remove))]
        if len(df_filtered) == original_count:
            logging.warning(f"Exclusion entry for SecurityID '{security_id_to_remove}' with AddDate '{add_date_str_to_remove}' not found for removal.")
            return False, "Exclusion entry not found."
        # Overwrite the CSV with the filtered data
        df_filtered.to_csv(exclusions_path, index=False)
        logging.info(f"Removed exclusion entry for SecurityID: {security_id_to_remove}, AddDate: {add_date_str_to_remove}")
        return True, "Exclusion removed successfully."
    except Exception as e:
        logging.error(f"Error removing exclusion from {exclusions_path}: {e}")
        return False, "An error occurred while removing the exclusion."
@exclusion_bp.route('/exclusions', methods=['GET', 'POST'])
def manage_exclusions():
    """
    Handles viewing and adding security exclusions.
    GET: Displays the list of current exclusions and the form to add new ones.
    POST: Processes the form submission to add a new exclusion.
    """
    message = None
    message_type = 'info' # Can be 'success' or 'error'
    if request.method == 'POST':
        security_id = request.form.get('security_id')
        end_date_str = request.form.get('end_date')
        comment = request.form.get('comment')
        success, msg = add_exclusion(security_id, end_date_str, comment)
        if success:
            # Redirect to the same page using GET to prevent form resubmission
            return redirect(url_for('exclusion_bp.manage_exclusions', _external=True))
        else:
            message = msg
            message_type = 'error'
            # Fall through to render the page again with the error message
    # For both GET requests and POST failures, load data and render template
    current_exclusions = load_exclusions()
    available_securities = load_available_securities()
    return render_template('exclusions_page.html',
                           exclusions=current_exclusions,
                           available_securities=available_securities,
                           message=message,
                           message_type=message_type)
@exclusion_bp.route('/exclusions/remove', methods=['POST'])
def remove_exclusion_route():
    """Handles the POST request to remove an exclusion."""
    security_id = request.form.get('security_id')
    add_date_str = request.form.get('add_date') # Get AddDate as string
    if not security_id or not add_date_str:
        # Handle missing identifiers (shouldn't happen with hidden fields but good practice)
        # Redirect back with an error message (using flash or query params)
        logging.warning("Remove exclusion attempt missing SecurityID or AddDate.")
        # For simplicity, redirect back to the main page; flash messages would be better
        return redirect(url_for('exclusion_bp.manage_exclusions'))
    success, msg = remove_exclusion(security_id, add_date_str)
    # Regardless of success/failure, redirect back to the main exclusions page.
    # Consider using flash messages to display the success/error message after redirect.
    # For now, the message `msg` is logged but not shown to the user on redirect.
    return redirect(url_for('exclusion_bp.manage_exclusions'))
</file>

<file path="views/fund_views.py">
"""Blueprint for fund-specific routes, currently focusing on duration details."""
from flask import Blueprint, render_template
import os
import pandas as pd
import traceback
# Import necessary functions from other modules
from config import DATA_FOLDER
from utils import _is_date_like, parse_fund_list # Import required utils
# Define the blueprint
fund_bp = Blueprint('fund', __name__, url_prefix='/fund')
@fund_bp.route('/duration_details/<fund_code>') # Corresponds to /fund/duration_details/...
def fund_duration_details(fund_code):
    """Renders a page showing duration changes for securities held by a specific fund."""
    duration_filename = "sec_duration.csv"
    data_filepath = os.path.join(DATA_FOLDER, duration_filename)
    print(f"--- Requesting Duration Details for Fund: {fund_code} --- File: {duration_filename}")
    if not os.path.exists(data_filepath):
        print(f"Error: Duration file '{duration_filename}' not found.")
        return f"Error: Data file '{duration_filename}' not found.", 404
    try:
        # 1. Load the duration data (only header first for column identification)
        header_df = pd.read_csv(data_filepath, nrows=0, encoding='utf-8')
        all_cols = [col.strip() for col in header_df.columns.tolist()]
        # Define ID column (specific to this file/route)
        id_col_name = 'Security Name' # Assuming this remains the ID for this specific file
        if id_col_name not in all_cols:
            print(f"Error: Expected ID column '{id_col_name}' not found in {duration_filename}.")
            return f"Error: Required ID column '{id_col_name}' not found in '{duration_filename}'.", 500
        # 2. Identify static and date columns dynamically
        date_cols = []
        static_cols = []
        for col in all_cols:
            if col == id_col_name:
                continue # Skip the ID column
            if _is_date_like(col): # Use the helper function from utils
                date_cols.append(col)
            else:
                static_cols.append(col) # Treat others as static
        print(f"Dynamically identified Static Cols: {static_cols}")
        print(f"Dynamically identified Date Cols (first 5): {date_cols[:5]}...")
        if not date_cols or len(date_cols) < 2:
             print("Error: Not enough date columns found in duration file to calculate change.")
             return f"Error: Insufficient date columns in '{duration_filename}' to calculate change.", 500
        # Now read the full data
        df = pd.read_csv(data_filepath, encoding='utf-8')
        df.columns = df.columns.str.strip() # Strip again after full read
        # Ensure the Funds column exists (still needed for filtering)
        funds_col = 'Funds' # Keep this assumption for now as it's key to filtering
        if funds_col not in static_cols:
             print(f"Warning: Expected column '{funds_col}' for filtering not found among static columns.")
             # Decide how to handle this - error or proceed without fund filtering? Let's error for now.
             return f"Error: Required column '{funds_col}' for fund filtering not found.", 500
        # Ensure date columns are sortable (attempt conversion if needed, basic check)
        try:
            # Check and sort date columns using the correct YYYY-MM-DD format
            pd.to_datetime(date_cols, format='%Y-%m-%d', errors='raise')
            date_cols = sorted(date_cols, key=lambda d: pd.to_datetime(d, format='%Y-%m-%d'))
            print(f"Identified and sorted date columns (YYYY-MM-DD): {date_cols[-5:]} (last 5 shown)")
        except ValueError:
            print("Warning: Could not parse all date columns using YYYY-MM-DD format. Using original order.")
            # Fallback remains, but hopefully won't be needed as often
        # Identify last two date columns based on sorted list (or original if parsing failed)
        if len(date_cols) < 2: # Double check after potential parsing failure
            return f"Error: Insufficient valid date columns in '{duration_filename}' to calculate change after sorting attempt.", 500
        last_date_col = date_cols[-1]
        second_last_date_col = date_cols[-2]
        print(f"Using dates for change calculation: {second_last_date_col} and {last_date_col}")
        # Ensure the relevant date columns are numeric for calculation
        df[last_date_col] = pd.to_numeric(df[last_date_col], errors='coerce')
        df[second_last_date_col] = pd.to_numeric(df[second_last_date_col], errors='coerce')
        # 3. Filter by Fund Code
        # Apply the parsing function from utils to the 'Funds' column
        fund_lists = df['Funds'].apply(parse_fund_list)
        # Create a boolean mask to filter rows where the fund_code is in the parsed list
        mask = fund_lists.apply(lambda funds: fund_code in funds)
        filtered_df = df[mask].copy() # Use copy to avoid SettingWithCopyWarning
        if filtered_df.empty:
            print(f"No securities found for fund '{fund_code}' in {duration_filename}.")
            # Render a template indicating no data found for this fund
            return render_template('fund_duration_details.html',
                                   fund_code=fund_code,
                                   securities_data=[],
                                   column_order=[],
                                   id_col_name=None,
                                   message=f"No securities found held by fund '{fund_code}' in {duration_filename}.")
        print(f"Found {len(filtered_df)} securities for fund '{fund_code}'. Calculating changes...")
        # 4. Calculate 1-day Change
        change_col_name = '1 Day Duration Change'
        filtered_df[change_col_name] = filtered_df[last_date_col] - filtered_df[second_last_date_col]
        # 5. Sort by Change (descending, NaN last)
        filtered_df.sort_values(by=change_col_name, ascending=False, na_position='last', inplace=True)
        print(f"Sorted securities by {change_col_name}.")
        # 6. Prepare data for template
        # Select columns for display - use the dynamically identified static_cols
        # ID column is already defined as id_col_name
        # Filter static_cols to ensure they exist in the filtered_df after operations
        existing_static_cols = [col for col in static_cols if col in filtered_df.columns]
        display_cols = [id_col_name] + existing_static_cols + [second_last_date_col, last_date_col, change_col_name]
        final_col_order = [col for col in display_cols if col in filtered_df.columns] # Ensure only existing columns are kept
        securities_data_list = filtered_df[final_col_order].round(3).to_dict(orient='records')
        # Handle potential NaN values for template rendering
        for row in securities_data_list:
             for key, value in row.items():
                 if pd.isna(value):
                     row[key] = None
        print(f"Final column order for display: {final_col_order}")
        return render_template('fund_duration_details.html',
                               fund_code=fund_code,
                               securities_data=securities_data_list,
                               column_order=final_col_order,
                               id_col_name=id_col_name,
                               message=None)
    except FileNotFoundError:
         return f"Error: Data file '{duration_filename}' not found.", 404
    except Exception as e:
        print(f"Error processing duration details for fund {fund_code}: {e}")
        traceback.print_exc()
        return f"An error occurred processing duration details for fund {fund_code}: {e}", 500
</file>

<file path="views/main_views.py">
# This file defines the routes related to the main, top-level views of the application.
# It primarily handles the dashboard or index page.
"""
Blueprint for main application routes, like the index page.
"""
from flask import Blueprint, render_template
import os
import pandas as pd
import traceback
# Import necessary functions/constants from other modules
from config import DATA_FOLDER
from data_loader import load_and_process_data
from metric_calculator import calculate_latest_metrics
# Define the blueprint for main routes
main_bp = Blueprint('main', __name__)
@main_bp.route('/')
def index():
    """Renders the main dashboard page (`index.html`).
    This view performs the following steps:
    1. Scans the `DATA_FOLDER` for time-series metric files (prefixed with `ts_`).
    2. For each `ts_` file found:
        a. Loads and processes the data using `data_loader.load_and_process_data`.
        b. Calculates metrics (including Z-scores) using `metric_calculator.calculate_latest_metrics`.
        c. Extracts the 'Change Z-Score' columns for both the benchmark and any specific fund columns.
    3. Aggregates all extracted 'Change Z-Score' columns from all files into a single pandas DataFrame (`summary_df`).
    4. Creates unique column names for the summary table by combining the original column name and the metric file name
       (e.g., 'Benchmark - Yield', 'FUND_A - Duration').
    5. Passes the list of available metric display names (filenames without `ts_`) and the aggregated Z-score
       DataFrame (`summary_df`) along with its corresponding column headers (`summary_metrics`) to the `index.html` template.
    This allows the dashboard to display a consolidated view of the most recent significant changes across all metrics.
    """
    # Find only files starting with ts_ and ending with .csv
    files = [f for f in os.listdir(DATA_FOLDER) if f.startswith('ts_') and f.endswith('.csv')]
    # Create two lists: one for filenames (with ts_), one for display (without ts_)
    metric_filenames = sorted([os.path.splitext(f)[0] for f in files])
    metric_display_names = sorted([name[3:] for name in metric_filenames]) # Remove 'ts_' prefix
    all_z_scores_list = []
    # Store the unique combined column names for the summary table header
    processed_summary_columns = []
    print("Starting Change Z-score aggregation for dashboard (ts_ files only)...")
    # Iterate using the filenames with prefix
    for metric_filename in metric_filenames:
        filename = f"{metric_filename}.csv"
        # Get the corresponding display name for this file
        display_name = metric_filename[3:]
        try:
            print(f"Processing {filename}...")
            df, fund_cols, benchmark_col = load_and_process_data(filename)
            # Skip if no benchmark AND no fund columns identified
            if not benchmark_col and not fund_cols:
                 print(f"Warning: No benchmark or fund columns identified in {filename}. Skipping.")
                 continue
            # Calculate metrics using the current function
            latest_metrics = calculate_latest_metrics(df, fund_cols, benchmark_col)
            # --- Extract Change Z-score for ALL columns (benchmark + funds) --- 
            if not latest_metrics.empty:
                columns_to_check = []
                if benchmark_col:
                    columns_to_check.append(benchmark_col)
                if fund_cols:
                    columns_to_check.extend(fund_cols)
                if not columns_to_check:
                    print(f"Warning: No columns to check for Z-scores in {filename} despite loading data.")
                    continue
                print(f"Checking for Z-scores for columns: {columns_to_check} in metric {display_name}")
                found_z_for_metric = False
                for original_col_name in columns_to_check:
                    z_score_col_name = f'{original_col_name} Change Z-Score'
                    if z_score_col_name in latest_metrics.columns:
                        # Create a unique name for the summary table column
                        summary_col_name = f"{original_col_name} - {display_name}"
                        # Extract and rename
                        metric_z_scores = latest_metrics[[z_score_col_name]].rename(columns={z_score_col_name: summary_col_name})
                        all_z_scores_list.append(metric_z_scores)
                        # Add the unique column name to our list if not already present (preserves order of discovery)
                        if summary_col_name not in processed_summary_columns:
                             processed_summary_columns.append(summary_col_name)
                        found_z_for_metric = True
                        print(f"  -> Extracted: {summary_col_name}")
                    else:
                        print(f"  -> Z-score column '{z_score_col_name}' not found.")
                if not found_z_for_metric:
                    print(f"Warning: No Z-score columns found for any checked column in metric {display_name} (from {filename}).")
            else:
                 print(f"Warning: Could not calculate latest_metrics for {filename}. Skipping Z-score extraction.")
        except FileNotFoundError:
            print(f"Error: Data file '{filename}' not found.")
        except ValueError as ve:
            print(f"Value Error processing {metric_filename}: {ve}") # Log with filename
        except Exception as e:
            print(f"Error processing {metric_filename} during dashboard aggregation: {e}") # Log with filename
            traceback.print_exc()
    # Combine all Z-score Series/DataFrames into one
    summary_df = pd.DataFrame()
    if all_z_scores_list:
        summary_df = pd.concat(all_z_scores_list, axis=1)
        # Ensure the columns are in the order they were discovered
        if processed_summary_columns:
             # Handle potential missing columns if a file failed processing midway
             cols_available_in_summary = [col for col in processed_summary_columns if col in summary_df.columns]
             summary_df = summary_df[cols_available_in_summary]
             # Update the list of columns to only those actually present
             processed_summary_columns = cols_available_in_summary
        print("Successfully combined Change Z-scores.")
        print(f"Summary DF columns: {summary_df.columns.tolist()}")
    else:
        print("No Change Z-scores could be extracted for the summary.")
    return render_template('index.html',
                           metrics=metric_display_names, # Still used for top-level metric links
                           summary_data=summary_df,
                           summary_metrics=processed_summary_columns) # Pass the NEW list of combined column names
</file>

<file path="views/metric_views.py">
# This file defines the routes for displaying detailed views of specific time-series metrics.
# It handles requests where the user wants to see the data and charts for a single metric
# (like 'Yield' or 'Spread Duration') across all applicable funds.
"""
Blueprint for metric-specific routes (e.g., displaying individual metric charts).
"""
from flask import Blueprint, render_template, jsonify
import os
import pandas as pd
import numpy as np
import traceback
# Import necessary functions/constants from other modules
from config import DATA_FOLDER, COLOR_PALETTE
from data_loader import load_and_process_data
from metric_calculator import calculate_latest_metrics
# Define the blueprint for metric routes, using '/metric' as the URL prefix
metric_bp = Blueprint('metric', __name__, url_prefix='/metric')
@metric_bp.route('/<metric_name>')
def metric_page(metric_name):
    """Renders the detailed page (`metric_page_js.html`) for a specific metric.
    This view takes the display name of a metric (e.g., 'Yield') from the URL,
    finds the corresponding data file (e.g., 'ts_Yield.csv'), and performs the following:
    1. Loads and processes the data using `data_loader.load_and_process_data`.
    2. Calculates summary metrics (latest value, change, Z-score, etc.) for each fund code
       within that metric using `metric_calculator.calculate_latest_metrics`.
    3. Identifies funds that might have missing data for the latest period (based on NaN Z-scores).
    4. Prepares data specifically for charting with Chart.js:
       - Extracts historical time-series data (dates and values) for the benchmark and each fund column
         for every fund code.
       - Packages this historical data along with the calculated summary metrics and a flag indicating
         if data was missing into a dictionary structure (`charts_data_for_js`).
    5. Converts the prepared data structure into a JSON string.
    6. Renders the `metric_page_js.html` template, passing:
       - The `metric_name` (for display).
       - The JSON string (`charts_data_json`) containing all data needed by the JavaScript.
       - The overall latest date found in the data.
       - A DataFrame (`missing_funds`) containing rows for funds flagged as potentially missing data.
       - The names of the fund and benchmark columns.
    This allows the template and its associated JavaScript to dynamically generate tables and charts
    for the selected metric.
    """
    # metric_name is the display name (e.g., 'Spread Duration')
    # Prepend 'ts_' to get the actual filename base
    metric_filename_base = f"ts_{metric_name}"
    filename = f"{metric_filename_base}.csv"
    fund_code = 'N/A' # For logging
    try:
        print(f"Loading data for display metric '{metric_name}' from file '{filename}'...")
        df, fund_cols, benchmark_col = load_and_process_data(filename)
        latest_date_overall = df.index.get_level_values(0).max()
        latest_date_str = latest_date_overall.strftime('%Y-%m-%d') # Use ISO format for JS
        print(f"Calculating latest metrics for {metric_name}...")
        latest_metrics = calculate_latest_metrics(df, fund_cols, benchmark_col)
        print(f"Identifying missing funds for {metric_name}...")
        # Construct the list of all possible Change Z-Score columns
        all_cols_for_z = []
        if benchmark_col: # Check if benchmark_col is not None
            all_cols_for_z.append(benchmark_col)
        all_cols_for_z.extend(fund_cols)
        z_score_cols = [f'{col} Change Z-Score' for col in all_cols_for_z
                        if f'{col} Change Z-Score' in latest_metrics.columns]
        if not z_score_cols:
            print(f"Warning: No 'Change Z-Score' columns found in latest_metrics for {metric_name} (from {filename})")
            # Fallback: Check for missing Latest Value if Z-score is absent
            latest_val_cols = [f'{col} Latest Value' for col in all_cols_for_z
                               if f'{col} Latest Value' in latest_metrics.columns]
            if latest_val_cols:
                 missing_latest = latest_metrics[latest_metrics[latest_val_cols].isna().any(axis=1)]
            else:
                 # If neither Z-score nor Latest Value columns exist, create an empty DataFrame
                 missing_latest = pd.DataFrame(index=latest_metrics.index)
        else:
             # If Z-score columns exist, use them to identify missing data
             missing_latest = latest_metrics[latest_metrics[z_score_cols].isna().any(axis=1)]
        # --- Prepare data for JavaScript --- 
        print(f"Preparing data structure for JavaScript for {metric_name}...")
        funds_data_for_js = {}
        # Iterate through sorted fund codes from latest_metrics
        for fund_code in latest_metrics.index:
            # Retrieve historical data for the specific fund (needed for charts)
            if not df.index.get_level_values(1).isin([fund_code]).any():
                print(f"Warning: Fund code {fund_code} not found in DataFrame index level 1 for {metric_name}. Skipping.")
                continue # Skip if fund code somehow isn't in the original DF index
            fund_hist_data = df.xs(fund_code, level=1).sort_index().copy()
            # Filter to include only business days (Mon-Fri)
            if isinstance(fund_hist_data.index, pd.DatetimeIndex):
                fund_hist_data = fund_hist_data[fund_hist_data.index.dayofweek < 5]
            else:
                print(f"Warning: Index for {fund_code} in {metric_name} is not DatetimeIndex, skipping business day filter.")
            # Retrieve the calculated latest metrics (flattened row) for this fund
            if fund_code not in latest_metrics.index:
                 print(f"Warning: Fund code {fund_code} not found in latest_metrics index for {metric_name}. Skipping.")
                 continue # Skip if fund code not in calculated metrics
            fund_latest_metrics_row = latest_metrics.loc[fund_code]
            # Check if this fund was flagged as missing (based on Z-score or fallback)
            is_missing_latest = fund_code in missing_latest.index
            # Prepare labels (dates) for the chart
            labels = fund_hist_data.index.strftime('%Y-%m-%d').tolist()
            datasets = []
            # Create datasets for the chart (raw values)
            # Add benchmark dataset
            if benchmark_col and benchmark_col in fund_hist_data.columns:
                # Replace NaN/Inf with None for JSON compatibility (JS handles null)
                bench_values = fund_hist_data[benchmark_col].round(3).replace([np.inf, -np.inf], np.nan).where(pd.notnull, None).tolist()
                datasets.append({
                    'label': benchmark_col,
                    'data': bench_values,
                    'borderColor': 'black', 'backgroundColor': 'grey',
                    'borderDash': [5, 5], 'tension': 0.1
                })
            # Add dataset for each fund column
            for i, fund_col in enumerate(fund_cols):
                 if fund_col in fund_hist_data.columns:
                    # Replace NaN/Inf with None for JSON compatibility (JS handles null)
                    fund_values = fund_hist_data[fund_col].round(3).replace([np.inf, -np.inf], np.nan).where(pd.notnull, None).tolist()
                    color = COLOR_PALETTE[i % len(COLOR_PALETTE)]
                    datasets.append({
                        'label': fund_col,
                        'data': fund_values,
                        'borderColor': color, 'backgroundColor': color + '40',
                        'tension': 0.1
                    })
            # Convert metrics row to dictionary, replacing NaN/Inf with None
            fund_latest_metrics_dict = fund_latest_metrics_row.round(3).replace([np.inf, -np.inf], np.nan).where(pd.notnull, None).to_dict()
            # Assemble data for JS for this fund
            funds_data_for_js[fund_code] = {
                'labels': labels,
                'datasets': datasets,
                'metrics': fund_latest_metrics_dict,
                'is_missing_latest': is_missing_latest
                # Removed redundant column names from here
            }
        # --- Create the final payload for JSON ---    
        json_payload = {
            "metadata": {
                "metric_name": metric_name,
                "latest_date": latest_date_str, 
                "fund_col_names": fund_cols, # The list of original fund column names
                "benchmark_col_name": benchmark_col # The standard benchmark name or None
            },
            "funds": funds_data_for_js # The dictionary containing data per fund
        }
        print(f"Finished preparing data for {metric_name}. Payload keys: {list(json_payload.keys())}")
        # Render the template, passing the *entire payload* as JSON
        return render_template('metric_page_js.html',
                               metric_name=metric_name, # Keep for page title
                               # Pass the complete payload as JSON
                               charts_data_json=jsonify(json_payload).get_data(as_text=True),
                               # Keep latest_date for display in template header (DD/MM/YYYY format)
                               latest_date=latest_date_overall.strftime('%d/%m/%Y'), 
                               missing_funds=missing_latest)
                               # Removed redundant fund/benchmark names passed separately
    except FileNotFoundError:
        # Use the display name in the error message for user clarity
        return f"Error: Data file for metric '{metric_name}' (expected: '{filename}') not found.", 404
    except ValueError as ve:
        # Use display name in error message
        print(f"Value Error processing {metric_name} (from {filename}): {ve}")
        return f"Error processing {metric_name}: {ve}", 400
    except Exception as e:
        # Use display name in error message
        print(f"Error processing {metric_name} (from {filename}) for fund {fund_code}: {e}")
        traceback.print_exc()
        return f"An error occurred processing {metric_name}: {e}", 500
</file>

<file path="views/security_views.py">
"""
Blueprint for security-related routes (e.g., summary page and individual details).
"""
from flask import Blueprint, render_template, jsonify, send_from_directory
import os
import pandas as pd
import numpy as np
import traceback
from urllib.parse import unquote
from datetime import datetime
from flask import request # Import request
# Import necessary functions/constants from other modules
from config import DATA_FOLDER, COLOR_PALETTE
from security_processing import load_and_process_security_data, calculate_security_latest_metrics
# Import the exclusion loading function
from views.exclusion_views import load_exclusions, get_data_path
# Define the blueprint
security_bp = Blueprint('security', __name__, url_prefix='/security')
def get_active_exclusions():
    """Loads exclusions and returns a set of SecurityIDs that are currently active."""
    exclusions = load_exclusions() # This returns a list of dicts
    active_exclusions = set()
    today = datetime.now().date()
    for ex in exclusions:
        try:
            add_date = ex['AddDate'].date() if pd.notna(ex['AddDate']) else None
            end_date = ex['EndDate'].date() if pd.notna(ex['EndDate']) else None
            security_id = str(ex['SecurityID']) # Ensure it's string for comparison
            if add_date and add_date <= today:
                if end_date is None or end_date >= today:
                    active_exclusions.add(security_id)
        except Exception as e:
            print(f"Error processing exclusion record {ex}: {e}") # Use logging in production
    print(f"Found {len(active_exclusions)} active exclusions: {active_exclusions}")
    return active_exclusions
@security_bp.route('/summary') # Renamed route for clarity, corresponds to /security/summary
def securities_page():
    """Renders a page summarizing potential issues in security-level data from sec_ files."""
    print("--- Starting Security Data Processing for Spread --- ")
    # --- Get Search Term from Request --- 
    search_term = request.args.get('search_term', '') # Get search term, default to empty string
    print(f"Search term received: '{search_term}'")
    spread_filename = "sec_Spread.csv"
    data_filepath = os.path.join(DATA_FOLDER, spread_filename)
    all_metrics_list = []
    all_static_columns = set() # Keep track of all unique static columns across files
    filter_options = {} # Dictionary to store unique values for each filterable static column
    combined_metrics_df = pd.DataFrame() # Initialize DataFrame
    if not os.path.exists(data_filepath):
        print(f"Error: The required file '{spread_filename}' was not found in the '{DATA_FOLDER}' directory.")
        return render_template('securities_page.html',
                           securities_data={},
                           filter_options={},
                           all_static_cols=[],
                           message=f"Error: Required data file '{spread_filename}' not found.")
    try:
        print(f"Processing security file: {spread_filename}")
        # 1. Load and process (melts data to long format)
        df_long, static_cols = load_and_process_security_data(spread_filename)
        if df_long is None or df_long.empty:
            print(f"Skipping {spread_filename} due to load/process errors or empty data after processing.")
            # Render template with a specific error message for the spread file
            return render_template('securities_page.html',
                                   securities_data={},
                                   filter_options={},
                                   all_static_cols=[],
                                   message=f"Error loading or processing '{spread_filename}'.")
        print(f"Loaded {spread_filename}. Identifying static columns: {static_cols}")
        all_static_columns.update(static_cols) # Add newly found static columns
        # 2. Calculate latest metrics
        latest_sec_metrics = calculate_security_latest_metrics(df_long, static_cols)
        if latest_sec_metrics.empty:
            print(f"No metrics calculated for {spread_filename}. Skipping.")
             # Render template indicating no metrics calculated
            return render_template('securities_page.html',
                                   securities_data={},
                                   filter_options={},
                                   all_static_cols=[],
                                   message=f"Could not calculate metrics from '{spread_filename}'.")
        # 3. Store the calculated metrics
        combined_metrics_df = latest_sec_metrics # Assign directly
        print(f"Successfully calculated metrics for {spread_filename}")
        # 4. Collect filter options from static columns
        current_static_in_df = [col for col in static_cols if col in combined_metrics_df.columns]
        for col in current_static_in_df:
            unique_vals = combined_metrics_df[col].unique().tolist()
            # Convert numpy types to standard Python types if necessary
            unique_vals = [item.item() if isinstance(item, np.generic) else item for item in unique_vals]
            unique_vals = [val for val in unique_vals if pd.notna(val)] # Remove NaN
            if col not in filter_options:
                filter_options[col] = set(unique_vals)
            else:
                filter_options[col].update(unique_vals)
        # --- Apply Search Term Filter --- 
        if search_term and not combined_metrics_df.empty:
            original_count = len(combined_metrics_df)
            # Ensure index is string for searching
            combined_metrics_df.index = combined_metrics_df.index.astype(str)
            # Case-insensitive search
            combined_metrics_df = combined_metrics_df[combined_metrics_df.index.str.contains(search_term, case=False, na=False)]
            filtered_count = len(combined_metrics_df)
            print(f"Filtered {original_count - filtered_count} securities based on search term '{search_term}'.")
            if combined_metrics_df.empty:
                print("No securities found matching the search term after initial loading.")
                return render_template('securities_page.html',
                                       securities_data={},
                                       filter_options={},
                                       all_static_cols=[],
                                       search_term=search_term, # Pass search term back
                                       message=f"No securities found matching '{search_term}'.")
    except Exception as e:
        print(f"Error processing security file {spread_filename}: {e}")
        traceback.print_exc()
        return render_template('securities_page.html',
                               securities_data={},
                               filter_options={},
                               all_static_cols=[],
                               message=f"An error occurred while processing '{spread_filename}'.")
    if combined_metrics_df.empty:
         print("No security metrics were generated from sec_Spread.csv.")
         # This case should be caught earlier, but added as a safeguard
         return render_template('securities_page.html',
                                securities_data={},
                                filter_options={},
                                all_static_cols=[],
                                message=f"Could not generate metrics from '{spread_filename}'.")
    # --- Load Active Exclusions --- 
    try:
        active_exclusion_ids = get_active_exclusions()
    except Exception as e:
        print(f"Error loading active exclusions: {e}")
        active_exclusion_ids = set() # Proceed without exclusions if loading fails
        # Optionally, add a message to the user
        # message = "Warning: Could not load security exclusions."
    # --- Filter out excluded securities --- 
    if not combined_metrics_df.empty and active_exclusion_ids:
        original_count = len(combined_metrics_df)
        # Ensure index is treated as string for matching
        combined_metrics_df.index = combined_metrics_df.index.astype(str)
        combined_metrics_df = combined_metrics_df[~combined_metrics_df.index.isin(active_exclusion_ids)]
        filtered_count = len(combined_metrics_df)
        print(f"Filtered out {original_count - filtered_count} excluded securities.")
        if combined_metrics_df.empty:
            print("All securities were filtered out by the exclusion list.")
            # Handle case where everything is excluded
            return render_template('securities_page.html',
                                   securities_data={},
                                   filter_options={},
                                   all_static_cols=[],
                                   search_term=search_term, # Pass search term back
                                   message="All securities matching the criteria are currently excluded.")
    # --- Sorting by Change Z-Score --- 
    if 'Change Z-Score' in combined_metrics_df.columns:
        # Handle potential NaNs before calculating absolute value and sorting
        combined_metrics_df['Abs Change Z-Score'] = combined_metrics_df['Change Z-Score'].fillna(0).abs()
        combined_metrics_df.sort_values(by='Abs Change Z-Score', ascending=False, inplace=True)
        combined_metrics_df.drop(columns=['Abs Change Z-Score'], inplace=True)
        print("Sorted combined security metrics by absolute Change Z-Score.")
    else:
        print("Warning: 'Change Z-Score' column not found in combined metrics. Cannot sort.")
    # Convert final filter options sets to sorted lists
    final_filter_options = {k: sorted(list(v)) for k, v in filter_options.items()}
    # Convert DataFrame to list of dictionaries for easier template processing
    securities_data_list = combined_metrics_df.reset_index().round(3).to_dict(orient='records')
    for row in securities_data_list:
        for key, value in row.items():
            if pd.isna(value):
                row[key] = None
    # Define order of columns for display (Security ID first, then Static, then Metrics)
    id_col_name = combined_metrics_df.index.name or 'Security ID' # Get index name
    ordered_static_cols = sorted(list(all_static_columns))
    metric_cols_ordered = ['Latest Value', 'Change', 'Change Z-Score', 'Mean', 'Max', 'Min']
    # Ensure only existing columns are included
    final_col_order = [id_col_name] + \
                      [col for col in ordered_static_cols if col in combined_metrics_df.columns] + \
                      [col for col in metric_cols_ordered if col in combined_metrics_df.columns]
    print(f"Final column order for display: {final_col_order}")
    return render_template('securities_page.html',
                           securities_data=securities_data_list,
                           filter_options=final_filter_options,
                           column_order=final_col_order, # Pass column order to template
                           id_col_name=id_col_name, # Pass the identified ID column name
                           search_term=search_term, # Pass search term back to template
                           message=None)
@security_bp.route('/details/<metric_name>/<path:security_id>') # Corresponds to /security/details/..., use path converter
def security_details_page(metric_name, security_id):
    """Renders a page showing the time series chart for a specific security from a specific metric file."""
    # Decode the security_id using standard library
    decoded_security_id = unquote(security_id)
    filename = f"sec_{metric_name}.csv"
    price_filename = "sec_Price.csv" # Define price filename
    duration_filename = "sec_Duration.csv" # Define duration filename
    print(f"--- Requesting Security Details --- Metric: {metric_name}, Security ID (Encoded): {security_id}, Security ID (Decoded): {decoded_security_id}, File: {filename}")
    try:
        # 1. Load and process the specific security data file (for the primary metric)
        df_long, static_cols = load_and_process_security_data(filename)
        if df_long is None or df_long.empty:
            return f"Error: Could not load or process data for file '{filename}'", 404
        # Check if the requested security ID exists in the data using the DECODED ID
        actual_id_col_name = df_long.index.names[1]
        if decoded_security_id not in df_long.index.get_level_values(actual_id_col_name):
             return f"Error: Security ID '{decoded_security_id}' not found in file '{filename}'.", 404
        # 2. Extract historical data for the specific security using the DECODED ID
        security_data = df_long.xs(decoded_security_id, level=actual_id_col_name).sort_index().copy()
        # Filter to include only business days (Mon-Fri)
        if isinstance(security_data.index, pd.DatetimeIndex):
            security_data = security_data[security_data.index.dayofweek < 5]
        else:
            print(f"Warning: Index for primary metric data ({metric_name}) for {decoded_security_id} is not DatetimeIndex, skipping business day filter.")
        if security_data.empty:
            # Check if empty *after* filtering too
            return f"Error: No historical data found for Security ID '{decoded_security_id}' in file '{filename}' (or only weekend data).", 404
        # Extract static dimension values for display
        static_info = {}
        if not security_data.empty:
            first_row = security_data.iloc[0]
            for col in static_cols:
                if col in first_row.index:
                    static_info[col] = first_row[col]
        # 3. Load Price and Duration data (if files exist)
        price_data = pd.DataFrame()
        duration_data = pd.DataFrame()
        # Load Price Data
        try:
            price_df_long, _ = load_and_process_security_data(price_filename)
            if price_df_long is not None and not price_df_long.empty:
                price_id_col = price_df_long.index.names[1]
                if decoded_security_id in price_df_long.index.get_level_values(price_id_col):
                    price_data = price_df_long.xs(decoded_security_id, level=price_id_col)[['Value']].sort_index().copy()
                    price_data.rename(columns={'Value': 'Price'}, inplace=True)
                    # Filter price data to business days
                    if isinstance(price_data.index, pd.DatetimeIndex):
                        price_data = price_data[price_data.index.dayofweek < 5]
                    else:
                        print(f"Warning: Index for price data for {decoded_security_id} is not DatetimeIndex, skipping business day filter.")
        except FileNotFoundError:
            print(f"Price file '{price_filename}' not found. Price chart will not be available.")
        except Exception as e:
            print(f"Error loading price data for {decoded_security_id}: {e}")
            traceback.print_exc()
        # Load Duration Data
        try:
            duration_df_long, _ = load_and_process_security_data(duration_filename)
            if duration_df_long is not None and not duration_df_long.empty:
                duration_id_col = duration_df_long.index.names[1]
                if decoded_security_id in duration_df_long.index.get_level_values(duration_id_col):
                    duration_data = duration_df_long.xs(decoded_security_id, level=duration_id_col)[['Value']].sort_index().copy()
                    duration_data.rename(columns={'Value': 'Duration'}, inplace=True)
                    # Filter duration data to business days
                    if isinstance(duration_data.index, pd.DatetimeIndex):
                        duration_data = duration_data[duration_data.index.dayofweek < 5]
                    else:
                        print(f"Warning: Index for duration data for {decoded_security_id} is not DatetimeIndex, skipping business day filter.")
        except FileNotFoundError:
            print(f"Duration file '{duration_filename}' not found. Duration chart will not be available.")
        except Exception as e:
            print(f"Error loading duration data for {decoded_security_id}: {e}")
            traceback.print_exc()
        # 4. Prepare data for the primary chart
        labels = security_data.index.strftime('%Y-%m-%d').tolist()
        # Initialize datasets list for the primary chart
        primary_datasets = [{
            'label': f'{decoded_security_id} - {metric_name} Value', # Use decoded ID in label
            'data': security_data['Value'].round(3).fillna(np.nan).tolist(),
            'borderColor': COLOR_PALETTE[0], # Use first color
            'backgroundColor': COLOR_PALETTE[0] + '40',
            'tension': 0.1,
            'yAxisID': 'y' # Assign primary metric to the default 'y' axis
        }]
        # Initialize chart_data_for_js with primary data
        chart_data_for_js = {
            'labels': labels,
            'primary_datasets': primary_datasets, # Use a specific key for primary chart datasets
            'duration_dataset': None # Initialize duration dataset as None
        }
        latest_date_overall = security_data.index.max()
        # --- Attempt to load and add Price data to the primary chart ---
        try:
            print(f"Attempting to load price data from {price_filename} for {decoded_security_id}...")
            price_df_long, _ = load_and_process_security_data(price_filename) # Ignore static cols from price file
            if price_df_long is not None and not price_df_long.empty:
                price_actual_id_col = price_df_long.index.names[1] # Get ID column name from price file
                if decoded_security_id in price_df_long.index.get_level_values(price_actual_id_col):
                    price_data = price_df_long.xs(decoded_security_id, level=price_actual_id_col).sort_index().copy()
                    # Reindex price data to align dates with the main metric data
                    price_data = price_data.reindex(security_data.index)
                    if not price_data.empty:
                        price_dataset = {
                            'label': f'{decoded_security_id} - Price',
                            'data': price_data['Value'].round(3).fillna(np.nan).tolist(),
                            'borderColor': COLOR_PALETTE[1 % len(COLOR_PALETTE)], # Use second color
                            'backgroundColor': COLOR_PALETTE[1 % len(COLOR_PALETTE)] + '40',
                            'tension': 0.1,
                            'yAxisID': 'y1' # Assign price data to the second y-axis
                        }
                        # Append price dataset to the primary_datasets list
                        chart_data_for_js['primary_datasets'].append(price_dataset)
                        print("Successfully added Price data overlay to primary chart.")
                    else:
                         print(f"Warning: Price data found for {decoded_security_id}, but was empty after aligning dates.")
                else:
                     print(f"Warning: Security ID {decoded_security_id} not found in {price_filename}.")
            else:
                 print(f"Warning: Could not load or process {price_filename}.")
        except FileNotFoundError:
            print(f"Warning: Price data file '{price_filename}' not found. Skipping price overlay.")
        except Exception as e_price:
            print(f"Warning: Error processing price data for {decoded_security_id} from {price_filename}: {e_price}")
            traceback.print_exc() # Log the price processing error but continue
        # --- Attempt to load and add Duration data for the second chart ---
        try:
            print(f"Attempting to load duration data from {duration_filename} for {decoded_security_id}...")
            duration_df_long, _ = load_and_process_security_data(duration_filename) # Ignore static cols
            if duration_df_long is not None and not duration_df_long.empty:
                duration_actual_id_col = duration_df_long.index.names[1] # Get ID column name
                if decoded_security_id in duration_df_long.index.get_level_values(duration_actual_id_col):
                    duration_data = duration_df_long.xs(decoded_security_id, level=duration_actual_id_col).sort_index().copy()
                    # Reindex duration data to align dates with the main metric data
                    duration_data = duration_data.reindex(security_data.index)
                    if not duration_data.empty:
                        duration_dataset = {
                            'label': f'{decoded_security_id} - Duration', # Use decoded ID
                            'data': duration_data['Value'].round(3).fillna(np.nan).tolist(),
                            'borderColor': COLOR_PALETTE[2 % len(COLOR_PALETTE)], # Use third color
                            'backgroundColor': COLOR_PALETTE[2 % len(COLOR_PALETTE)] + '40',
                            'tension': 0.1
                            # No yAxisID needed if it's a separate chart with its own scale
                        }
                        # Assign duration dataset to its specific key
                        chart_data_for_js['duration_dataset'] = duration_dataset
                        print("Successfully prepared Duration data for separate chart.")
                    else:
                         print(f"Warning: Duration data found for {decoded_security_id}, but was empty after aligning dates.")
                else:
                     print(f"Warning: Security ID {decoded_security_id} not found in {duration_filename}.")
            else:
                 print(f"Warning: Could not load or process {duration_filename}.")
        except FileNotFoundError:
            print(f"Warning: Duration data file '{duration_filename}' not found. Skipping duration chart.")
        except Exception as e_duration:
            print(f"Warning: Error processing duration data for {decoded_security_id} from {duration_filename}: {e_duration}")
            traceback.print_exc() # Log the duration processing error but continue
        # 4. Render the template
        return render_template('security_details_page.html',
                               metric_name=metric_name,
                               security_id=decoded_security_id,
                               static_info=static_info,
                               chart_data_json=jsonify(chart_data_for_js).get_data(as_text=True),
                               latest_date=latest_date_overall.strftime('%d/%m/%Y'))
    except FileNotFoundError:
        return f"Error: Data file '{filename}' not found.", 404
    except ValueError as ve:
        print(f"Value Error processing security details for {metric_name}/{decoded_security_id}: {ve}")
        return f"Error processing details for {decoded_security_id}: {ve}", 400
    except Exception as e:
        print(f"Error processing security details for {metric_name}/{decoded_security_id}: {e}")
        traceback.print_exc()
        return f"An error occurred processing details for {decoded_security_id}: {e}", 500 
# Add a route for static asset discovery (like in metric_views)
@security_bp.route('/static/<path:filename>')
def static_files(filename):
    return send_from_directory(os.path.join(security_bp.root_path, '..', 'static'), filename)
</file>

</files>
