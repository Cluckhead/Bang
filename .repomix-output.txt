This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-04-10T13:53:52.116Z

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
instance/
.gitignore
app.py
config.py
data_loader.py
data_validation.py
LICENSE
metric_calculator.py
process_data.py
README.md
requirements.txt
security_processing.py
static/css/style.css
static/js/main.js
static/js/modules/charts/timeSeriesChart.js
static/js/modules/ui/chartRenderer.js
static/js/modules/ui/securityTableFilter.js
static/js/modules/ui/tableSorter.js
static/js/modules/utils/helpers.js
templates/base.html
templates/comparison_details_page.html
templates/comparison_page.html
templates/delete_metric_page.html
templates/duration_comparison_details_page.html
templates/duration_comparison_page.html
templates/exclusions_page.html
templates/fund_detail_page.html
templates/fund_duration_details.html
templates/get_data.html
templates/index.html
templates/metric_page_js.html
templates/securities_page.html
templates/security_details_page.html
templates/spread_duration_comparison_details_page.html
templates/spread_duration_comparison_page.html
templates/weight_check_page.html
utils.py
views/__init__.py
views/api_views.py
views/comparison_views.py
views/curve_views.py
views/duration_comparison_views.py
views/exclusion_views.py
views/fund_views.py
views/main_views.py
views/metric_views.py
views/security_views.py
views/spread_duration_comparison_views.py
views/weight_views.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".gitignore">
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class
*.csv
# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
# Usually these files are written by a python script from a template
# before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
.hypothesis/
.pytest_cache/

# Translations
*.mo
*.pot
*.log

# Django stuff:
*.log
local_settings.py
db.sqlite3

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
.python-version

# celery beat schedule file
celerybeat-schedule

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/
</file>

<file path="app.py">
# This file defines the main entry point and structure for the Simple Data Checker Flask web application.
# It utilizes the Application Factory pattern (`create_app`) to initialize and configure the Flask app.
# Key responsibilities include:
# - Creating the Flask application instance.
# - Setting up basic configuration (like the secret key).
# - Ensuring necessary folders (like the instance folder) exist.
# - Registering Blueprints (`main_bp`, `metric_bp`, `security_bp`, `fund_bp`, `exclusion_bp`, `comparison_bp`, `duration_comparison_bp`, `spread_duration_comparison_bp`, `api_bp`, `weight_bp`) from the `views`
#   directory, which contain the application\'s routes and view logic.
# - Providing a conditional block (`if __name__ == '__main__':`) to run the development server
#   when the script is executed directly.
# This modular structure using factories and blueprints makes the application more organized and scalable.
# This file contains the main Flask application factory.
from flask import Flask, render_template, Blueprint, jsonify
import os
import logging
# --- Add imports for the new route ---
import subprocess
import sys # To get python executable path
# --- End imports ---
# Import configurations and utilities (potentially needed by factory setup later)
from config import DATA_FOLDER, COLOR_PALETTE # Uncommented import
# from utils import _is_date_like, parse_fund_list # Not directly used in factory itself yet
def create_app():
    """Factory function to create and configure the Flask app."""
    app = Flask(__name__, instance_relative_config=True) # instance_relative_config=True allows for instance folder config
    # Basic configuration (can be expanded later, e.g., loading from config file)
    app.config.from_mapping(
        SECRET_KEY='dev', # Default secret key for development. CHANGE for production!
        # Add other default configurations if needed
    )
    # Load configuration from config.py
    app.config.from_object('config')
    # Ensure the instance folder exists (if using instance_relative_config)
    try:
        os.makedirs(app.instance_path)
    except OSError:
        pass # Already exists
    # Serve static files (for JS, CSS, etc.)
    # Note: static_url_path defaults to /static, static_folder defaults to 'static' in root
    # No need to set app.static_folder = 'static' explicitly unless changing the folder name/path
    # --- Register Blueprints --- 
    from views.main_views import main_bp
    from views.metric_views import metric_bp
    from views.security_views import security_bp
    from views.fund_views import fund_bp
    from views.api_views import api_bp
    from views.exclusion_views import exclusion_bp
    from views.comparison_views import comparison_bp
    from views.weight_views import weight_bp
    # --- Import new blueprints ---
    from views.duration_comparison_views import duration_comparison_bp
    from views.spread_duration_comparison_views import spread_duration_comparison_bp
    from views.curve_views import curve_bp
    # --- End import new blueprints ---
    app.register_blueprint(main_bp)
    app.register_blueprint(metric_bp)
    app.register_blueprint(security_bp)
    app.register_blueprint(fund_bp)
    app.register_blueprint(api_bp)
    app.register_blueprint(exclusion_bp)
    app.register_blueprint(comparison_bp)
    app.register_blueprint(weight_bp)
    # --- Register new blueprints ---
    app.register_blueprint(duration_comparison_bp)
    app.register_blueprint(spread_duration_comparison_bp)
    app.register_blueprint(curve_bp)
    # --- End register new blueprints ---
    print("Registered Blueprints:")
    print(f"- {main_bp.name} (prefix: {main_bp.url_prefix})")
    print(f"- {metric_bp.name} (prefix: {metric_bp.url_prefix})")
    print(f"- {security_bp.name} (prefix: {security_bp.url_prefix})")
    print(f"- {fund_bp.name} (prefix: {fund_bp.url_prefix})")
    print(f"- {api_bp.name} (prefix: {api_bp.url_prefix})")
    print(f"- {exclusion_bp.name} (prefix: {exclusion_bp.url_prefix})")
    print(f"- {comparison_bp.name} (prefix: {comparison_bp.url_prefix})")
    print(f"- {weight_bp.name} (prefix: {weight_bp.url_prefix})")
    # --- Print new blueprints ---
    print(f"- {duration_comparison_bp.name} (prefix: {duration_comparison_bp.url_prefix})")
    print(f"- {spread_duration_comparison_bp.name} (prefix: {spread_duration_comparison_bp.url_prefix})")
    print(f"- {curve_bp.name} (prefix: {curve_bp.url_prefix})")
    # --- End print new blueprints ---
    # Add a simple test route to confirm app creation (optional)
    @app.route('/hello')
    def hello():
        return 'Hello, World! App factory is working.'
    # --- Add the new cleanup route ---
    @app.route('/run-cleanup', methods=['POST'])
    def run_cleanup():
        """Endpoint to trigger the process_data.py script."""
        script_path = os.path.join(os.path.dirname(__file__), 'process_data.py')
        python_executable = sys.executable # Use the same python that runs flask
        if not os.path.exists(script_path):
            app.logger.error(f"Cleanup script not found at: {script_path}")
            return jsonify({'status': 'error', 'message': 'Cleanup script not found.'}), 500
        app.logger.info(f"Attempting to run cleanup script: {script_path}")
        try:
            # Run the script using the same Python interpreter that is running Flask
            # Capture stdout and stderr, decode as UTF-8, handle potential errors
            result = subprocess.run(
                [python_executable, script_path],
                capture_output=True,
                text=True,
                check=False, # Don't raise exception on non-zero exit code
                encoding='utf-8' # Explicitly set encoding
            )
            log_output = f"STDOUT:\n{result.stdout}\nSTDERR:\n{result.stderr}"
            if result.returncode == 0:
                app.logger.info(f"Cleanup script finished successfully. Output:\n{log_output}")
                return jsonify({'status': 'success', 'output': result.stdout or "No output", 'error': result.stderr}), 200
            else:
                app.logger.error(f"Cleanup script failed with return code {result.returncode}. Output:\n{log_output}")
                return jsonify({'status': 'error', 'message': 'Cleanup script failed.', 'output': result.stdout, 'error': result.stderr}), 500
        except Exception as e:
            app.logger.error(f"Exception occurred while running cleanup script: {e}", exc_info=True)
            return jsonify({'status': 'error', 'message': f'An exception occurred: {e}'}), 500
    # --- End new route ---
    return app
# --- Application Execution --- 
if __name__ == '__main__':
    app = create_app() # Create the app instance using the factory
    app.run(debug=True) # Run in debug mode for development
</file>

<file path="config.py">
# This file defines configuration variables for the Simple Data Checker application.
# It centralizes settings like file paths and visual parameters (e.g., chart colors)
# to make them easily adjustable without modifying the core application code.
"""
Configuration settings for the Flask application.
"""
DATA_FOLDER = 'Data'
# Define a list of distinct colors for chart lines
# Add more colors if you expect more fund columns
COLOR_PALETTE = [
    'blue', 'red', 'green', 'purple', '#FF7F50', # Coral
    '#6495ED', # CornflowerBlue
    '#DC143C', # Crimson
    '#00FFFF'  # Aqua
]
</file>

<file path="data_loader.py">
# This file is responsible for loading and preprocessing data from CSV files.
# It includes functions to dynamically identify essential columns (Date, Code, Benchmark)
# based on patterns, handle potential naming variations, parse dates, standardize column names,
# set appropriate data types, and prepare the data in a pandas DataFrame format
# suitable for further analysis and processing within the application.
# It also supports loading a secondary file (e.g., prefixed with 'sp_') for comparison.
# data_loader.py
# This file is responsible for loading and preprocessing data from time-series CSV files (typically prefixed with `ts_`).
# It includes functions to dynamically identify essential columns (Date, Code, Benchmark)
# based on patterns, handle potential naming variations, parse dates (handling 'YYYY-MM-DD' and 'DD/MM/YYYY'),
# standardize column names, set appropriate data types, and prepare the data in a pandas DataFrame format
# suitable for further analysis within the application. It includes robust error handling and logging.
# It now also supports loading and processing a secondary comparison file (e.g., sp_*.csv).
import pandas as pd
import os
import logging
from typing import List, Tuple, Optional
import re # Import regex for pattern matching
# --- Logging Setup ---
LOG_FILENAME = 'data_processing_errors.log'
LOG_FORMAT = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
# Get the logger for the current module
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO) # Set minimum level for the logger
# Prevent adding handlers multiple times
if not logger.handlers:
    # Console Handler (INFO and above)
    ch = logging.StreamHandler()
    ch.setLevel(logging.INFO)
    ch_formatter = logging.Formatter(LOG_FORMAT)
    ch.setFormatter(ch_formatter)
    logger.addHandler(ch)
    # File Handler (WARNING and above)
    try:
        # Attempt to create log file in the parent directory (project root)
        log_filepath = os.path.join(os.path.dirname(__file__), '..', LOG_FILENAME)
        fh = logging.FileHandler(log_filepath, mode='a') # Append mode
        fh.setLevel(logging.WARNING)
        fh_formatter = logging.Formatter(LOG_FORMAT)
        fh.setFormatter(fh_formatter)
        logger.addHandler(fh)
    except Exception as e:
        logger.error(f"Failed to configure file logging to {log_filepath}: {e}")
# --- End Logging Setup ---
# Define constants
DATA_FOLDER = 'Data'
# Standard internal column names after renaming
STD_DATE_COL = 'Date'
STD_CODE_COL = 'Code'
STD_BENCHMARK_COL = 'Benchmark'
def _find_column(pattern: str, columns: List[str], filename_for_logging: str, col_type: str) -> str:
    """Helper function to find a single column matching a pattern (case-insensitive)."""
    matches = [col for col in columns if re.search(pattern, col, re.IGNORECASE)]
    if len(matches) == 1:
        logger.info(f"Found {col_type} column in '{filename_for_logging}': '{matches[0]}'")
        return matches[0]
    elif len(matches) > 1:
        # Log error before raising
        logger.error(f"Multiple possible {col_type} columns found in '{filename_for_logging}' matching pattern '{pattern}': {matches}. Please ensure unique column names.")
        raise ValueError(f"Multiple possible {col_type} columns found in '{filename_for_logging}' matching pattern '{pattern}': {matches}. Please ensure unique column names.")
    else:
         # Log error before raising
        logger.error(f"No {col_type} column found in '{filename_for_logging}' matching pattern '{pattern}'. Found columns: {columns}")
        raise ValueError(f"No {col_type} column found in '{filename_for_logging}' matching pattern '{pattern}'. Found columns: {columns}")
def _create_empty_dataframe(original_fund_val_col_names: List[str], benchmark_col_present: bool) -> pd.DataFrame:
    """Creates an empty DataFrame with the expected structure."""
    final_benchmark_col_name = STD_BENCHMARK_COL if benchmark_col_present else None
    expected_cols = [STD_DATE_COL, STD_CODE_COL] + original_fund_val_col_names
    if final_benchmark_col_name:
        expected_cols.append(final_benchmark_col_name)
    # Create an empty df with the right index and columns
    empty_index = pd.MultiIndex(levels=[[], []], codes=[[], []], names=[STD_DATE_COL, STD_CODE_COL])
    value_cols = [col for col in expected_cols if col not in [STD_DATE_COL, STD_CODE_COL]]
    return pd.DataFrame(index=empty_index, columns=value_cols)
def _process_single_file(
    filepath: str,
    filename_for_logging: str
) -> Optional[Tuple[pd.DataFrame, List[str], Optional[str]]]:
    """Internal helper to load and process a single CSV file.
    Handles finding columns, parsing dates, renaming, indexing, and type conversion.
    Returns None if the file is not found or critical processing steps fail.
    Returns:
        Optional[Tuple[pd.DataFrame, List[str], Optional[str]]]:
               Processed DataFrame, list of original fund value column names,
               and the standardized benchmark column name if present, otherwise None.
               Returns None if processing fails critically.
    """
    if not os.path.exists(filepath):
        logger.warning(f"File not found, skipping: {filepath}")
        return None # Return None if file doesn't exist
    try:
        # Read only the header first
        header_df = pd.read_csv(filepath, nrows=0, encoding='utf-8', encoding_errors='replace', on_bad_lines='skip')
        original_cols = [col.strip() for col in header_df.columns.tolist()]
        logger.info(f"Processing file: '{filename_for_logging}'. Original columns: {original_cols}")
        # Dynamically find required columns
        date_pattern = r'\b(Position\s*)?Date\b'
        actual_date_col = _find_column(date_pattern, original_cols, filename_for_logging, 'Date')
        code_pattern = r'\b(Fund\s*)?Code\b' # Allow 'Fund Code' or 'Code'
        actual_code_col = _find_column(code_pattern, original_cols, filename_for_logging, 'Code')
        benchmark_col_present = False
        actual_benchmark_col = None
        try:
            benchmark_pattern = r'\b(Benchmark|Bench)\b' # Allow 'Benchmark' or 'Bench'
            actual_benchmark_col = _find_column(benchmark_pattern, original_cols, filename_for_logging, 'Benchmark')
            benchmark_col_present = True
        except ValueError:
            logger.info(f"No Benchmark column found in '{filename_for_logging}' matching pattern. Proceeding without benchmark.")
        # Identify original fund value columns
        excluded_cols_for_funds = {actual_date_col, actual_code_col}
        if benchmark_col_present and actual_benchmark_col:
            excluded_cols_for_funds.add(actual_benchmark_col)
        original_fund_val_col_names = [col for col in original_cols if col not in excluded_cols_for_funds]
        if not original_fund_val_col_names and not benchmark_col_present:
             logger.error(f"No fund value columns and no benchmark column identified in '{filename_for_logging}'. Cannot process.")
             return None # Cannot proceed
        # Read the full CSV
        df = pd.read_csv(filepath, encoding='utf-8', encoding_errors='replace', on_bad_lines='skip', dtype={actual_date_col: str})
        df.columns = df.columns.str.strip()
        # Rename columns
        rename_map = {
            actual_date_col: STD_DATE_COL,
            actual_code_col: STD_CODE_COL
        }
        if benchmark_col_present and actual_benchmark_col:
            rename_map[actual_benchmark_col] = STD_BENCHMARK_COL
        df.rename(columns=rename_map, inplace=True)
        logger.info(f"Renamed columns in '{filename_for_logging}': {list(rename_map.keys())} -> {list(rename_map.values())}")
        # Robust Date Parsing
        date_series = df[STD_DATE_COL]
        parsed_dates = pd.to_datetime(date_series, errors='coerce', dayfirst=None, yearfirst=None) # Let pandas infer
        # Check if all parsing failed
        if parsed_dates.isnull().all() and len(date_series) > 0:
             # Try again with dayfirst=True if initial inference failed
            logger.warning(f"Initial date parsing failed for {filename_for_logging}. Trying with dayfirst=True.")
            parsed_dates = pd.to_datetime(date_series, errors='coerce', dayfirst=True)
            if parsed_dates.isnull().all() and len(date_series) > 0:
                logger.error(f"Could not parse any dates in column '{STD_DATE_COL}' (original: '{actual_date_col}') in file {filename_for_logging} even with dayfirst=True.")
                return None # Cannot proceed without valid dates
        nat_count = parsed_dates.isnull().sum()
        total_count = len(parsed_dates)
        success_count = total_count - nat_count
        logger.info(f"Parsed {success_count}/{total_count} dates in {filename_for_logging}. ({nat_count} resulted in NaT).")
        if nat_count > 0:
             logger.warning(f"{nat_count} date values in '{STD_DATE_COL}' from {filename_for_logging} became NaT.")
        df[STD_DATE_COL] = parsed_dates
        original_row_count = len(df)
        df.dropna(subset=[STD_DATE_COL], inplace=True)
        rows_dropped = original_row_count - len(df)
        if rows_dropped > 0:
            logger.warning(f"Dropped {rows_dropped} rows from {filename_for_logging} due to failed date parsing.")
        # Set Index
        if df.empty:
            logger.warning(f"DataFrame became empty after dropping rows with unparseable dates in {filename_for_logging}.")
            # Return empty structure but indicate success in file processing up to this point
            empty_df = _create_empty_dataframe(original_fund_val_col_names, benchmark_col_present)
            final_bm_col = STD_BENCHMARK_COL if benchmark_col_present else None
            return empty_df, original_fund_val_col_names, final_bm_col
        df.set_index([STD_DATE_COL, STD_CODE_COL], inplace=True)
        # Convert value columns to numeric
        value_cols_to_convert = original_fund_val_col_names[:]
        if benchmark_col_present:
            value_cols_to_convert.append(STD_BENCHMARK_COL)
        valid_cols_for_conversion = [col for col in value_cols_to_convert if col in df.columns]
        if not valid_cols_for_conversion:
             logger.error(f"No valid fund or benchmark value columns found to convert in {filename_for_logging} after processing.")
             # Return partially processed DF but log error
             final_bm_col = STD_BENCHMARK_COL if benchmark_col_present else None
             return df, original_fund_val_col_names, final_bm_col # Return what we have
        df[valid_cols_for_conversion] = df[valid_cols_for_conversion].apply(pd.to_numeric, errors='coerce')
        nan_check_cols = [col for col in valid_cols_for_conversion if col in df.columns]
        if nan_check_cols and df[nan_check_cols].isnull().all().all():
            logger.warning(f"All values in value columns {nan_check_cols} became NaN after conversion in file {filename_for_logging}. Check data types.")
        final_benchmark_col_name = STD_BENCHMARK_COL if benchmark_col_present else None
        logger.info(f"Successfully processed file: '{filename_for_logging}'. Index: {df.index.names}. Columns: {df.columns.tolist()}")
        return df, original_fund_val_col_names, final_benchmark_col_name
    except FileNotFoundError:
        logger.error(f"File not found during processing: {filepath}")
        return None # Handled above, but belt-and-suspenders
    except ValueError as e:
        logger.error(f"Value error processing {filename_for_logging}: {e}")
        return None # Return None on critical errors like missing columns
    except Exception as e:
        logger.exception(f"Unexpected error processing {filename_for_logging}: {e}") # Log full traceback
        return None # Return None on unexpected errors
# Simplified return type: focus on the dataframes and metadata needed downstream
LoadResult = Tuple[
    Optional[pd.DataFrame],      # Primary DataFrame
    Optional[List[str]],         # Primary original value columns
    Optional[str],               # Primary benchmark column name (standardized)
    Optional[pd.DataFrame],      # Secondary DataFrame
    Optional[List[str]],         # Secondary original value columns
    Optional[str]                # Secondary benchmark column name (standardized)
]
def load_and_process_data(
    primary_filename: str,
    secondary_filename: Optional[str] = None,
    data_folder: str = DATA_FOLDER
) -> LoadResult:
    """Loads and processes a primary CSV file and optionally a secondary CSV file.
    Uses the internal _process_single_file helper for processing each file.
    Args:
        primary_filename (str): The name of the primary CSV file.
        secondary_filename (Optional[str]): The name of the secondary CSV file. Defaults to None.
        data_folder (str): The path to the folder containing the data files. Defaults to DATA_FOLDER.
    Returns:
        LoadResult: A tuple containing the processed DataFrames and metadata for
                    primary and (optionally) secondary files. Elements corresponding
                    to a file will be None if the file doesn't exist or processing fails.
    """
    primary_filepath = os.path.join(data_folder, primary_filename)
    logger.info(f"--- Starting data load for primary: {primary_filename} ---")
    primary_result = _process_single_file(primary_filepath, primary_filename)
    primary_df = None
    primary_original_val_cols = None
    primary_benchmark_col = None
    if primary_result:
        primary_df, primary_original_val_cols, primary_benchmark_col = primary_result
        logger.info(f"Primary file '{primary_filename}' loaded. Shape: {primary_df.shape if primary_df is not None else 'None'}")
    else:
        logger.error(f"Failed to process primary file: {primary_filename}")
        # Still return structure, but with Nones for primary
        primary_df = _create_empty_dataframe([], False) # Provide minimal empty DF
        primary_original_val_cols = []
        primary_benchmark_col = None
    secondary_df = None
    secondary_original_val_cols = None
    secondary_benchmark_col = None
    if secondary_filename:
        secondary_filepath = os.path.join(data_folder, secondary_filename)
        logger.info(f"--- Checking for secondary file: {secondary_filename} ---")
        if os.path.exists(secondary_filepath):
            logger.info(f"Secondary file found. Processing: {secondary_filename}")
            secondary_result = _process_single_file(secondary_filepath, secondary_filename)
            if secondary_result:
                secondary_df, secondary_original_val_cols, secondary_benchmark_col = secondary_result
                logger.info(f"Secondary file '{secondary_filename}' loaded. Shape: {secondary_df.shape if secondary_df is not None else 'None'}")
            else:
                 logger.warning(f"Failed to process secondary file: {secondary_filename}. Proceeding without it.")
                 # Keep secondary parts as None if processing fails
        else:
            logger.info(f"Secondary file not found: {secondary_filename}. Proceeding with primary only.")
    else:
        logger.info("No secondary filename provided.")
    return (
        primary_df, primary_original_val_cols, primary_benchmark_col,
        secondary_df, secondary_original_val_cols, secondary_benchmark_col
    )
# Example Usage (optional, for testing)
# if __name__ == '__main__':
#     logging.basicConfig(level=logging.INFO, format=LOG_FORMAT)
#     logger.info("Running data_loader directly for testing...")
#
#     # Test case 1: Primary file only (exists)
#     print("\n--- Test Case 1: Primary Only (ts_Duration.csv) ---")
#     try:
#         pri_df, pri_ovc, pri_bc, sec_df, sec_ovc, sec_bc = load_and_process_data('ts_Duration.csv')
#         if pri_df is not None:
#             print(f"Primary loaded successfully. Shape: {pri_df.shape}")
#             print(f"Primary Orig Val Cols: {pri_ovc}")
#             print(f"Primary Bench Col: {pri_bc}")
#             print(pri_df.head())
#         else:
#             print("Primary loading failed.")
#         print(f"Secondary DF is None: {sec_df is None}")
#
#     except Exception as e:
#         print(f"Error in Test Case 1: {e}")
#
#     # Test case 2: Primary and Secondary (both exist)
#     print("\n--- Test Case 2: Primary (ts_Duration.csv) and Secondary (sp_ts_Duration.csv) ---")
#     # Ensure you have a 'sp_ts_Duration.csv' file in 'Data/' for this test
#     secondary_test_file = 'sp_ts_Duration.csv'
#     if not os.path.exists(os.path.join(DATA_FOLDER, secondary_test_file)):
#         print(f"WARNING: Secondary test file '{secondary_test_file}' not found. Skipping Test Case 2.")
#     else:
#         try:
#             pri_df, pri_ovc, pri_bc, sec_df, sec_ovc, sec_bc = load_and_process_data('ts_Duration.csv', secondary_filename=secondary_test_file)
#             if pri_df is not None:
#                 print(f"Primary loaded successfully. Shape: {pri_df.shape}")
#                 print(f"Primary Orig Val Cols: {pri_ovc}")
#                 print(f"Primary Bench Col: {pri_bc}")
#             else:
#                 print("Primary loading failed.")
#
#             if sec_df is not None:
#                 print(f"Secondary loaded successfully. Shape: {sec_df.shape}")
#                 print(f"Secondary Orig Val Cols: {sec_ovc}")
#                 print(f"Secondary Bench Col: {sec_bc}")
#                 print(sec_df.head())
#             else:
#                 print("Secondary loading failed or file not found.")
#
#         except Exception as e:
#             print(f"Error in Test Case 2: {e}")
#
#     # Test case 3: Primary exists, Secondary does not
#     print("\n--- Test Case 3: Primary (ts_Duration.csv) and Secondary (non_existent.csv) ---")
#     try:
#         pri_df, pri_ovc, pri_bc, sec_df, sec_ovc, sec_bc = load_and_process_data('ts_Duration.csv', secondary_filename='non_existent.csv')
#         if pri_df is not None:
#             print(f"Primary loaded successfully. Shape: {pri_df.shape}")
#         else:
#             print("Primary loading failed.")
#         print(f"Secondary DF is None: {sec_df is None}")
#         print(f"Secondary Orig Val Cols is None: {sec_ovc is None}")
#         print(f"Secondary Bench Col is None: {sec_bc is None}")
#
#     except Exception as e:
#         print(f"Error in Test Case 3: {e}")
#
#     # Test case 4: Primary does not exist
#     print("\n--- Test Case 4: Primary (bad_file.csv) ---")
#     try:
#         pri_df, pri_ovc, pri_bc, sec_df, sec_ovc, sec_bc = load_and_process_data('bad_file.csv')
#         print(f"Primary DF is not None: {pri_df is not None}") # Should be True, but df will be empty
#         print(f"Primary DF shape: {pri_df.shape if pri_df is not None else 'None'}")
#         print(f"Primary Orig Val Cols: {pri_ovc}")
#         print(f"Primary Bench Col: {pri_bc}")
#         print(f"Secondary DF is None: {sec_df is None}")
#
#     except Exception as e:
#         print(f"Error in Test Case 4: {e}")
</file>

<file path="data_validation.py">
'''
Placeholder module for validating data retrieved from the API.
This module will contain functions to check the structure, data types,
and potentially the content consistency of the DataFrames returned by the
Rex API before they are saved as CSV files.
'''
import pandas as pd
def validate_data(df: pd.DataFrame, filename: str):
    """
    Validates the structure and types of the DataFrame based on filename conventions.
    This is a placeholder function. Implement specific checks based on the
    expected format for different file types (e.g., 'ts_*.csv', 'sec_*.csv').
    Args:
        df (pd.DataFrame): The DataFrame returned by the API call.
        filename (str): The intended filename for the data (e.g., 'ts_Duration.csv').
    Returns:
        tuple[bool, list[str]]: A tuple containing:
            - bool: True if the data is valid, False otherwise.
            - list[str]: A list of validation error messages, empty if valid.
    """
    errors = []
    if df is None or not isinstance(df, pd.DataFrame):
        errors.append("Invalid input: DataFrame is None or not a pandas DataFrame.")
        return False, errors
    if df.empty:
        # It might be valid for some queries to return no data, but flag it for review.
        errors.append("Warning: DataFrame is empty.")
        # Decide if empty is truly invalid or just a warning.
        # For now, let's consider it potentially valid but issue a warning.
        # return False, errors # Uncomment if empty df is strictly invalid
    # Example checks based on filename conventions:
    if filename.startswith('ts_'):
        # Checks for time-series files
        required_cols = ['Date', 'Code'] # Assuming these are standard post-processing names
        if not all(col in df.columns for col in required_cols):
            errors.append(f"Missing required columns for time-series data: Expected {required_cols}, got {list(df.columns)}")
        # Check if 'Date' column is datetime type (or can be coerced)
        # try:
        #     pd.to_datetime(df['Date'])
        # except Exception as e:
        #     errors.append(f"'Date' column cannot be parsed as datetime: {e}")
        # Check if value columns (excluding Date, Code, Benchmark if exists) are numeric
        value_cols = [col for col in df.columns if col not in ['Date', 'Code', 'Benchmark']]
        for col in value_cols:
            if not pd.api.types.is_numeric_dtype(df[col]):
                 errors.append(f"Column '{col}' in time-series data is not numeric.")
    elif filename.startswith('sec_'):
        # Checks for security-level files
        # Example: Check for an ID column (e.g., 'Security ID', 'ISIN')
        # Example: Check if columns intended as dates are parseable
        # Example: Check if value columns are numeric
        pass # Add specific checks here
    elif filename == 'FundList.csv':
        # Example: Check required columns for FundList
        required_cols = ['Fund Code', 'Total Asset Value USD', 'Picked']
        if not all(col in df.columns for col in required_cols):
             errors.append(f"Missing required columns for FundList.csv: Expected {required_cols}, got {list(df.columns)}")
    # --- Add more specific validation rules as needed based on data specs --- 
    is_valid = len(errors) == 0
    return is_valid, errors
# Example Usage (can be run manually for testing):
if __name__ == '__main__':
    # Create dummy dataframes for testing validation logic
    print("Testing validation functions...")
    # Test case 1: Valid time-series data
    valid_ts_data = {
        'Date': pd.to_datetime(['2023-01-01', '2023-01-01', '2023-01-02', '2023-01-02']),
        'Code': ['FUNDA', 'FUNDB', 'FUNDA', 'FUNDB'],
        'Value': [10.1, 20.2, 10.5, 20.8],
        'Benchmark': [10.0, 20.0, 10.4, 20.7]
    }
    valid_ts_df = pd.DataFrame(valid_ts_data)
    is_valid, errors = validate_data(valid_ts_df, 'ts_ExampleMetric.csv')
    print(f"Valid TS Data Test: Valid={is_valid}, Errors={errors}")
    assert is_valid
    # Test case 2: Invalid time-series data (missing column)
    invalid_ts_data = {
        'Date': pd.to_datetime(['2023-01-01']),
        # 'Code': ['FUNDA'], # Missing Code column
        'Value': [10.1]
    }
    invalid_ts_df = pd.DataFrame(invalid_ts_data)
    is_valid, errors = validate_data(invalid_ts_df, 'ts_AnotherMetric.csv')
    print(f"Invalid TS Data Test (Missing Col): Valid={is_valid}, Errors={errors}")
    assert not is_valid
    assert "Missing required columns" in errors[0]
    # Test case 3: Invalid time-series data (non-numeric value)
    invalid_ts_data_type = {
        'Date': pd.to_datetime(['2023-01-01']),
        'Code': ['FUNDA'],
        'Value': ['abc'] # Non-numeric value
    }
    invalid_ts_df_type = pd.DataFrame(invalid_ts_data_type)
    is_valid, errors = validate_data(invalid_ts_df_type, 'ts_BadData.csv')
    print(f"Invalid TS Data Test (Bad Type): Valid={is_valid}, Errors={errors}")
    # Note: This specific check might depend on when type conversion happens.
    # If conversion happens *before* validation, this might pass if 'abc' becomes NaN.
    # The check here assumes the raw data from API might be non-numeric.
    assert not is_valid # Assuming the validation catches non-numeric directly
    assert "not numeric" in errors[0]
    # Test case 4: Empty DataFrame
    empty_df = pd.DataFrame()
    is_valid, errors = validate_data(empty_df, 'ts_EmptyData.csv')
    print(f"Empty DF Test: Valid={is_valid}, Errors={errors}")
    assert is_valid # Currently allows empty with warning
    assert "DataFrame is empty" in errors[0]
    # Test case 5: None DataFrame
    none_df = None
    is_valid, errors = validate_data(none_df, 'ts_NoneData.csv')
    print(f"None DF Test: Valid={is_valid}, Errors={errors}")
    assert not is_valid
    assert "DataFrame is None" in errors[0]
    print("Validation tests completed.")
</file>

<file path="LICENSE">
MIT License

Copyright (c) [2025] [Robert Clark]

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</file>

<file path="metric_calculator.py">
# This file provides functions for calculating various statistical metrics from the preprocessed data.
# Key functionalities include calculating historical statistics (mean, max, min), latest values,
# period-over-period changes, and Z-scores for changes for both benchmark and fund columns.
# It operates on a pandas DataFrame indexed by Date and Fund Code, producing a summary DataFrame
# containing these metrics for each fund, often sorted by the most significant recent changes (Z-scores).
# It now supports calculating metrics for both a primary and an optional secondary DataFrame.
# metric_calculator.py
# This file contains functions for calculating metrics from the processed data.
# Updated to handle primary and optional secondary data sources.
import pandas as pd
import numpy as np
import logging
import os # Needed for logging setup
from typing import List, Dict, Any, Tuple, Optional
# --- Logging Setup ---
# Use the same log file as data_loader
LOG_FILENAME = 'data_processing_errors.log'
LOG_FORMAT = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
# Get the logger for the current module
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
# Prevent adding handlers multiple times (especially if imported by other modules)
if not logger.handlers:
    # Console Handler (INFO and above)
    ch = logging.StreamHandler()
    ch.setLevel(logging.INFO)
    ch_formatter = logging.Formatter(LOG_FORMAT)
    ch.setFormatter(ch_formatter)
    logger.addHandler(ch)
    # File Handler (WARNING and above)
    try:
        # Create log file path relative to this file's location
        log_filepath = os.path.join(os.path.dirname(__file__), '..', LOG_FILENAME)
        fh = logging.FileHandler(log_filepath, mode='a')
        fh.setLevel(logging.WARNING)
        fh_formatter = logging.Formatter(LOG_FORMAT)
        fh.setFormatter(fh_formatter)
        logger.addHandler(fh)
    except Exception as e:
        # Log to stderr if file logging setup fails
        import sys
        print(f"Error setting up file logging for metric_calculator: {e}", file=sys.stderr)
# --- End Logging Setup ---
# Configure logging (can be configured globally elsewhere if part of a larger app)
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
def _calculate_column_stats(
    col_series: pd.Series,
    col_change_series: pd.Series,
    latest_date: pd.Timestamp,
    col_name: str,
    prefix: str = "" # Optional prefix for metric names (e.g., "S&P " )
) -> Dict[str, Any]:
    """Helper function to calculate stats for a single column series.
    Calculates historical mean/max/min, latest value, latest change, and change z-score.
    Handles potential NaN values resulting from calculations or missing data gracefully.
    Args:
        col_series (pd.Series): The historical data for the column.
        col_change_series (pd.Series): The historical changes for the column.
        latest_date (pd.Timestamp): The overall latest date in the dataset.
        col_name (str): The name of the column being processed.
        prefix (str): A prefix to add to the metric names in the output dictionary.
    Returns:
        Dict[str, Any]: A dictionary containing the calculated metrics for this column.
    """
    metrics = {}
    # Calculate base historical stats for the column level
    # Pandas functions like mean, max, min typically handle NaNs by skipping them.
    metrics[f'{prefix}{col_name} Mean'] = col_series.mean()
    metrics[f'{prefix}{col_name} Max'] = col_series.max()
    metrics[f'{prefix}{col_name} Min'] = col_series.min()
    # Calculate stats for the column change
    change_mean = col_change_series.mean()
    change_std = col_change_series.std()
    # Get latest values if data exists for the latest date
    # Check if the latest_date exists in the specific series index
    if latest_date in col_series.index:
        latest_value = col_series.loc[latest_date]
        # Use .get() for change series to handle potential index mismatch (though unlikely if derived correctly)
        latest_change = col_change_series.get(latest_date, np.nan)
        metrics[f'{prefix}{col_name} Latest Value'] = latest_value
        metrics[f'{prefix}{col_name} Change'] = latest_change
        # Calculate Change Z-Score: (latest_change - change_mean) / change_std
        change_z_score = np.nan # Default to NaN
        if pd.notna(latest_change) and pd.notna(change_mean) and pd.notna(change_std) and change_std != 0:
            change_z_score = (latest_change - change_mean) / change_std
        elif change_std == 0 and pd.notna(latest_change) and pd.notna(change_mean):
             # Handle case where std dev is zero (e.g., constant series)
             if latest_change == change_mean:
                 change_z_score = 0.0 # No deviation
             else:
                 change_z_score = np.inf if latest_change > change_mean else -np.inf # Infinite deviation
             logger.debug(f"Standard deviation of change for '{prefix}{col_name}' is zero. Z-score set to {change_z_score}.")
        else:
            # Log if Z-score calculation couldn't be performed due to NaNs
            if not (pd.notna(latest_change) and pd.notna(change_mean) and pd.notna(change_std)):
                 logger.debug(f"Cannot calculate Z-score for '{prefix}{col_name}' due to NaN inputs (latest_change={latest_change}, change_mean={change_mean}, change_std={change_std})")
        metrics[f'{prefix}{col_name} Change Z-Score'] = change_z_score
    else:
        # Data for the latest date is missing for this specific column/fund
        logger.debug(f"Latest date {latest_date} not found for column '{prefix}{col_name}'. Setting latest metrics to NaN.")
        metrics[f'{prefix}{col_name} Latest Value'] = np.nan
        metrics[f'{prefix}{col_name} Change'] = np.nan
        metrics[f'{prefix}{col_name} Change Z-Score'] = np.nan
    return metrics
def _process_dataframe_metrics(
    df: pd.DataFrame,
    fund_codes: pd.Index,
    fund_cols: List[str],
    benchmark_col: Optional[str],
    latest_date: pd.Timestamp,
    metric_prefix: str = ""
) -> Tuple[List[Dict[str, Any]], Dict[str, float]]:
    """Processes a single DataFrame (primary or secondary) to calculate metrics.
    Args:
        df (pd.DataFrame): The DataFrame to process (already sorted by date index).
        fund_codes (pd.Index): Unique fund codes from the combined data.
        fund_cols (List[str]): List of original fund value column names for this df.
        benchmark_col (Optional[str]): Standardized benchmark column name for this df, if present.
        latest_date (pd.Timestamp): The latest date across combined data.
        metric_prefix (str): Prefix to add to metric names (e.g., "S&P ").
    Returns:
        Tuple[List[Dict[str, Any]], Dict[str, float]]:
            - List of metric dictionaries, one per fund.
            - Dictionary mapping fund code to its max absolute change Z-score for sorting.
    """
    if df is None or df.empty:
        logger.warning(f"Input DataFrame for prefix '{metric_prefix}' is None or empty. Returning empty results.")
        return [], {}
    # Determine which columns to actually process based on presence in df
    cols_to_process = []
    output_col_name_map = {} # Map processed col name to output name (original/std)
    if benchmark_col and benchmark_col in df.columns:
        cols_to_process.append(benchmark_col)
        output_col_name_map[benchmark_col] = benchmark_col
    elif benchmark_col:
        logger.warning(f"Specified {metric_prefix}benchmark column '{benchmark_col}' not found in DataFrame columns: {df.columns.tolist()}")
    for f_col in fund_cols:
        if f_col in df.columns:
            cols_to_process.append(f_col)
            output_col_name_map[f_col] = f_col
        else:
            logger.warning(f"Specified {metric_prefix}fund column '{f_col}' not found in DataFrame columns: {df.columns.tolist()}")
    if not cols_to_process:
        logger.error(f"No valid columns (benchmark or funds) found in the {metric_prefix}DataFrame to calculate metrics for.")
        return [], {}
    logger.info(f"Calculating {metric_prefix}metrics for columns: {cols_to_process}")
    fund_metrics_list = []
    max_abs_z_scores: Dict[str, float] = {}
    for fund_code in fund_codes:
        fund_specific_metrics: Dict[str, Any] = {'Fund Code': fund_code} # Initialize with Fund Code
        current_fund_max_abs_z: float = -1.0
        try:
            # Check if fund exists in this specific dataframe
            if fund_code not in df.index.get_level_values(1):
                logger.debug(f"Fund code '{fund_code}' not found in {metric_prefix}DataFrame. Adding empty metrics.")
                # Add NaN placeholders for all expected metrics for this source
                for col_name_proc in cols_to_process:
                    output_name = output_col_name_map[col_name_proc]
                    fund_specific_metrics[f'{metric_prefix}{output_name} Mean'] = np.nan
                    fund_specific_metrics[f'{metric_prefix}{output_name} Max'] = np.nan
                    fund_specific_metrics[f'{metric_prefix}{output_name} Min'] = np.nan
                    fund_specific_metrics[f'{metric_prefix}{output_name} Latest Value'] = np.nan
                    fund_specific_metrics[f'{metric_prefix}{output_name} Change'] = np.nan
                    fund_specific_metrics[f'{metric_prefix}{output_name} Change Z-Score'] = np.nan
                fund_metrics_list.append(fund_specific_metrics)
                max_abs_z_scores[fund_code] = np.nan # No Z-score if fund not present
                continue # Move to the next fund code
            # Extract data for the fund
            fund_data_hist = df.loc[(slice(None), fund_code), cols_to_process]
            fund_data_hist = fund_data_hist.reset_index(level=1, drop=True).sort_index()
            for col_name in cols_to_process:
                if col_name not in fund_data_hist.columns:
                    logger.warning(f"Column '{col_name}' unexpectedly not found for fund '{fund_code}' in {metric_prefix}DF. Skipping metrics.")
                    continue
                col_hist = fund_data_hist[col_name]
                col_change_hist = pd.Series(index=col_hist.index, dtype=np.float64)
                if not col_hist.dropna().empty and len(col_hist.dropna()) > 1:
                    col_change_hist = col_hist.diff()
                else:
                    logger.debug(f"Cannot calculate difference for {metric_prefix}column '{col_name}', fund '{fund_code}' due to insufficient data.")
                # Calculate stats for this specific column
                output_name = output_col_name_map[col_name]
                col_stats = _calculate_column_stats(col_hist, col_change_hist, latest_date, output_name, prefix=metric_prefix)
                fund_specific_metrics.update(col_stats)
                # Update the fund's max absolute Z-score *for this source*
                col_z_score = col_stats.get(f'{metric_prefix}{output_name} Change Z-Score', np.nan)
                compare_z_score = col_z_score
                if np.isinf(compare_z_score):
                    compare_z_score = 1e9 * np.sign(compare_z_score)
                if pd.notna(compare_z_score):
                    current_fund_max_abs_z = max(current_fund_max_abs_z, abs(compare_z_score))
            fund_metrics_list.append(fund_specific_metrics)
            max_abs_z_scores[fund_code] = current_fund_max_abs_z if current_fund_max_abs_z >= 0 else np.nan
        except Exception as e:
            logger.error(f"Error processing {metric_prefix}metrics for fund code '{fund_code}': {e}", exc_info=True)
            # Add placeholder with NaNs if error occurs mid-fund processing
            if 'Fund Code' not in fund_specific_metrics: # Ensure Fund Code is there
                fund_specific_metrics['Fund Code'] = fund_code
            # Add NaN placeholders for potentially missing metrics
            for col_name_proc in cols_to_process:
                output_name = output_col_name_map[col_name_proc]
                if f'{metric_prefix}{output_name} Mean' not in fund_specific_metrics: fund_specific_metrics[f'{metric_prefix}{output_name} Mean'] = np.nan
                if f'{metric_prefix}{output_name} Max' not in fund_specific_metrics: fund_specific_metrics[f'{metric_prefix}{output_name} Max'] = np.nan
                # ... (add for all metrics) ...
                if f'{metric_prefix}{output_name} Change Z-Score' not in fund_specific_metrics: fund_specific_metrics[f'{metric_prefix}{output_name} Change Z-Score'] = np.nan
            fund_metrics_list.append(fund_specific_metrics)
            max_abs_z_scores[fund_code] = np.nan # Mark as NaN for sorting if error occurred
    return fund_metrics_list, max_abs_z_scores
def calculate_latest_metrics(
    primary_df: Optional[pd.DataFrame],
    primary_fund_cols: Optional[List[str]],
    primary_benchmark_col: Optional[str],
    secondary_df: Optional[pd.DataFrame] = None,
    secondary_fund_cols: Optional[List[str]] = None,
    secondary_benchmark_col: Optional[str] = None,
    secondary_prefix: str = "S&P " # Prefix for secondary metrics
) -> pd.DataFrame:
    """Calculates latest metrics for primary and optional secondary data.
    Merges metrics from both sources based on Fund Code.
    Sorts the final DataFrame by the maximum absolute Z-score from the *primary* source.
    Args:
        primary_df (Optional[pd.DataFrame]): Primary processed DataFrame.
        primary_fund_cols (Optional[List[str]]): List of primary fund value column names.
        primary_benchmark_col (Optional[str]): Standardized primary benchmark column name.
        secondary_df (Optional[pd.DataFrame]): Secondary processed DataFrame. Defaults to None.
        secondary_fund_cols (Optional[List[str]]): List of secondary fund value column names. Defaults to None.
        secondary_benchmark_col (Optional[str]): Standardized secondary benchmark column name. Defaults to None.
        secondary_prefix (str): Prefix for secondary metric columns. Defaults to "S&P ".
    Returns:
        pd.DataFrame: Combined metrics indexed by Fund Code, sorted by primary max abs Z-score.
                      Returns an empty DataFrame if primary data is missing or processing fails critically.
    """
    if primary_df is None or primary_df.empty or primary_fund_cols is None:
        logger.warning("Primary DataFrame or fund columns are missing. Cannot calculate metrics.")
        return pd.DataFrame()
    if primary_df.index.nlevels != 2:
        logger.error("Primary DataFrame must have a MultiIndex with 2 levels (Date, Fund Code).")
        return pd.DataFrame()
    # Combine fund codes and find the overall latest date
    all_dfs = [df for df in [primary_df, secondary_df] if df is not None and not df.empty]
    if not all_dfs:
        logger.warning("No valid DataFrames provided. Cannot calculate metrics.")
        return pd.DataFrame()
    try:
        combined_index = pd.concat(all_dfs).index
        latest_date = combined_index.get_level_values(0).max()
        fund_codes = combined_index.get_level_values(1).unique()
        # Ensure DataFrames are sorted by date index for diff calculation
        primary_df_sorted = primary_df.sort_index(level=0)
        secondary_df_sorted = secondary_df.sort_index(level=0) if secondary_df is not None else None
    except Exception as e:
        logger.error(f"Error preparing combined data for metric calculation: {e}", exc_info=True)
        return pd.DataFrame()
    # --- Calculate Metrics for Primary Data --- #
    primary_metrics_list, primary_max_abs_z = _process_dataframe_metrics(
        primary_df_sorted,
        fund_codes, # Use combined fund codes
        primary_fund_cols,
        primary_benchmark_col,
        latest_date,
        metric_prefix="" # No prefix for primary
    )
    # --- Calculate Metrics for Secondary Data (if present) --- #
    secondary_metrics_list = []
    if secondary_df_sorted is not None and secondary_fund_cols is not None:
        logger.info(f"Processing secondary data with prefix: '{secondary_prefix}'")
        secondary_metrics_list, _ = _process_dataframe_metrics(
            secondary_df_sorted,
            fund_codes, # Use combined fund codes
            secondary_fund_cols,
            secondary_benchmark_col,
            latest_date,
            metric_prefix=secondary_prefix
        )
    else:
        logger.info("No valid secondary data provided or fund columns missing, skipping secondary metrics.")
    # --- Combine Metrics --- #
    if not primary_metrics_list:
        logger.warning("Primary metric calculation resulted in empty list. Returning empty DataFrame.")
        return pd.DataFrame()
    # Convert lists of dicts to DataFrames
    primary_metrics_df = pd.DataFrame(primary_metrics_list).set_index('Fund Code')
    if secondary_metrics_list:
        secondary_metrics_df = pd.DataFrame(secondary_metrics_list).set_index('Fund Code')
        # Merge based on Fund Code index, keeping all funds (outer merge)
        combined_metrics_df = primary_metrics_df.merge(
            secondary_metrics_df, left_index=True, right_index=True, how='outer'
        )
    else:
        combined_metrics_df = primary_metrics_df
    # --- Sort Results --- #
    # Add the primary max abs Z-score as a temporary column for sorting
    # Use .get() on the dictionary to handle funds potentially missing from primary results
    combined_metrics_df['_sort_z'] = combined_metrics_df.index.map(lambda fc: primary_max_abs_z.get(fc, np.nan))
    # Sort by the temporary Z-score column (descending), put NaNs last
    combined_metrics_df_sorted = combined_metrics_df.sort_values(by='_sort_z', ascending=False, na_position='last')
    # Drop the temporary sorting column
    combined_metrics_df_sorted = combined_metrics_df_sorted.drop(columns=['_sort_z'])
    logger.info(f"Successfully calculated and combined metrics. Final shape: {combined_metrics_df_sorted.shape}")
    return combined_metrics_df_sorted
</file>

<file path="process_data.py">
# This script serves as a pre-processing step for specific CSV files within the 'Data' directory.
# It targets files prefixed with 'pre_', reads them, and performs data aggregation and cleaning.
# The core logic involves grouping rows based on identical values across most columns, excluding 'Funds' and 'Security Name'.
# For rows sharing the same 'Security Name' but differing in other data points, the 'Security Name' is suffixed
# (e.g., _1, _2) to ensure uniqueness. The 'Funds' associated with identical data rows are aggregated
# into a single list-like string representation (e.g., '[FUND1,FUND2]').
# The processed data is then saved to a new CSV file prefixed with 'new_'.
# process_data.py
# This script processes CSV files in the 'Data' directory that start with 'pre_'.
# It merges rows based on identical values in all columns except 'Funds'.
# Duplicated 'Security Name' entries with differing data are suffixed (_1, _2, etc.).
# The aggregated 'Funds' are stored as a list in the output file.
import os
import pandas as pd
import logging
# Add datetime for date parsing and sorting
from datetime import datetime
import io
# --- Logging Setup ---
# Use the same log file as other data processing scripts
LOG_FILENAME = 'data_processing_errors.log'
LOG_FORMAT = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
# Get the logger for the current module
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
# Prevent adding handlers multiple times
if not logger.handlers:
    # Console Handler (INFO and above)
    ch = logging.StreamHandler()
    ch.setLevel(logging.INFO)
    ch_formatter = logging.Formatter(LOG_FORMAT)
    ch.setFormatter(ch_formatter)
    logger.addHandler(ch)
    # File Handler (WARNING and above)
    try:
        # Create log file path relative to this file's location (assuming it's in the project root)
        log_filepath = os.path.join(os.path.dirname(__file__), LOG_FILENAME) # If script is in root
        # If script is in a sub-directory, adjust path:
        # log_filepath = os.path.join(os.path.dirname(__file__), '..', LOG_FILENAME)
        fh = logging.FileHandler(log_filepath, mode='a')
        fh.setLevel(logging.WARNING)
        fh_formatter = logging.Formatter(LOG_FORMAT)
        fh.setFormatter(fh_formatter)
        logger.addHandler(fh)
    except Exception as e:
        # Log to stderr if file logging setup fails
        import sys
        print(f"Error setting up file logging for process_data: {e}", file=sys.stderr)
# --- End Logging Setup ---
# Define a constant for the dates file path
DATES_FILE_PATH = os.path.join('Data', 'dates.csv') # Define path to dates file
def read_and_sort_dates(dates_file):
    """
    Reads dates from a CSV file, sorts them, and returns them as a list of strings.
    Args:
        dates_file (str): Path to the CSV file containing dates (expected single column).
    Returns:
        list[str] | None: A sorted list of date strings (YYYY-MM-DD) or None if an error occurs.
    """
    try:
        dates_df = pd.read_csv(dates_file, parse_dates=[0]) # Assume date is the first column
        # Handle potential parsing errors if the column isn't purely dates
        if dates_df.iloc[:, 0].isnull().any():
             logger.warning(f"Warning: Some values in {dates_file} could not be parsed as dates.")
             # Attempt to drop NaT values and proceed
             dates_df = dates_df.dropna(subset=[dates_df.columns[0]])
             if dates_df.empty:
                 logger.error(f"Error: No valid dates found in {dates_file} after handling parsing issues.")
                 return None
        # Sort dates chronologically
        sorted_dates = dates_df.iloc[:, 0].sort_values()
        # Format dates as 'YYYY-MM-DD' strings for column headers
        date_strings = sorted_dates.dt.strftime('%Y-%m-%d').tolist()
        logger.info(f"Successfully read and sorted {len(date_strings)} dates from {dates_file}.")
        # --- Deduplicate the date list while preserving order --- 
        unique_date_strings = []
        seen_dates = set()
        duplicates_found = False
        for date_str in date_strings:
            if date_str not in seen_dates:
                unique_date_strings.append(date_str)
                seen_dates.add(date_str)
            else:
                duplicates_found = True
        if duplicates_found:
            logger.warning(f"Duplicate dates found in {dates_file}. Using unique sorted dates: {len(unique_date_strings)} unique dates.")
        # --- End Deduplication ---
        return unique_date_strings # Return the deduplicated list
    except FileNotFoundError:
        logger.error(f"Error: Dates file not found at {dates_file}")
        return None
    except pd.errors.EmptyDataError:
        logger.error(f"Error: Dates file is empty - {dates_file}")
        return None
    except IndexError:
        logger.error(f"Error: Dates file {dates_file} seems to have no columns.")
        return None
    except Exception as e:
        logger.error(f"An unexpected error occurred reading dates from {dates_file}: {e}", exc_info=True)
        return None
# Modify process_csv_file signature to accept date_columns
def process_csv_file(input_path, output_path, date_columns):
    """
    Reads a 'pre_' CSV file, potentially replaces placeholder columns with dates,
    processes it according to the rules, and writes the result to a 'new_' CSV file.
    Args:
        input_path (str): Path to the input CSV file (e.g., 'Data/pre_sec_duration.csv').
        output_path (str): Path to the output CSV file (e.g., 'Data/new_sec_duration.csv').
        date_columns (list[str] | None): Sorted list of date strings for headers, or None if dates couldn't be read.
    """
    # If date_columns is None (due to error reading dates.csv), log and skip processing files needing date replacement.
    # We'll handle the actual replacement logic further down.
    if date_columns is None:
         logger.warning(f"Skipping {input_path} because date information is unavailable (check logs for errors reading dates.csv).")
         # A file might still be processable if its columns are *already* correct dates,
         # but the current logic requires date_columns for the check/replacement.
         # To proceed without dates.csv, the logic would need significant changes.
         return # Skip processing this file if dates aren't loaded.
    try:
        # Read the input CSV - add robustness
        # Skip bad lines, handle encoding errors
        df = pd.read_csv(input_path, on_bad_lines='skip', encoding='utf-8', encoding_errors='replace')
        logger.info(f"Processing file: {input_path}")
        # Log DataFrame info right after reading
        logger.info(f"DataFrame info after read:")
        # Use a buffer to capture df.info() output for logging
        buf = io.StringIO()
        df.info(verbose=True, buf=buf)
        logger.info(buf.getvalue())
        if df.empty:
            logger.warning(f"Input file {input_path} is empty or contains only invalid lines. Skipping processing.")
            return
        # --- Column Header Replacement & Validation Logic ---
        original_cols = df.columns.tolist()
        # Define core columns, allowing for 'Fund' or 'Funds'
        fund_col_name = None
        if 'Funds' in original_cols:
            fund_col_name = 'Funds'
        elif 'Fund' in original_cols:
            fund_col_name = 'Fund'
            logger.info(f"Found 'Fund' column in {input_path}. Will rename to 'Funds' for processing.")
            # Rename the column IN PLACE for subsequent operations
            df.rename(columns={'Fund': 'Funds'}, inplace=True)
        else:
            logger.error(f"Skipping {input_path}: Missing required fund column (neither 'Funds' nor 'Fund' found). Found columns: {original_cols}")
            return
        # Now define required cols using the standardized 'Funds' name
        required_cols = ['Funds', 'Security Name']
        # Check for 'Security Name' (already checked for fund column)
        if 'Security Name' not in original_cols:
            logger.error(f"Skipping {input_path}: Missing required column 'Security Name'. Found columns: {original_cols}")
            return
        logger.debug(f"Required columns {required_cols} confirmed present (or renamed) in {input_path}.")
        # Find the index after the last required column to start searching for placeholders
        # Use the potentially renamed df.columns
        current_df_cols = df.columns.tolist()
        last_required_idx = -1
        for req_col in required_cols:
            try:
                last_required_idx = max(last_required_idx, current_df_cols.index(req_col))
            except ValueError: # Should not happen due to checks above, but safeguard
                 logger.error(f"Required column '{req_col}' unexpectedly not found after initial check/rename in {input_path}. Skipping.")
                 return
        candidate_start_index = last_required_idx + 1
        # Use current_df_cols which includes the renamed column if applicable
        candidate_cols = current_df_cols[candidate_start_index:]
        # Decide on the final columns to use for processing
        current_cols = current_df_cols # Default to current (potentially renamed) columns
        # --- Enhanced Placeholder Detection ---
        # Detect sequences like 'Col', 'Col.1', 'Col.2', ... (Pandas default)
        # AND sequences like 'Base', 'Base', 'Base', ... (Target for date replacement)
        potential_placeholder_base_patternA = None # For Base, Base.1, ...
        detected_sequence_patternA = []
        start_index_patternA = -1
        potential_placeholder_base_patternB = None # For Base, Base, Base, ...
        detected_sequence_patternB = []
        start_index_patternB = -1
        is_patternB_dominant = False # Flag if Pattern B is found and should trigger date replacement
        if not candidate_cols:
            logger.warning(f"File {input_path} has no columns after required columns {required_cols}. Cannot check for date placeholders.")
        else:
            # Check for Pattern B first: 'Base', 'Base', 'Base', ...
            first_candidate = candidate_cols[0]
            # Check if ALL candidate columns are identical to the first one
            if all(col == first_candidate for col in candidate_cols):
                potential_placeholder_base_patternB = first_candidate
                detected_sequence_patternB = candidate_cols
                start_index_patternB = 0 # Starts at the beginning of candidates
                logger.info(f"Detected Pattern B: Repeated column name '{potential_placeholder_base_patternB}' for all {len(detected_sequence_patternB)} candidate columns.")
                # If we find Pattern B covering *all* candidates, we prioritize it for date replacement check
                is_patternB_dominant = True
            else:
                 logger.info(f"Candidate columns are not all identical (Pattern B check failed). First candidate: '{first_candidate}'. Candidates: {candidate_cols[:5]}...")
                 # If Pattern B check fails, proceed to check for Pattern A ('Base', 'Base.1', ...)
                 # Iterate through candidate columns to find the *start* of the sequence 'Base', 'Base.1', ...
                 found_sequence_A = False
                 for start_idx in range(len(candidate_cols)):
                     current_potential_base = candidate_cols[start_idx]
                     # Check if it's a potential base name (no '.' suffix)
                     if '.' not in current_potential_base:
                         logger.debug(f"Checking for Pattern A starting with '{current_potential_base}' at index {start_idx} in candidate columns.")
                         temp_sequence = [current_potential_base]
                         # Check subsequent columns for the pattern 'base.1', 'base.2', etc.
                         for i in range(1, len(candidate_cols) - start_idx):
                             expected_col = f"{current_potential_base}.{i}"
                             actual_col_index = start_idx + i
                             if candidate_cols[actual_col_index] == expected_col:
                                 temp_sequence.append(candidate_cols[actual_col_index])
                             else:
                                 logger.debug(f"Pattern A sequence broken at index {actual_col_index}. Expected '{expected_col}', found '{candidate_cols[actual_col_index]}'.")
                                 break # Stop checking for this base
                         if len(temp_sequence) > 1: # Found Base, Base.1 at minimum
                             potential_placeholder_base_patternA = current_potential_base
                             detected_sequence_patternA = temp_sequence
                             start_index_patternA = start_idx
                             logger.info(f"Found Pattern A sequence starting with '{potential_placeholder_base_patternA}' at candidate index {start_index_patternA} with length {len(detected_sequence_patternA)}.")
                             found_sequence_A = True
                             break # Exit the outer loop for Pattern A search
                         else:
                             logger.debug(f"Only base '{current_potential_base}' found or Pattern A sequence too short. Continuing search.")
                             # Continue loop to check next candidate as potential base
                     else:
                         logger.debug(f"Column '{current_potential_base}' at index {start_idx} has '.' suffix, skipping as potential Pattern A base.")
                 if not found_sequence_A:
                     logger.info(f"No Pattern A sequence ('Base', 'Base.1', ...) found in candidate columns of {input_path}.")
        # --- Date Replacement Logic using Detected Patterns ---
        if date_columns is None:
            logger.warning(f"Date information from {DATES_FILE_PATH} is unavailable. Cannot check or replace headers in {input_path}. Processing with original headers: {original_cols}")
        # --- Prioritize Pattern B for Date Replacement ---
        elif is_patternB_dominant:
             placeholder_count_B = len(detected_sequence_patternB)
             original_placeholder_start_index_B = candidate_start_index + start_index_patternB # Should be last_required_idx + 1
             logger.info(f"Processing based on detected Pattern B ('{potential_placeholder_base_patternB}' repeated {placeholder_count_B} times), starting at original index {original_placeholder_start_index_B}.")
             if len(date_columns) == placeholder_count_B:
                 logger.info(f"Replacing {placeholder_count_B} repeated '{potential_placeholder_base_patternB}' columns with dates.")
                 # Use current_cols here to respect potential prior rename ('Fund' -> 'Funds')
                 cols_before = current_cols[:original_placeholder_start_index_B]
                 cols_after = current_cols[original_placeholder_start_index_B + placeholder_count_B:]
                 new_columns = cols_before + date_columns + cols_after
                 if len(new_columns) != len(current_cols): # Compare against current_cols length
                     logger.error(f"Internal error (Pattern B): Column count mismatch after constructing new columns ({len(new_columns)} vs {len(current_cols)}). Reverting to original headers.")
                     # Revert logic might need refinement, but for now, keep current_cols as is.
                     # current_cols = original_cols # Reverting might lose the 'Fund'->'Funds' rename
                 else:
                     df.columns = new_columns
                     current_cols = new_columns
                     logger.info(f"Columns after Pattern B date replacement: {current_cols}")
             else:
                 logger.warning(f"Count mismatch for Pattern B in {input_path}: Found {placeholder_count_B} repeated '{potential_placeholder_base_patternB}' columns, but expected {len(date_columns)} dates. Skipping date replacement. Processing with original headers.")
                 # current_cols remains potentially renamed cols
        # --- Handle Pattern A or No Pattern ---
        # No Pattern B found, or it didn't match date count. Check Pattern A or if columns already match dates.
        else:
             if potential_placeholder_base_patternA:
                 # Pattern A ('Base', 'Base.1', ...) was found.
                 placeholder_count_A = len(detected_sequence_patternA)
                 original_placeholder_start_index_A = candidate_start_index + start_index_patternA
                 logger.info(f"Detected Pattern A sequence based on '{potential_placeholder_base_patternA}' (length {placeholder_count_A}) starting at original index {original_placeholder_start_index_A}.")
                 # --- Attempt Date Replacement for Pattern A if lengths match ---
                 if len(date_columns) == placeholder_count_A:
                     logger.info(f"Replacing {placeholder_count_A} Pattern A columns ('{potential_placeholder_base_patternA}', '{potential_placeholder_base_patternA}.1', ...) with dates.")
                     # Use current_cols here to respect potential prior rename ('Fund' -> 'Funds')
                     cols_before = current_cols[:original_placeholder_start_index_A]
                     cols_after = current_cols[original_placeholder_start_index_A + placeholder_count_A:]
                     new_columns = cols_before + date_columns + cols_after
                     if len(new_columns) != len(current_cols): # Compare against current_cols length
                         logger.error(f"Internal error (Pattern A): Column count mismatch after constructing new columns ({len(new_columns)} vs {len(current_cols)}). Reverting to original headers.")
                         # Keep current_cols as is to avoid losing potential rename
                         # current_cols = original_cols
                     else:
                         df.columns = new_columns
                         current_cols = new_columns
                         logger.info(f"Columns after Pattern A date replacement: {current_cols}")
                 else:
                     # Lengths don't match, log warning and proceed with original (Pattern A) headers
                     logger.warning(f"Count mismatch for Pattern A in {input_path}: Found {placeholder_count_A} columns in sequence ('{potential_placeholder_base_patternA}', '{potential_placeholder_base_patternA}.1', ...), but expected {len(date_columns)} dates. Skipping date replacement. Processing with original headers.")
                     # current_cols remains potentially renamed cols
                 # --- End Date Replacement Logic for Pattern A ---
             elif candidate_cols == date_columns:
                 # No patterns found, but candidates already match dates
                 logger.info(f"Columns in {input_path} (after required ones) already match the expected dates. No replacement needed.")
        # --- End Column Header Replacement Logic ---
        # Identify columns to check for identity (all except Funds and Security Name) using the CURRENT columns
        # These might be the original placeholders or the replaced dates.
        # Crucially, this now correctly includes any original static columns that were *not* replaced.
        id_cols = [col for col in current_cols if col not in required_cols]
        processed_rows = []
        # Convert 'Security Name' and 'Funds' to string first to handle potential non-string types causing issues later
        # Use 'Funds' as it has been standardized by rename operation if necessary
        df['Security Name'] = df['Security Name'].astype(str)
        df['Funds'] = df['Funds'].astype(str)
        # Group by the primary identifier 'Security Name'
        # Convert 'Security Name' to string first to handle potential non-string types causing groupby issues
        # df['Security Name'] = df['Security Name'].astype(str) # Already done above
        # Ensure 'Funds' is also string for consistent processing later
        # df['Funds'] = df['Funds'].astype(str) # Already done above
        # Use the potentially renamed DataFrame for grouping
        grouped_by_sec = df.groupby('Security Name', sort=False, dropna=False)
        for sec_name, sec_group in grouped_by_sec:
            # Within each security group, further group by all other identifying columns (which might now be dates)
            # This separates rows where the same Security Name has different associated data
            distinct_versions = []
            if id_cols: # Only subgroup if there are other identifying columns
                try:
                    # dropna=False treats NaNs in id_cols as equal for grouping
                    sub_grouped = sec_group.groupby(id_cols, dropna=False, sort=False)
                    distinct_versions = [group for _, group in sub_grouped]
                except KeyError as e:
                    logger.error(f"KeyError during sub-grouping for Security Name '{sec_name}' in {input_path}. Column: {e}. Grouping columns: {id_cols}. Skipping this security.", exc_info=True)
                    continue # Skip this security name if subgrouping fails
                except Exception as e:
                     logger.error(f"Unexpected error during sub-grouping for Security Name '{sec_name}' in {input_path}: {e}. Grouping columns: {id_cols}. Skipping this security.", exc_info=True)
                     continue
            else:
                # If only Security Name and Funds exist (after potential date column issues), treat the whole sec_group as one version
                 distinct_versions = [sec_group]
            num_versions = len(distinct_versions)
            # Iterate through each distinct version found for the current Security Name
            for i, current_version_df in enumerate(distinct_versions):
                if current_version_df.empty:
                    continue # Should not happen, but safeguard
                # Aggregate the unique 'Funds' for this specific version
                # Handle potential NaN values in Funds column before aggregation
                unique_funds = current_version_df['Funds'].dropna().unique()
                # Convert funds to string before joining
                funds_list = sorted([str(f) for f in unique_funds])
                # Take the first row of this version as the template for the output row
                # Use .iloc[0] safely as we checked current_version_df is not empty
                new_row_series = current_version_df.iloc[0].copy()
                # Assign the aggregated funds as a string formatted like a list: "[FUND1,FUND2,...]"
                new_row_series['Funds'] = f"[{','.join(funds_list)}]"
                # If there was more than one distinct version for this Security Name, suffix the name
                if num_versions > 1:
                    # Ensure sec_name is a string before formatting
                    new_row_series['Security Name'] = f"{str(sec_name)}_{i+1}"
                # Else: keep the original Security Name (already stringified and set in new_row_series)
                # Append the processed row (as a dictionary) to our results list
                processed_rows.append(new_row_series.to_dict())
        if not processed_rows:
            logger.warning(f"No data rows processed for {input_path}. Output file will not be created.")
            # Changed behavior: Do not create an empty output file if no rows are processed.
            return
            # Create an empty DataFrame with original columns if no rows processed
            # output_df = pd.DataFrame(columns=original_cols)
        else:
             # Create the final DataFrame from the list of processed rows
            output_df = pd.DataFrame(processed_rows)
             # Ensure the column order reflects the potentially updated columns (current_cols)
             # Filter current_cols to only those present in output_df to avoid KeyErrors if a column was unexpectedly dropped
            final_cols = [col for col in current_cols if col in output_df.columns]
            output_df = output_df[final_cols]
        # Fill NaN values with 0 before saving
        output_df = output_df.fillna(0)
        # Log DataFrame info just before saving
        logger.info(f"Output DataFrame info before save (after NaN fill):")
        buf = io.StringIO()
        output_df.info(verbose=True, buf=buf)
        logger.info(buf.getvalue())
        # Write the processed data to the new CSV file
        # The Funds column now contains comma-separated strings, which pandas will quote if necessary.
        output_df.to_csv(output_path, index=False, encoding='utf-8')
        logger.info(f"Successfully created: {output_path} with {len(output_df)} rows.")
    except FileNotFoundError:
        logger.error(f"Error: Input file not found - {input_path}")
    except pd.errors.EmptyDataError:
         logger.warning(f"Input file is empty or contains only header - {input_path}. Skipping.")
    except pd.errors.ParserError as pe:
        logger.error(f"Error parsing CSV file {input_path}: {pe}. Check file format and integrity.", exc_info=True)
    except Exception as e:
        logger.error(f"An unexpected error occurred processing {input_path}: {e}", exc_info=True)
def main():
    """
    Main function to find and process all 'pre_*.csv' files in the 'Data' directory.
    """
    # --- Read Dates ---
    # Define the path to the dates file
    dates_file_path = DATES_FILE_PATH # Use the constant defined at the top
    # Attempt to read and sort dates
    date_columns = read_and_sort_dates(dates_file_path)
    # If dates couldn't be read, date_columns will be None.
    # process_csv_file will handle this by skipping files.
    # Log message indicating status of date loading is handled within read_and_sort_dates.
    # --- End Read Dates ---
    input_dir = 'Data'
    input_prefix = 'pre_'
    # output_prefix = 'new_' #disabled
    output_prefix = ''
    if not os.path.isdir(input_dir):
        logger.error(f"Input directory '{input_dir}' not found.")
        return
    logger.info(f"Starting pre-processing scan in directory: '{input_dir}'")
    processed_count = 0
    skipped_count = 0
    # Iterate through all files in the specified directory
    for filename in os.listdir(input_dir):
        # Check if the file matches the pattern 'pre_*.csv'
        if filename.startswith(input_prefix) and filename.endswith('.csv'):
            input_file_path = os.path.join(input_dir, filename)
            # Construct the output filename by replacing 'pre_' with 'new_'
            output_filename = filename.replace(input_prefix, output_prefix, 1)
            output_file_path = os.path.join(input_dir, output_filename)
            # Process the individual CSV file
            try:
                # Pass the loaded date_columns to the processing function
                process_csv_file(input_file_path, output_file_path, date_columns)
                # Simple check if output exists might not be enough if process_csv_file skips creation
                # We rely on logs from process_csv_file to indicate success/failure/skip
                processed_count +=1 # Increment even if skipped internally, as we attempted it.
            except Exception as e:
                 # Catch any unexpected errors bubbling up from process_csv_file
                 logger.error(f"Unhandled exception processing {input_file_path} in main loop: {e}", exc_info=True)
                 skipped_count += 1
        else:
            # Optionally log files that don't match the pattern if needed for debugging
            # logger.debug(f"Skipping file '{filename}' as it does not match pattern 'pre_*.csv'")
            pass
    logger.info(f"Pre-processing scan finished. Attempted processing {processed_count} files. Encountered errors in {skipped_count} files during main loop (check logs for details).")
if __name__ == "__main__":
    # Ensure the script runs the main function when executed directly
    main()
</file>

<file path="README.md">
# Simple Data Checker

This application provides a web interface to load, process, and check financial data, primarily focusing on time-series metrics and security-level data. It helps identify potential data anomalies by calculating changes and Z-scores.

## Features

*   **Time-Series Metric Analysis:** Load `ts_*.csv` files, view latest changes, Z-scores, and historical data charts for various metrics per fund.
*   **Security-Level Analysis:** Load wide-format `sec_*.csv` files, view latest changes and Z-scores across securities, and drill down into historical charts (Value, Price, Duration) for individual securities.
    *   **Performance:** Uses server-side pagination, filtering (search, dropdowns), and sorting for improved performance with large datasets.
*   **Fund-Specific Views:** Analyze data aggregated or filtered by specific funds (e.g., Fund Duration Details, General Fund Overview).
*   **Security Exclusions:** Maintain a list of securities to temporarily exclude from the main Security Summary page (`/security/summary`). Exclusions can have start/end dates and comments.
*   **Weight Check:** Load fund (`w_Funds.csv`) and benchmark (`w_Bench.csv`) weight files and display them side-by-side, highlighting any daily weights that are not exactly 100% via `/weights/check`.
*   **Data Comparison:**
    *   Compare two spread files (`sec_spread.csv` vs `sec_spreadSP.csv`) via `/comparison/summary`.
    *   Compare two duration files (`sec_duration.csv` vs `sec_durationSP.csv`) via `/duration_comparison/summary`.
    *   Compare two spread duration files (`sec_Spread duration.csv` vs `sec_Spread durationSP.csv`) via `/spread_duration_comparison/summary`.
    *   All comparison pages include summary statistics and side-by-side detail charts.
    *   **Performance:** Uses server-side pagination, filtering, and sorting for summary views.
*   **Data Simulation & Management:**
    *   Simulate API calls to fetch data via the `/get_data` page.
    *   Run a data cleanup process via a button on the `/get_data` page.

## File Structure Overview

```mermaid
graph TD
    A[Simple Data Checker] --> B(app.py);
    A --> C{Python Modules};
    A --> D{Views};
    A --> E{Templates};
    A --> F{Static Files};
    A --> G(Data);
    A --> H(Config/Utils);

    C --> C1(data_loader.py);
    C --> C2(metric_calculator.py);
    C --> C3(security_processing.py);
    C --> C4(process_data.py);

    D --> D1(main_views.py);
    D --> D2(metric_views.py);
    D --> D3(security_views.py);
    D --> D4(fund_views.py);
    D --> D5(exclusion_views.py);
    D --> D6(comparison_views.py);
    D --> D7(weight_views.py);
    D --> D8(api_views.py);
    D --> D9(duration_comparison_views.py);
    D --> D10(spread_duration_comparison_views.py);

    E --> E1(base.html);
    E --> E2(index.html);
    E --> E3(metric_page_js.html);
    E --> E4(securities_page.html);
    E --> E5(security_details_page.html);
    E --> E6(fund_duration_details.html);
    E --> E7(exclusions_page.html);
    E --> E8(get_data.html);
    E --> E9(comparison_page.html);
    E --> E10(comparison_details_page.html);
    E --> E11(fund_detail_page.html);
    E --> E12(weight_check.html);
    E --> E13(duration_comparison_page.html);
    E --> E14(duration_comparison_details_page.html);
    E --> E15(spread_duration_comparison_page.html);
    E --> E16(spread_duration_comparison_details_page.html);

    F --> F1(js);
    F1 --> F1a(main.js);
    F1 --> F1b(modules);
    F1b --> F1b1(ui);
    F1b1 --> F1b1a(chartRenderer.js);
    F1b1 --> F1b1b(securityTableFilter.js);
    F1b1 --> F1b1c(tableSorter.js);
    F1b --> F1b2(utils);
    F1b2 --> F1b2a(helpers.js);
    F1b --> F1b3(charts);
    F1b3 --> F1b3a(timeSeriesChart.js);

    G --> G1(ts_*.csv);
    G --> G2(sec_*.csv);
    G --> G3(pre_*.csv);
    G --> G4(new_*.csv);
    G --> G5(exclusions.csv);
    G --> G6(QueryMap.csv);
    G --> G7(FundList.csv);
    G --> G8(w_Funds.csv);
    G --> G9(w_Bench.csv);

    H --> H1(config.py);
    H --> H2(utils.py);

    B --> D;
    D --> C;
    D --> H;
    D --> E;
    E --> F;
```

## Data Files (`Data/`)

*   `ts_*.csv`: Time-series data, indexed by Date and Code (Fund/Benchmark).
*   `sec_*.csv`: Security-level data, typically wide format with dates as columns.
*   `pre_*.csv`: Input files for the `process_data.py` script.
*   `new_*.csv`: Output files from the `process_data.py` script.
*   **`exclusions.csv`**: Stores the list of excluded securities. Contains columns: `SecurityID`, `AddDate`, `EndDate`, `Comment`.
*   `QueryMap.csv`: Maps query IDs to filenames for the API simulation.
*   `FundList.csv`: Contains fund codes and metadata used on the API simulation page.
*   `Dates.csv`: May exist for specific configurations or helper data.
*   `w_Funds.csv`: Wide format file containing daily fund weights (expected to be 100%). Used by the Weight Check page.
*   `w_Bench.csv`: Wide format file containing daily benchmark weights (expected to be 100%). Used by the Weight Check page.

## Python Files

### `app.py`
*   **Purpose:** Defines the main entry point and structure for the Simple Data Checker Flask web application. It utilizes the Application Factory pattern (`create_app`) to initialize and configure the Flask app.
*   **Key Responsibilities:**
    *   Creating the Flask application instance.
    *   Setting up basic configuration (like the secret key).
    *   Ensuring necessary folders (like the instance folder) exist.
    *   Registering all Blueprints (e.g., `main_bp`, `metric_bp`, `security_bp`, `fund_bp`, `exclusion_bp`, `comparison_bp`, `duration_comparison_bp`, `spread_duration_comparison_bp`, `api_bp`, `weight_bp`) from the `views` directory.
    *   Includes an endpoint (`/run-cleanup`) to trigger the `process_data.py` script.
    *   Providing a conditional block (`if __name__ == '__main__':`) to run the development server.
*   **Functions:**
    *   `create_app()`: Factory function to create and configure the Flask app.
    *   `run_cleanup()`: Endpoint to run the cleanup script.
    *   `hello()`: Simple test route (can be removed).

### `config.py`
*   **Purpose:** Defines configuration variables for the Simple Data Checker application.
*   **Variables:**
    *   `DATA_FOLDER`: Specifies the directory containing data files.
    *   `COLOR_PALETTE`: Defines a list of colors for chart lines.

### `data_loader.py`
*   **Purpose:** Responsible for loading and preprocessing data from time-series CSV files (`ts_*.csv`).
*   **Key Features:**
    *   Dynamically identifies 'Date' (using `Date` or `Position Date`), 'Code', and optional benchmark columns (using `Bench`).
    *   Parses dates using pandas `to_datetime` (handles various formats).
    *   Standardizes key column names (`Date`, `Code`, `Benchmark`).
    *   Sets a MultiIndex (`Date`, `Code`).
    *   Converts value columns to numeric using `pd.to_numeric(errors='coerce')`.
    *   Logs progress and errors to `data_processing_errors.log`.
*   **Functions:**
    *   `_find_column(...)`: Helper to find columns by regex.
    *   `load_and_process_data(...)`: Main function for loading and processing.

### `metric_calculator.py`
*   **Purpose:** Provides functions for calculating statistical metrics (mean, max, min, latest value, change, Z-score) from preprocessed time-series data.
*   **Key Features:**
    *   Operates on a DataFrame indexed by Date and Fund Code.
    *   Calculates metrics for both fund and benchmark columns (if present).
    *   Handles `NaN` values gracefully.
*   **Functions:**
    *   `_calculate_column_stats(...)`: Helper for single-column stats.
    *   `calculate_latest_metrics(...)`: Calculates latest metrics per fund, sorted by max absolute Z-score.

### `process_data.py`
*   **Purpose:** Serves as a pre-processing step for specific CSV files (usually `pre_*.csv`), aggregating rows and handling duplicates.
*   **Functions:**
    *   `process_csv_file(...)`: Processes a single input file.
    *   `main()`: Processes all `pre_*.csv` files in the `Data` directory.

### `security_processing.py`
*   **Purpose:** Handles loading, processing, and analysis of security-level data (usually `sec_*.csv`).
*   **Key Features:**
    *   Assumes wide format (dates as columns).
    *   Dynamically identifies ID, static, and date columns (using `utils._is_date_like`).
    *   Melts data into long format (Date, Security ID).
    *   Robust type conversion (`pd.to_datetime`, `pd.to_numeric`, `errors='coerce'`).
    *   Calculates latest metrics (Latest Value, Change, Mean, Max, Min, Change Z-Score) per security.
*   **Functions:**
    *   `_is_date_like(...)`: Moved to `utils.py`.
    *   `load_and_process_security_data(...)`: Loads and melts wide-format data.
    *   `calculate_security_latest_metrics(...)`: Calculates metrics on long-format data.

### `utils.py`
*   **Purpose:** Contains common utility functions.
*   **Functions:**
    *   `_is_date_like(column_name)`: Checks if a column name resembles `YYYY-MM-DD` or `DD/MM/YYYY` format.
    *   `parse_fund_list(fund_string)`: Parses a string like `'[FUND1,FUND2]'` into a list.

## View Modules (`views/`)

These modules contain the Flask Blueprints defining the application's routes.

### `views/main_views.py` (`main_bp`)
*   **Purpose:** Main dashboard/index page.
*   **Routes:**
    *   `/`: Renders `index.html`, showing links to metric pages and a summary table of latest Z-Scores across all metrics and funds. Fund codes link to the general fund detail page (`/fund/<fund_code>`).

### `views/metric_views.py` (`metric_bp`)
*   **Purpose:** Detailed views for specific time-series metrics.
*   **Routes:**
    *   `/metric/<metric_name>`: Renders `metric_page_js.html`. Loads `ts_*.csv`, calculates metrics, and passes JSON data (metadata, fund data) to the template for JavaScript chart rendering. Includes link to fund duration details if metric is 'Duration'.

### `views/security_views.py` (`security_bp`)
*   **Purpose:** Security-level data checks.
*   **Routes:**
    *   `/security/summary`: Renders `securities_page.html`.
        *   **Handles server-side pagination, filtering (search, static columns), and sorting.**
        *   Loads spread data (`sec_Spread.csv`), applies filters/search/exclusions.
        *   Calculates metrics, sorts data, and selects the current page.
        *   Passes paginated data and metadata to the template.
        *   Security IDs link to the details page.
    *   `/security/details/<metric_name>/<path:security_id>`: Renders `security_details_page.html`. Shows historical charts for a specific security and metric.

### `views/fund_views.py` (`fund_bp`)
*   **Purpose:** Fund-specific views.
*   **Routes:**
    *   `/fund/duration_details/<fund_code>`: Renders `fund_duration_details.html`. Loads `sec_duration.csv`, filters by fund, calculates recent duration changes, and displays results.
    *   `/fund/<fund_code>`: Renders `fund_detail_page.html`. Finds all `ts_*.csv` files, loads data for the specified fund, and prepares data for rendering multiple time-series charts on a single page.

### `views/exclusion_views.py` (`exclusion_bp`)
*   **Purpose:** Managing the security exclusion list (`Data/exclusions.csv`).
*   **Routes:**
    *   `/exclusions` (GET/POST): Renders `exclusions_page.html` to view/add exclusions.
    *   `/exclusions/remove` (POST): Removes an exclusion.

### `views/comparison_views.py` (`comparison_bp`)
*   **Purpose:** Comparing two spread files (`sec_spread.csv` vs. `sec_spreadSP.csv`).
*   **Routes:**
    *   `/comparison/summary`: Renders `comparison_page.html`.
        *   **Handles server-side pagination, filtering (static columns), and sorting.**
        *   Loads both files, calculates comparison statistics (correlation, diffs, date ranges).
        *   Applies filters, sorts data, and selects the current page.
        *   Passes paginated data and metadata to the template.
    *   `/comparison/details/<path:security_id>`: Renders `comparison_details_page.html`. Shows side-by-side historical charts for a specific security.

### `views/duration_comparison_views.py` (`duration_comparison_bp`)
*   **Purpose:** Comparing two duration files (`sec_duration.csv` vs. `sec_durationSP.csv`).
*   **Routes:**
    *   `/duration_comparison/summary`: Renders `duration_comparison_page.html`.
        *   Handles server-side pagination, filtering (static columns), and sorting.
        *   Loads both files, calculates comparison statistics.
        *   Applies filters, sorts data, and selects the current page.
        *   Passes paginated data and metadata to the template.
    *   `/duration_comparison/details/<path:security_id>`: Renders `duration_comparison_details_page.html`. Shows side-by-side historical charts for a specific security.

### `views/spread_duration_comparison_views.py` (`spread_duration_comparison_bp`)
*   **Purpose:** Comparing two spread duration files (`sec_Spread duration.csv` vs. `sec_Spread durationSP.csv`).
*   **Routes:**
    *   `/spread_duration_comparison/summary`: Renders `spread_duration_comparison_page.html`.
        *   Handles server-side pagination, filtering (static columns), and sorting.
        *   Loads both files, calculates comparison statistics.
        *   Applies filters, sorts data, and selects the current page.
        *   Passes paginated data and metadata to the template.
    *   `/spread_duration_comparison/details/<path:security_id>`: Renders `spread_duration_comparison_details_page.html`. Shows side-by-side historical charts for a specific security.

### `views/api_views.py` (`api_bp`)
*   **Purpose:** Handling the API simulation page interactions.
*   **Routes:**
    *   `/get_data`: Renders `get_data.html` (GET). Shows data file statuses, fund selection, and date inputs.
    *   `/run-api-calls`: Handles the POST request from `get_data.html` to simulate API calls based on `QueryMap.csv`. Reads data, merges/overwrites based on `overwrite_mode` flag, and saves to `Data/` folder.
    *   `/rerun-api-call`: Handles POST requests to rerun a single API call for a specific fund.

### `views/weight_views.py` (`weight_bp`)
*   **Purpose:** Handles the weight checking functionality.
*   **Routes:**
    *   `/weights/check`: Renders `weight_check_page.html`. Loads data from `w_Funds.csv` and `w_Bench.csv`, processes percentage values, checks if they equal 100%, and passes the processed data to the template for display.

## HTML Templates (`templates/`)

*   **`base.html`:** Main layout, includes Bootstrap, navbar, common structure. All other templates extend this.
*   **`index.html`:** Dashboard page. Displays metric links and Z-Score summary table.
*   **`metric_page_js.html`:** Detail page for a time-series metric (rendered via JS).
*   **`securities_page.html`:** Security check summary table. Includes filter/search form, sortable headers, table body, and pagination controls.
*   **`security_details_page.html`:** Detail page for a single security (charts).
*   **`fund_duration_details.html`:** Table showing security duration changes for a specific fund.
*   **`exclusions_page.html`:** UI for managing security exclusions.
*   **`get_data.html`:** UI for API simulation. Includes data status table, fund selection, date inputs, status/results area, and buttons for simulation, overwrite, and cleanup.
*   **`comparison_page.html`:** Comparison summary table. Includes filter form, sortable headers, table body, and pagination controls.
*   **`comparison_details_page.html`:** Side-by-side chart comparison for a single security (Spread).
*   **`fund_detail_page.html`:** Displays multiple charts for different metrics for a single fund.
*   **`weight_check.html`:** Placeholder page for weight checks.
*   **`duration_comparison_page.html`:** Comparison summary table for Duration.
*   **`duration_comparison_details_page.html`:** Side-by-side chart comparison for a single security (Duration).
*   **`spread_duration_comparison_page.html`:** Comparison summary table for Spread Duration.
*   **`spread_duration_comparison_details_page.html`:** Side-by-side chart comparison for a single security (Spread Duration).
```
</file>

<file path="requirements.txt">
Flask
pandas
plotly
</file>

<file path="security_processing.py">
# This file handles the loading, processing, and analysis of security-level data.
# It assumes input CSV files are structured with one security per row and time series data
# spread across columns where headers represent dates (e.g., YYYY-MM-DD).
# Key functions:
# - `load_and_process_security_data`: Reads a wide-format CSV, identifies the security ID column,
#   static attribute columns, and date columns. It then 'melts' the data into a long format,
#   converting date strings to datetime objects and setting a MultiIndex (Date, Security ID).
# - `calculate_security_latest_metrics`: Takes the processed long-format DataFrame and calculates
#   various metrics for each security's 'Value' over time, including latest value, change,
#   historical stats (mean, max, min), and change Z-score. It also preserves the static attributes.
import pandas as pd
import os
import numpy as np
import re # For checking date-like column headers
import logging
import traceback
# --- Logging Setup ---
# Use the same log file as data_loader and metric_calculator
LOG_FILENAME = 'data_processing_errors.log'
LOG_FORMAT = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
# Get the logger for the current module
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
# Prevent adding handlers multiple times
if not logger.handlers:
    # Console Handler (INFO and above)
    ch = logging.StreamHandler()
    ch.setLevel(logging.INFO)
    ch_formatter = logging.Formatter(LOG_FORMAT)
    ch.setFormatter(ch_formatter)
    logger.addHandler(ch)
    # File Handler (WARNING and above)
    try:
        # Create log file path relative to this file's location
        log_filepath = os.path.join(os.path.dirname(__file__), '..', LOG_FILENAME)
        fh = logging.FileHandler(log_filepath, mode='a')
        fh.setLevel(logging.WARNING)
        fh_formatter = logging.Formatter(LOG_FORMAT)
        fh.setFormatter(fh_formatter)
        logger.addHandler(fh)
    except Exception as e:
        # Log to stderr if file logging setup fails
        import sys
        print(f"Error setting up file logging for security_processing: {e}", file=sys.stderr)
# --- End Logging Setup ---
DATA_FOLDER = 'Data'
def _is_date_like(column_name):
    """Check if a column name looks like a common date format.
    Recognizes formats like YYYY-MM-DD, YYYY/MM/DD, MM/DD/YYYY, M/D/YYYY, YYYYMMDD.
    """
    col_str = str(column_name)
    # Regex to match common date patterns
    # - YYYY[-/]MM[-/]DD
    # - MM[-/]DD[-/]YYYY (allows 1-2 digits for M, D and 2 or 4 for Y)
    # - YYYYMMDD
    pattern = r'^(\d{4}[-/]\d{1,2}[-/]\d{1,2}|\d{1,2}[-/]\d{1,2}[-/](\d{4}|\d{2})|\d{8})$'
    return bool(re.match(pattern, col_str))
def load_and_process_security_data(filename):
    """Loads security data, identifies static/date columns, and melts to long format.
    Args:
        filename (str): The name of the CSV file (e.g., 'sec_Spread.csv').
    Returns:
        tuple: (pandas.DataFrame, list[str])
               - Processed DataFrame in long format with MultiIndex (Date, Security ID).
               - List of identified static column names (excluding Security ID).
        Returns (pd.DataFrame(), []) if a critical error occurs during loading or processing.
    """
    filepath = os.path.join(DATA_FOLDER, filename)
    logger.info(f"Attempting to load security data from: {filepath}")
    try:
        # Read just the header to identify column types
        # Use on_bad_lines='skip' for robustness
        header_df = pd.read_csv(filepath, nrows=0, on_bad_lines='skip', encoding='utf-8', encoding_errors='replace')
        all_cols = [str(col).strip() for col in header_df.columns.tolist()] # Ensure string type and strip
        if not all_cols:
            logger.error(f"CSV file '{filename}' appears to be empty or header is missing.")
            raise ValueError(f"CSV file '{filename}' appears to be empty or header is missing.")
        # Assume first column is the Security ID
        id_col = all_cols[0]
        # Identify static and date columns dynamically using the updated _is_date_like
        static_cols = []
        date_cols = []
        for col in all_cols[1:]: # Skip the ID column
            if _is_date_like(col):
                date_cols.append(col)
            else:
                static_cols.append(col) # Already stripped
        if not date_cols:
            logger.error(f"No date-like columns found in '{filename}' using flexible patterns. Cannot process as security time series.")
            raise ValueError("No date-like columns found using flexible patterns.")
        if not id_col:
             # This case should technically not be reachable if all_cols is not empty
             logger.error(f"Could not identify the Security ID column (expected first column) in '{filename}'.")
             raise ValueError("Could not identify the Security ID column (expected first column).")
        logger.info(f"Identified ID Col: {id_col}")
        logger.info(f"Identified Static Cols: {static_cols}")
        # logger.info(f"Identified Date Cols: {date_cols[:5]}...") # Avoid excessive logging
        # Read the full data
        # Use on_bad_lines='skip' again for robustness
        df_wide = pd.read_csv(filepath, encoding='utf-8', on_bad_lines='skip', encoding_errors='replace')
        df_wide.columns = df_wide.columns.map(lambda x: str(x).strip()) # Ensure string type and strip
        # Ensure ID column name is also stripped if needed for melting (already done)
        # id_col = id_col.strip() 
        # Ensure static columns used for id_vars exist after stripping
        # The id_vars must match column names *exactly* after stripping
        id_vars = [id_col] + [col for col in static_cols if col in df_wide.columns]
        value_vars = [col for col in date_cols if col in df_wide.columns] # Ensure date columns exist
        if not value_vars:
             logger.error(f"Date columns identified in header of '{filename}' not found in data frame after loading. Columns available: {df_wide.columns.tolist()}")
             raise ValueError("Date columns identified in header not found in data frame after loading.")
        # Melt the DataFrame
        df_long = pd.melt(df_wide,
                          id_vars=id_vars,
                          value_vars=value_vars,
                          var_name='Date_Str',
                          value_name='Value')
        # Process Date and Value columns
        # Coerce errors: invalid date formats will become NaT
        df_long['Date'] = pd.to_datetime(df_long['Date_Str'], errors='coerce')
        # Check how many dates failed to parse after trying inference
        failed_date_parse_count = df_long['Date'].isna().sum()
        original_date_str_count = len(df_long['Date_Str'])
        if failed_date_parse_count > 0:
             logger.warning(f"Could not parse {failed_date_parse_count} out of {original_date_str_count} date strings in '{filename}' using pandas format inference.")
             # Keep only successfully parsed dates before proceeding
             df_long = df_long.dropna(subset=['Date']) 
             if df_long.empty:
                  logger.error(f"No dates could be parsed successfully in '{filename}'. Aborting processing for this file.")
                  return pd.DataFrame(), [] # Return empty as no valid date data
        # Coerce errors: non-numeric values will become NaN
        df_long['Value'] = pd.to_numeric(df_long['Value'], errors='coerce')
        # Drop rows where value is missing after conversion or ID is missing
        # Date NaNs were handled above
        initial_rows = len(df_long)
        df_long.dropna(subset=['Value', id_col], inplace=True)
        rows_dropped = initial_rows - len(df_long)
        if rows_dropped > 0:
             logger.warning(f"Dropped {rows_dropped} rows from '{filename}' due to missing Values or Security IDs after melting/conversion (excluding date parse failures handled separately).")
        if df_long.empty:
             logger.warning(f"DataFrame for '{filename}' is empty after melting, value conversion, and NaN drop. No data to process.")
             # Return empty df, list as per function spec
             return pd.DataFrame(), static_cols
        # Set MultiIndex
        # Use the identified (and potentially stripped) id_col name
        df_long.set_index(['Date', id_col], inplace=True)
        df_long.sort_index(inplace=True)
        # Drop the original string date column
        df_long.drop(columns=['Date_Str'], inplace=True)
        logger.info(f"Successfully loaded and processed '{filename}'. Shape: {df_long.shape}")
        # Return only the static columns that were actually found and used as id_vars (excluding the ID col itself)
        final_static_cols = [col for col in id_vars if col != id_col]
        return df_long, final_static_cols
    except FileNotFoundError:
        logger.error(f"Error: File not found at {filepath}")
        return pd.DataFrame(), [] # Return empty dataframe and list
    except ValueError as ve:
        logger.error(f"Error processing header or columns in {filename}: {ve}")
        return pd.DataFrame(), []
    except KeyError as ke:
        logger.error(f"Error melting DataFrame for {filename}, likely due to missing column used as id_var or value_var: {ke}")
        return pd.DataFrame(), []
    except Exception as e:
        logger.error(f"An unexpected error occurred loading/processing {filename}: {e}", exc_info=True)
        # traceback.print_exc() # Logger handles traceback now
        return pd.DataFrame(), []
def calculate_security_latest_metrics(df, static_cols):
    """Calculates latest metrics for each security based on its 'Value' column.
    Args:
        df (pd.DataFrame): Processed long-format DataFrame with MultiIndex (Date, Security ID).
                           Must contain a 'Value' column.
        static_cols (list[str]): List of static column names present in the DataFrame's columns (not index).
    Returns:
        pandas.DataFrame: DataFrame indexed by Security ID, including static columns and
                          calculated metrics (Latest Value, Change, Mean, Max, Min, Change Z-Score).
                          Returns an empty DataFrame if input is empty or processing fails.
    """
    if df is None or df.empty:
        logger.warning("Input DataFrame is None or empty. Cannot calculate security metrics.")
        return pd.DataFrame()
    if 'Value' not in df.columns:
        logger.error("Input DataFrame for security metrics calculation must contain a 'Value' column.")
        return pd.DataFrame()
    # Ensure index has two levels and get their names dynamically
    if df.index.nlevels != 2:
        logger.error("Input DataFrame for security metrics must have 2 index levels (Date, Security ID).")
        return pd.DataFrame()
    date_level_name, id_level_name = df.index.names
    try:
        latest_date = df.index.get_level_values(date_level_name).max()
        security_ids = df.index.get_level_values(id_level_name).unique()
        all_metrics_list = []
        for sec_id in security_ids:
            try:
                # Extract data for the current security ID
                # Use .loc for potentially cleaner selection and ensure sorting
                sec_data_hist = df.loc[(slice(None), sec_id), :].reset_index(level=id_level_name, drop=True).sort_index()
                if sec_data_hist.empty:
                     logger.debug(f"No data found for security '{sec_id}' after extraction. Skipping.")
                     continue
                sec_metrics = {} # Dictionary to hold metrics for this security
                # Add static columns first
                # Take the first available row's values, assuming they are constant per security
                # Need to handle potential multi-index if static_cols contains index names by mistake
                valid_static_cols = [col for col in static_cols if col in sec_data_hist.columns]
                if not sec_data_hist.empty:
                    static_data_row = sec_data_hist.iloc[0]
                    for static_col in valid_static_cols:
                        sec_metrics[static_col] = static_data_row.get(static_col, np.nan)
                else: # Should not happen due to check above, but safeguard
                    for static_col in valid_static_cols:
                         sec_metrics[static_col] = np.nan 
                # Ensure all expected static cols are present in the dict, even if missing from data
                for static_col in static_cols:
                     if static_col not in sec_metrics:
                          logger.warning(f"Static column '{static_col}' not found in data for security '{sec_id}', adding as NaN.")
                          sec_metrics[static_col] = np.nan
                # Calculate metrics for the 'Value' column
                value_hist = sec_data_hist['Value']
                # Calculate diff only if series has enough data
                value_change_hist = pd.Series(index=value_hist.index, dtype=np.float64)
                if not value_hist.dropna().empty and len(value_hist.dropna()) > 1:
                    value_change_hist = value_hist.diff()
                else:
                    logger.debug(f"Cannot calculate difference for 'Value' column, security '{sec_id}' due to insufficient data.")
                # Base historical stats (level) - handle potential all-NaN series
                sec_metrics['Mean'] = value_hist.mean() if value_hist.notna().any() else np.nan
                sec_metrics['Max'] = value_hist.max() if value_hist.notna().any() else np.nan
                sec_metrics['Min'] = value_hist.min() if value_hist.notna().any() else np.nan
                # Stats for change
                change_mean = value_change_hist.mean() if value_change_hist.notna().any() else np.nan
                change_std = value_change_hist.std() if value_change_hist.notna().any() else np.nan
                # Latest values
                # Check if latest_date exists in this security's specific history
                if latest_date in sec_data_hist.index:
                    latest_value = sec_data_hist.loc[latest_date, 'Value']
                    latest_change = value_change_hist.get(latest_date, np.nan)
                    sec_metrics['Latest Value'] = latest_value
                    sec_metrics['Change'] = latest_change
                    # Calculate Change Z-Score
                    change_z_score = np.nan
                    if pd.notna(latest_change) and pd.notna(change_mean) and pd.notna(change_std) and change_std != 0:
                        change_z_score = (latest_change - change_mean) / change_std
                    elif change_std == 0 and pd.notna(latest_change) and pd.notna(change_mean):
                         # Handle zero standard deviation
                         if latest_change == change_mean:
                              change_z_score = 0.0
                         else:
                             change_z_score = np.inf if latest_change > change_mean else -np.inf
                         logger.debug(f"Std dev of change for security '{sec_id}' is zero. Z-score set to {change_z_score}.")
                    else:
                         # Log if Z-score calculation failed due to NaNs
                        if not (pd.notna(latest_change) and pd.notna(change_mean) and pd.notna(change_std)):
                             logger.debug(f"Cannot calculate Z-score for security '{sec_id}' due to NaN inputs (latest_change={latest_change}, change_mean={change_mean}, change_std={change_std})")
                    sec_metrics['Change Z-Score'] = change_z_score
                else:
                    # Security missing the overall latest date
                    logger.debug(f"Security '{sec_id}' missing data for latest date {latest_date}. Setting latest metrics to NaN.")
                    sec_metrics['Latest Value'] = np.nan
                    sec_metrics['Change'] = np.nan
                    sec_metrics['Change Z-Score'] = np.nan
                # Add the security ID itself for setting the index later
                sec_metrics[id_level_name] = sec_id 
                all_metrics_list.append(sec_metrics)
            except Exception as inner_e:
                logger.error(f"Error calculating metrics for security '{sec_id}': {inner_e}", exc_info=True)
                # Optionally add a placeholder row with NaNs? Or just skip. Let's skip.
                continue
        if not all_metrics_list:
            logger.warning("No security metrics were successfully calculated. Returning empty DataFrame.")
            return pd.DataFrame()
        # Create DataFrame and set index
        latest_metrics_df = pd.DataFrame(all_metrics_list)
        # id_col_name = df.index.names[1] # Get the actual ID column name used
        if id_level_name in latest_metrics_df.columns:
             latest_metrics_df.set_index(id_level_name, inplace=True)
        else:
             logger.error(f"Security ID column '{id_level_name}' not found in the created metrics list for setting index. Columns: {latest_metrics_df.columns.tolist()}")
             # Fallback or error? Let's return as is for now, index might be RangeIndex.
        # Reorder columns to have static columns first, then calculated metrics
        metric_cols = ['Latest Value', 'Change', 'Mean', 'Max', 'Min', 'Change Z-Score']
        # Get static cols that are actually present in the final df columns (excluding the ID index)
        present_static_cols = [col for col in static_cols if col in latest_metrics_df.columns]
        final_col_order = present_static_cols + [m_col for m_col in metric_cols if m_col in latest_metrics_df.columns]
        try:
            latest_metrics_df = latest_metrics_df[final_col_order]
        except KeyError as ke:
            logger.error(f"Error reordering columns, likely a metric column is missing: {ke}. Columns available: {latest_metrics_df.columns.tolist()}")
            # Proceed with potentially incorrect order
        # Sorting (e.g., by Z-score) should be done in the view function where it's displayed
        logger.info(f"Successfully calculated metrics for {len(latest_metrics_df)} securities.")
        return latest_metrics_df
    except Exception as e:
        logger.error(f"An unexpected error occurred during security metric calculation: {e}", exc_info=True)
        # traceback.print_exc() # Logger handles traceback
        return pd.DataFrame()
</file>

<file path="static/css/style.css">
/* Basic styling for sortable table headers */
th.sortable {
    cursor: pointer;
    position: relative; /* Needed for absolute positioning of indicator */
}
/* Hide default indicator span content */
.sort-indicator {
    display: inline-block;
    width: 1em;
    height: 1em;
    margin-left: 5px;
    vertical-align: middle;
    content: "";
}
/* Style for ascending sort indicator */
th.sortable.sort-asc .sort-indicator::before {
    content: "\25B2"; /* Up arrow ▲ */
    font-size: 0.8em;
}
/* Style for descending sort indicator */
th.sortable.sort-desc .sort-indicator::before {
    content: "\25BC"; /* Down arrow ▼ */
    font-size: 0.8em;
}
/* Hover effect for sortable headers */
th.sortable:hover {
    background-color: #e9ecef; /* Light grey background on hover */
}
</file>

<file path="static/js/main.js">
// This file acts as the main entry point for the application's JavaScript.
// It runs after the DOM is fully loaded and performs several key initializations:
// 1. Imports necessary functions from UI modules (chart rendering, table filtering).
// 2. Checks for the presence of specific elements on the page to determine the context
//    (e.g., metric details page, securities list page, single security detail page).
// 3. If on a metric details page (`metric_page_js.html`):
//    - Finds the embedded JSON data (`<script id="chartData">`).
//    - Parses the JSON data containing historical values and calculated metrics for all funds.
//    - Calls `renderChartsAndTables` from `chartRenderer.js` to dynamically create
//      the metric tables and time-series charts for each fund code.
// 4. If on a securities list page (`securities_page.html`):
//    - Finds the main securities table (`<table id="securities-table">`).
//    - Calls `initSecurityTableFilter` from `securityTableFilter.js` to add
//      interactive filtering capabilities to the table header.
// 5. If on a single security detail page (`security_details_page.html`):
//    - Finds the chart canvas (`<canvas id="securityChart">`) and its associated JSON data (`<script id="chartJsonData">`).
//    - Parses the JSON data containing the time-series for that specific security.
//    - Calls `renderSingleSecurityChart` from `chartRenderer.js` to display the chart.
// This modular approach ensures that initialization code only runs when the corresponding HTML elements are present.
// static/js/main.js
// Purpose: Main entry point for client-side JavaScript. Initializes modules based on page content.
import { renderChartsAndTables, renderSingleSecurityChart, renderFundCharts } from './modules/ui/chartRenderer.js';
import { initSecurityTableFilter } from './modules/ui/securityTableFilter.js';
import { initTableSorter } from './modules/ui/tableSorter.js';
document.addEventListener('DOMContentLoaded', () => {
    console.log("DOM fully loaded and parsed");
    // --- Metric Page (Multiple Charts per Metric) ---    
    const metricChartDataElement = document.getElementById('chartData');
    const metricChartsArea = document.getElementById('chartsArea');
    if (metricChartDataElement && metricChartsArea) {
        console.log("Metric page detected. Initializing charts.");
        try {
            const chartDataJson = metricChartDataElement.textContent;
            console.log("Raw JSON string from script tag:", chartDataJson);
            const fullChartData = JSON.parse(chartDataJson);
            console.log('Parsed fullChartData object:', fullChartData);
            console.log('Checking fullChartData.metadata:', fullChartData ? fullChartData.metadata : 'fullChartData is null/undefined');
            console.log('Checking fullChartData.funds:', fullChartData ? fullChartData.funds : 'fullChartData is null/undefined');
            if (fullChartData && fullChartData.metadata && fullChartData.funds && Object.keys(fullChartData.funds).length > 0) {
                console.log("Conditional check passed. Calling renderChartsAndTables...");
                renderChartsAndTables(
                    metricChartsArea,
                    fullChartData
                );
            } else {
                console.error('Parsed metric chart data is missing expected structure or funds are empty:', fullChartData);
                metricChartsArea.innerHTML = '<div class="alert alert-danger">Error: Invalid data structure or no fund data.</div>';
            }
        } catch (e) {
            console.error('Error processing metric chart data:', e);
            metricChartsArea.innerHTML = '<div class="alert alert-danger">Error loading chart data. Check console.</div>';
        }
    }
    // --- Fund Detail Page (Multiple Charts per Fund) ---    
    const fundChartDataElement = document.getElementById('fundChartData');
    const fundChartsArea = document.getElementById('fundChartsArea');
    if (fundChartDataElement && fundChartsArea) {
        console.log("Fund detail page detected. Initializing charts.");
        try {
            const fundChartDataJson = fundChartDataElement.textContent;
            const allChartData = JSON.parse(fundChartDataJson);
            console.log('Parsed fund chart data:', JSON.parse(JSON.stringify(allChartData)));
            if (Array.isArray(allChartData) && allChartData.length > 0) {
                renderFundCharts(fundChartsArea, allChartData);
            } else if (Array.isArray(allChartData) && allChartData.length === 0) {
                console.log("No chart data provided for this fund.");
                // Message is already shown by the template
            } else {
                 console.error('Parsed fund chart data is not an array or is invalid:', allChartData);
                fundChartsArea.innerHTML = '<div class="alert alert-danger">Error: Invalid chart data received.</div>';
            }
        } catch (e) {
            console.error('Error processing fund chart data:', e);
            fundChartsArea.innerHTML = '<div class="alert alert-danger">Error loading fund charts. Check console.</div>';
        }
    }
    // --- Securities Summary Page (Filterable & Sortable Table) ---
    const securitiesTable = document.getElementById('securities-table');
    if (securitiesTable) {
        console.log("Securities page table detected. Initializing client-side sorter (filtering is server-side).");
        // initSecurityTableFilter('securities-table'); // REMOVED: Filtering is now server-side
        initTableSorter('securities-table'); // Keep client-side sorting for instant feedback after load
    } else {
        // console.log("Securities table not found, skipping table features initialization.");
    }
    // --- Comparison Summary Page (Filterable & Sortable Table) ---
    const comparisonTable = document.getElementById('comparison-table');
    if (comparisonTable) {
        console.log("Comparison page table detected. Initializing sorter.");
        // Note: Filters are handled server-side via form submission for this table
        initTableSorter('comparison-table'); // Enable client-side sorting
    }
    // --- Security Details Page (Single Chart) ---
    const securityChartCanvas = document.getElementById('primarySecurityChart');
    const securityJsonDataElement = document.getElementById('chartJsonData');
    if (securityChartCanvas && securityJsonDataElement) {
        console.log("Security details page detected. Initializing single chart.");
        try {
            const securityChartData = JSON.parse(securityJsonDataElement.textContent);
            if (securityChartData && securityChartData.primary && securityChartData.primary.labels && securityChartData.primary.datasets) {
                renderSingleSecurityChart(
                    securityChartCanvas.id,
                    securityChartData.primary.labels,
                    securityChartData.primary.datasets,
                    securityChartData.security_id + ' - ' + securityChartData.metric_name
                );
                const durationChartCanvas = document.getElementById('durationSecurityChart');
                if(durationChartCanvas && securityChartData.duration && securityChartData.duration.labels && securityChartData.duration.datasets) {
                    renderSingleSecurityChart(
                        durationChartCanvas.id,
                        securityChartData.duration.labels,
                        securityChartData.duration.datasets,
                        securityChartData.security_id + ' - Duration'
                    );
                }
            } else {
                console.warn('Security chart JSON data is incomplete or invalid.', securityChartData);
            }
        } catch (error) {
            console.error('Error parsing security chart data or rendering chart:', error);
        }
    } else {
       // console.log("Security chart canvas or JSON data element not found, skipping single chart rendering.");
    }
    // Add any other global initializations here
});
</file>

<file path="static/js/modules/charts/timeSeriesChart.js">
// This file contains the specific logic for creating and configuring time-series line charts
// using the Chart.js library. It's designed to be reusable for generating consistent charts
// across different metrics and funds.
// static/js/modules/charts/timeSeriesChart.js
// Encapsulates Chart.js configuration and rendering for multiple time series datasets
/**
 * Creates and renders a time series chart using Chart.js.
 * @param {string} canvasId - The ID of the canvas element.
 * @param {object} chartData - Data object containing labels, multiple datasets, metrics.
 * @param {string} metricName - Name of the overall metric (e.g., Duration).
 * @param {string} fundCode - Code of the specific fund.
 * @param {number | null} maxZScore - The maximum absolute Z-score for this fund (used in title).
 * @param {boolean} isMissingLatest - Flag indicating if the latest point is missing for any spread.
 */
export function createTimeSeriesChart(canvasId, chartData, metricName, fundCode, maxZScore, isMissingLatest) {
    const ctx = document.getElementById(canvasId).getContext('2d');
    if (!ctx) {
        console.error(`[createTimeSeriesChart] Failed to get 2D context for canvas ID: ${canvasId}`);
        return; // Exit if canvas context is not available
    }
    // --- Prepare Chart Title (Adjusted for different contexts) --- 
    let chartTitle = metricName; // Default to just the metric name
    if (fundCode) { // If fundCode is provided (called from metric page)
        let titleSuffix = maxZScore !== null ? `(Max Spread Z: ${maxZScore.toFixed(2)})` : '(Z-Score N/A)';
        if (isMissingLatest) {
            titleSuffix = "(MISSING LATEST DATA)";
        }
        chartTitle = `${metricName} for ${fundCode} ${titleSuffix}`; 
    }
    // If fundCode is null (called from fund page), title remains just metricName
    console.log(`[createTimeSeriesChart] Using chart title: "${chartTitle}" for canvas ${canvasId}`);
    // --- Prepare Chart Data & Styling --- 
    const datasets = chartData.datasets.map((ds, index) => {
        const isBenchmark = ds.label.includes('Benchmark'); // Basic check, refine if needed
        const isLastDataset = index === chartData.datasets.length - 1; // Check if it's the benchmark dataset based on order from app.py
        return {
            ...ds,
            // Style points - highlight last point for non-benchmark lines
            pointRadius: (context) => {
                const isLastPoint = context.dataIndex === (ds.data.length - 1);
                // Only show large radius for last point of non-benchmark datasets
                return isLastPoint && !isLastDataset ? 6 : 0;
            },
            pointHoverRadius: (context) => {
                const isLastPoint = context.dataIndex === (ds.data.length - 1);
                return isLastPoint && !isLastDataset ? 8 : 5;
            },
            pointBackgroundColor: isLastDataset ? 'darkgrey' : ds.borderColor, // Use border color for fund points, grey for benchmark
            borderWidth: isLastDataset ? 2 : 1.5, // Slightly thicker benchmark line
        };
    });
    // --- Chart Configuration --- 
    const config = {
        type: 'line',
        data: {
            labels: chartData.labels, // Dates as strings
            datasets: datasets // Now includes multiple fund series + benchmark
        },
        options: {
            responsive: true,
            maintainAspectRatio: false, 
            plugins: {
                title: { display: true, text: chartTitle, font: { size: 16 } },
                legend: { position: 'top' },
                tooltip: { 
                    mode: 'index', 
                    intersect: false, 
                }
            },
            hover: { mode: 'nearest', intersect: true },
            scales: {
                x: {
                    type: 'time',
                    time: {
                        unit: 'day',
                        tooltipFormat: 'MMM dd, yyyy',
                        displayFormats: { day: 'MMM dd', week: 'MMM dd yyyy', month: 'MMM yyyy' }
                    },
                    title: { display: true, text: 'Date' }
                },
                y: {
                    display: true,
                    title: { display: true, text: metricName },
                    // Dynamic scaling based on *all* datasets
                    suggestedMin: Math.min(...datasets.flatMap(ds => ds.data.filter(d => d !== null && !isNaN(d)))),
                    suggestedMax: Math.max(...datasets.flatMap(ds => ds.data.filter(d => d !== null && !isNaN(d))))
                }
            }
        }
    };
    // --- Create Chart Instance (with Error Handling) --- 
    try {
        // Check if a chart instance already exists on the canvas and destroy it
        let existingChart = Chart.getChart(canvasId);
        if (existingChart) {
            console.log(`[createTimeSeriesChart] Destroying existing chart on canvas ${canvasId}`);
            existingChart.destroy();
        }
        // Attempt to create the new chart
        console.log(`[createTimeSeriesChart] Attempting to create new Chart on canvas ${canvasId}`);
        const chartInstance = new Chart(ctx, config); // Store the instance
        // Log success *after* instantiation
        console.log(`[createTimeSeriesChart] Successfully created chart for "${chartTitle}" on ${canvasId}`);
        return chartInstance; // Return the created chart instance
    } catch (error) {
        // Log any error during chart instantiation
        console.error(`[createTimeSeriesChart] Error creating chart on canvas ${canvasId} for "${chartTitle}":`, error);
        // Optionally display an error message in the canvas container
        const errorP = document.createElement('p');
        errorP.textContent = `Error rendering chart: ${error.message}`;
        errorP.className = 'text-danger';
        // Attempt to add error message to parent, replacing canvas if needed
        const canvasElement = document.getElementById(canvasId);
        if (canvasElement && canvasElement.parentElement) {
            canvasElement.parentElement.appendChild(errorP);
            canvasElement.style.display = 'none'; // Hide broken canvas
        }    
    }
}
</file>

<file path="static/js/modules/ui/chartRenderer.js">
// This file is responsible for dynamically creating and rendering the user interface elements
// related to charts and associated metric tables within the application.
// It separates the logic for generating the visual components from the main application flow.
// Updated to handle optional secondary data sources and toggle visibility.
// static/js/modules/ui/chartRenderer.js
// Handles creating DOM elements for charts and tables
import { createTimeSeriesChart } from '../charts/timeSeriesChart.js';
import { formatNumber } from '../utils/helpers.js';
// Store chart instances to manage them later (e.g., for toggling)
const chartInstances = {};
/**
 * Renders charts and metric tables into the specified container.
 * Handles primary and optional secondary (S&P) data source toggle.
 * @param {HTMLElement} container - The parent element to render into.
 * @param {object} payload - The full data payload object from Flask (contains metadata and funds data).
 */
export function renderChartsAndTables(container, payload) {
    const metadata = payload.metadata;
    const chartsData = payload.funds;
    const metricName = metadata.metric_name;
    const latestDate = metadata.latest_date;
    const fundColNames = metadata.fund_col_names;
    const benchmarkColName = metadata.benchmark_col_name;
    const secondaryDataAvailable = metadata.secondary_data_available;
    const secondaryFundColNames = metadata.secondary_fund_col_names;
    const secondaryBenchmarkColName = metadata.secondary_benchmark_col_name;
    console.log("[chartRenderer] Rendering charts for metric:", metricName, "Latest Date:", latestDate);
    console.log("[chartRenderer] Metadata:", metadata);
    console.log("[chartRenderer] Fund Data Keys:", Object.keys(chartsData || {}));
    // Clear previous content and chart instances
    container.innerHTML = '';
    Object.keys(chartInstances).forEach(key => delete chartInstances[key]);
    if (!chartsData || Object.keys(chartsData).length === 0) {
        console.warn("[chartRenderer] No fund data available for metric:", metricName);
        container.innerHTML = '<p>No fund data available for this metric.</p>';
        return;
    }
    // --- Setup Toggle Switch --- 
    const toggleContainer = document.getElementById('sp-toggle-container');
    const toggleSwitch = document.getElementById('toggleSpData');
    if (toggleContainer && toggleSwitch) {
        if (secondaryDataAvailable) {
            console.log("[chartRenderer] Secondary data available, showing toggle switch.");
            toggleContainer.style.display = 'block'; // Show the toggle
            // Remove previous listeners if any
            toggleSwitch.replaceWith(toggleSwitch.cloneNode(true));
            const newToggleSwitch = document.getElementById('toggleSpData'); // Get the new cloned element
            // Add new listener
            newToggleSwitch.addEventListener('change', (event) => {
                const showSecondary = event.target.checked;
                console.log(`[chartRenderer] Toggle changed. Show Secondary: ${showSecondary}`);
                toggleSecondaryDataVisibility(showSecondary);
            });
        } else {
            console.log("[chartRenderer] Secondary data not available, hiding toggle switch.");
            toggleContainer.style.display = 'none'; // Ensure toggle is hidden
        }
    } else {
        console.warn("[chartRenderer] Toggle switch container or input not found in the DOM.");
    }
    // --- Render Chart and Table for Each Fund --- 
    for (const [fundCode, data] of Object.entries(chartsData)) {
        console.log(`[chartRenderer] Processing fund: ${fundCode}`);
        const metrics = data.metrics; // Flattened metrics for this fund
        const isMissingLatest = data.is_missing_latest;
        // Find max absolute Z-score from PRIMARY metrics for section highlight
        let maxAbsPrimaryZScore = 0;
        let primaryZScoreForTitle = null;
        if (metrics) {
            const primaryColsToCheck = [];
            if (benchmarkColName) primaryColsToCheck.push(benchmarkColName);
            if (fundColNames && Array.isArray(fundColNames)) primaryColsToCheck.push(...fundColNames);
            primaryColsToCheck.forEach(colName => {
                if (!colName) return;
                // Look for the non-prefixed Z-score key
                const zScoreKey = `${colName} Change Z-Score`; 
                const zScore = metrics[zScoreKey];
                 if (zScore !== null && typeof zScore !== 'undefined' && !isNaN(zScore)) {
                     const absZ = Math.abs(zScore);
                     if (absZ > maxAbsPrimaryZScore) {
                         maxAbsPrimaryZScore = absZ;
                         primaryZScoreForTitle = zScore; 
                     }
                 }
            });
        }
        // Determine CSS class for the wrapper based on primary Z-score
        let zClass = '';
        if (maxAbsPrimaryZScore > 3) { zClass = 'very-high-z'; }
        else if (maxAbsPrimaryZScore > 2) { zClass = 'high-z'; }
        // --- Create DOM Elements --- 
        const wrapper = document.createElement('div');
        wrapper.className = `chart-container-wrapper ${zClass}`;
        wrapper.id = `chart-wrapper-${fundCode}`;
        // Add Duration Details Link (if applicable)
        if (metricName === 'Duration') {
            const linkDiv = document.createElement('div');
            linkDiv.className = 'mb-2 text-end';
            const link = document.createElement('a');
            link.href = `/fund/duration_details/${fundCode}`;
            link.className = 'btn btn-info btn-sm';
            link.textContent = `View Security Duration Changes for ${fundCode} →`;
            linkDiv.appendChild(link);
            wrapper.appendChild(linkDiv);
        }
        // Create Chart Canvas
        const canvas = document.createElement('canvas');
        canvas.id = `chart-${fundCode}`;
        canvas.className = 'chart-canvas';
        wrapper.appendChild(canvas);
        // Create Metrics Table (handles primary and secondary columns)
        const table = createMetricsTable(
            metrics,
            latestDate,
            fundColNames,
            benchmarkColName,
            secondaryDataAvailable,
            secondaryFundColNames,
            secondaryBenchmarkColName,
            "S&P " // Prefix for secondary
        );
        wrapper.appendChild(table);
        container.appendChild(wrapper);
        // --- Render Chart --- 
        setTimeout(() => {
            const chartCanvas = document.getElementById(canvas.id);
             if (chartCanvas && chartCanvas.getContext('2d')) {
                 console.log(`[chartRenderer] Rendering chart for ${fundCode}`);
                 // Pass the specific Z-score corresponding to the max absolute primary Z
                 const chart = createTimeSeriesChart(canvas.id, data, metricName, fundCode, primaryZScoreForTitle, isMissingLatest);
                 if (chart) {
                     chartInstances[fundCode] = chart; // Store chart instance
                     console.log(`[chartRenderer] Stored chart instance for ${fundCode}`);
                 } else {
                      console.error(`[chartRenderer] Failed to create chart instance for ${fundCode}`);
                 }
            } else {
                console.error(`[chartRenderer] Could not get 2D context for canvas ${canvas.id}`);
                const errorP = document.createElement('p');
                errorP.textContent = 'Error rendering chart.';
                errorP.className = 'text-danger';
                if (chartCanvas && chartCanvas.parentNode) {
                    chartCanvas.parentNode.replaceChild(errorP, chartCanvas);
                } else if (wrapper) {
                    wrapper.appendChild(errorP);
                }
            }
        }, 0); 
    }
    console.log("[chartRenderer] Finished processing all funds.");
}
/**
 * Updates the visibility of secondary data datasets across all rendered charts.
 * @param {boolean} show - Whether to show or hide the secondary datasets.
 */
function toggleSecondaryDataVisibility(show) {
    console.log(`[chartRenderer] Toggling secondary data visibility to: ${show}`);
    Object.values(chartInstances).forEach(chart => {
        chart.data.datasets.forEach((dataset, index) => {
            if (dataset.source === 'secondary') {
                // Use setDatasetVisibility for better control than just 'hidden' property
                chart.setDatasetVisibility(index, show);
                console.log(`[chartRenderer] Chart ${chart.canvas.id} - Setting dataset ${index} ('${dataset.label}') visibility to ${show}`);
            }
        });
        chart.update(); // Update the chart to reflect visibility changes
        console.log(`[chartRenderer] Updated chart ${chart.canvas.id}`);
    });
}
/**
 * Creates the HTML table element displaying metrics.
 * Includes primary columns and optionally secondary (prefixed) columns.
 *
 * @param {object | null} metrics - Flattened metrics object from Flask.
 * @param {string} latestDate - The latest date string.
 * @param {string[]} fundColNames - List of primary fund value column names.
 * @param {string | null} benchmarkColName - Primary benchmark column name.
 * @param {boolean} secondaryAvailable - Flag indicating if secondary data exists.
 * @param {string[] | null} secondaryFundColNames - List of secondary fund value column names.
 * @param {string | null} secondaryBenchmarkColName - Secondary benchmark column name.
 * @param {string} secondaryPrefix - Prefix for secondary metric keys (e.g., "S&P ").
 * @returns {HTMLTableElement} The created table element.
 */
function createMetricsTable(
    metrics, 
    latestDate, 
    fundColNames, 
    benchmarkColName, 
    secondaryAvailable, 
    secondaryFundColNames, 
    secondaryBenchmarkColName, 
    secondaryPrefix
) {
    const table = document.createElement('table');
    table.className = 'table table-sm table-bordered metrics-table'; // Apply base classes
    const thead = table.createTHead();
    const headerRow = thead.insertRow();
    // Define headers dynamically based on secondary data availability
    headerRow.innerHTML = `
        <th>Column</th>
        <th>Latest Value (${latestDate})</th>
        <th>Change</th>
        <th>Mean</th> 
        <th>Max</th> 
        <th>Min</th> 
        <th>Change Z-Score</th>
        ${secondaryAvailable ? `
        <th class="text-muted">S&P Latest</th>
        <th class="text-muted">S&P Change</th>
        <th class="text-muted">S&P Mean</th>
        <th class="text-muted">S&P Max</th>
        <th class="text-muted">S&P Min</th>
        <th class="text-muted">S&P Z-Score</th>
        ` : ''} 
    `;
    const tbody = table.createTBody();
    if (!metrics) {
        console.warn("[createMetricsTable] Metrics object is null.");
        const row = tbody.insertRow();
        const cell = row.insertCell();
        cell.colSpan = secondaryAvailable ? 13 : 7; // Adjust colspan based on headers
        cell.textContent = 'Metrics not available.';
        return table;
    }
    // Combine primary columns for iteration
    const allPrimaryColumns = [];
    if (benchmarkColName) allPrimaryColumns.push(benchmarkColName);
    if (fundColNames && Array.isArray(fundColNames)) allPrimaryColumns.push(...fundColNames);
    // Create one row per primary column
    allPrimaryColumns.forEach(colName => {
        if (!colName) return;
        const row = tbody.insertRow();
        // --- Primary Metrics --- 
        const pri_latestValKey = `${colName} Latest Value`;
        const pri_changeKey = `${colName} Change`;
        const pri_meanKey = `${colName} Mean`;
        const pri_maxKey = `${colName} Max`;
        const pri_minKey = `${colName} Min`;
        const pri_zScoreKey = `${colName} Change Z-Score`;
        const pri_zScoreValue = metrics[pri_zScoreKey];
        let pri_zScoreClass = ''; // Default class
        if (pri_zScoreValue !== null && typeof pri_zScoreValue !== 'undefined' && !isNaN(pri_zScoreValue)) {
             const absZ = Math.abs(pri_zScoreValue);
             if (absZ > 3) { pri_zScoreClass = 'very-high-z'; }
             else if (absZ > 2) { pri_zScoreClass = 'high-z'; }
        }
        // Populate primary cells
        row.innerHTML = `
            <td>${colName}</td>
            <td>${formatNumber(metrics[pri_latestValKey])}</td>
            <td>${formatNumber(metrics[pri_changeKey])}</td>
            <td>${formatNumber(metrics[pri_meanKey])}</td>
            <td>${formatNumber(metrics[pri_maxKey])}</td>
            <td>${formatNumber(metrics[pri_minKey])}</td>
            <td class="${pri_zScoreClass}">${formatNumber(pri_zScoreValue)}</td>
        `;
        // --- Secondary Metrics (if available) --- 
        if (secondaryAvailable) {
            // Try to find the corresponding secondary column name (simple exact match)
            let secColName = null;
            if (benchmarkColName === colName && secondaryBenchmarkColName) {
                secColName = secondaryBenchmarkColName;
            } else if (secondaryFundColNames && secondaryFundColNames.includes(colName)) {
                secColName = colName;
            }
            let secCellsHTML = '<td colspan="6" class="text-muted text-center">(N/A)</td>'; // Default placeholder
            // Check if a corresponding secondary column exists and has metrics
            if (secColName && `${secondaryPrefix}${secColName} Latest Value` in metrics) {
                const sec_latestValKey = `${secondaryPrefix}${secColName} Latest Value`;
                const sec_changeKey = `${secondaryPrefix}${secColName} Change`;
                const sec_meanKey = `${secondaryPrefix}${secColName} Mean`;
                const sec_maxKey = `${secondaryPrefix}${secColName} Max`;
                const sec_minKey = `${secondaryPrefix}${secColName} Min`;
                const sec_zScoreKey = `${secondaryPrefix}${secColName} Change Z-Score`;
                const sec_zScoreValue = metrics[sec_zScoreKey];
                let sec_zScoreClass = 'text-muted'; // Default secondary class
                if (sec_zScoreValue !== null && typeof sec_zScoreValue !== 'undefined' && !isNaN(sec_zScoreValue)) {
                    const absZ = Math.abs(sec_zScoreValue);
                    // Apply similar Z-score highlighting, but keep text muted unless significant
                    if (absZ > 3) { sec_zScoreClass = 'very-high-z text-muted'; } 
                    else if (absZ > 2) { sec_zScoreClass = 'high-z text-muted'; } 
                }
                secCellsHTML = `
                    <td class="text-muted">${formatNumber(metrics[sec_latestValKey])}</td>
                    <td class="text-muted">${formatNumber(metrics[sec_changeKey])}</td>
                    <td class="text-muted">${formatNumber(metrics[sec_meanKey])}</td>
                    <td class="text-muted">${formatNumber(metrics[sec_maxKey])}</td>
                    <td class="text-muted">${formatNumber(metrics[sec_minKey])}</td>
                    <td class="${sec_zScoreClass}">${formatNumber(sec_zScoreValue)}</td>
                `;
            } else if (secColName) {
                // If secColName was found but metrics weren't in the object (e.g., secondary processing failed for this col)
                secCellsHTML = '<td colspan="6" class="text-muted text-center">(Metrics missing)</td>';
            }
            // Append the secondary cells (either data or placeholder)
            row.innerHTML += secCellsHTML;
        }
    });
    console.log("[createMetricsTable] Table created.");
    return table;
} 
/**
 * Renders a single time series chart for a specific security.
 * @param {string} canvasId - The ID of the canvas element.
 * @param {object} chartData - The chart data (labels, datasets) from Flask.
 * @param {string} securityId - The ID of the security.
 * @param {string} metricName - The name of the metric.
 */
export function renderSingleSecurityChart(canvasId, chartData, securityId, metricName) {
    const ctx = document.getElementById(canvasId);
    if (!ctx) {
        console.error(`Canvas element with ID '${canvasId}' not found.`);
        return;
    }
    if (!chartData || !chartData.labels || !chartData.datasets) {
        console.error('Invalid or incomplete chart data provided.');
        ctx.parentElement.innerHTML = '<p class="text-danger">Error: Invalid chart data.</p>';
        return;
    }
    try {
        new Chart(ctx, {
            type: 'line',
            data: {
                labels: chartData.labels,
                datasets: chartData.datasets
            },
            options: {
                responsive: true,
                maintainAspectRatio: false,
                plugins: {
                    title: {
                        display: true,
                        text: `${securityId} - ${metricName} Time Series`,
                        font: { size: 16 }
                    },
                    legend: {
                        position: 'top',
                    }
                },
                scales: {
                    x: {
                        title: {
                            display: true,
                            text: 'Date'
                        }
                    },
                    y: {
                        type: 'linear',
                        display: true,
                        position: 'left',
                        title: {
                            display: true,
                            text: `${metricName} Value`
                        },
                        beginAtZero: false,
                        ticks: {
                            maxTicksLimit: 8
                        }
                    },
                    y1: {
                        type: 'linear',
                        display: true,
                        position: 'right',
                        title: {
                            display: true,
                            text: 'Price'
                        },
                        grid: {
                            drawOnChartArea: false,
                        },
                        beginAtZero: false
                    }
                },
                interaction: {
                    intersect: false,
                    mode: 'index',
                },
            }
        });
        console.log(`Chart rendered for ${securityId} - ${metricName}`);
    } catch (error) {
        console.error(`Error creating chart for ${securityId} - ${metricName}:`, error);
        ctx.parentElement.innerHTML = '<p class="text-danger">Error rendering chart.</p>';
    }
} 
/**
 * Renders multiple time series charts into the specified container for the fund detail page.
 * Iterates through metrics for a single fund.
 * @param {HTMLElement} container - The parent element to render into (e.g., #fundChartsArea).
 * @param {Array<object>} allChartData - An array where each object contains data for one metric's chart.
 *                                       Expected structure: [{ metricName: '...', labels: [...], datasets: [...] }, ...]
 */
export function renderFundCharts(container, allChartData) {
    console.log("[chartRenderer] Rendering charts for fund detail page.");
    console.log("[chartRenderer] Received Data:", JSON.parse(JSON.stringify(allChartData))); // Deep copy for logging
    container.innerHTML = ''; // Clear previous content
    if (!allChartData || !Array.isArray(allChartData) || allChartData.length === 0) {
        console.warn("[chartRenderer] No chart data provided for the fund page.");
        // Message should be handled by the template, but log it here.
        return;
    }
    // Iterate through each metric's chart data
    allChartData.forEach((metricData, index) => {
        if (!metricData || !metricData.metricName || !metricData.labels || !metricData.datasets) {
            console.warn(`[chartRenderer] Skipping chart at index ${index} due to missing data:`, metricData);
            return;
        }
        const metricName = metricData.metricName;
        const safeMetricName = metricName.replace(/[^a-zA-Z0-9]/g, '-') || 'metric'; // Create a CSS-safe ID part
        console.log(`[chartRenderer] Processing metric: ${metricName}`);
        // Create wrapper div for each chart (using Bootstrap columns for layout)
        const wrapper = document.createElement('div');
        // Uses the col classes defined in the template's fundChartsArea (row-cols-1 row-cols-lg-2)
        wrapper.className = `chart-container-wrapper fund-chart-item`; 
        wrapper.id = `fund-chart-wrapper-${safeMetricName}-${index}`;
        // Create Chart Canvas
        const canvas = document.createElement('canvas');
        // Ensure unique ID for each canvas
        canvas.id = `fund-chart-${safeMetricName}-${index}`; 
        canvas.className = 'chart-canvas';
        wrapper.appendChild(canvas);
        console.log(`[chartRenderer] Created canvas with id: ${canvas.id} for metric: ${metricName}`);
        // Append the wrapper to the main container
        container.appendChild(wrapper);
        console.log(`[chartRenderer] Appended wrapper for ${metricName} to container.`);
        // Render Chart using the existing time series chart function
        // Use setTimeout to ensure the canvas is in the DOM and sized
        setTimeout(() => {
            console.log(`[chartRenderer] Preparing to render chart for metric: ${metricName} in setTimeout.`);
             if (canvas.getContext('2d')) {
                 console.log(`[chartRenderer] Canvas context obtained. Calling createTimeSeriesChart with:`, {
                    canvasId: canvas.id,
                    data: JSON.parse(JSON.stringify(metricData)), // Log deep copy
                    titlePrefix: metricName, // Use metric name as the main title part
                    fundCodeOrSecurityId: null, // Not needed for title here
                    zScoreForTitle: null, // No specific Z-score for the whole page/chart
                    is_missing_latest: null // Not applicable here
                 });
                 createTimeSeriesChart(
                     canvas.id,         // The unique canvas ID
                     metricData,        // Data object with labels and datasets
                     metricName,        // Title prefix (e.g., "Yield")
                     null,              // fundCodeOrSecurityId (not needed for title)
                     null,              // zScoreForTitle (not applicable)
                     null               // is_missing_latest (not applicable)
                 );
                 console.log(`[chartRenderer] createTimeSeriesChart call finished for metric: ${metricName}.`);
            } else {
                console.error(`[chartRenderer] Could not get 2D context for canvas ${canvas.id} (Metric: ${metricName})`);
                const errorP = document.createElement('p');
                errorP.textContent = `Error rendering chart for ${metricName}.`;
                errorP.className = 'text-danger';
                canvas.parentNode.replaceChild(errorP, canvas); // Replace canvas with error message
            }
        }, 0); 
    });
    console.log("[chartRenderer] Finished rendering all fund charts.");
}
</file>

<file path="static/js/modules/ui/securityTableFilter.js">
// This file implements client-side filtering for the HTML table displaying security-level metrics.
// It enhances the user experience by allowing interactive filtering based on the values
// in specific static columns (e.g., Sector, Rating) without requiring a page reload.
// static/js/modules/ui/securityTableFilter.js
// This module handles client-side filtering for the securities table.
/**
 * Initializes the filtering functionality for the securities table.
 */
export function initSecurityTableFilter() {
    const filterSelects = document.querySelectorAll('.security-filter-select');
    const tableBody = document.getElementById('securities-table-body');
    if (!tableBody || filterSelects.length === 0) {
        console.log("Security table body or filter selects not found. Filtering disabled.");
        return; // Exit if necessary elements aren't present
    }
    // Store all original rows. Use querySelectorAll for robustness.
    const originalRows = Array.from(tableBody.querySelectorAll('tr'));
    if (originalRows.length === 0) {
        console.log("No rows found in the table body.");
        return; // Exit if no data rows
    }
    // Function to get current filter values
    const getCurrentFilters = () => {
        const filters = {};
        filterSelects.forEach(select => {
            if (select.value) { // Only add if a filter is selected (not 'All')
                filters[select.dataset.column] = select.value;
            }
        });
        return filters;
    };
    // Function to perform filtering and update the table
    const applyFilters = () => {
        const currentFilters = getCurrentFilters();
        const filterKeys = Object.keys(currentFilters);
        // Clear current table body content efficiently
        tableBody.innerHTML = ''; 
        originalRows.forEach(row => {
            let matches = true;
            // Get all cells in the current row
            const cells = row.querySelectorAll('td');
            // Assuming the order of cells matches the order of `column_order` from Python
            // We need a way to map filter column names to cell indices
            // Let's get the header names to map column names to indices
            const headerCells = document.querySelectorAll('#securities-table th');
            const columnNameToIndexMap = {};
            headerCells.forEach((th, index) => {
                columnNameToIndexMap[th.textContent.trim()] = index;
            });
            for (const column of filterKeys) {
                const columnIndex = columnNameToIndexMap[column];
                if (columnIndex !== undefined) {
                    const cellValue = cells[columnIndex]?.textContent.trim(); // Use optional chaining
                    // Strict comparison - ensure types match if needed, or use == for type coercion
                    if (cellValue !== currentFilters[column]) {
                        matches = false;
                        break; // No need to check other filters for this row
                    }
                }
                 else {
                      console.warn(`Column "${column}" not found in table header for filtering.`);
                      // Decide how to handle: skip filter, always fail match? Let's skip filter for robustness.
                 }
            }
            if (matches) {
                // Append the row if it matches all active filters
                tableBody.appendChild(row.cloneNode(true)); // Append a clone to avoid issues
            }
        });
        // Display a message if no rows match
        if (tableBody.children.length === 0) {
             const noMatchRow = tableBody.insertRow();
             const cell = noMatchRow.insertCell();
             cell.colSpan = headerCells.length; // Span across all columns
             cell.textContent = 'No securities match the current filter criteria.';
             cell.style.textAlign = 'center';
             cell.style.fontStyle = 'italic';
        }
    };
    // Add event listeners to all filter dropdowns
    filterSelects.forEach(select => {
        select.addEventListener('change', applyFilters);
    });
    console.log("Security table filtering initialized.");
}
</file>

<file path="static/js/modules/ui/tableSorter.js">
// static/js/modules/ui/tableSorter.js
// Purpose: Handles client-side sorting for HTML tables.
/**
 * Initializes sorting functionality for a specified table.
 * @param {string} tableId The ID of the table element to make sortable.
 */
export function initTableSorter(tableId) {
    const table = document.getElementById(tableId);
    if (!table) {
        console.warn(`Table sorter: Table with ID '${tableId}' not found.`);
        return;
    }
    const headers = table.querySelectorAll('thead th.sortable');
    const tbody = table.querySelector('tbody');
    if (!tbody) {
        console.warn(`Table sorter: Table with ID '${tableId}' does not have a tbody.`);
        return;
    }
    headers.forEach(header => {
        header.addEventListener('click', () => {
            // Get column name from data attribute
            const columnName = header.dataset.columnName;
            const currentIsAscending = header.classList.contains('sort-asc');
            const direction = currentIsAscending ? -1 : 1; // -1 for desc, 1 for asc
            // Find the index of the clicked column
            const columnIndex = Array.from(header.parentNode.children).indexOf(header);
            // Remove sorting indicators from other columns
            headers.forEach(h => {
                if (h !== header) {
                  h.classList.remove('sort-asc', 'sort-desc');
                }
            });
            // Set sorting indicator for the current column
            header.classList.toggle('sort-asc', !currentIsAscending);
            header.classList.toggle('sort-desc', currentIsAscending);
            // Sort the rows, passing the column name
            sortRows(tbody, columnIndex, direction, columnName);
        });
    });
}
/**
 * Sorts the rows within a table body.
 * @param {HTMLElement} tbody The table body element containing the rows.
 * @param {number} columnIndex The index of the column to sort by.
 * @param {number} direction 1 for ascending, -1 for descending.
 * @param {string} columnName The name of the column being sorted.
 */
function sortRows(tbody, columnIndex, direction, columnName) {
    const rows = Array.from(tbody.querySelectorAll('tr'));
    // Get the correct comparison function, passing the column name
    const compareFunction = getCompareFunction(rows, columnIndex, columnName);
    // Sort the rows
    rows.sort((rowA, rowB) => {
        const cellA = rowA.children[columnIndex];
        const cellB = rowB.children[columnIndex];
        // Use data-value attribute primarily, fall back to textContent
        const valueA = cellA?.dataset.value ?? cellA?.textContent?.trim() ?? '';
        const valueB = cellB?.dataset.value ?? cellB?.textContent?.trim() ?? '';
        return compareFunction(valueA, valueB) * direction;
    });
    // Re-append sorted rows
    tbody.append(...rows); // More efficient way to re-append
}
/**
 * Determines the appropriate comparison function (numeric or text) based on column content.
 * @param {Array<HTMLElement>} rows Array of table row elements.
 * @param {number} columnIndex The index of the column to check.
 * @param {string} columnName The name of the column being sorted.
 * @returns {function(string, string): number} The comparison function.
 */
function getCompareFunction(rows, columnIndex, columnName) {
    // Check the first few rows (up to 5 data rows) to guess the data type
    let isNumeric = true;
    for (let i = 0; i < Math.min(rows.length, 5); i++) {
        const cell = rows[i].children[columnIndex];
        // Use data-value attribute primarily for checking type
        const value = cell?.dataset.value ?? cell?.textContent?.trim() ?? '';
        // Allow empty strings in numeric columns, but if we find something non-numeric (and not empty), switch to text sort
        if (value !== '' && isNaN(Number(value.replace(/,/g, '')))) {
            isNumeric = false;
            break;
        }
    }
    if (isNumeric) {
        // Check if it's the special column 'Change Z-Score'
        if (columnName === 'Change Z-Score') {
             // Use absolute value for comparison
            return (a, b) => {
                const numA = Math.abs(parseNumber(a));
                const numB = Math.abs(parseNumber(b));
                return numA - numB;
            };
        } else {
            // Standard numeric comparison for other numeric columns
            return (a, b) => {
                const numA = parseNumber(a);
                const numB = parseNumber(b);
                return numA - numB;
            };
        }
    } else {
        // Case-insensitive text comparison
        return (a, b) => a.toLowerCase().localeCompare(b.toLowerCase());
    }
}
/**
 * Helper to parse number, handling empty strings and NaN.
 * Returns -Infinity for values that cannot be parsed as numbers or are empty,
 * ensuring they sort consistently.
 * @param {string} val The string value to parse.
 * @returns {number}
 */
function parseNumber(val) {
    if (val === null || val === undefined || val.trim() === '') {
        return -Infinity; // Treat empty/null/undefined as very small
    }
    const num = Number(val.replace(/,/g, ''));
    // Treat non-numeric as very small. Math.abs(-Infinity) is Infinity, which might be desired
    // when sorting absolute values (non-numbers/empty go to the end when ascending by abs value).
    return isNaN(num) ? -Infinity : num;
}
</file>

<file path="static/js/modules/utils/helpers.js">
// This file contains general JavaScript utility functions that can be reused across different modules.
// It helps keep common tasks, like formatting numbers for display, consistent and DRY (Don't Repeat Yourself).
// static/js/modules/utils/helpers.js
// Utility functions
/**
 * Formats a number for display, handling null/undefined.
 * @param {number | null | undefined} value - The number to format.
 * @param {number} [digits=2] - Number of decimal places.
 * @returns {string} Formatted number or 'N/A'.
 */
export function formatNumber(value, digits = 2) {
    if (value === null || typeof value === 'undefined' || isNaN(value)) {
        return 'N/A';
    }
    return Number(value).toFixed(digits);
}
</file>

<file path="templates/base.html">
<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>{% block title %}Data Checker{% endblock %}</title>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/css/bootstrap.min.css" rel="stylesheet">
    <style>
        /* Basic styles - can be expanded */
        body { padding-top: 6rem; } /* Increased top padding */
        .sticky-top {
            top: 56px; /* Adjust based on navbar height */
        }
        /* Add any custom global styles here */
        .table-danger {
            background-color: #f8d7da !important; /* Red for high Z */
        }
        .table-warning {
            background-color: #fff3cd !important; /* Yellow for medium Z */
        }
        /* Give chart canvases a default aspect ratio */
        .chart-canvas {
            aspect-ratio: 16 / 9; /* Default widescreen aspect ratio */
            width: 100%; /* Ensure it fills container width */
            max-width: 100%; /* Prevent overflow */
            min-height: 250px; /* Optional: Ensure a minimum height */
        }
        /* Navbar brand adjustments */
        .navbar-brand {
            display: flex; /* Use flexbox for alignment */
            align-items: center; /* Vertically center items */
            font-size: 1.5rem; /* Increase font size */
        }
        .navbar-brand img {
            height: 50px; /* Reduced logo height */
            margin-right: 0.5rem; /* Space between logo and text */
        }
    </style>
</head>
<body>
    <nav class="navbar navbar-expand-md navbar-dark bg-dark fixed-top">
        <div class="container-fluid">
            <a class="navbar-brand" href="{{ url_for('main.index') }}">
                <img src="{{ url_for('static', filename='images/bang.jpg') }}" alt="Logo">
                Data Checker
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarCollapse">
                <ul class="navbar-nav me-auto mb-2 mb-md-0">
                    <li class="nav-item">
                        <a class="nav-link" href="{{ url_for('main.index') }}">Time Series Dashboard</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="{{ url_for('security.securities_page') }}">Securities Check</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="{{ url_for('comparison_bp.summary') }}">Spread Comparison</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="{{ url_for('duration_comparison_bp.summary') }}">Duration Comparison</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="{{ url_for('spread_duration_comparison_bp.summary') }}">Spread Duration Comparison</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="{{ url_for('curve_bp.summary_page') }}">Yield Curve Check</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="{{ url_for('weight.weight_check') }}">Weight Check</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>
    <main role="main" class="container">
        {% block content %}
        {# Page specific content will go here #}
        {% endblock %}
    </main>
    <!-- Bootstrap Bundle with Popper -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/js/bootstrap.bundle.min.js"></script>
    <!-- Load Chart.js Library -->
    <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
    <!-- Load Date Adapter (e.g., date-fns) - MUST be after Chart.js -->
    <script src="https://cdn.jsdelivr.net/npm/chartjs-adapter-date-fns@3.0.0/dist/chartjs-adapter-date-fns.bundle.min.js"></script>
    <!-- Load Main Application JS (after libraries are loaded) -->
    <script type="module" src="{{ url_for('static', filename='js/main.js') }}"></script>
    {% block scripts %}
    {# Page specific scripts can go here #}
    {% endblock %}
</body>
</html>
</file>

<file path="templates/comparison_details_page.html">
{% extends "base.html" %}
{% block title %}Spread Comparison Details: {{ security_name }}{% endblock %}
{% block content %}
<div class="container mt-4">
    <nav aria-label="breadcrumb">
        <ol class="breadcrumb">
            <li class="breadcrumb-item"><a href="{{ url_for('comparison_bp.summary') }}">Comparison Summary</a></li>
            <li class="breadcrumb-item active" aria-current="page">{{ security_name }} ({{ security_id }})</li>
        </ol>
    </nav>
    <h1>Spread Comparison Details: {{ security_name }}</h1>
    <h5 class="text-muted">Security ID: {{ security_id }}</h5>
    <div class="row mt-4 mb-4">
        <div class="col-md-6">
            <h2>Comparison Statistics</h2>
            <ul class="list-group">
                <li class="list-group-item d-flex justify-content-between align-items-center">
                    Level Correlation
                    <span class="badge bg-primary rounded-pill">{{ "%.4f"|format(stats.Level_Correlation) if stats.Level_Correlation is not none else 'N/A' }}</span>
                </li>
                <li class="list-group-item d-flex justify-content-between align-items-center">
                    Change Correlation
                    <span class="badge bg-primary rounded-pill">{{ "%.4f"|format(stats.Change_Correlation) if stats.Change_Correlation is not none else 'N/A' }}</span>
                </li>
                <li class="list-group-item d-flex justify-content-between align-items-center">
                    Mean Absolute Difference
                    <span class="badge bg-secondary rounded-pill">{{ "%.2f"|format(stats.Mean_Abs_Diff) if stats.Mean_Abs_Diff is not none else 'N/A' }}</span>
                </li>
                <li class="list-group-item d-flex justify-content-between align-items-center">
                    Max Absolute Difference
                    <span class="badge bg-secondary rounded-pill">{{ "%.2f"|format(stats.Max_Abs_Diff) if stats.Max_Abs_Diff is not none else 'N/A' }}</span>
                </li>
                 <li class="list-group-item d-flex justify-content-between align-items-center">
                    Data Points (Original)
                    <span class="badge bg-info rounded-pill">{{ stats.Total_Points - stats.NaN_Count_Orig }} / {{ stats.Total_Points }}</span>
                </li>
                 <li class="list-group-item d-flex justify-content-between align-items-center">
                    Data Points (New)
                    <span class="badge bg-info rounded-pill">{{ stats.Total_Points - stats.NaN_Count_New }} / {{ stats.Total_Points }}</span>
                </li>
                 <li class="list-group-item d-flex justify-content-between align-items-center">
                    Same Date Range?
                    <span class="badge {{ 'bg-success' if stats.Same_Date_Range else 'bg-warning' }} rounded-pill">{{ 'Yes' if stats.Same_Date_Range else 'No' }}</span>
                </li>
            </ul>
        </div>
        {# Placeholder for additional stats or info if needed #}
        {# <div class="col-md-6">
             <h2>Other Info</h2>
        </div> #}
    </div>
    <h2>Time Series Comparison</h2>
    <p class="text-muted">Overlayed credit spreads from Original (sec_spread) and New (sec_spreadSP) datasets.</p>
    <div>
        <canvas id="comparisonChart"></canvas>
    </div>
    {# Embed chart data as JSON for JavaScript #}
    <script type="application/json" id="comparisonChartData">
        {{ chart_data | tojson | safe }}
    </script>
</div>
{% endblock %}
{% block scripts %}
{{ super() }}
{# We need Chart.js - ensure it's included in base.html or here #}
<script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
{# We also need the JS to render this specific chart #}
<script>
    document.addEventListener('DOMContentLoaded', function() {
        const chartDataElement = document.getElementById('comparisonChartData');
        const comparisonChartCanvas = document.getElementById('comparisonChart');
        if (chartDataElement && comparisonChartCanvas) {
            try {
                const chartData = JSON.parse(chartDataElement.textContent);
                const ctx = comparisonChartCanvas.getContext('2d');
                // --- REVERTING TO SIMPLE CONFIG --- 
                // Remove scriptable options for now
                /*
                // Define base colors (assuming these are the intended original colors)
                const baseColors = [
                    'rgba(13, 110, 253, 1)', // Bootstrap Blue (or COLOR_PALETTE[0])
                    'rgba(220, 53, 69, 1)'  // Bootstrap Red (or COLOR_PALETTE[1])
                ];
                const gapColor = 'rgba(150, 150, 150, 0.7)'; // Semi-transparent gray for gaps
                // Prepare datasets with scriptable borderColor
                console.log("Preparing datasets with scriptable colors...");
                const datasetsWithScriptableColors = chartData.datasets.map((dataset, index) => {
                     console.log(`Mapping dataset index: ${index}`);
                     return {
                        ...dataset, // Keep original label, data, tension, spanGaps etc.
                        borderColor: context => {
                            // Check if drawing a line segment and context is valid
                            if (context.type === 'segment' && context.p0 && context.p1) {
                                // --- NEW APPROACH: Check the 'skip' property of the points (safely) --- 
                                // Ensure p0 and p1 exist before accessing skip
                                const p0skip = context.p0 ? context.p0.skip : false;
                                const p1skip = context.p1 ? context.p1.skip : false;
                                if (p0skip || p1skip) {
                                     // --- DEBUG LOGGING START ---
                                    // Log when gap is detected using the 'skip' property
                                    console.log(`>>> Gap DETECTED (via skip): p0.skip=${p0skip}, p1.skip=${p1skip}, datasetIndex=${context.datasetIndex}. Applying gapColor.`);
                                    // --- DEBUG LOGGING END ---
                                    return gapColor;
                                }
                            }
                            // Default color for non-gap segments, points, legend
                            return baseColors[context.datasetIndex % baseColors.length];
                        },
                        // Ensure point colors match the line start/end unless hovered
                        pointBorderColor: context => baseColors[context.datasetIndex % baseColors.length],
                        pointBackgroundColor: context => baseColors[context.datasetIndex % baseColors.length],
                    }
                });
                console.log("Data prepared. Initializing Chart...");
                */
                // --- END REVERT --- 
                console.log("Initializing Chart with original data..."); // Log before init
                new Chart(ctx, {
                    type: 'line',
                    // Use the ORIGINAL datasets from Python/JSON
                    data: chartData, 
                    options: {
                        responsive: true,
                        maintainAspectRatio: true, // Adjust as needed
                        plugins: {
                            legend: {
                                position: 'top',
                            },
                            title: {
                                display: true,
                                text: 'Spread Comparison: {{ security_name|tojson }}'
                            }
                        },
                        scales: {
                            x: {
                                // Assuming labels are date strings, configure time scale if needed
                                // type: 'time',
                                // time: {
                                //     unit: 'day' // or week, month, etc.
                                // },
                                title: {
                                    display: true,
                                    text: 'Date'
                                }
                            },
                            y: {
                                title: {
                                    display: true,
                                    text: 'Spread'
                                }
                            }
                        },
                        interaction: {
                             intersect: false,
                             mode: 'index',
                        },
                        // Add other options from existing charts for consistency
                    }
                });
            } catch (error) {
                console.error("Error parsing chart data or rendering chart:", error);
                comparisonChartCanvas.parentElement.innerHTML = '<p class="text-danger">Error rendering chart.</p>';
            }
        } else {
             console.warn("Chart data or canvas element not found for comparison chart.");
        }
    });
</script>
{% endblock %}
</file>

<file path="templates/comparison_page.html">
{% extends "base.html" %}
{% block title %}Spread Comparison Summary{% endblock %}
{% block content %}
<div class="container-fluid mt-4"> {# Use container-fluid for wider view #}
    <h1>Spread Comparison: Original (sec_spread) vs. New (sec_spreadSP)</h1>
    <p class="text-muted">Comparing credit spreads between the two datasets. Click on a Security ID/Name to see details. Use filters or click column headers to sort. Pagination applied.</p>
    {# Display message if any #}
    {% if message %}
    <div class="alert alert-warning alert-dismissible fade show" role="alert">
        {{ message }}
        <button type="button" class="btn-close" data-bs-dismiss="alert" aria-label="Close"></button>
    </div>
    {% endif %}
    {# --- Filter Form --- #}
    {% if filter_options %}
    <form method="GET" action="{{ url_for('comparison_bp.summary') }}" class="mb-3 p-3 border rounded bg-light" id="filter-form">
        <h5>Filters</h5>
        <div class="row g-2 align-items-end">
            {% for column, options in filter_options.items() %}
            <div class="col-md-2 mb-2">
                <label for="filter-{{ column }}" class="form-label">{{ column }}</label>
                <select id="filter-{{ column }}" name="filter_{{ column }}" class="form-select form-select-sm">
                    <option value="">All</option>
                    {% for option in options %}
                    <option value="{{ option }}" {% if active_filters.get(column) == option|string %}selected{% endif %}>{{ option }}</option>
                    {% endfor %}
                </select>
            </div>
            {% endfor %}
            <div class="col-md-auto">
                <button type="submit" class="btn btn-primary btn-sm">Apply Filters</button>
                {# Add a clear button only if filters are active #}
                {% if active_filters %}
                <a href="{{ url_for('comparison_bp.summary') }}" class="btn btn-secondary btn-sm">Clear Filters</a>
                {% endif %}
            </div>
        </div>
        {# Hidden fields to preserve current sort order when applying filters - page is implicitly reset #}
        <input type="hidden" name="sort_by" value="{{ current_sort_by }}">
        <input type="hidden" name="sort_order" value="{{ current_sort_order }}">
    </form>
    {% endif %}
    {# --- Data Table --- #}
    <div class="table-responsive">
        <table class="table table-striped table-hover table-sm caption-top" id="comparison-table">
             {# Add table caption for summary #}
             {% if pagination %}
             <caption class="pb-1">
                 Displaying {{ table_data|length }} of {{ pagination.total_items }} total securities.
                 (Page {{ pagination.page }} of {{ pagination.total_pages }})
             </caption>
             {% endif %}
            <thead class="table-light sticky-top">
                <tr>
                    {# Loop through the columns passed from the view #}
                    {% for col_name in columns_to_display %}
                        {% set is_sort_col = (col_name == current_sort_by) %}
                        {% set next_sort_order = 'asc' if is_sort_col and current_sort_order == 'desc' else 'desc' %}
                        {# Base arguments, including current filters #}
                        {% set sort_args = request.args.to_dict() %}
                        {% set _ = sort_args.pop('page', None) %}
                        {% set _ = sort_args.update({'sort_by': col_name, 'sort_order': next_sort_order}) %}
                        {# Generate URL for this header #}
                        {% set sort_url = url_for('comparison_bp.summary', **sort_args) %}
                        {# Add classes for styling and JS #}
                        <th class="sortable {{ 'sorted-' + current_sort_order if is_sort_col else '' }}" 
                            data-column-name="{{ col_name }}">
                            <a href="{{ sort_url }}" class="text-decoration-none text-dark">
                                {{ col_name.replace('_', ' ') | title }} 
                                {% if is_sort_col %}
                                    <span class="sort-indicator ms-1">{{ '▲' if current_sort_order == 'asc' else '▼' }}</span>
                                {% endif %}
                            </a>
                        </th>
                    {% endfor %}
                </tr>
            </thead>
            <tbody id="comparison-table-body">
                {% set id_col = id_column_name %}
                {% for row in table_data %}
                <tr>
                    {# Loop through the same columns to ensure order matches header #}
                    {% for col_name in columns_to_display %}
                        <td>
                            {% if col_name == id_col %}
                                <a href="{{ url_for('comparison_bp.details', security_id=row[id_col]|urlencode) }}">{{ row[id_col] }}</a>
                            {% elif col_name in ['Level_Correlation', 'Change_Correlation'] and row[col_name] is not none %}
                                {{ "%.3f"|format(row[col_name]) }}
                            {% elif col_name in ['Mean_Abs_Diff', 'Max_Abs_Diff'] and row[col_name] is not none %}
                                {{ "%.2f"|format(row[col_name]) }}
                            {% elif col_name == 'Same_Date_Range' %}
                                {{ 'Yes' if row[col_name] else 'No' }}
                            {% elif row[col_name] is number %}
                                {{ row[col_name]|round(3) }} {# General numeric formatting #}
                            {% else %}
                                {{ row[col_name] if row[col_name] is not none else '' }} {# Display strings or empty #}
                            {% endif %}
                        </td>
                    {% endfor %}
                </tr>
                {% else %}
                {# This message is now shown above if filtered_stats is empty #}
                {# <tr> <td colspan="{{ columns_to_display|length }}" class="text-center">No comparison data available matching the current filters.</td> </tr> #}
                {% endfor %}
            </tbody>
        </table>
    </div>
    {# --- Pagination Controls --- #}
    {% if pagination and pagination.total_pages > 1 %}
        <nav aria-label="Comparison data navigation">
            <ul class="pagination pagination-sm justify-content-center">
                {# Previous Page Link #}
                <li class="page-item {{ 'disabled' if not pagination.has_prev }}">
                    <a class="page-link" href="{{ pagination.url_for_page(pagination.prev_num) if pagination.has_prev else '#' }}" aria-label="Previous">&laquo;</a>
                </li>
                {# Page Number Links (using context variables calculated in view) #}
                 {% set start_page = pagination.start_page_display %}
                 {% set end_page = pagination.end_page_display %}
                 {% if start_page > 1 %}
                     <li class="page-item"><a class="page-link" href="{{ pagination.url_for_page(1) }}">1</a></li>
                     {% if start_page > 2 %}
                         <li class="page-item disabled"><span class="page-link">...</span></li>
                     {% endif %}
                 {% endif %}
                 {% for p in range(start_page, end_page + 1) %}
                    <li class="page-item {{ 'active' if p == pagination.page }}">
                        <a class="page-link" href="{{ pagination.url_for_page(p) }}">{{ p }}</a>
                    </li>
                {% endfor %}
                 {% if end_page < pagination.total_pages %}
                     {% if end_page < pagination.total_pages - 1 %}
                         <li class="page-item disabled"><span class="page-link">...</span></li>
                     {% endif %}
                     <li class="page-item"><a class="page-link" href="{{ pagination.url_for_page(pagination.total_pages) }}">{{ pagination.total_pages }}</a></li>
                 {% endif %}
                {# Next Page Link #}
                <li class="page-item {{ 'disabled' if not pagination.has_next }}">
                    <a class="page-link" href="{{ pagination.url_for_page(pagination.next_num) if pagination.has_next else '#' }}" aria-label="Next">&raquo;</a>
                </li>
            </ul>
        </nav>
    {% endif %}
</div>
{% endblock %}
{% block scripts %}
{{ super() }}
{# Client-side sorting can still be enabled via tableSorter.js if desired, #}
{# but core pagination/filtering/sorting is server-side. #}
{# Ensure tableSorter.js is loaded via base.html if you keep initTableSorter in main.js #}
{% endblock %}
</file>

<file path="templates/delete_metric_page.html">
<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <title>{{ metric_name }} Check</title>
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
    <style>
        body { padding-top: 5rem; }
        .chart-container { margin-bottom: 15px; }
        .metrics-table { margin-top: 5px; margin-bottom: 25px; font-size: 0.9em; }
        .metrics-table th, .metrics-table td { padding: 4px 8px; border: 1px solid #dee2e6; }
        .missing-warning { color: red; font-weight: bold; }
        .high-z { background-color: #fff3cd; }
        .very-high-z { background-color: #f8d7da; font-weight: bold; }
    </style>
</head>
<body>
    <nav class="navbar navbar-expand-md navbar-dark bg-dark fixed-top">
        <a class="navbar-brand" href="/">Data Verification</a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarsExampleDefault" aria-controls="navbarsExampleDefault" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="navbarsExampleDefault">
            <ul class="navbar-nav mr-auto">
                <li class="nav-item">
                    <a class="nav-link" href="/">Dashboard</a>
                </li>
            </ul>
        </div>
    </nav>
    <main role="main" class="container">
        <h1>{{ metric_name }} Check</h1>
        <p>Latest Data Date: <strong>{{ latest_date }}</strong></p>
        <p>Charts sorted by the absolute Z-score of the latest <strong>Fund - Benchmark Spread</strong> (most deviation first).</p>
        {% if not missing_funds.empty %}
            <div class="alert alert-warning" role="alert">
                <strong>Warning:</strong> The following funds are missing data for the latest date ({{ latest_date }}):
                {{ missing_funds.index.tolist() | join(', ') }}
            </div>
        {% endif %}
        {% for fund_code, data in charts_data.items() %}
            {% set metrics = data.metrics %}
            {% set z_score = metrics['Spread Z-Score'] %}
            {% set z_class = 'high-z' if z_score and z_score|abs > 2 else ('very-high-z' if z_score and z_score|abs > 3 else '') %}
            <div class="chart-container {{ z_class }}">
                {{ data.chart_html|safe }}
            </div>
            <table class="table table-sm table-bordered metrics-table {{ z_class }}">
                <thead>
                    <tr>
                        <th>Metric</th>
                        <th>Latest Value ({{ latest_date }})</th>
                        <th>Change from Previous</th>
                        <th>Historical Spread</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Fund Value</td>
                        <td>{{ metrics['Latest Fund Value']|round(2) if metrics['Latest Fund Value'] is not none else 'N/A' }}</td>
                        <td>{{ metrics['Fund Value Change']|round(2) if metrics['Fund Value Change'] is not none else 'N/A' }}</td>
                        <td rowspan="2">Mean: {{ metrics['Historical Spread Mean']|round(2) if metrics['Historical Spread Mean'] is not none else 'N/A' }}</td>
                    </tr>
                    <tr>
                        <td>Benchmark Value</td>
                        <td>{{ metrics['Latest Benchmark Value']|round(2) if metrics['Latest Benchmark Value'] is not none else 'N/A' }}</td>
                        <td>N/A</td> {# Change not calculated for benchmark #}
                    </tr>
                    <tr>
                        <td>Fund - Benchmark Spread</td>
                        <td>{{ metrics['Latest Spread']|round(2) if metrics['Latest Spread'] is not none else 'N/A' }}</td>
                        <td>{{ metrics['Spread Change']|round(2) if metrics['Spread Change'] is not none else 'N/A' }}</td>
                        <td>Std Dev: {{ metrics['Historical Spread Std Dev']|round(2) if metrics['Historical Spread Std Dev'] is not none else 'N/A' }}</td>
                    </tr>
                     <tr>
                        <td><strong>Spread Z-Score</strong></td>
                        <td colspan="3"><strong>{{ z_score|round(2) if z_score is not none else 'N/A' }}</strong></td>
                    </tr>
                </tbody>
            </table>
            {# Conditionally add link to fund duration details page #}
            {% if metric_name == 'Duration' %}
                <div class="mb-4 text-right">
                     <a href="{{ url_for('fund_duration_details', fund_code=fund_code) }}" class="btn btn-info btn-sm">View Security Duration Changes for {{ fund_code }} &rarr;</a>
                </div>
            {% endif %}
        {% else %}
            <p>No data processed for this metric.</p>
        {% endfor %}
    </main>
    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.5.4/dist/umd/popper.min.js"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"></script>
</body>
</html>
</file>

<file path="templates/duration_comparison_details_page.html">
{% extends "base.html" %}
{# Note: security_name is not explicitly passed, using security_id for title/breadcrumb #}
{% block title %}Duration Comparison Details: {{ security_id }}{% endblock %}
{% block content %}
<div class="container mt-4">
    <nav aria-label="breadcrumb">
        <ol class="breadcrumb">
            <li class="breadcrumb-item"><a href="{{ url_for('duration_comparison_bp.summary') }}">Duration Comparison Summary</a></li> {# Updated Link #}
            {# Displaying ID as primary identifier if name isn't guaranteed #}
            <li class="breadcrumb-item active" aria-current="page">{{ security_id }}</li>
        </ol>
    </nav>
    <h1>Duration Comparison Details: {{ security_id }}</h1> {# Updated Title #}
    {# Add static info if available #}
    {% if static_info %}
        {% for key, value in static_info.items() %}
             {% if key != id_column_name %} {# Avoid repeating the ID #}
                <span class="text-muted me-3"><strong>{{ key }}:</strong> {{ value }}</span>
             {% endif %}
        {% endfor %}
    {% endif %}
    <div class="row mt-4 mb-4">
        <div class="col-md-6">
            <h2>Comparison Statistics</h2>
            {% if stats_summary %}
                <ul class="list-group">
                    <li class="list-group-item d-flex justify-content-between align-items-center">
                        Level Correlation
                        <span class="badge bg-primary rounded-pill">{{ stats_summary.Level_Correlation if stats_summary.Level_Correlation is not none else 'N/A' }}</span>
                    </li>
                    <li class="list-group-item d-flex justify-content-between align-items-center">
                        Change Correlation
                        <span class="badge bg-primary rounded-pill">{{ stats_summary.Change_Correlation if stats_summary.Change_Correlation is not none else 'N/A' }}</span>
                    </li>
                    <li class="list-group-item d-flex justify-content-between align-items-center">
                        Mean Absolute Difference
                        <span class="badge bg-secondary rounded-pill">{{ stats_summary.Mean_Abs_Diff if stats_summary.Mean_Abs_Diff is not none else 'N/A' }}</span>
                    </li>
                    <li class="list-group-item d-flex justify-content-between align-items-center">
                        Max Absolute Difference
                        <span class="badge bg-secondary rounded-pill">{{ stats_summary.Max_Abs_Diff if stats_summary.Max_Abs_Diff is not none else 'N/A' }}</span>
                    </li>
                    <li class="list-group-item d-flex justify-content-between align-items-center">
                        Data Points (Original)
                        <span class="badge bg-info rounded-pill">{{ stats_summary.Total_Points - stats_summary.NaN_Count_Orig }} / {{ stats_summary.Total_Points }}</span>
                    </li>
                    <li class="list-group-item d-flex justify-content-between align-items-center">
                        Data Points (New)
                        <span class="badge bg-info rounded-pill">{{ stats_summary.Total_Points - stats_summary.NaN_Count_New }} / {{ stats_summary.Total_Points }}</span>
                    </li>
                    <li class="list-group-item d-flex justify-content-between align-items-center">
                        Same Date Range?
                        <span class="badge {{ 'bg-success' if stats_summary.Same_Date_Range else 'bg-warning' }} rounded-pill">{{ 'Yes' if stats_summary.Same_Date_Range else 'No' }}</span>
                    </li>
                    {# Add date range details #}
                    <li class="list-group-item">
                        <small>Orig Range: {{ stats_summary.Start_Date_Orig or 'N/A' }} to {{ stats_summary.End_Date_Orig or 'N/A' }}</small><br>
                        <small>New Range: {{ stats_summary.Start_Date_New or 'N/A' }} to {{ stats_summary.End_Date_New or 'N/A' }}</small>
                    </li>
                </ul>
            {% else %}
                <p>No comparison statistics could be calculated.</p>
            {% endif %}
        </div>
        {# Placeholder for additional stats or info if needed #}
    </div>
    <h2>Time Series Comparison</h2>
    <p class="text-muted">Overlayed Duration from Original (sec_duration) and New (sec_durationSP) datasets.</p> {# Updated Text #}
    <div>
        <canvas id="comparisonChart"></canvas>
    </div>
    {# Embed chart data as JSON for JavaScript #}
    <script type="application/json" id="comparisonChartData">
        {{ chart_data | tojson | safe }}
    </script>
</div>
{% endblock %}
{% block scripts %}
{{ super() }}
{# We need Chart.js - ensure it's included in base.html or here #}
<script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
<script src="https://cdn.jsdelivr.net/npm/chartjs-adapter-date-fns/dist/chartjs-adapter-date-fns.bundle.min.js"></script> {# Include Date Adapter #}
<script>
    document.addEventListener('DOMContentLoaded', function() {
        const chartDataElement = document.getElementById('comparisonChartData');
        const comparisonChartCanvas = document.getElementById('comparisonChart');
        if (chartDataElement && comparisonChartCanvas) {
            try {
                const chartData = JSON.parse(chartDataElement.textContent);
                const ctx = comparisonChartCanvas.getContext('2d');
                console.log("Initializing Duration Comparison Chart...");
                new Chart(ctx, {
                    type: 'line',
                    data: chartData, // Direct use of data from JSON
                    options: {
                        responsive: true,
                        maintainAspectRatio: true,
                        plugins: {
                            legend: {
                                position: 'top',
                            },
                            title: {
                                display: true,
                                text: 'Duration Comparison: {{ security_id|tojson }}' // Use security_id
                            },
                            tooltip: {
                                mode: 'index', // Show tooltips for all datasets at the same index
                                intersect: false
                            }
                        },
                        scales: {
                            x: {
                                type: 'time', // Use time scale
                                time: {
                                    unit: 'day',
                                    tooltipFormat: 'yyyy-MM-dd', // Format for tooltip
                                    displayFormats: { // Formats for axis labels
                                        day: 'MMM d, yyyy'
                                    }
                                },
                                title: {
                                    display: true,
                                    text: 'Date'
                                }
                            },
                            y: {
                                title: {
                                    display: true,
                                    text: 'Duration' // Updated Axis Label
                                }
                            }
                        },
                        interaction: {
                             intersect: false,
                             mode: 'index',
                        },
                         elements: {
                            point:{ // Reduce point size for potentially dense data
                                radius: 2
                            }
                        }
                        // No complex scriptable options needed for basic display
                    }
                });
            } catch (error) {
                console.error("Error parsing duration chart data or rendering chart:", error);
                if (comparisonChartCanvas.parentElement) {
                    comparisonChartCanvas.parentElement.innerHTML = '<p class="text-danger">Error rendering duration chart.</p>';
                }
            }
        } else {
             console.warn("Chart data or canvas element not found for duration comparison chart.");
        }
    });
</script>
{% endblock %}
</file>

<file path="templates/duration_comparison_page.html">
{% extends "base.html" %}
{% block title %}Duration Comparison Summary{% endblock %}
{% block content %}
<div class="container-fluid mt-4"> {# Use container-fluid for wider view #}
    <h1>Duration Comparison: Original (sec_duration) vs. New (sec_durationSP)</h1> {# Updated Title #}
    <p class="text-muted">Comparing Duration between the two datasets. Click on a Security ID/Name to see details. Use filters or click column headers to sort. Pagination applied.</p> {# Updated Text #}
    {# Display message if any #}
    {% if message %}
    <div class="alert alert-warning alert-dismissible fade show" role="alert">
        {{ message }}
        <button type="button" class="btn-close" data-bs-dismiss="alert" aria-label="Close"></button>
    </div>
    {% endif %}
    {# --- Filter Form --- #}
    {% if filter_options %}
    <form method="GET" action="{{ url_for('duration_comparison_bp.summary') }}" class="mb-3 p-3 border rounded bg-light" id="filter-form"> {# Updated Action URL #}
        <h5>Filters</h5>
        <div class="row g-2 align-items-end">
            {% for column, options in filter_options.items() %}
            <div class="col-md-2 mb-2">
                <label for="filter-{{ column }}" class="form-label">{{ column }}</label>
                <select id="filter-{{ column }}" name="filter_{{ column }}" class="form-select form-select-sm">
                    <option value="">All</option>
                    {% for option in options %}
                    <option value="{{ option }}" {% if active_filters.get(column) == option|string %}selected{% endif %}>{{ option }}</option>
                    {% endfor %}
                </select>
            </div>
            {% endfor %}
            <div class="col-md-auto">
                <button type="submit" class="btn btn-primary btn-sm">Apply Filters</button>
                {# Add a clear button only if filters are active #}
                {% if active_filters %}
                <a href="{{ url_for('duration_comparison_bp.summary') }}" class="btn btn-secondary btn-sm">Clear Filters</a> {# Updated Clear URL #}
                {% endif %}
            </div>
        </div>
        {# Hidden fields to preserve current sort order when applying filters - page is implicitly reset #}
        <input type="hidden" name="sort_by" value="{{ current_sort_by }}">
        <input type="hidden" name="sort_order" value="{{ current_sort_order }}">
    </form>
    {% endif %}
    {# --- Data Table --- #}
    <div class="table-responsive">
        <table class="table table-striped table-hover table-sm caption-top" id="duration-comparison-table"> {# Updated Table ID #}
             {# Add table caption for summary #}
             {% if pagination %}
             <caption class="pb-1">
                 Displaying {{ table_data|length }} of {{ pagination.total_items }} total securities.
                 (Page {{ pagination.page }} of {{ pagination.total_pages }})
             </caption>
             {% endif %}
            <thead class="table-light sticky-top">
                <tr>
                    {# Loop through the columns passed from the view #}
                    {% for col_name in columns_to_display %}
                        {% set is_sort_col = (col_name == current_sort_by) %}
                        {% set next_sort_order = 'asc' if is_sort_col and current_sort_order == 'desc' else 'desc' %}
                        {# Base arguments, including current filters #}
                        {% set sort_args = request.args.to_dict() %}
                        {% set _ = sort_args.pop('page', None) %}
                        {% set _ = sort_args.update({'sort_by': col_name, 'sort_order': next_sort_order}) %}
                        {# Generate URL for this header #}
                        {% set sort_url = url_for('duration_comparison_bp.summary', **sort_args) %} {# Updated Sort URL #}
                        {# Add classes for styling and JS #}
                        <th class="sortable {{ 'sorted-' + current_sort_order if is_sort_col else '' }}"
                            data-column-name="{{ col_name }}">
                            <a href="{{ sort_url }}" class="text-decoration-none text-dark">
                                {{ col_name.replace('_', ' ') | title }}
                                {% if is_sort_col %}
                                    <span class="sort-indicator ms-1">{{ '▲' if current_sort_order == 'asc' else '▼' }}</span>
                                {% endif %}
                            </a>
                        </th>
                    {% endfor %}
                </tr>
            </thead>
            <tbody id="duration-comparison-table-body"> {# Updated tbody ID #}
                {% set id_col = id_column_name %}
                {% for row in table_data %}
                <tr>
                    {# Loop through the same columns to ensure order matches header #}
                    {% for col_name in columns_to_display %}
                        <td>
                            {% if col_name == id_col %}
                                <a href="{{ url_for('duration_comparison_bp.details', security_id=row[id_col]|urlencode) }}">{{ row[id_col] }}</a> {# Updated Detail URL #}
                            {% elif col_name in ['Level_Correlation', 'Change_Correlation'] and row[col_name] is not none %}
                                {# Attempt to format as float, handle potential errors gracefully #}
                                {% set formatted_val = "%.3f"|format(row[col_name]|float) if row[col_name] is number else row[col_name] %}
                                {{ formatted_val }}
                            {% elif col_name in ['Mean_Abs_Diff', 'Max_Abs_Diff'] and row[col_name] is not none %}
                                {% set formatted_val = "%.2f"|format(row[col_name]|float) if row[col_name] is number else row[col_name] %}
                                {{ formatted_val }}
                             {% elif col_name == 'Same_Date_Range' %}
                                 <span class="badge {{ 'bg-success' if row[col_name] else 'bg-warning' }}">{{ 'Yes' if row[col_name] else 'No' }}</span>
                            {% elif col_name.endswith('_Date') and row[col_name] %}
                                 {# Assume date strings are already YYYY-MM-DD from view #}
                                {{ row[col_name] }}
                            {% elif row[col_name] is number %}
                                {{ row[col_name]|round(3) }} {# General numeric formatting #}
                            {% else %}
                                {{ row[col_name] if row[col_name] is not none else '' }} {# Display strings or empty #}
                            {% endif %}
                        </td>
                    {% endfor %}
                </tr>
                {% else %}
                <tr>
                    <td colspan="{{ columns_to_display|length }}" class="text-center">No duration comparison data available matching the current filters.</td> {# Updated Message #}
                </tr>
                {% endfor %}
            </tbody>
        </table>
    </div>
    {# --- Pagination Controls --- #}
    {% if pagination and pagination.total_pages > 1 %}
    <nav aria-label="Duration comparison data navigation">
        <ul class="pagination pagination-sm justify-content-center">
            {# Helper macro for generating pagination links #}
            {% macro page_link(page_num, text=None, is_disabled=False, is_active=False) %}
                {% set link_args = request.args.to_dict() %}
                {% set _ = link_args.update({'page': page_num, 'sort_by': current_sort_by, 'sort_order': current_sort_order}) %}
                {% set url = url_for('duration_comparison_bp.summary', **link_args) if page_num else '#' %} {# Updated URL #}
                <li class="page-item {{ 'disabled' if is_disabled }} {{ 'active' if is_active }}">
                    <a class="page-link" href="{{ url }}" {% if is_active %}aria-current="page"{% endif %}>{{ text or page_num }}</a>
                </li>
            {% endmacro %}
            {{ page_link(pagination.prev_num, '&laquo;', is_disabled=not pagination.has_prev) }}
            {# Simplified pagination display logic #}
            {% set window = 2 %}
            {% set start_page = [1, pagination.page - window] | max %}
            {% set end_page = [pagination.total_pages, pagination.page + window] | min %}
            {% if start_page > 1 %}
                {{ page_link(1) }}
                {% if start_page > 2 %}
                    <li class="page-item disabled"><span class="page-link">...</span></li>
                {% endif %}
            {% endif %}
            {% for p in range(start_page, end_page + 1) %}
                {{ page_link(p, is_active=(p == pagination.page)) }}
            {% endfor %}
            {% if end_page < pagination.total_pages %}
                {% if end_page < pagination.total_pages - 1 %}
                    <li class="page-item disabled"><span class="page-link">...</span></li>
                {% endif %}
                {{ page_link(pagination.total_pages) }}
            {% endif %}
            {{ page_link(pagination.next_num, '&raquo;', is_disabled=not pagination.has_next) }}
        </ul>
    </nav>
    {% endif %}
</div>
{% endblock %}
{% block scripts %}
{{ super() }}
{# No specific JS needed for this page unless client-side sorting is added back #}
{% endblock %}
</file>

<file path="templates/exclusions_page.html">
{% extends 'base.html' %}
{% block title %}Manage Security Exclusions{% endblock %}
{% block content %}
<div class="container mt-4">
    <h2>Manage Security Exclusions</h2>
    <hr>
    {# Display messages if any #}
    {% if message %}
        <div class="alert alert-{{ message_type }} alert-dismissible fade show" role="alert">
            {{ message }}
            <button type="button" class="btn-close" data-bs-dismiss="alert" aria-label="Close"></button>
        </div>
    {% endif %}
    <div class="row">
        {# Left Column: Display Current Exclusions #}
        <div class="col-md-7">
            <h4>Current Exclusions</h4>
            {% if exclusions %}
                <table class="table table-striped table-sm">
                    <thead>
                        <tr>
                            <th>Security ID</th>
                            <th>Date Added</th>
                            <th>End Date</th>
                            <th>Comment</th>
                            <th>Action</th>
                        </tr>
                    </thead>
                    <tbody>
                        {% for exclusion in exclusions %}
                            <tr>
                                <td>{{ exclusion.SecurityID }}</td>
                                <td>{{ exclusion.AddDate.strftime('%Y-%m-%d') if exclusion.AddDate else 'N/A' }}</td>
                                <td>{{ exclusion.EndDate.strftime('%Y-%m-%d') if exclusion.EndDate else '' }}</td>
                                <td>{{ exclusion.Comment }}</td>
                                <td>
                                    <form method="POST" action="{{ url_for('exclusion_bp.remove_exclusion_route') }}" style="display: inline;">
                                        <input type="hidden" name="security_id" value="{{ exclusion.SecurityID }}">
                                        <input type="hidden" name="add_date" value="{{ exclusion.AddDate.strftime('%Y-%m-%d') if exclusion.AddDate else '' }}">
                                        <button type="submit" class="btn btn-danger btn-sm" onclick="return confirm('Are you sure you want to remove this exclusion?');">Remove</button>
                                    </form>
                                </td>
                            </tr>
                        {% endfor %}
                    </tbody>
                </table>
            {% else %}
                <p>No securities are currently excluded.</p>
            {% endif %}
        </div>
        {# Right Column: Add New Exclusion Form #}
        <div class="col-md-5">
            <h4>Add New Exclusion</h4>
            <form method="POST" action="{{ url_for('exclusion_bp.manage_exclusions') }}">
                <div class="mb-3">
                    <label for="security-search-input" class="form-label">Search & Select Security ID:</label>
                    <input type="text" id="security-search-input" class="form-control mb-2" placeholder="Type to filter securities...">
                    <select class="form-select" id="security-select" name="security_id" required>
                        <option value="" disabled selected>Select a Security ID</option>
                        {% for sec_id in available_securities %}
                            <option value="{{ sec_id }}">{{ sec_id }}</option>
                        {% endfor %}
                    </select>
                </div>
                <div class="mb-3">
                    <label for="end_date" class="form-label">End Date (Optional):</label>
                    <input type="date" class="form-control" id="end_date" name="end_date">
                </div>
                <div class="mb-3">
                    <label for="comment" class="form-label">Comment (Required):</label>
                    <textarea class="form-control" id="comment" name="comment" rows="3" required></textarea>
                </div>
                <button type="submit" class="btn btn-primary">Add Exclusion</button>
            </form>
        </div>
    </div>
</div>
{% endblock %}
{% block scripts %}
{{ super() }} {# Include scripts from base.html #}
{# We will add specific JS for the dynamic dropdown here later #}
<script>
    // Basic dynamic filtering for the dropdown
    document.getElementById('security-search-input').addEventListener('input', function() {
        let filter = this.value.toLowerCase();
        let select = document.getElementById('security-select');
        let options = select.options;
        let firstVisibleOption = null;
        for (let i = 0; i < options.length; i++) {
            let option = options[i];
            // Skip the placeholder option
            if (option.value === "") {
                option.style.display = ""; // Always show placeholder if input is empty, hide otherwise
                option.style.display = filter ? "none" : "";
                continue;
            }
            let txtValue = option.textContent || option.innerText;
            if (txtValue.toLowerCase().indexOf(filter) > -1) {
                option.style.display = "";
                if (!firstVisibleOption) {
                     firstVisibleOption = option; // Keep track of the first match
                }
            } else {
                option.style.display = "none";
            }
        }
         // Optionally, select the first visible option if the user hasn't selected one manually
        // This part can be enhanced, maybe select only if input length > N or on specific event
        // if (filter && firstVisibleOption && select.selectedIndex <= 0) {
            // select.value = firstVisibleOption.value;
        // }
    });
    // Reset filter when dropdown is clicked (to show all options again initially)
    document.getElementById('security-select').addEventListener('mousedown', function(){
       // Optional: Uncomment below to clear search on dropdown click
       // document.getElementById('security-search-input').value = '';
       // let event = new Event('input');
       // document.getElementById('security-search-input').dispatchEvent(event);
    });
</script>
{% endblock %}
</file>

<file path="templates/fund_detail_page.html">
{% extends "base.html" %}
{% block title %}Fund Details: {{ fund_code }}{% endblock %}
{% block content %}
<div class="container mt-4">
    <h1 class="mb-4">Fund Details: {{ fund_code }}</h1>
    {% if message %}
        <div class="alert alert-info" role="alert">
            {{ message }}
        </div>
    {% endif %}
    {% if chart_data_json and chart_data_json != '[]' %}
        <!-- Embed JSON data for JavaScript -->
        <script id="fundChartData" type="application/json">
            {{ chart_data_json | safe }}
        </script>
        <!-- Area where charts will be rendered by JavaScript -->
        <div id="fundChartsArea" class="row row-cols-1 row-cols-lg-2 g-4">
            <!-- Charts will be dynamically inserted here -->
        </div>
    {% elif not message %}
         <div class="alert alert-warning" role="alert">
            No chart data available to display for this fund.
        </div>
    {% endif %}
     <div class="mt-4">
        <a href="{{ url_for('main.index') }}" class="btn btn-secondary">Back to Dashboard</a>
    </div>
</div>
{% endblock %}
{% block scripts %}
{{ super() }}
<!-- Chart.js is already included in base.html if needed by main.js -->
{% endblock %}
</file>

<file path="templates/fund_duration_details.html">
{% extends 'base.html' %}
{% block title %}Duration Change Details for {{ fund_code }}{% endblock %}
{% block content %}
<div class="container mt-4">
    <h2>Duration Change Details for Fund: {{ fund_code }}</h2>
    <p>Showing securities from <code>sec_duration.csv</code> held by <strong>{{ fund_code }}</strong>, sorted by the latest 1-day change in duration (largest change first).</p>
    {% if message %}
    <div class="alert alert-warning" role="alert">
        {{ message }}
    </div>
    {% endif %}
    {# Data Table Section #}
    {% if securities_data %}
    <div class="table-responsive">
        <table class="table table-striped table-hover table-sm small" id="fund-duration-table">
            <thead class="table-light">
                <tr>
                    {# Use the column_order provided by the backend #}
                    {% for col_name in column_order %}
                        <th>{{ col_name }}</th>
                    {% endfor %}
                </tr>
            </thead>
            <tbody id="fund-duration-table-body">
                {% for row in securities_data %}
                     {# Optionally add row highlighting based on the change magnitude if needed #}
                     {% set change_value = row['1 Day Duration Change'] %}
                     {% set row_class = '' %} {# Add logic here if desired e.g., based on change_value sign or magnitude #}
                     {# Example highlighting:
                     {% if change_value is not none %}
                        {% if change_value > 0.5 %}
                            {% set row_class = 'table-warning' %}
                        {% elif change_value < -0.5 %}
                             {% set row_class = 'table-info' %}
                        {% endif %}
                     {% endif %}
                     #}
                    <tr class="{{ row_class }}">
                        {% for col_name in column_order %}
                            <td>
                                {# Special formatting for the change column or others if needed #}
                                {% if col_name == id_col_name %}
                                     {# Make the Security Name a link #}
                                     <a href="{{ url_for('security.security_details_page', metric_name='Duration', security_id=row[col_name]|urlencode) }}">{{ row[col_name] }}</a>
                                {% elif row[col_name] is number %}
                                    {# Format numeric columns, maybe specific format for change #}
                                     {{ "%.3f"|format(row[col_name]) }}
                                {% else %}
                                     {{ row[col_name] if row[col_name] is not none else '' }}
                                {% endif %}
                            </td>
                        {% endfor %}
                    </tr>
                {% endfor %}
            </tbody>
        </table>
    </div>
    {% elif not message %}
     <div class="alert alert-info" role="alert">
        No securities data to display for fund {{ fund_code }}.
    </div>
    {% endif %}
    <div class="mt-3">
         <a href="{{ url_for('metric.metric_page', metric_name='Duration') }}" class="btn btn-secondary btn-sm">&larr; Back to Duration Metric Page</a>
    </div>
</div>
{% endblock %}
</file>

<file path="templates/get_data.html">
{% extends "base.html" %}
{% block title %}Get Data via API{% endblock %}
{% block content %}
<div class="container mt-4">
    {# --- Display Data File Statuses --- #}
    <div class="card mb-4">
        <div class="card-header">
            Current Data File Status
        </div>
        <div class="card-body">
            {% if data_file_statuses %}
            <table class="table table-sm table-striped table-bordered">
                <thead>
                    <tr>
                        <th>File Name</th>
                        <th>Latest Data Date (in file)</th>
                        <th>File Last Modified</th>
                        <th>Funds Included</th>
                    </tr>
                </thead>
                <tbody>
                    {% for status in data_file_statuses %}
                    <tr>
                        <td>{{ status.filename }}</td>
                        <td>
                            {% if status.exists %}
                                {{ status.latest_data_date }}
                            {% else %}
                                <span class="text-muted">File Not Found</span>
                            {% endif %}
                        </td>
                        <td>
                             {% if status.exists %}
                                {{ status.last_modified }}
                            {% else %}
                                <span class="text-muted">N/A</span>
                            {% endif %}
                        </td>
                        <td>
                             {% if status.exists %}
                                {{ status.funds_included }}
                            {% else %}
                                <span class="text-muted">N/A</span>
                            {% endif %}
                        </td>
                    </tr>
                    {% endfor %}
                </tbody>
            </table>
            {% else %}
            <p class="text-muted">Could not retrieve data file statuses. Check QueryMap.csv or server logs.</p>
            {% endif %}
        </div>
    </div>
    {# --- End Display Data File Statuses --- #}
    <h2>Get Data via Simulated API (Rex)</h2>
    <p>Select funds and date range to simulate retrieving data using the Rex API.</p>
    <p>The simulated API calls will be printed in the terminal where the Flask app is running.</p>
    <form id="get-data-form">
        <div class="row mb-3">
            <div class="col-md-4">
                <label for="daysBack" class="form-label">Days Back:</label>
                <input type="number" class="form-control" id="daysBack" name="days_back" value="30" required>
                <div class="form-text">Number of days of history to retrieve ending on the End Date.</div>
            </div>
            <div class="col-md-4">
                <label for="endDate" class="form-label">End Date:</label>
                <input type="date" class="form-control" id="endDate" name="end_date" value="{{ default_end_date }}" required>
                <div class="form-text">Defaults to the previous business day.</div>
            </div>
        </div>
        <div class="mb-3">
            <label class="form-label">Select Funds:</label>
             <button type="button" class="btn btn-sm btn-outline-secondary ms-2" id="select-all-funds">Select All</button>
             <button type="button" class="btn btn-sm btn-outline-secondary ms-1" id="deselect-all-funds">Deselect All</button>
            <div id="fund-list" class="border p-3" style="max-height: 300px; overflow-y: auto;">
                {% for fund in funds %}
                <div class="form-check">
                    <input class="form-check-input fund-checkbox" type="checkbox" value="{{ fund['Fund Code'] }}" id="fund-{{ fund['Fund Code'] }}" name="funds"
                           {% if fund['Picked'] %}checked{% endif %}>
                    <label class="form-check-label" for="fund-{{ fund['Fund Code'] }}">
                        {{ fund['Fund Code'] }} (AUM: {{ fund['Total Asset Value USD']|int }})
                    </label>
                </div>
                {% else %}
                <p class="text-danger">No funds found or FundList.csv could not be loaded correctly.</p>
                {% endfor %}
            </div>
             <div class="form-text text-danger d-none" id="fund-selection-error">Please select at least one fund.</div>
        </div>
        <button type="submit" class="btn btn-primary">Simulate API Calls</button>
        <button type="button" id="run-overwrite-button" class="btn btn-warning ms-2">Run and Overwrite Data</button>
        <button type="button" id="run-cleanup-button" class="btn btn-secondary ms-2">Run Data Cleanup</button>
    </form>
    <div id="status-area" class="mt-4" style="display: none;">
        <h4>Processing Status</h4>
        <div class="progress mb-2" style="height: 20px;">
            <div id="progress-bar" class="progress-bar progress-bar-striped progress-bar-animated" role="progressbar" style="width: 0%;" aria-valuenow="0" aria-valuemin="0" aria-valuemax="100">0%</div>
        </div>
        <p id="status-message"></p>
        <div id="results-summary" class="mt-3">
            <h5>Results Summary</h5>
            <table class="table table-sm table-striped">
                <thead>
                    <tr>
                        <th>Query ID</th>
                        <th>File Name</th>
                        <th>Simulated Rows Returned</th>
                        <th>Simulated File Lines</th>
                        <th>Status</th>
                        <th>Actions</th>
                    </tr>
                </thead>
                <tbody id="results-table-body">
                    <!-- Results will be populated here -->
                </tbody>
            </table>
        </div>
         <div id="error-message" class="alert alert-danger mt-3" style="display: none;">
             <!-- Errors shown here -->
         </div>
    </div>
</div>
{% endblock %}
{% block scripts %}
{{ super() }} {# Include scripts from base.html #}
<script>
document.addEventListener('DOMContentLoaded', function() {
    const form = document.getElementById('get-data-form');
    const statusArea = document.getElementById('status-area');
    const statusMessage = document.getElementById('status-message');
    const progressBar = document.getElementById('progress-bar');
    const resultsTableBody = document.getElementById('results-table-body');
    const errorMessageDiv = document.getElementById('error-message');
    const fundSelectionError = document.getElementById('fund-selection-error');
    const cleanupButton = document.getElementById('run-cleanup-button');
    const runOverwriteButton = document.getElementById('run-overwrite-button');
    const cleanupStatus = document.createElement('div');
    cleanupStatus.id = 'cleanup-status';
    cleanupStatus.className = 'mt-2';
    cleanupButton.parentNode.insertBefore(cleanupStatus, cleanupButton.nextSibling);
    const fundCheckboxes = document.querySelectorAll('.fund-checkbox');
    const selectAllButton = document.getElementById('select-all-funds');
    const deselectAllButton = document.getElementById('deselect-all-funds');
    // Select/Deselect All Funds buttons
    selectAllButton.addEventListener('click', () => {
        fundCheckboxes.forEach(checkbox => checkbox.checked = true);
    });
    deselectAllButton.addEventListener('click', () => {
        fundCheckboxes.forEach(checkbox => checkbox.checked = false);
    });
    // --- Function to handle the API call logic ---
    async function handleApiCall(overwriteMode = false) {
        // Clear previous results and errors
        statusArea.style.display = 'none';
        resultsTableBody.innerHTML = '';
        errorMessageDiv.style.display = 'none';
        errorMessageDiv.textContent = '';
        cleanupButton.style.display = 'none';
        cleanupStatus.textContent = '';
        cleanupStatus.className = 'mt-2';
        statusMessage.textContent = '';
        progressBar.style.width = '0%';
        progressBar.textContent = '0%';
        progressBar.classList.remove('bg-success', 'bg-danger');
        fundSelectionError.classList.add('d-none');
        // Get selected funds
        const selectedFunds = Array.from(document.querySelectorAll('input[name="funds"]:checked'))
                                 .map(cb => cb.value);
        // Basic client-side validation
        if (selectedFunds.length === 0) {
            fundSelectionError.classList.remove('d-none');
            return; // Stop submission
        }
        const daysBack = document.getElementById('daysBack').value;
        const endDate = document.getElementById('endDate').value;
        if (!endDate) {
             errorMessageDiv.textContent = 'Please select an End Date.';
             errorMessageDiv.style.display = 'block';
            return; // Stop submission
        }
        // Show status area and indicate processing
        statusArea.style.display = 'block';
        statusMessage.textContent = `Starting ${overwriteMode ? 'overwrite' : 'simulation/merge'}...`;
        progressBar.classList.add('progress-bar-animated');
        progressBar.classList.remove('bg-success', 'bg-danger');
        progressBar.style.width = '5%'; // Initial small progress
        progressBar.textContent = '5%';
        try {
            // Prepare request body, including the overwrite_mode flag
            const requestBody = {
                days_back: parseInt(daysBack, 10),
                end_date: endDate,
                funds: selectedFunds,
                overwrite_mode: overwriteMode // Add the flag here
            };
            const response = await fetch('{{ url_for("api_bp.run_api_calls") }}', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                },
                body: JSON.stringify(requestBody) // Send the body with the flag
            });
            const result = await response.json();
            // Stop animation
             progressBar.classList.remove('progress-bar-animated');
            if (response.ok && (result.status === 'completed' || result.status === 'completed_with_errors')) {
                statusMessage.textContent = result.message;
                progressBar.style.width = '100%';
                progressBar.textContent = '100%';
                progressBar.classList.add(result.status === 'completed_with_errors' ? 'bg-warning' : 'bg-success');
                // Populate results table
                resultsTableBody.innerHTML = ''; // Clear any potential previous entries
                if (result.summary && result.summary.length > 0) {
                    result.summary.forEach(item => {
                        // Determine row content based on real vs simulated
                        let rowsCellContent = 'N/A';
                        let linesCellContent = 'N/A';
                        if (item.actual_rows !== undefined && item.actual_rows !== null) { // Real API mode was used
                            rowsCellContent = item.actual_rows;
                            linesCellContent = item.actual_lines !== undefined ? item.actual_lines : (rowsCellContent > 0 ? rowsCellContent + 1 : 0);
                        } else if (item.simulated_rows !== undefined && item.simulated_rows !== null) { // Simulated mode was used
                            rowsCellContent = item.simulated_rows;
                            linesCellContent = item.simulated_lines !== undefined ? item.simulated_lines : (rowsCellContent > 0 ? rowsCellContent + 1 : 0);
                        }
                        // Define fundCode. Prefer 'fund_code' if present, else derive from 'query_id' if possible
                        let fundCodeForRerun = item.fund_code || null;
                        // Basic attempt to extract from query_id if needed (adjust regex/logic if format differs)
                        if (!fundCodeForRerun && item.query_id && typeof item.query_id === 'string') {
                            const match = item.query_id.match(/some_pattern_to_extract_fund_code/); // Replace with actual pattern if applicable
                            if (match && match[1]) {
                                fundCodeForRerun = match[1];
                            }
                        }
                        const rerunButtonHtml = fundCodeForRerun
                            ? `<button class="btn btn-sm btn-outline-primary rerun-button" data-fund-code="${fundCodeForRerun}">Rerun</button>`
                            : `<span class="text-muted">Rerun N/A</span>`; // No rerun if fund code unknown
                        const row = `<tr data-query-id="${item.query_id}">
                                        <td>${item.query_id}</td>
                                        <td>${item.file_name}</td>
                                        <td>${rowsCellContent}</td>
                                        <td>${linesCellContent}</td>
                                        <td><span class="badge ${item.status.includes('OK') || item.status.includes('Saved') || item.status.includes('Simulated') ? 'bg-success' : (item.status.includes('Warning') ? 'bg-warning' : 'bg-danger')}">${item.status}</span></td>
                                        <td>${rerunButtonHtml}</td>
                                     </tr>`;
                        resultsTableBody.innerHTML += row;
                    });
                } else {
                    resultsTableBody.innerHTML = '<tr><td colspan="6">No summary data returned.</td></tr>'; // colspan is 6 now
                }
                 errorMessageDiv.style.display = 'none'; // Hide error div if successful
                 cleanupButton.style.display = 'inline-block';
            } else {
                // Handle errors reported by the server (e.g., validation errors, file not found)
                statusMessage.textContent = 'Processing failed.';
                progressBar.style.width = '100%';
                progressBar.textContent = 'Error';
                progressBar.classList.add('bg-danger');
                errorMessageDiv.textContent = `Error: ${result.message || 'Unknown error'}`;
                errorMessageDiv.style.display = 'block';
            }
        } catch (error) {
            // Handle network errors or issues with the fetch itself
            console.error("Fetch error:", error);
            progressBar.classList.remove('progress-bar-animated');
            progressBar.style.width = '100%';
            progressBar.textContent = 'Error';
            progressBar.classList.add('bg-danger');
            statusMessage.textContent = 'An error occurred during the request.';
            errorMessageDiv.textContent = 'Network error or server unreachable. Check console for details.';
            errorMessageDiv.style.display = 'block';
        }
    }
    // --- End of API call handler function ---
    // --- Event Listener for the original "Simulate API Calls" button (which is type="submit") ---
    form.addEventListener('submit', function(event) {
        event.preventDefault(); // Prevent traditional form submission
        handleApiCall(false); // Call the handler function with overwriteMode = false
    });
    // --- Event Listener for the new "Run and Overwrite Data" button ---
    runOverwriteButton.addEventListener('click', function() {
        // No need for event.preventDefault() as it's not a submit button
        handleApiCall(true); // Call the handler function with overwriteMode = true
    });
    // Add event listener for the Rerun buttons using event delegation
    resultsTableBody.addEventListener('click', async function(event) {
        if (event.target.classList.contains('rerun-button')) {
            const button = event.target;
            const row = button.closest('tr');
            const queryId = row.dataset.queryId;
            const fundCode = button.dataset.fundCode;
            const daysBack = document.getElementById('daysBack').value;
            const endDate = document.getElementById('endDate').value;
            const cells = row.cells; // Define cells here so it's available in all blocks
            if (!fundCode || !queryId) {
                console.error('Missing fund code or query ID for rerun');
                // Optionally display an error to the user near the button/row
                return;
            }
            // Provide visual feedback
            button.disabled = true;
            button.textContent = 'Running...';
            // You could also add a temporary status cell or highlight the row
            try {
                const response = await fetch('/rerun-api-call', { // New endpoint needed
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                    },
                    body: JSON.stringify({
                        query_id: queryId,
                        // Send fundCode as a list in the 'funds' key
                        funds: [fundCode],
                        days_back: parseInt(daysBack, 10),
                        end_date: endDate
                    })
                });
                const result = await response.json();
                if (response.ok && result.status.includes('OK')) {
                    // Update the specific row in the table
                    cells[2].textContent = result.simulated_rows !== undefined ? result.simulated_rows : 'N/A'; // Simulated Rows
                    cells[3].textContent = result.simulated_lines !== undefined ? result.simulated_lines : 'N/A'; // Simulated Lines
                    cells[4].innerHTML = `<span class="badge bg-success">${result.status}</span>`; // Status
                    // Optional: Add a temporary success indicator
                    button.textContent = 'Rerun Success';
                    setTimeout(() => { button.textContent = 'Rerun'; }, 2000); // Reset after 2s
                } else {
                    // Handle error - update status cell, show message
                    cells[4].innerHTML = `<span class="badge bg-danger">Error</span>`;
                    console.error("Rerun failed:", result.message || 'Unknown error');
                    // Optionally display error details near the row or in the main error area
                     button.textContent = 'Rerun Failed';
                     setTimeout(() => { button.textContent = 'Rerun'; }, 3000); // Reset after 3s
                }
            } catch (error) {
                console.error("Rerun fetch error:", error);
                 cells[4].innerHTML = `<span class="badge bg-danger">Network Error</span>`;
                 button.textContent = 'Rerun Error';
                 setTimeout(() => { button.textContent = 'Rerun'; }, 3000); // Reset after 3s
                // Optionally display error details
            } finally {
                 button.disabled = false; // Re-enable button
                 // Remove any temporary status indicators if needed
            }
        }
    });
    // Add event listener for the new Cleanup button
    cleanupButton.addEventListener('click', async function() {
        cleanupStatus.textContent = 'Starting cleanup process...';
        cleanupStatus.className = 'mt-2 alert alert-info'; // Show feedback
        cleanupButton.disabled = true; // Disable button while running
        try {
            const response = await fetch('/run-cleanup', { // Call the new endpoint
                method: 'POST',
                 headers: {
                    'Content-Type': 'application/json', // Optional: Send empty JSON or adjust endpoint
                },
                // body: JSON.stringify({}) // Optional: Send empty JSON or adjust endpoint
            });
            const result = await response.json();
            if (response.ok && result.status === 'success') {
                cleanupStatus.textContent = `Cleanup process finished successfully. Output:\n${result.output}`;
                cleanupStatus.className = 'mt-2 alert alert-success';
            } else {
                 cleanupStatus.textContent = `Cleanup process failed. Error:\n${result.error || result.message || 'Unknown error'}`;
                 cleanupStatus.className = 'mt-2 alert alert-danger';
            }
        } catch (error) {
            console.error("Cleanup fetch error:", error);
            cleanupStatus.textContent = 'Failed to trigger cleanup process. Network error or server unreachable.';
            cleanupStatus.className = 'mt-2 alert alert-danger';
        } finally {
             cleanupButton.disabled = false; // Re-enable button
        }
    });
});
</script>
{% endblock %}
</file>

<file path="templates/index.html">
{% extends "base.html" %}
{% block title %}Data Verification Dashboard{% endblock %}
{% block content %}
    <div class="jumbotron mt-4"> {# Added mt-4 for spacing below fixed navbar #}
        <h1>Dashboard</h1>
        <p class="lead">Select a metric below to view the detailed checks, or see the latest Z-Score summary below.</p>
        <div class="row row-cols-1 row-cols-md-2 row-cols-lg-3 g-4">
            {% for metric in metrics %}
            <div class="col">
                <div class="card h-100 metric-card">
                    <div class="card-body d-flex flex-column">
                        <h5 class="card-title">{{ metric }}</h5>
                        <p class="card-text flex-grow-1">View details for {{ metric }}.</p>
                        {# Generate the URL using url_for #}
                        {% set metric_url = url_for('metric.metric_page', metric_name=metric) %}
                        <a href="{{ metric_url }}" class="btn btn-primary metric-link">View Details</a>
                        {# Debug: Display the generated URL #}
                        <span class="text-muted small mt-1">Debug URL: {{ metric_url }}</span>
                    </div>
                </div>
            </div>
            {% endfor %}
        </div>
    </div>
    <!-- Z-Score Summary Table -->
    {% if not summary_data.empty %}
    <h2>Latest Change Z-Score Summary</h2>
    <div class="table-responsive"> <!-- Make table scrollable on small screens -->
        <table class="table table-striped table-bordered table-hover table-sm">
            <thead class="thead-light"> {# thead-light might not be standard BS5, but harmless #}
                <tr>
                    <th>Fund Code</th>
                    {# Use the new summary_metrics list which contains combined names #}
                    {% for full_metric_name in summary_metrics %}
                    <th>{{ full_metric_name }}</th>
                    {% endfor %}
                </tr>
            </thead>
            <tbody>
                {% for fund_code, row in summary_data.iterrows() %}
                <tr>
                    {# Corrected url_for to point to the general fund detail page #}
                    <td><a href="{{ url_for('fund.fund_detail', fund_code=fund_code) }}" title="View all metrics for {{ fund_code }}">{{ fund_code }}</a></td>
                    {# Iterate through the same new list for data access #}
                    {% for full_metric_name in summary_metrics %}
                        {% set z_score = row[full_metric_name] %}
                        {% if z_score is none or z_score != z_score %}
                            {# Handle NaN/None - use base.html styling implicitly #}
                            <td class="text-muted fst-italic">N/A</td> {# Using BS5 classes #}
                        {% else %}
                            {# Apply conditional styling based on Z-score value - Use classes defined in base.html #}
                            {% set z_abs = z_score|abs %}
                            {% set cell_class = '' %}
                            {% if z_abs > 3.0 %}
                                {% set cell_class = 'table-danger' %}
                            {% elif z_abs > 2.0 %}
                                {% set cell_class = 'table-warning' %}
                            {% endif %}
                            <td class="{{ cell_class }}">{{ "%.2f"|format(z_score) }}</td>
                        {% endif %}
                    {% endfor %}
                </tr>
                {% endfor %}
            </tbody>
        </table>
    </div>
    {% else %}
    <div class="alert alert-warning" role="alert">
        No Z-score data could be generated for the summary table. Check the console logs for errors.
    </div>
    {% endif %}
    <!-- Add a link to the new API Data Retrieval page - Kept original url_for -->
    <div class="mt-4 mb-4 p-3 border rounded bg-light">
        <h5>Get Data via API (Simulated)</h5>
        <p>Select funds and dates to simulate retrieving data from the Rex API.</p>
        <a href="{{ url_for('api_bp.get_data_page') }}" class="btn btn-success btn-sm">Go to Get Data Page</a>
    </div>
    <!-- Add a link to the Securities page - url_for already matches base.html -->
    <div class="mt-4 p-3 border rounded bg-light">
        <h5>Securities Data Check</h5>
        <p>View checks for individual securities based on latest daily changes.</p>
        <a href="{{ url_for('security.securities_page') }}" class="btn btn-info btn-sm">View Securities Check</a>
    </div>
{% endblock %}
{% block scripts %}
{# Add any page-specific scripts here if needed in the future #}
{% endblock %}
</file>

<file path="templates/metric_page_js.html">
<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <title>{{ metric_name }} Check (JS)</title>
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
    <!-- Include Chart.js and the date adapter -->
    <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-adapter-date-fns/dist/chartjs-adapter-date-fns.bundle.min.js"></script>
    <style>
        body { padding-top: 5rem; }
        .chart-container-wrapper { margin-bottom: 15px; padding: 10px; border: 1px solid #eee; }
        .chart-canvas { max-height: 400px; }
        .metrics-table { margin-top: 15px; margin-bottom: 25px; font-size: 0.9em; }
        .metrics-table th, .metrics-table td { padding: 4px 8px; border: 1px solid #dee2e6; }
        .missing-warning { color: red; font-weight: bold; }
        .high-z { background-color: #fff3cd; }
        .very-high-z { background-color: #f8d7da; font-weight: bold; }
    </style>
</head>
<body>
    <nav class="navbar navbar-expand-md navbar-dark bg-dark fixed-top">
        <a class="navbar-brand" href="/">Data Verification</a>
        <!-- Navbar content -->
    </nav>
    <main role="main" class="container">
        <h1>{{ metric_name }} Check</h1>
        <p>Latest Data Date: <strong>{{ latest_date }}</strong></p>
        <p>Charts sorted by the maximum absolute <strong>Change Z-Score</strong> across all columns for the fund (most deviation first).</p>
        <!-- Toggle Switch for S&P Data -->
        <div id="sp-toggle-container" class="form-group" style="display: none;"> <!-- Initially hidden, JS will show if needed -->
            <div class="form-check form-switch"><input class="form-check-input" type="checkbox" role="switch" id="toggleSpData">
                <label class="form-check-label" for="toggleSpData">Show S&P Comparison Data</label>
            </div>
        </div>
        <!-- End Toggle Switch -->
        {% if not missing_funds.empty %}
            <div class="alert alert-warning" role="alert">
                <strong>Warning:</strong> The following funds are missing data for the latest date ({{ latest_date }}):
                {{ missing_funds.index.tolist() | join(', ') }}
            </div>
        {% endif %}
        {% if error_message %}
        <div class="alert alert-danger" role="alert">
          {{ error_message }}
        </div>
        {% endif %}
        <!-- Data passed from Flask, embedded as JSON -->
        <script type="application/json" id="chartData">
            {{ charts_data_json | safe }}
        </script>
        <div id="chartsArea">
            <!-- Charts will be rendered here by JavaScript -->
        </div>
        <!-- END: Metrics Table -->
    </main>
    <!-- Link to the external JavaScript module -->
    <script type="module" src="{{ url_for('static', filename='js/main.js') }}"></script>
    <!-- Bootstrap JS (optional) -->
    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.5.4/dist/umd/popper.min.js"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"></script>
</body>
</html>
</file>

<file path="templates/securities_page.html">
lockdown-install.js:1 Removing unpermitted intrinsics
Alpha002:181 Initializing Chart with original data...
main.js:29 DOM fully loaded and parsed
{% extends 'base.html' %}
{% block title %}Security Data Check{% endblock %}
{% block head_extra %}
  {# Link the new CSS file #}
  <link rel="stylesheet" href="{{ url_for('static', filename='css/style.css') }}">
{% endblock %}
{% block content %}
<div class="container-fluid mt-4"> {# Use container-fluid for wider tables #}
    <div class="d-flex justify-content-between align-items-center mb-3">
        <h2>Security Data Check</h2>
        <a href="{{ url_for('exclusion_bp.manage_exclusions') }}" class="btn btn-outline-secondary btn-sm">Manage Exclusions</a>
    </div>
    <p class="text-muted">Potential data issues based on the latest daily change Z-score. Filters and sorting are applied server-side.</p>
    {% if message %}
    <div class="alert alert-warning alert-dismissible fade show" role="alert">
        {{ message }}
        <button type="button" class="btn-close" data-bs-dismiss="alert" aria-label="Close"></button>
    </div>
    {% endif %}
    {# --- Search and Filter Form --- #}
    {# Combined into one form submitting GET requests #}
    <form method="GET" action="{{ url_for('security.securities_page') }}" class="mb-3 p-3 border rounded bg-light" id="filter-form">
        <div class="row g-2 align-items-end">
            {# Search Box #}
            <div class="col-md-4">
                <label for="search_term" class="form-label">Search by {{ id_col_name }}</label>
                <input type="text" name="search_term" id="search_term" class="form-control form-control-sm" placeholder="Enter search term..." value="{{ search_term or '' }}">
            </div>
            {# Dynamic Filters #}
            {% if filter_options %}
                {% for column, options in filter_options.items() %}
                <div class="col-md-2">
                    <label for="filter-{{ column|replace(' ', '_') }}" class="form-label">{{ column }}</label>
                    <select id="filter-{{ column|replace(' ', '_') }}" name="filter_{{ column }}" class="form-select form-select-sm">
                        <option value="">All</option>
                        {% for option in options %}
                        <option value="{{ option }}" {% if active_filters.get(column) == option|string %}selected{% endif %}>{{ option }}</option>
                        {% endfor %}
                    </select>
                </div>
                {% endfor %}
            {% endif %}
            {# Buttons #}
            <div class="col-md-auto">
                <button class="btn btn-primary btn-sm" type="submit">Apply Filters</button>
                {# Clear button redirects to the base URL without filters/search #}
                 {% if search_term or active_filters %}
                    <a href="{{ url_for('security.securities_page') }}" class="btn btn-secondary btn-sm">Clear All</a>
                {% endif %}
            </div>
        </div>
        {# Hidden fields to preserve pagination/sorting state if needed (though typically sorting/filtering resets page to 1) #}
        {# <input type="hidden" name="sort_by" value="{{ current_sort_by }}"> #}
        {# <input type="hidden" name="sort_order" value="{{ current_sort_order }}"> #}
    </form>
    {# --- Data Table Section --- #}
    {% if securities_data %}
    <div class="table-responsive">
        <table class="table table-striped table-hover table-sm small caption-top" id="securities-table">
            {# Add table caption for summary #}
            {% if pagination %}
            <caption class="pb-1">
                Displaying {{ securities_data|length }} of {{ pagination.total_items }} total securities. 
                (Page {{ pagination.page }} of {{ pagination.total_pages }})
            </caption>
            {% endif %}
            <thead class="table-light sticky-top"> {# Make header sticky #}
                <tr>
                    {# Generate sortable headers #}
                    {% for col_name in column_order %}
                        {% set is_sort_col = (col_name == current_sort_by or (col_name == 'Change Z-Score' and current_sort_by is none)) %}
                        {# Determine next sort order: flip if current column, default to asc otherwise #}
                        {% set next_sort_order = 'asc' if is_sort_col and current_sort_order == 'desc' else 'desc' %}
                        {# Base arguments, including current search and filters #}
                        {% set sort_args = request.args.to_dict() %}
                        {% set _ = sort_args.pop('page', None) %}
                        {% set _ = sort_args.update({'sort_by': col_name, 'sort_order': next_sort_order}) %}
                        {# Construct the URL for the sort link #}
                        {% set sort_url = url_for('security.securities_page', **sort_args) %}
                        {# Add classes for styling and potential JS hooks #}
                        <th class="sortable {{ 'sorted-' + current_sort_order if is_sort_col else '' }}" 
                            data-column-name="{{ col_name }}">
                            <a href="{{ sort_url }}" class="text-decoration-none text-dark">
                                {{ col_name }} 
                                {% if is_sort_col %}
                                    <span class="sort-indicator ms-1">{{ '▲' if current_sort_order == 'asc' else '▼' }}</span>
                                {% endif %}
                            </a>
                        </th>
                    {% endfor %}
                </tr>
            </thead>
            <tbody id="securities-table-body">
                {% for row in securities_data %}
                    {% set z_score = row['Change Z-Score'] %}
                    {% set abs_z_score = z_score|abs if z_score is not none else 0 %} {# Calculate here #}
                    {% set row_class = 'table-danger' if abs_z_score >= 3 else ('table-warning' if abs_z_score >= 2 else '') %}
                    <tr class="{{ row_class }}">
                        {% for col_name in column_order %}
                            <td>
                                {# Link for ID column #}
                                {% if col_name == id_col_name %}
                                    <a href="{{ url_for('security.security_details_page', metric_name='Spread', security_id=row[col_name]|urlencode) }}">
                                        {{ row[col_name] }}
                                    </a>
                                {# Formatting for numeric columns #}
                                {% elif col_name == 'Change Z-Score' and row[col_name] is not none %}
                                    {{ "%.2f"|format(row[col_name]) }}
                                {% elif row[col_name] is number %}
                                    {# Consider if specific formatting is needed for other numbers #}
                                    {{ row[col_name]|round(3) }}
                                {# Display other values (string, None) #}
                                {% else %}
                                    {{ row[col_name] if row[col_name] is not none else '' }} {# Display empty for None #}
                                {% endif %}
                            </td>
                        {% endfor %}
                    </tr>
                {% endfor %}
            </tbody>
        </table>
    </div>
    {# --- Pagination Controls --- #}
    {% if pagination and pagination.total_pages > 1 %}
        <nav aria-label="Security data navigation">
            <ul class="pagination pagination-sm justify-content-center">
                {# Previous Page Link #}
                <li class="page-item {{ 'disabled' if not pagination.has_prev }}">
                    <a class="page-link" href="{{ pagination.url_for_page(pagination.prev_num) if pagination.has_prev else '#' }}" aria-label="Previous">
                        <span aria-hidden="true">&laquo;</span>
                    </a>
                </li>
                {# Page Number Links (using context variables calculated in view) #}
                 {% set start_page = pagination.start_page_display %}
                 {% set end_page = pagination.end_page_display %}
                 {% if start_page > 1 %}
                     <li class="page-item"><a class="page-link" href="{{ pagination.url_for_page(1) }}">1</a></li>
                     {% if start_page > 2 %}
                         <li class="page-item disabled"><span class="page-link">...</span></li>
                     {% endif %}
                 {% endif %}
                 {% for p in range(start_page, end_page + 1) %}
                    <li class="page-item {{ 'active' if p == pagination.page }}">
                        <a class="page-link" href="{{ pagination.url_for_page(p) }}">{{ p }}</a>
                    </li>
                {% endfor %}
                 {% if end_page < pagination.total_pages %}
                     {% if end_page < pagination.total_pages - 1 %}
                         <li class="page-item disabled"><span class="page-link">...</span></li>
                     {% endif %}
                     <li class="page-item"><a class="page-link" href="{{ pagination.url_for_page(pagination.total_pages) }}">{{ pagination.total_pages }}</a></li>
                 {% endif %}
                {# Next Page Link #}
                <li class="page-item {{ 'disabled' if not pagination.has_next }}">
                    <a class="page-link" href="{{ pagination.url_for_page(pagination.next_num) if pagination.has_next else '#' }}" aria-label="Next">
                        <span aria-hidden="true">&raquo;</span>
                    </a>
                </li>
            </ul>
        </nav>
    {% endif %}
    {# Handle case where filters resulted in no data (message displayed above) #}
    {% elif not message %}
     <div class="alert alert-info mt-3" role="alert">
        No security metrics data is currently available or matches the selected criteria.
    </div>
    {% endif %}
</div>
{% endblock %}
{% block scripts %}
{{ super() }}
{# Remove client-side filter script block - All filtering/sorting is server-side #}
{# Optional: Add JS for minor enhancements like highlighting sort column, but core logic is server-side #}
{% endblock %}
</file>

<file path="templates/security_details_page.html">
{% extends 'base.html' %}
{% block title %}Security Details: {{ security_id }} - {{ metric_name }}{% endblock %}
{% block content %}
<div class="container mt-4">
    <h1>{{ security_id }} - {{ metric_name }}</h1>
    <p>Latest data as of: <strong>{{ latest_date }}</strong></p>
    {# Display Static Info #}
    {% if static_info %}
    <div class="mb-3 p-3 border rounded bg-light small">
        <strong>Static Details:</strong> 
        {% for key, value in static_info.items() %}
            <span class="me-3">{{ key }}: {{ value }}</span>
        {% endfor %}
    </div>
    {% endif %}
    {# Primary Chart Area (Metric + Price Overlay) #}
    <h2>{{ metric_name }} and Price Time Series</h2>
    <div id="primary-chart-container" class="mb-4" style="height: 450px; position: relative;">
        <canvas id="primarySecurityChart"></canvas>
    </div>
    {# Duration Chart Area #}
    <h2>Duration Time Series</h2>
    <div id="duration-chart-container" class="mb-4" style="height: 450px; position: relative;">
        <canvas id="durationSecurityChart"></canvas>
    </div>
    {# Hidden script tag for chart data #}
    <script id="chartJsonData" type="application/json">
        {{ chart_data_json|safe }}
    </script>
</div>
{% endblock %}
{% block scripts %}
{# Include Chart.js library (Likely inherited from base.html, but included here for safety/explicitness if needed) #}
{# If base.html already includes it, this line can be removed #}
<script src="https://cdn.jsdelivr.net/npm/chart.js"></script> 
<script>
    document.addEventListener('DOMContentLoaded', function() {
        // Get the chart data passed from Flask
        const chartDataElement = document.getElementById('chartJsonData');
        const chartData = JSON.parse(chartDataElement.textContent);
        // --- Render Primary Chart (Metric + Price) ---
        const primaryCtx = document.getElementById('primarySecurityChart').getContext('2d');
        if (chartData.primary_datasets && chartData.primary_datasets.length > 0) {
            new Chart(primaryCtx, {
                type: 'line',
                data: {
                    labels: chartData.labels,
                    datasets: chartData.primary_datasets // Use the primary datasets array
                },
                options: {
                    responsive: true,
                    maintainAspectRatio: false,
                    scales: {
                        x: {
                            title: {
                                display: true,
                                text: 'Date'
                            }
                        },
                        y: { // Primary Y-axis (for the main metric)
                            position: 'left',
                            title: {
                                display: true,
                                text: {{ metric_name|tojson }} + ' Value' // Use the metric name dynamically with tojson filter
                            }
                        },
                        y1: { // Secondary Y-axis (for Price)
                            position: 'right',
                            title: {
                                display: true,
                                text: 'Price'
                            },
                            grid: {
                                drawOnChartArea: false, // only draw grid lines for the first Y axis
                            },
                        }
                    },
                    plugins: {
                        legend: {
                            position: 'top',
                        },
                        tooltip: {
                            mode: 'index',
                            intersect: false,
                        }
                    }
                }
            });
        } else {
            console.error('No primary datasets found for the primary chart.');
            document.getElementById('primary-chart-container').innerHTML = '<p class="text-danger">Could not render primary chart: No data available.</p>';
        }
        // --- Render Duration Chart ---
        const durationCtx = document.getElementById('durationSecurityChart').getContext('2d');
        if (chartData.duration_dataset) {
             // Check if there's actual data in the duration dataset
            const hasDurationData = chartData.duration_dataset.data && chartData.duration_dataset.data.some(value => value !== null);
            if (hasDurationData) {
                new Chart(durationCtx, {
                    type: 'line',
                    data: {
                        labels: chartData.labels,
                        datasets: [chartData.duration_dataset] // Pass the single duration dataset in an array
                    },
                    options: {
                        responsive: true,
                        maintainAspectRatio: false,
                        scales: {
                            x: {
                                title: {
                                    display: true,
                                    text: 'Date'
                                }
                            },
                            y: { // Single Y-axis for duration
                                title: {
                                    display: true,
                                    text: 'Duration Value'
                                }
                            }
                        },
                        plugins: {
                            legend: {
                                position: 'top',
                            },
                            tooltip: {
                                mode: 'index',
                                intersect: false,
                            }
                        }
                    }
                });
            } else {
                 console.log('No non-null data points found for the duration chart.');
                 document.getElementById('duration-chart-container').innerHTML = '<p class="text-info">No duration data available for this security over the selected period.</p>';
            }
        } else {
            console.log('Duration dataset not provided or empty.');
            document.getElementById('duration-chart-container').innerHTML = '<p class="text-info">Duration data not available for this security.</p>';
        }
    });
</script>
{% endblock %}
</file>

<file path="templates/spread_duration_comparison_details_page.html">
{% extends "base.html" %}
{# Note: security_name is not explicitly passed, using security_id for title/breadcrumb #}
{% block title %}Spread Duration Comparison Details: {{ security_id }}{% endblock %}
{% block content %}
<div class="container mt-4">
    <nav aria-label="breadcrumb">
        <ol class="breadcrumb">
            <li class="breadcrumb-item"><a href="{{ url_for('spread_duration_comparison_bp.summary') }}">Spread Duration Comparison Summary</a></li> {# Updated Link #}
            {# Displaying ID as primary identifier if name isn't guaranteed #}
            <li class="breadcrumb-item active" aria-current="page">{{ security_id }}</li>
        </ol>
    </nav>
    <h1>Spread Duration Comparison Details: {{ security_id }}</h1> {# Updated Title #}
    {# Add static info if available #}
    {% if static_info %}
        {% for key, value in static_info.items() %}
             {% if key != id_column_name %} {# Avoid repeating the ID #}
                <span class="text-muted me-3"><strong>{{ key }}:</strong> {{ value }}</span>
             {% endif %}
        {% endfor %}
    {% endif %}
    <div class="row mt-4 mb-4">
        <div class="col-md-6">
            <h2>Comparison Statistics</h2>
            {% if stats_summary %}
                <ul class="list-group">
                    <li class="list-group-item d-flex justify-content-between align-items-center">
                        Level Correlation
                        <span class="badge bg-primary rounded-pill">{{ stats_summary.Level_Correlation if stats_summary.Level_Correlation is not none else 'N/A' }}</span>
                    </li>
                    <li class="list-group-item d-flex justify-content-between align-items-center">
                        Change Correlation
                        <span class="badge bg-primary rounded-pill">{{ stats_summary.Change_Correlation if stats_summary.Change_Correlation is not none else 'N/A' }}</span>
                    </li>
                    <li class="list-group-item d-flex justify-content-between align-items-center">
                        Mean Absolute Difference
                        <span class="badge bg-secondary rounded-pill">{{ stats_summary.Mean_Abs_Diff if stats_summary.Mean_Abs_Diff is not none else 'N/A' }}</span>
                    </li>
                    <li class="list-group-item d-flex justify-content-between align-items-center">
                        Max Absolute Difference
                        <span class="badge bg-secondary rounded-pill">{{ stats_summary.Max_Abs_Diff if stats_summary.Max_Abs_Diff is not none else 'N/A' }}</span>
                    </li>
                    <li class="list-group-item d-flex justify-content-between align-items-center">
                        Data Points (Original)
                        <span class="badge bg-info rounded-pill">{{ stats_summary.Total_Points - stats_summary.NaN_Count_Orig }} / {{ stats_summary.Total_Points }}</span>
                    </li>
                    <li class="list-group-item d-flex justify-content-between align-items-center">
                        Data Points (New)
                        <span class="badge bg-info rounded-pill">{{ stats_summary.Total_Points - stats_summary.NaN_Count_New }} / {{ stats_summary.Total_Points }}</span>
                    </li>
                    <li class="list-group-item d-flex justify-content-between align-items-center">
                        Same Date Range?
                        <span class="badge {{ 'bg-success' if stats_summary.Same_Date_Range else 'bg-warning' }} rounded-pill">{{ 'Yes' if stats_summary.Same_Date_Range else 'No' }}</span>
                    </li>
                    {# Add date range details #}
                    <li class="list-group-item">
                        <small>Orig Range: {{ stats_summary.Start_Date_Orig or 'N/A' }} to {{ stats_summary.End_Date_Orig or 'N/A' }}</small><br>
                        <small>New Range: {{ stats_summary.Start_Date_New or 'N/A' }} to {{ stats_summary.End_Date_New or 'N/A' }}</small>
                    </li>
                </ul>
            {% else %}
                <p>No comparison statistics could be calculated.</p>
            {% endif %}
        </div>
        {# Placeholder for additional stats or info if needed #}
    </div>
    <h2>Time Series Comparison</h2>
    <p class="text-muted">Overlayed Spread Duration from Original (sec_Spread duration) and New (sec_Spread durationSP) datasets.</p> {# Updated Text #}
    <div>
        <canvas id="comparisonChart"></canvas>
    </div>
    {# Embed chart data as JSON for JavaScript #}
    <script type="application/json" id="comparisonChartData">
        {{ chart_data | tojson | safe }}
    </script>
</div>
{% endblock %}
{% block scripts %}
{{ super() }}
{# We need Chart.js - ensure it's included in base.html or here #}
<script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
<script src="https://cdn.jsdelivr.net/npm/chartjs-adapter-date-fns/dist/chartjs-adapter-date-fns.bundle.min.js"></script> {# Include Date Adapter #}
<script>
    document.addEventListener('DOMContentLoaded', function() {
        const chartDataElement = document.getElementById('comparisonChartData');
        const comparisonChartCanvas = document.getElementById('comparisonChart');
        if (chartDataElement && comparisonChartCanvas) {
            try {
                const chartData = JSON.parse(chartDataElement.textContent);
                const ctx = comparisonChartCanvas.getContext('2d');
                console.log("Initializing Spread Duration Comparison Chart...");
                new Chart(ctx, {
                    type: 'line',
                    data: chartData, // Direct use of data from JSON
                    options: {
                        responsive: true,
                        maintainAspectRatio: true,
                        plugins: {
                            legend: {
                                position: 'top',
                            },
                            title: {
                                display: true,
                                text: 'Spread Duration Comparison: {{ security_id|tojson }}' // Use security_id
                            },
                            tooltip: {
                                mode: 'index', // Show tooltips for all datasets at the same index
                                intersect: false
                            }
                        },
                        scales: {
                            x: {
                                type: 'time', // Use time scale
                                time: {
                                    unit: 'day',
                                    tooltipFormat: 'yyyy-MM-dd', // Format for tooltip
                                    displayFormats: { // Formats for axis labels
                                        day: 'MMM d, yyyy'
                                    }
                                },
                                title: {
                                    display: true,
                                    text: 'Date'
                                }
                            },
                            y: {
                                title: {
                                    display: true,
                                    text: 'Spread Duration' // Updated Axis Label
                                }
                            }
                        },
                        interaction: {
                             intersect: false,
                             mode: 'index',
                        },
                         elements: {
                            point:{ // Reduce point size for potentially dense data
                                radius: 2
                            }
                        }
                        // No complex scriptable options needed for basic display
                    }
                });
            } catch (error) {
                console.error("Error parsing spread duration chart data or rendering chart:", error);
                if (comparisonChartCanvas.parentElement) {
                    comparisonChartCanvas.parentElement.innerHTML = '<p class="text-danger">Error rendering spread duration chart.</p>';
                }
            }
        } else {
             console.warn("Chart data or canvas element not found for spread duration comparison chart.");
        }
    });
</script>
{% endblock %}
</file>

<file path="templates/spread_duration_comparison_page.html">
{% extends "base.html" %}
{% block title %}Spread Duration Comparison Summary{% endblock %}
{% block content %}
<div class="container-fluid mt-4"> {# Use container-fluid for wider view #}
    <h1>Spread Duration Comparison: Original (sec_Spread duration) vs. New (sec_Spread durationSP)</h1> {# Updated Title #}
    <p class="text-muted">Comparing Spread Duration between the two datasets. Click on a Security ID/Name to see details. Use filters or click column headers to sort. Pagination applied.</p> {# Updated Text #}
    {# Display message if any #}
    {% if message %}
    <div class="alert alert-warning alert-dismissible fade show" role="alert">
        {{ message }}
        <button type="button" class="btn-close" data-bs-dismiss="alert" aria-label="Close"></button>
    </div>
    {% endif %}
    {# --- Filter Form --- #}
    {% if filter_options %}
    <form method="GET" action="{{ url_for('spread_duration_comparison_bp.summary') }}" class="mb-3 p-3 border rounded bg-light" id="filter-form"> {# Updated Action URL #}
        <h5>Filters</h5>
        <div class="row g-2 align-items-end">
            {% for column, options in filter_options.items() %}
            <div class="col-md-2 mb-2">
                <label for="filter-{{ column }}" class="form-label">{{ column }}</label>
                <select id="filter-{{ column }}" name="filter_{{ column }}" class="form-select form-select-sm">
                    <option value="">All</option>
                    {% for option in options %}
                    <option value="{{ option }}" {% if active_filters.get(column) == option|string %}selected{% endif %}>{{ option }}</option>
                    {% endfor %}
                </select>
            </div>
            {% endfor %}
            <div class="col-md-auto">
                <button type="submit" class="btn btn-primary btn-sm">Apply Filters</button>
                {# Add a clear button only if filters are active #}
                {% if active_filters %}
                <a href="{{ url_for('spread_duration_comparison_bp.summary') }}" class="btn btn-secondary btn-sm">Clear Filters</a> {# Updated Clear URL #}
                {% endif %}
            </div>
        </div>
        {# Hidden fields to preserve current sort order when applying filters - page is implicitly reset #}
        <input type="hidden" name="sort_by" value="{{ current_sort_by }}">
        <input type="hidden" name="sort_order" value="{{ current_sort_order }}">
    </form>
    {% endif %}
    {# --- Data Table --- #}
    <div class="table-responsive">
        <table class="table table-striped table-hover table-sm caption-top" id="spread-duration-comparison-table"> {# Updated Table ID #}
             {# Add table caption for summary #}
             {% if pagination %}
             <caption class="pb-1">
                 Displaying {{ table_data|length }} of {{ pagination.total_items }} total securities.
                 (Page {{ pagination.page }} of {{ pagination.total_pages }})
             </caption>
             {% endif %}
            <thead class="table-light sticky-top">
                <tr>
                    {# Loop through the columns passed from the view #}
                    {% for col_name in columns_to_display %}
                        {% set is_sort_col = (col_name == current_sort_by) %}
                        {% set next_sort_order = 'asc' if is_sort_col and current_sort_order == 'desc' else 'desc' %}
                        {# Base arguments, including current filters #}
                        {% set sort_args = request.args.to_dict() %}
                        {% set _ = sort_args.pop('page', None) %}
                        {% set _ = sort_args.update({'sort_by': col_name, 'sort_order': next_sort_order}) %}
                        {# Generate URL for this header #}
                        {% set sort_url = url_for('spread_duration_comparison_bp.summary', **sort_args) %} {# Updated Sort URL #}
                        {# Add classes for styling and JS #}
                        <th class="sortable {{ 'sorted-' + current_sort_order if is_sort_col else '' }}"
                            data-column-name="{{ col_name }}">
                            <a href="{{ sort_url }}" class="text-decoration-none text-dark">
                                {{ col_name.replace('_', ' ') | title }}
                                {% if is_sort_col %}
                                    <span class="sort-indicator ms-1">{{ '▲' if current_sort_order == 'asc' else '▼' }}</span>
                                {% endif %}
                            </a>
                        </th>
                    {% endfor %}
                </tr>
            </thead>
            <tbody id="spread-duration-comparison-table-body"> {# Updated tbody ID #}
                {% set id_col = id_column_name %}
                {% for row in table_data %}
                <tr>
                    {# Loop through the same columns to ensure order matches header #}
                    {% for col_name in columns_to_display %}
                        <td>
                            {% if col_name == id_col %}
                                <a href="{{ url_for('spread_duration_comparison_bp.details', security_id=row[id_col]|urlencode) }}">{{ row[id_col] }}</a> {# Updated Detail URL #}
                            {% elif col_name in ['Level_Correlation', 'Change_Correlation'] and row[col_name] is not none %}
                                {# Attempt to format as float, handle potential errors gracefully #}
                                {% set formatted_val = "%.3f"|format(row[col_name]|float) if row[col_name] is number else row[col_name] %}
                                {{ formatted_val }}
                            {% elif col_name in ['Mean_Abs_Diff', 'Max_Abs_Diff'] and row[col_name] is not none %}
                                {% set formatted_val = "%.2f"|format(row[col_name]|float) if row[col_name] is number else row[col_name] %}
                                {{ formatted_val }}
                             {% elif col_name == 'Same_Date_Range' %}
                                 <span class="badge {{ 'bg-success' if row[col_name] else 'bg-warning' }}">{{ 'Yes' if row[col_name] else 'No' }}</span>
                            {% elif col_name.endswith('_Date') and row[col_name] %}
                                 {# Assume date strings are already YYYY-MM-DD from view #}
                                {{ row[col_name] }}
                            {% elif row[col_name] is number %}
                                {{ row[col_name]|round(3) }} {# General numeric formatting #}
                            {% else %}
                                {{ row[col_name] if row[col_name] is not none else '' }} {# Display strings or empty #}
                            {% endif %}
                        </td>
                    {% endfor %}
                </tr>
                {% else %}
                <tr>
                    <td colspan="{{ columns_to_display|length }}" class="text-center">No spread duration comparison data available matching the current filters.</td> {# Updated Message #}
                </tr>
                {% endfor %}
            </tbody>
        </table>
    </div>
    {# --- Pagination Controls --- #}
    {% if pagination and pagination.total_pages > 1 %}
    <nav aria-label="Spread duration comparison data navigation">
        <ul class="pagination pagination-sm justify-content-center">
            {# Helper macro for generating pagination links #}
            {% macro page_link(page_num, text=None, is_disabled=False, is_active=False) %}
                {% set link_args = request.args.to_dict() %}
                {% set _ = link_args.update({'page': page_num, 'sort_by': current_sort_by, 'sort_order': current_sort_order}) %}
                {% set url = url_for('spread_duration_comparison_bp.summary', **link_args) if page_num else '#' %} {# Updated URL #}
                <li class="page-item {{ 'disabled' if is_disabled }} {{ 'active' if is_active }}">
                    <a class="page-link" href="{{ url }}" {% if is_active %}aria-current="page"{% endif %}>{{ text or page_num }}</a>
                </li>
            {% endmacro %}
            {{ page_link(pagination.prev_num, '&laquo;', is_disabled=not pagination.has_prev) }}
            {# Simplified pagination display logic #}
            {% set window = 2 %}
            {% set start_page = [1, pagination.page - window] | max %}
            {% set end_page = [pagination.total_pages, pagination.page + window] | min %}
            {% if start_page > 1 %}
                {{ page_link(1) }}
                {% if start_page > 2 %}
                    <li class="page-item disabled"><span class="page-link">...</span></li>
                {% endif %}
            {% endif %}
            {% for p in range(start_page, end_page + 1) %}
                {{ page_link(p, is_active=(p == pagination.page)) }}
            {% endfor %}
            {% if end_page < pagination.total_pages %}
                {% if end_page < pagination.total_pages - 1 %}
                    <li class="page-item disabled"><span class="page-link">...</span></li>
                {% endif %}
                {{ page_link(pagination.total_pages) }}
            {% endif %}
            {{ page_link(pagination.next_num, '&raquo;', is_disabled=not pagination.has_next) }}
        </ul>
    </nav>
    {% endif %}
</div>
{% endblock %}
{% block scripts %}
{{ super() }}
{# No specific JS needed for this page unless client-side sorting is added back #}
{% endblock %}
</file>

<file path="templates/weight_check_page.html">
{% extends "base.html" %}
{% block title %}Weight Check (100% Target){% endblock %}
{% block content %}
<div class="container-fluid mt-4">
    <h1 class="mb-4">Weight Check (100% Target)</h1>
    <p>The following tables show weights from <code>{{ fund_filename }}</code> and <code>{{ bench_filename }}</code>. Cells highlighted in <span class="text-danger fw-bold">red</span> indicate weights that are not exactly 100.00%.</p>
    {% macro render_weight_table(title, filename, data, date_headers) %}
        <h2 class="mt-4">{{ title }} <small class="text-muted fs-6">({{ filename }})</small></h2>
        {% if data %}
            <div class="table-responsive">
                <table class="table table-bordered table-sm table-hover weight-check-table">
                    <thead class="thead-light">
                        <tr>
                            <th>Fund Code</th>
                            {% for date in date_headers %}
                                <th class="text-center">{{ date }}</th>
                            {% endfor %}
                        </tr>
                    </thead>
                    <tbody>
                        {% for fund_code, date_values in data.items()|sort %}
                            <tr>
                                <td class="fw-bold">{{ fund_code }}</td>
                                {% for date in date_headers %}
                                    {% set cell_data = date_values.get(date) %}
                                    {% if cell_data %}
                                        {% set value_str = cell_data.value_str %}
                                        {% set is_100 = cell_data.is_100 %}
                                        <td class="text-center {{ 'table-danger' if not is_100 }}">
                                            {{ value_str }}
                                        </td>
                                    {% else %}
                                        <td class="text-center text-muted">-</td> {# Data missing for this date #}
                                    {% endif %}
                                {% endfor %}
                            </tr>
                        {% else %}
                            <tr>
                                <td colspan="{{ date_headers|length + 1 }}" class="text-center">No data found in {{ filename }}.</td>
                            </tr>
                        {% endfor %}
                    </tbody>
                </table>
            </div>
        {% else %}
            <div class="alert alert-warning" role="alert">
                Could not load or process data from <code>{{ filename }}</code>. Check server logs.
            </div>
        {% endif %}
    {% endmacro %}
    {{ render_weight_table("Fund Weights", fund_filename, fund_data, date_headers) }}
    {{ render_weight_table("Benchmark Weights", bench_filename, bench_data, date_headers) }}
</div>
<style>
/* Optional: Adjust table layout for wide tables */
.weight-check-table th,
.weight-check-table td {
    white-space: nowrap; /* Prevent wrapping in cells */
    font-size: 0.85rem; /* Slightly smaller font */
    padding: 0.3rem 0.5rem; /* Adjust padding */
}
.weight-check-table th:first-child,
.weight-check-table td:first-child {
    position: sticky;
    left: 0;
    background-color: #f8f9fa; /* Light background for sticky column */
    z-index: 1;
}
.weight-check-table thead th {
    position: sticky;
    top: 0; /* Stick headers to top */
    background-color: #e9ecef;
    z-index: 2;
}
</style>
{% endblock %}
</file>

<file path="utils.py">
# This file contains utility functions used throughout the Simple Data Checker application.
# These functions provide common helper functionalities like parsing specific string formats
# or validating data types, helping to keep the main application logic cleaner.
"""
Utility functions for the Flask application.
"""
import re
import pandas as pd
def _is_date_like(column_name):
    """Check if a column name looks like a date (e.g., YYYY-MM-DD or DD/MM/YYYY).
    Updated regex to match both common formats.
    Ensures the pattern matches the entire string.
    """
    # Regex explanation:
    # ^            - Start of string
    # (\d{4}-\d{2}-\d{2}) - Group 1: YYYY-MM-DD format
    # |            - OR
    # (\d{2}/\d{2}/\d{4}) - Group 2: DD/MM/YYYY format
    # $            - End of string
    pattern = r'^((\d{4}-\d{2}-\d{2})|(\d{2}/\d{2}/\d{4}))$'
    return bool(re.match(pattern, str(column_name)))
def parse_fund_list(fund_string):
    """Safely parses the fund list string like '[FUND1,FUND2]' or '[FUND1]' into a list.
       Handles potential errors and variations in spacing.
    """
    if not isinstance(fund_string, str) or not fund_string.startswith('[') or not fund_string.endswith(']'):
        return [] # Return empty list if format is unexpected
    try:
        # Remove brackets and split by comma
        content = fund_string[1:-1]
        # Split by comma, strip whitespace from each element
        funds = [f.strip() for f in content.split(',') if f.strip()]
        return funds
    except Exception as e:
        print(f"Error parsing fund string '{fund_string}': {e}")
        return []
</file>

<file path="views/__init__.py">
"""
This file makes the 'views' directory a Python package.
"""
# You can leave this file empty or use it to import blueprints
# for easier registration in the app factory, though explicit imports
# in the factory as done currently are also perfectly fine.
</file>

<file path="views/api_views.py">
'''
Defines the Flask Blueprint for handling API data retrieval requests, including simulation,
fetching real data, saving data (with options for merging or overwriting),
and rerunning specific API calls.
'''
import os
import pandas as pd
from flask import Blueprint, render_template, request, current_app, jsonify
import datetime
from pandas.tseries.offsets import BDay
import time # Import the time module
#from tqs import tqs_query as tqs
# # Import the placeholder validation function
from data_validation import validate_data
# --- Feature Switch --- 
# Set to True to attempt real API calls, validation, and saving.
# Set to False to only simulate the API call (print to console).
USE_REAL_TQS_API = False
# ----------------------
# Blueprint Configuration
api_bp = Blueprint(
    'api_bp', __name__,
    template_folder='../templates',
    static_folder='../static'
)
def _simulate_and_print_tqs_call(QueryID, FundCodeList, StartDate, EndDate):
    '''Simulates calling the TQS API by printing the call signature.
    This function is used when USE_REAL_TQS_API is False.
    It does NOT interact with any external API.
    Returns:
        int: A simulated number of rows for status reporting.
    '''
    # Format the call signature exactly as requested: tqs(QueryID,[FundList],StartDate,EndDate)
    call_signature = f"tqs({QueryID}, {FundCodeList}, {StartDate}, {EndDate})"
    print(f"--- SIMULATING TQS API CALL (USE_REAL_TQS_API = False) ---")
    print(call_signature)
    print(f"--------------------------------------------------------")
    # Return a simulated row count for the summary table
    simulated_row_count = len(FundCodeList) * 10 if FundCodeList else 0 # Dummy calculation
    return simulated_row_count
def _fetch_real_tqs_data(QueryID, FundCodeList, StartDate, EndDate):
    '''Fetches real data from the TQS API.
    This function is called when USE_REAL_TQS_API is True.
    Replace the placeholder logic with the actual API interaction code.
    Args:
        QueryID: The query identifier.
        FundCodeList: List of fund codes.
        StartDate: Start date string (YYYY-MM-DD).
        EndDate: End date string (YYYY-MM-DD).
    Returns:
        pd.DataFrame or None: The DataFrame containing the fetched data,
                              or None if the API call fails or returns no data.
    '''
    current_app.logger.info(f"Attempting real TQS API call for QueryID: {QueryID}")
    print(f"--- EXECUTING REAL TQS API CALL (USE_REAL_TQS_API = True) --- ")
    print(f"tqs({QueryID}, {FundCodeList}, {StartDate}, {EndDate})")
    print(f"--------------------------------------------------------")
    dataframe = None # Initialize dataframe to None
    try:
        # --- !!! Replace this comment and the line below with the actual API call !!! ---
        # Ensure the `tqs` function/library is imported (commented out at the top)
        # dataframe = tqs.get_data(QueryID, FundCodeList, StartDate, EndDate) # Example real call
        print(dataframe.head()) if dataframe is not None else print("No data to display")
        pass # Remove this pass when uncommenting the line above
        # --- End of section to replace --- 
        # Check if the API returned valid data (e.g., a DataFrame)
        if dataframe is not None and isinstance(dataframe, pd.DataFrame):
            current_app.logger.info(f"Real TQS API call successful for QueryID: {QueryID}, Rows: {len(dataframe)}")
            return dataframe
        elif dataframe is None:
             # Explicitly handle the case where the API call itself returned None (e.g., planned failure or empty result coded as None)
             current_app.logger.warning(f"Real TQS API call for QueryID: {QueryID} returned None.")
             return None
        else:
            # Handle cases where the API returned something unexpected (not a DataFrame)
            current_app.logger.warning(f"Real TQS API call for QueryID: {QueryID} returned an unexpected data type: {type(dataframe)}.")
            return None # Treat unexpected types as failure
    except NameError as ne:
         # Specific handling if the tqs function isn't defined (import is commented out)
         current_app.logger.error(f"Real TQS API call failed for QueryID: {QueryID}. TQS function not imported/defined. Error: {ne}")
         print(f"    ERROR: TQS function not available. Ensure 'from tqs import tqs_query as tqs' is uncommented and the library is installed.")
         return None
    except Exception as e:
        # Handle API call errors (timeout, connection issues, authentication, etc.)
        current_app.logger.error(f"Real TQS API call failed for QueryID: {QueryID}. Error: {e}", exc_info=True)
        print(f"    ERROR during real API call: {e}")
        return None
# --- Helper Function to Get File Statuses ---
def get_data_file_statuses(data_folder):
    """
    Scans the data folder based on QueryMap.csv and returns status for each file.
    """
    statuses = []
    query_map_path = os.path.join(data_folder, 'QueryMap.csv')
    if not os.path.exists(query_map_path):
        current_app.logger.warning(f"QueryMap.csv not found at {query_map_path} for status check.")
        return statuses # Return empty list if map is missing
    try:
        query_map_df = pd.read_csv(query_map_path)
        if 'FileName' not in query_map_df.columns:
             current_app.logger.warning(f"QueryMap.csv at {query_map_path} is missing 'FileName' column.")
             return statuses
        date_column_candidates = ['Date', 'date', 'AsOfDate', 'ASOFDATE', 'Effective Date', 'Trade Date', 'Position Date'] # Add more candidates if needed
        for index, row in query_map_df.iterrows():
            filename = row['FileName']
            file_path = os.path.join(data_folder, filename)
            status_info = {
                'filename': filename,
                'exists': False,
                'last_modified': 'N/A',
                'latest_data_date': 'N/A',
                'funds_included': 'N/A' # Initialize new key
            }
            if os.path.exists(file_path):
                status_info['exists'] = True
                try:
                    # Get file modification time
                    mod_timestamp = os.path.getmtime(file_path)
                    status_info['last_modified'] = datetime.datetime.fromtimestamp(mod_timestamp).strftime('%Y-%m-%d %H:%M:%S')
                    # Try to read the CSV and find the latest date
                    try:
                        df = pd.read_csv(file_path, low_memory=False) # low_memory=False can help with mixed types
                        df_head = df.head()
                        # Determine the actual date column name
                        date_col = None
                        # --- Update Date Column Candidates --- 
                        date_column_candidates = ['Date', 'date', 'AsOfDate', 'ASOFDATE', 'Effective Date', 'Trade Date', 'Position Date'] 
                        found_cols = df_head.columns.str.strip()
                        current_app.logger.info(f"[{filename}] Checking for date columns: {date_column_candidates} in columns {found_cols.tolist()}") 
                        for candidate in date_column_candidates:
                            # Case-insensitive check
                            matching_cols = [col for col in found_cols if col.lower() == candidate.lower()]
                            if matching_cols:
                                date_col = matching_cols[0] # Use the actual name found
                                current_app.logger.info(f"[{filename}] Found date column: '{date_col}'") 
                                break # Found the first match
                        if date_col:
                            try:
                                # --- FIX: Use the full DataFrame's date column --- 
                                if date_col not in df.columns:
                                    # Handle case where column name from head differs slightly after full read (e.g., whitespace)
                                    # Find it again in the full df columns, case-insensitively
                                    corrected_date_col = None
                                    for col in df.columns:
                                        if col.strip().lower() == date_col.lower():
                                            corrected_date_col = col
                                            break
                                    if not corrected_date_col:
                                         raise ValueError(f"Date column '{date_col}' found in header but not in full DataFrame columns: {df.columns.tolist()}")
                                    date_col = corrected_date_col # Use the name from the full df
                                date_series = df[date_col] # Use the full series from the complete DataFrame
                                # --------------------------------------------------
                                current_app.logger.info(f"[{filename}] Attempting to parse full date column '{date_col}' (length: {len(date_series)}). Top 5 values: {date_series.head().to_list()}") 
                                # Try standard YYYY-MM-DD first
                                parsed_dates = pd.to_datetime(date_series, format='%Y-%m-%d', errors='coerce')
                                # If all are NaT, try DD/MM/YYYY
                                if parsed_dates.isnull().all():
                                    current_app.logger.info(f"[{filename}] Format YYYY-MM-DD failed, trying DD/MM/YYYY...") 
                                    parsed_dates = pd.to_datetime(date_series, format='%d/%m/%Y', errors='coerce')
                                # If still all NaT, try inferring (less reliable but fallback)
                                if parsed_dates.isnull().all():
                                     current_app.logger.warning(f"[{filename}] Both specific formats failed, trying to infer date format...") 
                                     parsed_dates = pd.to_datetime(date_series, errors='coerce', infer_datetime_format=True)
                                # Check if any dates were successfully parsed
                                if not parsed_dates.isnull().all():
                                    latest_date = parsed_dates.max()
                                    if pd.notna(latest_date):
                                        status_info['latest_data_date'] = latest_date.strftime('%Y-%m-%d')
                                        current_app.logger.info(f"[{filename}] Successfully found latest date: {status_info['latest_data_date']}") 
                                    else:
                                        status_info['latest_data_date'] = 'No Valid Dates Found'
                                        current_app.logger.warning(f"[{filename}] Parsed dates but found no valid max date (all NaT?).") 
                                else:
                                    status_info['latest_data_date'] = 'Date Parsing Failed'
                                    current_app.logger.warning(f"[{filename}] All parsing attempts failed for date column '{date_col}'.") 
                            except Exception as date_err:
                                current_app.logger.error(f"Error parsing date column '{date_col}' in {file_path}: {date_err}", exc_info=True)
                                status_info['latest_data_date'] = f'Error Parsing Date ({date_col})'
                        else:
                            status_info['latest_data_date'] = 'No Date Column Found/Parsed'
                            current_app.logger.warning(f"[{filename}] Could not find a suitable date column.") 
                        # --- Add Fund Code Extraction --- 
                        code_col = None
                        # FIX: Search for 'code' OR 'fund code' (case-insensitive)
                        code_candidates = ['code', 'fund code'] 
                        found_code_col_name = None
                        for candidate in code_candidates:
                             matches = [c for c in df.columns if c.strip().lower() == candidate]
                             if matches:
                                 found_code_col_name = matches[0] # Use the actual column name found
                                 break # Stop searching once found
                        if found_code_col_name:
                            code_col = found_code_col_name # Assign the found name to code_col
                            current_app.logger.info(f"[{filename}] Found Code column: '{code_col}'")
                            if not df.empty and code_col in df:
                                try:
                                    unique_funds = sorted([str(f) for f in df[code_col].unique() if pd.notna(f)])
                                    if unique_funds:
                                        if len(unique_funds) <= 5:
                                            status_info['funds_included'] = ', '.join(unique_funds)
                                        else:
                                            status_info['funds_included'] = ', '.join(unique_funds[:5]) + f' ... ({len(unique_funds)} total)'
                                        current_app.logger.info(f"[{filename}] Found funds: {status_info['funds_included']}")
                                    else:
                                        status_info['funds_included'] = 'No Codes Found'
                                except Exception as fund_err:
                                     current_app.logger.error(f"[{filename}] Error extracting funds from column '{code_col}': {fund_err}")
                                     status_info['funds_included'] = 'Error Extracting Funds'
                            else:
                                 status_info['funds_included'] = 'Code Column Empty?' # Should be covered by EmptyDataError usually
                        else:
                            status_info['funds_included'] = 'Code Column Missing'
                            current_app.logger.warning(f"[{filename}] Code column ('Code' or 'Fund Code') not found.")
                        # --- End Fund Code Extraction ---
                    except pd.errors.EmptyDataError:
                         status_info['latest_data_date'] = 'File is Empty'
                         status_info['funds_included'] = 'File is Empty' # Also set for funds
                         current_app.logger.warning(f"CSV file is empty: {file_path}")
                    except Exception as read_err:
                        status_info['latest_data_date'] = 'Read Error'
                        current_app.logger.error(f"Error reading CSV {file_path} for status check: {read_err}", exc_info=True)
                except Exception as file_err:
                     current_app.logger.error(f"Error accessing file properties for {file_path}: {file_err}", exc_info=True)
                     status_info['last_modified'] = 'Error Accessing File'
            statuses.append(status_info)
    except Exception as e:
        current_app.logger.error(f"Failed to process QueryMap.csv for file statuses: {e}", exc_info=True)
        # Optionally return a status indicating the map couldn't be processed
        return [{'filename': 'QueryMap Error', 'exists': False, 'last_modified': str(e), 'latest_data_date': '', 'funds_included': ''}]
    return statuses
# --- End Helper Function ---
@api_bp.route('/get_data')
def get_data_page():
    '''Renders the page for users to select parameters for API data retrieval.'''
    try:
        # Construct the path to FundList.csv relative to the app's instance path or root
        # Assuming DATA_FOLDER is configured relative to the app root
        data_folder = current_app.config.get('DATA_FOLDER', 'Data')
        fund_list_path = os.path.join(data_folder, 'FundList.csv')
        if not os.path.exists(fund_list_path):
            current_app.logger.error(f"FundList.csv not found at {fund_list_path}")
            return "Error: FundList.csv not found.", 500
        fund_df = pd.read_csv(fund_list_path)
        # Ensure required columns exist
        if not {'Fund Code', 'Total Asset Value USD', 'Picked'}.issubset(fund_df.columns):
             current_app.logger.error(f"FundList.csv is missing required columns.")
             return "Error: FundList.csv is missing required columns (Fund Code, Total Asset Value USD, Picked).", 500
        # Convert Total Asset Value to numeric, coercing errors
        fund_df['Total Asset Value USD'] = pd.to_numeric(fund_df['Total Asset Value USD'], errors='coerce')
        fund_df.dropna(subset=['Total Asset Value USD'], inplace=True) # Remove rows where conversion failed
        # Sort by Total Asset Value USD descending
        fund_df = fund_df.sort_values(by='Total Asset Value USD', ascending=False)
        # Convert Picked to boolean
        fund_df['Picked'] = fund_df['Picked'].astype(bool)
        # Prepare fund data for the template
        funds = fund_df.to_dict('records')
        # Calculate default end date (previous business day)
        default_end_date = (datetime.datetime.today() - BDay(1)).strftime('%Y-%m-%d')
        # --- Get Data File Statuses ---
        data_file_statuses = get_data_file_statuses(data_folder)
        # --- End Get Data File Statuses ---
    except Exception as e:
        current_app.logger.error(f"Error preparing get_data page: {e}", exc_info=True)
        # Provide a user-friendly error message, specific details are logged
        return f"An error occurred while preparing the data retrieval page: {e}", 500
    return render_template('get_data.html', funds=funds, default_end_date=default_end_date, data_file_statuses=data_file_statuses)
# --- Helper function to find key columns ---
def _find_key_columns(df, filename):
    """Attempts to find the date and fund/identifier columns."""
    date_col = None
    fund_col = None
    # Date column candidates (add more if needed)
    date_candidates = ['Date', 'date', 'AsOfDate', 'ASOFDATE', 'Effective Date', 'Trade Date', 'Position Date']
    # Fund/ID column candidates
    fund_candidates = ['Code', 'Fund Code', 'Fundcode', 'security id', 'SecurityID', 'Security Name'] # Broadened list
    found_cols = df.columns.str.strip().str.lower()
    for candidate in date_candidates:
        if candidate.lower() in found_cols:
            # Find the original casing
            original_cols = [col for col in df.columns if col.strip().lower() == candidate.lower()]
            if original_cols:
                date_col = original_cols[0]
                current_app.logger.info(f"[{filename}] Found date column: '{date_col}'")
                break
    for candidate in fund_candidates:
        if candidate.lower() in found_cols:
            # Find the original casing
            original_cols = [col for col in df.columns if col.strip().lower() == candidate.lower()]
            if original_cols:
                fund_col = original_cols[0]
                current_app.logger.info(f"[{filename}] Found fund/ID column: '{fund_col}'")
                break
    if not date_col:
        current_app.logger.warning(f"[{filename}] Could not reliably identify a date column from candidates: {date_candidates}")
    if not fund_col:
        current_app.logger.warning(f"[{filename}] Could not reliably identify a fund/ID column from candidates: {fund_candidates}")
    return date_col, fund_col
@api_bp.route('/run_api_calls', methods=['POST'])
def run_api_calls():
    '''Handles the form submission to trigger API calls (real or simulated).'''
    try:
        # Get data from form
        data = request.get_json()
        days_back = int(data.get('days_back', 30)) # Default to 30 days if not provided
        end_date_str = data.get('end_date')
        selected_funds = data.get('funds', [])
        overwrite_mode = data.get('overwrite_mode', False) # Get the new overwrite flag
        if not end_date_str:
            # Should have been validated client-side, but handle defensively
            return jsonify({"status": "error", "message": "End date is required."}), 400
        if not selected_funds:
             return jsonify({"status": "error", "message": "At least one fund must be selected."}), 400
        # Calculate dates
        end_date = pd.to_datetime(end_date_str)
        start_date = end_date - pd.Timedelta(days=days_back)
        # Format dates as YYYY-MM-DD for the TQS call
        start_date_tqs_str = start_date.strftime('%Y-%m-%d')
        end_date_tqs_str = end_date.strftime('%Y-%m-%d')
        # --- Get Query Map ---
        data_folder = current_app.config.get('DATA_FOLDER', 'Data')
        query_map_path = os.path.join(data_folder, 'QueryMap.csv')
        if not os.path.exists(query_map_path):
            return jsonify({"status": "error", "message": f"QueryMap.csv not found at {query_map_path}"}), 500
        query_map_df = pd.read_csv(query_map_path)
        if not {'QueryID', 'FileName'}.issubset(query_map_df.columns):
            return jsonify({"status": "error", "message": "QueryMap.csv missing required columns (QueryID, FileName)."}), 500
        # Sort queries: ts_*, pre_*, others
        def sort_key(query):
            filename = query.get('FileName', '').lower()
            if filename.startswith('ts_'):
                return 0
            elif filename.startswith('pre_'):
                return 1
            else:
                # Keep original order for non-ts/pre files relative to each other
                # Or assign a consistent rank if needed (e.g., based on original index)
                return 2 # All others get rank 2 for now
        # Add original index to preserve relative order for non-ts/pre files
        queries_with_indices = list(enumerate(query_map_df.to_dict('records')))
        def sort_key_with_index(item):
            index, query = item
            filename = query.get('FileName', '').lower()
            if filename.startswith('ts_'):
                return (0, index) # Sort by ts_ first, then original index
            elif filename.startswith('pre_'):
                return (1, index) # Sort by pre_ next, then original index
            else:
                return (2, index) # Others last, sorted by original index
        queries_with_indices.sort(key=sort_key_with_index)
        # Extract the sorted queries list
        queries = [item[1] for item in queries_with_indices]
        current_app.logger.info(f"Processing order after sorting: {[q.get('FileName', 'N/A') for q in queries]}")
        results_summary = []
        total_queries = len(queries)
        completed_queries = 0
        all_ts_files_succeeded = True # Flag to track success of ts_ files
        # Determine mode for logging/messaging
        current_mode_desc = "SIMULATED mode" if not USE_REAL_TQS_API else ("REAL API mode (Overwrite Enabled)" if overwrite_mode else "REAL API mode (Merge/Append)")
        current_app.logger.info(f"--- Starting /run_api_calls in {current_mode_desc} ---")
        # Loop through sorted queries
        for query_info in queries:
            # Extract query details safely
            query_id = query_info.get('QueryID')
            file_name = query_info.get('FileName')
            # Make sure QueryID and FileName exist
            if not query_id or not file_name:
                 current_app.logger.warning(f"Skipping entry due to missing QueryID or FileName: {query_info}")
                 # Add a summary entry indicating the skip?
                 summary = {
                    "query_id": query_id or "N/A", "file_name": file_name or "N/A",
                    "status": "Skipped (Missing QueryID/FileName)",
                    "simulated_rows": None, "simulated_lines": None,
                    "actual_rows": None, "actual_lines": None,
                    "save_action": "N/A", "validation_status": "Not Run"
                 }
                 results_summary.append(summary)
                 # Don't increment completed_queries if it fundamentally couldn't run
                 continue 
            output_path = os.path.join(data_folder, file_name)
            # Initialize summary for this query (moved after basic validation)
            summary = {
                "query_id": query_id,
                "file_name": file_name,
                "status": "Pending", # Initial status
                "simulated_rows": None, # Initialize keys
                "simulated_lines": None,
                "actual_rows": None,
                "actual_lines": None,
                "save_action": "N/A",
                "validation_status": "Not Run"
            }
            # Determine file type
            file_type = 'other'
            if file_name.lower().startswith('ts_'):
                file_type = 'ts'
            elif file_name.lower().startswith('pre_'):
                file_type = 'pre'
            current_app.logger.info(f"--- Starting Process for QueryID: {query_id}, File: {file_name} (Type: {file_type}) ---")
            # Skip pre_ files if any ts_ file failed
            if file_type == 'pre' and not all_ts_files_succeeded:
                current_app.logger.warning(f"[{file_name}] Skipping pre_ file because a previous ts_ file failed processing.")
                summary['status'] = 'Skipped (Previous TS Failure)'
                summary['validation_status'] = 'Not Run'
                summary['save_action'] = 'Skipped'
                results_summary.append(summary)
                completed_queries += 1 # It was processed (by skipping)
                continue # Move to the next query
            # --- Existing try block for processing a single query ---
            try:
                if USE_REAL_TQS_API:
                    # --- Real API Call, Validation, and Save Logic ---
                    df_new = None
                    df_to_save = None # Will hold the final DF to be saved
                    force_overwrite = overwrite_mode # Use the flag passed from frontend
                    try:
                        # 1. Fetch Real Data (Common step)
                        df_new = _fetch_real_tqs_data(query_id, selected_funds, start_date_tqs_str, end_date_tqs_str)
                        # --- Handle fetch result ---
                        if df_new is None:
                            current_app.logger.warning(f"[{file_name}] No data returned from API call for QueryID {query_id}.")
                            summary['status'] = 'Warning - No data returned from API'
                            summary['validation_status'] = 'Skipped (API Returned None)'
                            if file_type == 'ts': all_ts_files_succeeded = False # Mark failure for ts_ files
                        elif df_new.empty:
                            current_app.logger.warning(f"[{file_name}] Empty DataFrame returned from API call for QueryID {query_id}.")
                            summary['status'] = 'Warning - Empty data returned from API'
                            # Don't skip validation if empty, allow saving empty file
                            summary['validation_status'] = 'OK (Empty Data)'
                            # An empty dataframe is still data, proceed to save/overwrite logic below
                            df_to_save = df_new # Allow overwriting with empty data if needed
                            summary['actual_rows'] = 0 # Explicitly set 0 rows
                        else: # Data fetched successfully (and not empty)
                            current_app.logger.info(f"[{file_name}] Fetched {len(df_new)} new rows.")
                            summary['actual_rows'] = len(df_new)
                            df_to_save = df_new # Prepare to save this new data (might be modified below)
                        # --- Type-Specific Processing (only if df_new is not None) ---
                        if df_new is not None: # Includes empty DataFrame case
                            # == TS File Processing ==
                            if file_type == 'ts':
                                current_app.logger.info(f"[{file_name}] Processing as ts_ file (Overwrite Mode: {force_overwrite}).")
                                try:
                                    # 2. Identify Key Columns in New Data (TS specific)
                                    if not df_new.empty: # Only check non-empty DFs
                                        date_col_new, fund_col_new = _find_key_columns(df_new, f"{file_name} (New TS Data)")
                                        if not date_col_new or not fund_col_new:
                                            err_msg = f"Could not find essential date/fund columns in fetched ts_ data for {file_name}. Cannot proceed."
                                            current_app.logger.error(f"[{file_name}] {err_msg}")
                                            raise ValueError(err_msg) # Caught below
                                    else:
                                        date_col_new, fund_col_new = None, None # Cannot find cols in empty df
                                        current_app.logger.info(f"[{file_name}] Skipping key column check for empty TS data.")
                                    # 3. Handle Existing File (TS specific - merge/append logic)
                                    if force_overwrite:
                                        current_app.logger.info(f"[{file_name}] Overwrite Mode enabled. Skipping check for existing file.")
                                        if os.path.exists(output_path):
                                            summary['save_action'] = 'Overwritten (User Request)'
                                        else:
                                            summary['save_action'] = 'Created (Overwrite Mode)'
                                        # df_to_save is already df_new
                                    elif os.path.exists(output_path):
                                        current_app.logger.info(f"[{file_name}] TS file exists. Reading existing data for merge/append.")
                                        try:
                                            df_existing = pd.read_csv(output_path, low_memory=False)
                                            if not df_existing.empty:
                                                # --- Existing TS merge/append logic ---
                                                if date_col_new and fund_col_new: # Requires new data cols to be found
                                                    date_col_existing, fund_col_existing = _find_key_columns(df_existing, f"{file_name} (Existing TS)")
                                                    if date_col_existing == date_col_new and fund_col_existing == fund_col_new:
                                                        date_col = date_col_new # Use consistent names
                                                        fund_col = fund_col_new
                                                        # Date range comparison (optional check)
                                                        # ... (keep existing date comparison logic if desired) ...
                                                        # Combine Data (Append/Overwrite) logic
                                                        current_app.logger.info(f"[{file_name}] Combining new data for funds/IDs {df_new[fund_col].unique()} with existing data.")
                                                        funds_in_new_data = df_new[fund_col].unique()
                                                        # Ensure fund columns have compatible types (e.g., strings)
                                                        try:
                                                            df_existing[fund_col] = df_existing[fund_col].astype(str)
                                                            df_new[fund_col] = df_new[fund_col].astype(str) # Ensure new is also str
                                                            funds_in_new_data = [str(f) for f in funds_in_new_data]
                                                        except Exception as type_err:
                                                             current_app.logger.warning(f"[{file_name}] Potential type mismatch in fund column \'{fund_col}\' during filtering: {type_err}. Filtering might be incomplete.")
                                                        df_existing_filtered = df_existing[~df_existing[fund_col].isin(funds_in_new_data)]
                                                        current_app.logger.info(f"[{file_name}] Kept {len(df_existing_filtered)} rows from existing file (other funds).")
                                                        # Concatenate
                                                        df_combined = pd.concat([df_existing_filtered, df_new], ignore_index=True)
                                                        # Optional Sort
                                                        try:
                                                            df_combined = df_combined.sort_values(by=[date_col, fund_col])
                                                        except Exception as sort_err:
                                                             current_app.logger.warning(f"[{file_name}] Could not sort combined data: {sort_err}")
                                                        df_to_save = df_combined # Update the df to save
                                                        summary['save_action'] = 'Combined (Append/Overwrite)'
                                                        current_app.logger.info(f"[{file_name}] Prepared combined data ({len(df_to_save)} rows).")
                                                    else: # Key columns mismatch
                                                        current_app.logger.warning(f"[{file_name}] Key columns mismatch between existing ({date_col_existing}, {fund_col_existing}) and new ({date_col_new}, {fund_col_new}). Overwriting entire file.")
                                                        # df_to_save is already df_new
                                                        summary['save_action'] = 'Overwritten (Column Mismatch)'
                                                else: # Cannot proceed with merge if new cols weren't found (e.g., new data was empty)
                                                    current_app.logger.warning(f"[{file_name}] Cannot merge TS data as key columns were not identified in new data. Overwriting.")
                                                    # df_to_save is already df_new
                                                    summary['save_action'] = 'Overwritten (Merge Skipped)'
                                            else: # Existing file is empty
                                                current_app.logger.warning(f"[{file_name}] Existing TS file is empty. Overwriting.")
                                                # df_to_save is already df_new
                                                summary['save_action'] = 'Overwritten (Existing Empty)'
                                        except pd.errors.EmptyDataError:
                                            current_app.logger.warning(f"[{file_name}] Existing TS file is empty (EmptyDataError). Overwriting.")
                                            # df_to_save is already df_new
                                            summary['save_action'] = 'Overwritten (Existing Empty)'
                                        except Exception as read_err:
                                            current_app.logger.error(f"[{file_name}] Error reading existing TS file: {read_err}. Overwriting.", exc_info=True)
                                            # df_to_save is already df_new
                                            summary['save_action'] = 'Overwritten (Read Error)'
                                            all_ts_files_succeeded = False # Failed to read existing TS file properly
                                    else: # No existing file (and not forcing overwrite)
                                        current_app.logger.info(f"[{file_name}] TS file does not exist (or overwrite mode is on and file was absent). Creating new file.")
                                        # df_to_save is already df_new
                                        summary['save_action'] = 'Created'
                                except ValueError as ve: # Catch _find_key_columns error
                                    current_app.logger.error(f"[{file_name}] TS validation failed: {ve}")
                                    summary['status'] = f'Error - TS Validation Failed: {ve}'
                                    summary['validation_status'] = 'Failed (Missing Columns)'
                                    all_ts_files_succeeded = False # TS validation failed
                                    df_to_save = None # Don't save if validation fails
                            # == PRE File Processing ==
                            elif file_type == 'pre':
                                current_app.logger.info(f"[{file_name}] Processing as pre_ file (checking column count).")
                                # df_to_save is already df_new (or empty df) - Always overwritten
                                if os.path.exists(output_path):
                                    try:
                                        # Check column count consistency (only if new data is not empty)
                                        if not df_new.empty:
                                            existing_header_df = pd.read_csv(output_path, nrows=0, low_memory=False) # Read only header
                                            existing_cols = existing_header_df.columns.tolist()
                                            new_cols = df_new.columns.tolist()
                                            if force_overwrite or len(existing_cols) != len(new_cols) or set(existing_cols) != set(new_cols):
                                                if force_overwrite:
                                                    current_app.logger.info(f"[{file_name}] Overwriting pre_ file as requested by user.")
                                                    summary['save_action'] = 'Overwritten (User Request)'
                                                else:
                                                    current_app.logger.warning(f"[{file_name}] Column count/names mismatch between existing pre_ file ({len(existing_cols)} cols: {existing_cols}) and new data ({len(new_cols)} cols: {new_cols}). Overwriting.")
                                                    summary['save_action'] = 'Overwritten (Column Mismatch)'
                                            else:
                                                current_app.logger.info(f"[{file_name}] Existing pre_ file found with matching columns. Overwriting.")
                                                summary['save_action'] = 'Overwritten'
                                        else: # New data is empty, just overwrite
                                            current_app.logger.info(f"[{file_name}] New data for pre_ file is empty. Overwriting existing file.")
                                            summary['save_action'] = 'Overwritten (New Data Empty)'
                                    except pd.errors.EmptyDataError:
                                         current_app.logger.warning(f"[{file_name}] Existing pre_ file is empty (EmptyDataError). Overwriting.")
                                         summary['save_action'] = 'Overwritten (Existing Empty)'
                                    except Exception as read_err:
                                         current_app.logger.error(f"[{file_name}] Error reading existing pre_ file header: {read_err}. Overwriting.", exc_info=True)
                                         summary['save_action'] = 'Overwritten (Read Error)'
                                else: # No existing pre_ file
                                    current_app.logger.info(f"[{file_name}] Pre_ file does not exist. Creating new file.")
                                    summary['save_action'] = 'Created'
                                # Note: df_to_save remains df_new for pre_ files.
                            # == Other File Processing ==
                            else: # Handle other files (e.g., sec_*)
                                # For now, treat 'other' files like 'pre_' files (overwrite, check column counts)
                                # This avoids the date/fund column check which might fail for sec_* files too
                                current_app.logger.info(f"[{file_name}] Processing as 'other' file type (using column count check).")
                                # df_to_save is already df_new (or empty df)
                                if os.path.exists(output_path):
                                    try:
                                        if not df_new.empty:
                                            existing_header_df = pd.read_csv(output_path, nrows=0, low_memory=False)
                                            existing_cols = existing_header_df.columns.tolist()
                                            new_cols = df_new.columns.tolist()
                                            if force_overwrite or len(existing_cols) != len(new_cols) or set(existing_cols) != set(new_cols):
                                                if force_overwrite:
                                                    current_app.logger.info(f"[{file_name}] Overwriting 'other' file as requested by user.")
                                                    summary['save_action'] = 'Overwritten (User Request)'
                                                else:
                                                    current_app.logger.warning(f"[{file_name}] Column count/names mismatch for 'other' file. Overwriting.")
                                                    summary['save_action'] = 'Overwritten (Column Mismatch)'
                                            else:
                                                current_app.logger.info(f"[{file_name}] Existing 'other' file found with matching columns. Overwriting.")
                                                summary['save_action'] = 'Overwritten'
                                        else:
                                            current_app.logger.info(f"[{file_name}] New data for 'other' file is empty. Overwriting existing file.")
                                            summary['save_action'] = 'Overwritten (New Data Empty)'
                                    except pd.errors.EmptyDataError:
                                         current_app.logger.warning(f"[{file_name}] Existing 'other' file is empty (EmptyDataError). Overwriting.")
                                         summary['save_action'] = 'Overwritten (Existing Empty)'
                                    except Exception as read_err:
                                         current_app.logger.error(f"[{file_name}] Error reading existing 'other' file header: {read_err}. Overwriting.", exc_info=True)
                                         summary['save_action'] = 'Overwritten (Read Error)'
                                else:
                                    current_app.logger.info(f"[{file_name}] 'Other' file does not exist. Creating new file.")
                                    summary['save_action'] = 'Created'
                            # 4. Save the Final DataFrame (Common step, if df_to_save is valid)
                            if df_to_save is not None: # Allow saving empty dataframe to overwrite/create
                                current_app.logger.info(f"[{file_name}] Attempting to save {len(df_to_save)} rows to {output_path} (Action: {summary['save_action']})")
                                # ... existing warning log ...
                                try:
                                    df_to_save.to_csv(output_path, index=False, header=True)
                                    current_app.logger.info(f"[{file_name}] Successfully saved data to {output_path}")
                                    summary['status'] = 'OK - Data Saved'
                                    # Update lines_in_file count after successful save
                                    try:
                                         with open(output_path, 'r', encoding='utf-8') as f:
                                              # Store in actual_lines as this is real API mode
                                              summary['actual_lines'] = sum(1 for line in f)
                                    except Exception:
                                         summary['actual_lines'] = 'N/A' # Or len(df_to_save)+1?
                                    # Validation step (consider if validate_data needs adjustment for pre_/other files)
                                    summary['validation_status'] = validate_data(df_to_save, file_name)
                                    current_app.logger.info(f"[{file_name}] Validation status: {summary['validation_status']})")
                                    # If validation fails for a TS file, should it mark all_ts_files_succeeded = False? Maybe.
                                    # if file_type == 'ts' and 'Error' in summary['validation_status']:
                                    #    all_ts_files_succeeded = False
                                    #    current_app.logger.warning(f"[{file_name}] TS file validation failed, marking overall TS process as failed.")
                                except Exception as write_err:
                                    current_app.logger.error(f"[{file_name}] Error writing final data to {output_path}: {write_err}", exc_info=True)
                                    summary['status'] = f'Error - Failed to save file: {write_err}'
                                    summary['validation_status'] = 'Failed (Save Error)'
                                    if file_type == 'ts': all_ts_files_succeeded = False # Save failed for ts_ file
                            # This case handles if df_new was None initially, or if df_to_save was set to None due to TS validation error
                            elif df_new is None: 
                                pass # Status already set when df_new was None
                            elif df_to_save is None and file_type == 'ts':
                                pass # Status already set from TS validation error
                            else: # Should not happen? Log if it does.
                                 current_app.logger.error(f"[{file_name}] Reached unexpected state where df_to_save is None but no prior error logged.")
                                 summary['status'] = 'Error - Internal Logic Error (df_to_save is None)'
                    except Exception as proc_err: # Catch errors during the fetch/process stage for one file
                        current_app.logger.error(f"Error processing real data for QueryID {query_id}, File {file_name}: {proc_err}", exc_info=True)
                        summary['status'] = f'Error - Processing failed: {proc_err}'
                        summary['validation_status'] = 'Failed (Processing Error)'
                        if file_type == 'ts': all_ts_files_succeeded = False # Processing failed for ts_ file
                else: # Simulate API Call (keep existing)
                    # ... existing simulation logic ...
                    # status = "Simulated OK" # Update summary status if needed
                    simulated_rows = _simulate_and_print_tqs_call(query_id, selected_funds, start_date_tqs_str, end_date_tqs_str)
                    summary['status'] = "Simulated OK"
                    summary['simulated_rows'] = simulated_rows
                    summary['simulated_lines'] = simulated_rows + 1 if simulated_rows > 0 else 0
                    summary['actual_rows'] = None # Ensure actual keys are None in sim mode
                    summary['actual_lines'] = None
            except Exception as outer_err: # Catch unexpected errors during the processing of a single query's try block
                 current_app.logger.error(f"Unexpected outer error processing QueryID {query_id} ({file_name}): {outer_err}", exc_info=True)
                 # Ensure status reflects the outer error if not already set
                 if summary['status'] == 'Pending' or summary['status'].startswith('Warning'):
                      summary['status'] = f"Outer Processing Error: {outer_err}"
                 # Mark TS as failed if outer error occurs
                 if file_type == 'ts': all_ts_files_succeeded = False 
            # Append results for this query
            results_summary.append(summary)
            completed_queries += 1
            # Pause between real API calls (keep existing)
            if USE_REAL_TQS_API and completed_queries < total_queries: # Avoid pause after last call
                print(f"Pausing for 3 seconds before next real API call ({completed_queries}/{total_queries})...") # Optional status message
                time.sleep(3) 
        # After loop (keep existing)
        mode_message = "SIMULATED mode" if not USE_REAL_TQS_API else ("REAL API mode (Overwrite Enabled)" if overwrite_mode else "REAL API mode (Merge/Append)")
        final_status = "completed"
        if USE_REAL_TQS_API and not all_ts_files_succeeded:
             completion_message = f"Processed {completed_queries}/{total_queries} API calls ({mode_message}). WARNING: One or more ts_ files failed processing or validation."
             final_status = "completed_with_errors"
        else:
             completion_message = f"Processed {completed_queries}/{total_queries} API calls ({mode_message})."
        return jsonify({
            "status": final_status, # Provide more info on completion status
            "message": completion_message,
            "summary": results_summary
        })
    except ValueError as ve:
        # Handle potential errors like invalid integer conversion for days_back
        current_app.logger.error(f"Value error in /run_api_calls: {ve}", exc_info=True)
        return jsonify({"status": "error", "message": f"Invalid input value: {ve}"}), 400
    except FileNotFoundError as fnf:
        # Specific handling for file not found during setup (e.g., QueryMap)
        current_app.logger.error(f"File not found error in /run_api_calls: {fnf}", exc_info=True)
        return jsonify({"status": "error", "message": f"Required file not found: {fnf}"}), 500
    except Exception as e:
        # Catch-all for other unexpected errors during the process
        current_app.logger.error(f"Unexpected error in /run_api_calls: {e}", exc_info=True)
        return jsonify({"status": "error", "message": f"An unexpected error occurred: {e}"}), 500
# === NEW RERUN ROUTE ===
@api_bp.route('/rerun-api-call', methods=['POST'])
def rerun_api_call():
    '''Handles the request to rerun a single API call (real or simulated).'''
    try:
        data = request.get_json()
        query_id = data.get('query_id')
        days_back = int(data.get('days_back', 30))
        end_date_str = data.get('end_date')
        selected_funds = data.get('funds', []) # Get the list of funds
        overwrite_mode = data.get('overwrite_mode', False) # Get the new overwrite flag
        # --- Basic Input Validation ---
        if not query_id:
            return jsonify({"status": "error", "message": "Query ID is required."}), 400
        if not end_date_str:
            return jsonify({"status": "error", "message": "End date is required."}), 400
        if not selected_funds:
             # Allow rerunning even if no funds are selected? Decide based on API behavior.
             # For now, let's require funds similar to the initial run.
             return jsonify({"status": "error", "message": "At least one fund must be selected."}), 400
        # --- Calculate Dates ---
        end_date = pd.to_datetime(end_date_str)
        start_date = end_date - pd.Timedelta(days=days_back)
        start_date_tqs_str = start_date.strftime('%Y-%m-%d')
        end_date_tqs_str = end_date.strftime('%Y-%m-%d')
        # --- Find FileName from QueryMap ---
        data_folder = current_app.config.get('DATA_FOLDER', 'Data')
        query_map_path = os.path.join(data_folder, 'QueryMap.csv')
        if not os.path.exists(query_map_path):
            return jsonify({"status": "error", "message": f"QueryMap.csv not found at {query_map_path}"}), 500
        query_map_df = pd.read_csv(query_map_path)
        # Ensure comparison is string vs string
        query_map_df['QueryID'] = query_map_df['QueryID'].astype(str)
        if 'QueryID' not in query_map_df.columns or 'FileName' not in query_map_df.columns:
             return jsonify({"status": "error", "message": "QueryMap.csv missing required columns (QueryID, FileName)."}), 500
        # Compare string query_id from request with string QueryID column
        query_row = query_map_df[query_map_df['QueryID'] == query_id]
        if query_row.empty:
            # Log the types for debugging if it still fails
            current_app.logger.warning(f"QueryID '{query_id}' (type: {type(query_id)}) not found in QueryMap QueryIDs (types: {query_map_df['QueryID'].apply(type).unique()}).")
            return jsonify({"status": "error", "message": f"QueryID '{query_id}' not found in QueryMap.csv."}), 404
        file_name = query_row.iloc[0]['FileName']
        output_path = os.path.join(data_folder, file_name)
        # --- Execute Single API Call (Simulated or Real) ---
        status = "Rerun Error: Unknown"
        rows_returned = 0
        lines_in_file = 0
        actual_df = None
        simulated_rows = None # Initialize simulation keys too
        simulated_lines = None
        try:
            if USE_REAL_TQS_API:
                # --- Real API Call, Validation, and Save ---
                actual_df = _fetch_real_tqs_data(query_id, selected_funds, start_date_tqs_str, end_date_tqs_str)
                if actual_df is not None and isinstance(actual_df, pd.DataFrame):
                    rows_returned = len(actual_df)
                    if actual_df.empty:
                        current_app.logger.info(f"(Rerun) API returned empty DataFrame for {query_id} ({file_name}). Saving empty file.")
                        status = "Saved OK (Empty)"
                    else:
                        is_valid, validation_errors = validate_data(actual_df, file_name)
                        if not is_valid:
                            current_app.logger.warning(f"(Rerun) Data validation failed for {file_name}: {validation_errors}")
                            status = f"Validation Failed: {'; '.join(validation_errors)}"
                            lines_in_file = 0
                        # else: Validation passed
                    if not status.startswith("Validation Failed"):
                        try:
                            os.makedirs(os.path.dirname(output_path), exist_ok=True)
                            actual_df.to_csv(output_path, index=False)
                            current_app.logger.info(f"(Rerun) Successfully saved data to {output_path}")
                            lines_in_file = rows_returned + 1
                            if status != "Saved OK (Empty)":
                                status = "Saved OK"
                        except Exception as e:
                            current_app.logger.error(f"(Rerun) Error saving DataFrame to {output_path}: {e}", exc_info=True)
                            status = f"Save Error: {e}"
                            lines_in_file = 0
                elif actual_df is None:
                    current_app.logger.warning(f"(Rerun) Real API call/fetch for {query_id} ({file_name}) returned None.")
                    status = "No Data / API Error / TQS Missing"
                    rows_returned = 0
                    lines_in_file = 0
                else:
                    current_app.logger.error(f"(Rerun) Real API fetch for {query_id} ({file_name}) returned unexpected type: {type(actual_df)}.")
                    status = "API Returned Invalid Type"
                    rows_returned = 0
                    lines_in_file = 0
            else:
                # --- Simulate API Call ---
                simulated_rows = _simulate_and_print_tqs_call(query_id, selected_funds, start_date_tqs_str, end_date_tqs_str)
                rows_returned = simulated_rows
                lines_in_file = simulated_rows + 1 if simulated_rows > 0 else 0
                status = "Simulated OK"
        except Exception as e:
            current_app.logger.error(f"Error during single rerun for query {query_id} ({file_name}): {e}", exc_info=True)
            status = f"Processing Error: {e}"
            rows_returned = 0
            lines_in_file = 0
        # --- Return Result for the Single Query ---
        result_data = {
            "status": status,
             # Provide consistent keys for the frontend to update the table
            "simulated_rows": simulated_rows, # Value if simulated, None otherwise
            "actual_rows": rows_returned if USE_REAL_TQS_API else None, # Value if real, None otherwise
            "simulated_lines": simulated_lines, # Value if simulated, None otherwise
            "actual_lines": lines_in_file if USE_REAL_TQS_API else None # Value if real, None otherwise
        }
        return jsonify(result_data)
    except ValueError as ve:
        current_app.logger.error(f"Value error in /rerun-api-call: {ve}", exc_info=True)
        return jsonify({"status": "error", "message": f"Invalid input value: {ve}"}), 400
    except FileNotFoundError as fnf:
        current_app.logger.error(f"File not found error in /rerun-api-call: {fnf}", exc_info=True)
        return jsonify({"status": "error", "message": f"Required file not found: {fnf}"}), 500
    except Exception as e:
        current_app.logger.error(f"Unexpected error in /rerun-api-call: {e}", exc_info=True)
        return jsonify({"status": "error", "message": f"An unexpected error occurred: {e}"}), 500
# Ensure no code remains after this point in the file for this function.
</file>

<file path="views/comparison_views.py">
# views/comparison_views.py
# This module defines the Flask Blueprint for comparing two security spread datasets.
# It includes routes for a summary view listing securities with comparison metrics
# and a detail view showing overlayed time-series charts and statistics for a single security.
from flask import Blueprint, render_template, request, current_app, jsonify, url_for
import pandas as pd
import os
import logging
import math # Add math for pagination calculation
# Assuming security_processing and utils are in the parent directory or configured in PYTHONPATH
try:
    from security_processing import load_and_process_security_data, calculate_security_latest_metrics # May need adjustments
    from utils import parse_fund_list # Example utility
    from config import DATA_FOLDER, COLOR_PALETTE
except ImportError:
    # Handle potential import errors if the structure is different
    logging.error("Could not import required modules from parent directory.")
    # Add fallback imports or path adjustments if necessary
    # Example: sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
    from ..security_processing import load_and_process_security_data, calculate_security_latest_metrics
    from ..utils import parse_fund_list
    from ..config import DATA_FOLDER, COLOR_PALETTE
comparison_bp = Blueprint('comparison_bp', __name__,
                        template_folder='../templates',
                        static_folder='../static')
# Configure logging
log = logging.getLogger(__name__)
PER_PAGE_COMPARISON = 50 # Items per page for comparison summary
# --- Data Loading and Processing ---
def load_comparison_data(file1='sec_spread.csv', file2='sec_spreadSP.csv'):
    """Loads, processes, and merges data from two security spread files.
    Returns:
        tuple: (merged_df, static_data, common_static_cols, id_col_name)
               Returns (pd.DataFrame(), pd.DataFrame(), [], None) on error.
    """
    log.info(f"Loading comparison data: {file1} and {file2}")
    # Pass only the filename, as load_and_process_security_data prepends DATA_FOLDER internally
    df1, static_cols1 = load_and_process_security_data(file1)
    df2, static_cols2 = load_and_process_security_data(file2)
    if df1.empty or df2.empty:
        log.warning(f"One or both dataframes are empty. File1 empty: {df1.empty}, File2 empty: {df2.empty}")
        return pd.DataFrame(), pd.DataFrame(), [], None # Return None for id_col_name
    # Identify common static columns (excluding the ID column used for merging)
    common_static_cols = list(set(static_cols1) & set(static_cols2))
    # Get the actual ID column name (should be the same for both, use df1)
    if df1.index.nlevels == 2:
        id_col_name = df1.index.names[1] # Assuming 'Security ID'/Name is the second level
        log.info(f"Identified ID column from index: {id_col_name}")
    else:
        log.error("Processed DataFrame df1 does not have the expected 2-level MultiIndex.")
        return pd.DataFrame(), pd.DataFrame(), [], None # Return None for id_col_name
    # Prepare for merge - keep only necessary columns and rename Value columns
    df1_merge = df1.reset_index()[[id_col_name, 'Date', 'Value'] + common_static_cols].rename(columns={'Value': 'Value_Orig'})
    df2_merge = df2.reset_index()[[id_col_name, 'Date', 'Value']].rename(columns={'Value': 'Value_New'}) # Don't need static cols twice
    # Perform an outer merge to keep all dates and securities from both files
    merged_df = pd.merge(df1_merge, df2_merge, on=[id_col_name, 'Date'], how='outer')
    # Calculate daily changes
    merged_df = merged_df.sort_values(by=[id_col_name, 'Date'])
    merged_df['Change_Orig'] = merged_df.groupby(id_col_name)['Value_Orig'].diff()
    merged_df['Change_New'] = merged_df.groupby(id_col_name)['Value_New'].diff()
    # Store static data separately - get the latest version per security
    static_data = merged_df.groupby(id_col_name)[common_static_cols].last().reset_index()
    log.info(f"Successfully merged data. Shape: {merged_df.shape}")
    return merged_df, static_data, common_static_cols, id_col_name # Return the identified ID column name
def calculate_comparison_stats(merged_df, static_data, id_col):
    """Calculates comparison statistics for each security.
    Args:
        merged_df (pd.DataFrame): The merged dataframe of original and new values.
        static_data (pd.DataFrame): DataFrame with static info per security.
        id_col (str): The name of the column containing the Security ID/Name.
    """
    if merged_df.empty:
        return pd.DataFrame()
    if id_col not in merged_df.columns:
        log.error(f"Specified id_col '{id_col}' not found in merged_df columns: {merged_df.columns.tolist()}")
        return pd.DataFrame() # Cannot group without the ID column
    log.info(f"Calculating comparison statistics using ID column: {id_col}...")
    stats_list = []
    # Use the passed id_col here
    for sec_id, group in merged_df.groupby(id_col):
        sec_stats = {id_col: sec_id} # Use actual id_col name
        # Filter out rows where both values are NaN for overall analysis period
        group_valid_overall = group.dropna(subset=['Value_Orig', 'Value_New'], how='all')
        overall_min_date = group_valid_overall['Date'].min()
        overall_max_date = group_valid_overall['Date'].max()
        # Filter out rows where EITHER value is NaN for correlation/diff calculations
        valid_comparison = group.dropna(subset=['Value_Orig', 'Value_New'])
        # 1. Correlation of Levels
        if len(valid_comparison) >= 2: # Need at least 2 points for correlation
            # Use the NaN-dropped dataframe for correlation
            level_corr = valid_comparison['Value_Orig'].corr(valid_comparison['Value_New'])
            sec_stats['Level_Correlation'] = level_corr if pd.notna(level_corr) else None
        else:
             sec_stats['Level_Correlation'] = None
        # 2. Max / Min (use original group to get true max/min including non-overlapping points)
        sec_stats['Max_Orig'] = group['Value_Orig'].max()
        sec_stats['Min_Orig'] = group['Value_Orig'].min()
        sec_stats['Max_New'] = group['Value_New'].max()
        sec_stats['Min_New'] = group['Value_New'].min()
        # 3. Date Range Comparison - Refined Logic
        # Find min/max dates within the MERGED data where each series is individually valid
        min_date_orig_idx = group['Value_Orig'].first_valid_index()
        max_date_orig_idx = group['Value_Orig'].last_valid_index()
        min_date_new_idx = group['Value_New'].first_valid_index()
        max_date_new_idx = group['Value_New'].last_valid_index()
        sec_stats['Start_Date_Orig'] = group.loc[min_date_orig_idx, 'Date'] if min_date_orig_idx is not None else None
        sec_stats['End_Date_Orig'] = group.loc[max_date_orig_idx, 'Date'] if max_date_orig_idx is not None else None
        sec_stats['Start_Date_New'] = group.loc[min_date_new_idx, 'Date'] if min_date_new_idx is not None else None
        sec_stats['End_Date_New'] = group.loc[max_date_new_idx, 'Date'] if max_date_new_idx is not None else None
        # Check if the start and end dates MATCH for the valid periods of EACH series
        same_start = pd.Timestamp(sec_stats['Start_Date_Orig']) == pd.Timestamp(sec_stats['Start_Date_New']) if sec_stats['Start_Date_Orig'] and sec_stats['Start_Date_New'] else False
        same_end = pd.Timestamp(sec_stats['End_Date_Orig']) == pd.Timestamp(sec_stats['End_Date_New']) if sec_stats['End_Date_Orig'] and sec_stats['End_Date_New'] else False
        sec_stats['Same_Date_Range'] = same_start and same_end
        # Add overall date range for info
        sec_stats['Overall_Start_Date'] = overall_min_date
        sec_stats['Overall_End_Date'] = overall_max_date
        # 4. Correlation of Daily Changes (Volatility Alignment)
        # Use the dataframe where BOTH values are non-NaN to calculate changes for correlation
        valid_comparison = valid_comparison.copy() # Avoid SettingWithCopyWarning
        valid_comparison['Change_Orig_Corr'] = valid_comparison['Value_Orig'].diff()
        valid_comparison['Change_New_Corr'] = valid_comparison['Value_New'].diff()
        # Drop NaNs created by the diff() itself (first row)
        valid_changes = valid_comparison.dropna(subset=['Change_Orig_Corr', 'Change_New_Corr'])
        # --- Debug Logging Start ---
        # if sec_id == 'Alpha001': # Log only for a specific security to avoid flooding
        #     log.debug(f"Debug {sec_id} - valid_changes DataFrame (first 5 rows):\n{valid_changes.head()}")
        #     log.debug(f"Debug {sec_id} - valid_changes count: {len(valid_changes)}")
        # --- Debug Logging End ---
        if len(valid_changes) >= 2:
            change_corr = valid_changes['Change_Orig_Corr'].corr(valid_changes['Change_New_Corr'])
            sec_stats['Change_Correlation'] = change_corr if pd.notna(change_corr) else None
        else:
            sec_stats['Change_Correlation'] = None
            # Log why correlation is None
            log.debug(f"Cannot calculate Change_Correlation for {sec_id}. Need >= 2 valid change pairs, found {len(valid_changes)}.")
        # 5. Difference Statistics (use the valid_comparison df where both values exist)
        valid_comparison['Abs_Diff'] = (valid_comparison['Value_Orig'] - valid_comparison['Value_New']).abs()
        sec_stats['Mean_Abs_Diff'] = valid_comparison['Abs_Diff'].mean() # Mean diff where both values exist
        sec_stats['Max_Abs_Diff'] = valid_comparison['Abs_Diff'].max() # Max diff where both values exist
        # Count NaNs - use original group
        sec_stats['NaN_Count_Orig'] = group['Value_Orig'].isna().sum()
        sec_stats['NaN_Count_New'] = group['Value_New'].isna().sum()
        sec_stats['Total_Points'] = len(group)
        stats_list.append(sec_stats)
    summary_df = pd.DataFrame(stats_list)
    # Merge static data back
    if not static_data.empty and id_col in static_data.columns and id_col in summary_df.columns:
        summary_df = pd.merge(summary_df, static_data, on=id_col, how='left')
    elif not static_data.empty:
         log.warning(f"Could not merge static data back. ID column '{id_col}' missing from static_data ({id_col in static_data.columns}) or summary_df ({id_col in summary_df.columns}).")
    log.info(f"Finished calculating stats. Summary shape: {summary_df.shape}")
    return summary_df
# --- Routes ---
@comparison_bp.route('/comparison/summary')
def summary():
    """Displays the comparison summary page with server-side filtering, sorting, and pagination."""
    log.info("--- Starting Comparison Summary Request ---")
    try:
        # --- Get Request Parameters ---
        page = request.args.get('page', 1, type=int)
        sort_by = request.args.get('sort_by', 'Change_Correlation') # Default sort
        sort_order = request.args.get('sort_order', 'desc').lower()
        if sort_order not in ['asc', 'desc']:
            sort_order = 'desc'
        ascending = sort_order == 'asc'
        # Get active filters (ensuring keys are correct)
        active_filters = {k.replace('filter_', ''): v 
                          for k, v in request.args.items() 
                          if k.startswith('filter_') and v}
        log.info(f"Request Params: Page={page}, SortBy={sort_by}, Order={sort_order}, Filters={active_filters}")
        # --- Load and Prepare Data --- 
        # Capture the actual ID column name returned by the load function
        merged_data, static_data, static_cols, actual_id_col = load_comparison_data()
        if actual_id_col is None:
            log.error("Failed to get ID column name during data loading.")
            return "Error loading comparison data: Could not determine ID column.", 500
        # Pass the actual ID column name to the stats calculation function
        summary_stats = calculate_comparison_stats(merged_data, static_data, id_col=actual_id_col)
        if summary_stats.empty and not merged_data.empty:
             log.warning("Calculation resulted in empty stats DataFrame, but merged data was present.")
        elif summary_stats.empty:
             log.info("No summary statistics could be calculated.")
             # Render with message if empty even before filtering
             return render_template('comparison_page.html',
                                    table_data=[],
                                    columns_to_display=[],
                                    id_column_name=actual_id_col,
                                    filter_options={},
                                    active_filters={},
                                    current_sort_by=sort_by,
                                    current_sort_order=sort_order,
                                    pagination=None,
                                    message="No comparison data available.")
        # --- Collect Filter Options (From Full Dataset Before Filtering) --- 
        filter_options = {}
        potential_filter_cols = static_cols # Add other potential categorical columns from summary_stats if needed
        for col in potential_filter_cols:
            if col in summary_stats.columns:
                unique_vals = summary_stats[col].dropna().unique().tolist()
                # Basic type check and sort if possible - Improved Robust Sorting Key
                try:
                    unique_vals = sorted(unique_vals, key=lambda x: \
                        (0, float(x)) if isinstance(x, bool) else \
                        (0, x) if isinstance(x, (int, float)) else \
                        (0, float(x)) if isinstance(x, str) and x.replace('.', '', 1).lstrip('-').isdigit() else \
                        (1, x) if isinstance(x, str) else \
                        (2, x) # Fallback for other types
                    )
                except TypeError as e:
                    # If sorting fails (e.g., comparing incompatible types not caught by key),
                    # fall back to string sorting as a last resort.
                    log.warning(f"Type error during sorting unique values for column '{col}': {e}. Falling back to string sort.")
                    try:
                         unique_vals = sorted(str(x) for x in unique_vals)
                    except Exception as final_sort_err:
                        log.error(f"Final string sort failed for column '{col}': {final_sort_err}")
                        # Keep original unsorted list if all else fails
                if unique_vals:
                    filter_options[col] = unique_vals
        final_filter_options = dict(sorted(filter_options.items()))
        # --- Apply Filtering ---
        filtered_stats = summary_stats.copy()
        if active_filters:
            log.info(f"Applying filters: {active_filters}")
            for col, value in active_filters.items():
                if col in filtered_stats.columns and value:
                    # Handle potential type mismatches if filtering on numeric/boolean
                    try:
                        # Attempt direct comparison first
                        if filtered_stats[col].dtype == 'boolean':
                             filtered_stats = filtered_stats[filtered_stats[col] == (value.lower() == 'true')]
                        # Add more specific type handling if needed (e.g., numeric ranges)
                        else:
                             filtered_stats = filtered_stats[filtered_stats[col].astype(str).str.contains(value, case=False, na=False)] # Case-insensitive string contains
                    except Exception as e:
                        log.warning(f"Could not apply filter on column '{col}' with value '{value}'. Error: {e}")
                        # Optionally skip this filter or handle differently
            log.info(f"Stats shape after filtering: {filtered_stats.shape}")
        else:
             log.info("No active filters.")
        # --- Apply Sorting ---
        if sort_by in filtered_stats.columns:
            log.info(f"Sorting by '{sort_by}' {sort_order}")
            # Ensure numeric columns are sorted numerically, handle NaNs
            if pd.api.types.is_numeric_dtype(filtered_stats[sort_by]):
                 filtered_stats = filtered_stats.sort_values(by=sort_by, ascending=ascending, na_position='last')
            else:
                 # Attempt string sort for non-numeric, case-insensitive might be desired
                 filtered_stats = filtered_stats.sort_values(by=sort_by, ascending=ascending, na_position='last', key=lambda col: col.astype(str).str.lower())
        else:
            log.warning(f"Sort column '{sort_by}' not found in filtered data. Skipping sort.")
            sort_by = actual_id_col # Fallback sort? Or remove sort indicator in template?
            sort_order = 'asc'
            ascending = True
        # --- Define Columns to Display ---
        # Identify fund columns (case-insensitive check) within static columns
        fund_cols = sorted([col for col in static_cols if 'fund' in col.lower() and col != actual_id_col])
        # Identify other static columns (excluding ID and fund columns)
        other_static_cols = sorted([col for col in static_cols if col != actual_id_col and col not in fund_cols])
        # Identify calculated columns to display (excluding helper/raw date/count columns)
        calculated_cols = sorted([col for col in summary_stats.columns
                                 if col not in static_cols and col != actual_id_col and
                                 col not in ['Start_Date_Orig', 'End_Date_Orig', 'Start_Date_New', 'End_Date_New',
                                              'NaN_Count_Orig', 'NaN_Count_New', 'Total_Points',
                                              'Overall_Start_Date', 'Overall_End_Date']]) # Exclude helper/raw date columns
        # Assemble the final list: ID, Other Static, Calculated, Fund Columns
        columns_to_display = [actual_id_col] + other_static_cols + calculated_cols + fund_cols
        log.debug(f"Columns to display: {columns_to_display}")
        # --- Pagination ---
        total_items = len(filtered_stats)
        if total_items == 0:
             log.info("No data remaining after filtering.")
             # Render with message if filtering resulted in empty set
             return render_template('comparison_page.html',
                                    table_data=[],
                                    columns_to_display=columns_to_display, # Still pass columns for header
                                    id_column_name=actual_id_col,
                                    filter_options=filter_options,
                                    active_filters=active_filters,
                                    current_sort_by=sort_by,
                                    current_sort_order=sort_order,
                                    pagination=None,
                                    message="No data matches the current filters.")
        # Ensure PER_PAGE_COMPARISON is positive
        safe_per_page = max(1, PER_PAGE_COMPARISON)
        total_pages = math.ceil(total_items / safe_per_page)
        total_pages = max(1, total_pages) # Ensure at least 1 page
        page = max(1, min(page, total_pages)) # Ensure page is within valid range
        start_index = (page - 1) * safe_per_page
        end_index = start_index + safe_per_page
        log.info(f"Pagination: Total items={total_items}, Total pages={total_pages}, Current page={page}, Per page={safe_per_page}")
        # Calculate display page numbers
        page_window = 2
        start_page_display = max(1, page - page_window)
        end_page_display = min(total_pages, page + page_window)
        paginated_stats = filtered_stats.iloc[start_index:end_index]
        # --- Prepare Data for Template ---
        # Filter the DataFrame to only these columns AFTER pagination
        paginated_stats = paginated_stats[[col for col in columns_to_display if col in paginated_stats.columns]]
        table_data_list = paginated_stats.to_dict(orient='records')
        # Replace NaN with None for template rendering
        for row in table_data_list:
            for key, value in row.items():
                if pd.isna(value):
                    row[key] = None
        # Create pagination context
        pagination_context = {
            'page': page,
            'per_page': safe_per_page,
            'total_pages': total_pages,
            'total_items': total_items,
            'has_prev': page > 1,
            'has_next': page < total_pages,
            'prev_num': page - 1,
            'next_num': page + 1,
            'start_page_display': start_page_display, # Pass calculated start page
            'end_page_display': end_page_display,     # Pass calculated end page
            # Function to generate URLs for pagination links, preserving state
            'url_for_page': lambda p: url_for('comparison_bp.summary', 
                                              page=p, 
                                              sort_by=sort_by, 
                                              sort_order=sort_order, 
                                              **{f'filter_{k}': v for k, v in active_filters.items()})
        }
    except Exception as e:
        log.exception("Error occurred during comparison summary processing.") # Log full traceback
        return render_template('comparison_page.html', 
                               message=f"An unexpected error occurred: {e}", 
                               table_data=[], 
                               pagination=None,
                               filter_options={}, 
                               active_filters={}, 
                               columns_to_display=[],
                               id_column_name='Security') # Provide a default ID name
    # --- Render Template ---
    return render_template('comparison_page.html',
                           table_data=table_data_list,
                           columns_to_display=columns_to_display,
                           id_column_name=actual_id_col,
                           filter_options=filter_options,
                           active_filters=active_filters,
                           current_sort_by=sort_by,
                           current_sort_order=sort_order,
                           pagination=pagination_context, # Pass pagination object
                           message=None)
@comparison_bp.route('/comparison/details/<path:security_id>')
def details(security_id):
    """Displays the comparison details page for a single security."""
    try:
        # Reload or filter the merged data for the specific security
        # This might be inefficient - consider caching or passing data if possible
        merged_data, static_data, _, actual_id_col = load_comparison_data()
        if actual_id_col is None:
            log.error("Failed to get ID column name during data loading for details page.")
            return "Error loading comparison data: Could not determine ID column.", 500
        # Filter using the actual ID column name
        security_data = merged_data[merged_data[actual_id_col] == security_id].copy()
        if security_data.empty:
            return "Security ID not found", 404
        # Get the static data for this specific security
        sec_static_data = static_data[static_data[actual_id_col] == security_id]
        # Recalculate detailed stats for this security, passing the correct ID column
        stats_df = calculate_comparison_stats(security_data.copy(), sec_static_data, id_col=actual_id_col)
        security_stats = stats_df.iloc[0].where(pd.notnull(stats_df.iloc[0]), None).to_dict() if not stats_df.empty else {}
        # Prepare chart data
        security_data['Date_Str'] = security_data['Date'].dt.strftime('%Y-%m-%d')
        # Convert NaN to None using list comprehension after .tolist()
        data_orig = security_data['Value_Orig'].tolist()
        data_orig_processed = [None if pd.isna(x) else x for x in data_orig]
        data_new = security_data['Value_New'].tolist()
        data_new_processed = [None if pd.isna(x) else x for x in data_new]
        chart_data = {
            'labels': security_data['Date_Str'].tolist(),
            'datasets': [
                {
                    'label': 'Original Spread (Sec_spread)',
                    'data': data_orig_processed, # Use processed list
                    'borderColor': COLOR_PALETTE[0 % len(COLOR_PALETTE)],
                    'tension': 0.1
                },
                {
                    'label': 'New Spread (Sec_spreadSP)',
                    'data': data_new_processed, # Use processed list
                    'borderColor': COLOR_PALETTE[1 % len(COLOR_PALETTE)],
                    'tension': 0.1
                }
            ]
        }
        # Get static attributes for display (use actual_id_col if it's 'Security Name')
        # Best to get from security_stats which should now include merged static data
        security_name_display = security_stats.get('Security Name', security_id) if actual_id_col == 'Security Name' else security_id
        # If 'Security Name' is not the ID, try to get it from stats
        if actual_id_col != 'Security Name' and 'Security Name' in security_stats:
             security_name_display = security_stats.get('Security Name', security_id)
        return render_template('comparison_details_page.html',
                               security_id=security_id,
                               security_name=security_name_display,
                               chart_data=chart_data, # Pass as JSONifiable dict
                               stats=security_stats, # Pass comparison stats
                               id_column_name=actual_id_col) # Pass actual ID col name
    except Exception as e:
        log.exception(f"Error generating comparison details page for {security_id}.")
        return f"An error occurred: {e}", 500
</file>

<file path="views/curve_views.py">
# views/curve_views.py
# Purpose: Defines Flask routes for displaying yield curve analysis results.
import pandas as pd
from flask import Blueprint, render_template, jsonify, request, abort
from curve_analyzer import (
    load_curve_data,
    calculate_curve_changes,
    analyze_curve_jumps,
    get_summary_stats,
    get_curve_for_date,
    get_available_dates
)
import logging
# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
curve_bp = Blueprint('curve_bp', __name__, template_folder='templates')
# --- Data Loading and Caching (Simple Example) ---
# In a real app, consider more robust caching (e.g., Flask-Caching)
# or loading data only when needed if it's very large.
curve_data_raw = None
curve_data_analyzed = None
analysis_summary = None
def load_and_analyze_data():
    """Loads and analyzes data if not already done."""
    global curve_data_raw, curve_data_analyzed, analysis_summary
    if curve_data_raw is None:
        logger.info("Loading and processing curve data for the first time...")
        raw_data = load_curve_data()
        if not raw_data.empty:
            data_with_changes = calculate_curve_changes(raw_data)
            analysis_results = analyze_curve_jumps(data_with_changes) # Get the full df with flags
            summary = get_summary_stats(analysis_results) # Pass the df with flags
            curve_data_raw = raw_data # Store the raw data with TermInDays etc.
            curve_data_analyzed = analysis_results # Store summary of flagged points
            analysis_summary = summary # Store latest day summary counts
            logger.info("Curve data loaded and analyzed.")
        else:
            logger.error("Failed to load curve data.")
            # Set to empty DataFrames to avoid errors later
            curve_data_raw = pd.DataFrame()
            curve_data_analyzed = pd.DataFrame()
            analysis_summary = pd.DataFrame()
@curve_bp.before_app_first_request
def initial_data_load():
    """Ensure data is loaded before the first request to this blueprint."""
    load_and_analyze_data()
# --- Routes ---
@curve_bp.route('/curves/summary')
def summary_page():
    """Displays the summary of flagged issues for the latest date."""
    load_and_analyze_data() # Ensure data is loaded/updated if needed
    if analysis_summary is None or analysis_summary.empty:
         logger.warning("No analysis summary data available for summary page.")
         summary_data = []
         latest_date_str = "N/A"
    else:
        # Get latest date from the raw data index if possible
        if not curve_data_raw.empty:
            latest_date = curve_data_raw.index.get_level_values('Date').max()
            latest_date_str = latest_date.strftime('%Y-%m-%d')
        else:
            latest_date_str = "N/A"
        summary_data = analysis_summary.reset_index().to_dict('records')
    return render_template('curve_summary_page.html',
                           summary_data=summary_data,
                           latest_date=latest_date_str)
@curve_bp.route('/curves/details/<currency_code>')
def details_page(currency_code):
    """Displays the curve chart for a specific currency and allows date selection."""
    load_and_analyze_data() # Ensure data is loaded
    if curve_data_raw is None or curve_data_raw.empty:
        logger.error(f"Curve data not loaded, cannot show details for {currency_code}")
        abort(404, description="Curve data not available.")
    currency_code = currency_code.upper()
    available_dates = get_available_dates(curve_data_raw, currency_code)
    if not available_dates:
        logger.warning(f"No data or dates found for currency: {currency_code}")
        # Render template with message indicating no data?
        return render_template('curve_details_page.html',
                               currency=currency_code,
                               available_dates=[],
                               latest_date_str=None,
                               initial_chart_data=None,
                               error_message=f"No data found for currency code: {currency_code}")
    latest_date = available_dates[0] # Dates are sorted descending
    latest_date_str = latest_date.strftime('%Y-%m-%d')
    # Fetch initial data for the latest date chart
    initial_curve_data = get_curve_for_date(curve_data_raw, currency_code, latest_date_str)
    if initial_curve_data.empty:
         logger.warning(f"Could not retrieve initial curve data for {currency_code} on {latest_date_str}")
         initial_chart_data = None # Handle in template
    else:
         initial_chart_data = initial_curve_data.to_dict('records')
    # Format dates for the dropdown
    date_options = [{'value': d.strftime('%Y-%m-%d'), 'text': d.strftime('%d-%b-%Y')} for d in available_dates]
    return render_template('curve_details_page.html',
                           currency=currency_code,
                           available_dates=date_options,
                           latest_date_str=latest_date_str,
                           initial_chart_data=initial_chart_data)
@curve_bp.route('/curves/data/<currency_code>/<date_str>')
def get_curve_data_api(currency_code, date_str):
    """API endpoint to fetch curve data for a specific currency and date."""
    load_and_analyze_data() # Ensure data is loaded
    if curve_data_raw is None or curve_data_raw.empty:
        logger.error(f"Curve data not loaded, cannot fetch data for {currency_code}/{date_str}")
        return jsonify({"error": "Curve data not available"}), 500
    currency_code = currency_code.upper()
    # Validate date_str format? Basic check here.
    try:
        pd.to_datetime(date_str)
    except ValueError:
        logger.error(f"Invalid date format requested: {date_str}")
        return jsonify({"error": "Invalid date format. Use YYYY-MM-DD."}), 400
    curve_data = get_curve_for_date(curve_data_raw, currency_code, date_str)
    if curve_data.empty:
        logger.warning(f"No curve data found for {currency_code} on {date_str}")
        # Return empty list or error? Empty list might be better for chart updates
        return jsonify([])
        # return jsonify({"error": f"No data found for {currency_code} on {date_str}"}), 404
    # Convert TermInDays back for potential use, though Term might be sufficient
    chart_data = curve_data.to_dict('records')
    return jsonify(chart_data)
</file>

<file path="views/duration_comparison_views.py">
# views/duration_comparison_views.py
# This module defines the Flask Blueprint for comparing two security duration datasets.
# It includes routes for a summary view listing securities with comparison metrics
# and a detail view showing overlayed time-series charts and statistics for a single security.
from flask import Blueprint, render_template, request, current_app, jsonify, url_for
import pandas as pd
import os
import logging
import math # Add math for pagination calculation
# Assuming security_processing and utils are in the parent directory or configured in PYTHONPATH
try:
    from security_processing import load_and_process_security_data # May need adjustments
    from utils import parse_fund_list # Example utility
    from config import DATA_FOLDER, COLOR_PALETTE
except ImportError:
    # Handle potential import errors if the structure is different
    logging.error("Could not import required modules from parent directory.")
    # Add fallback imports or path adjustments if necessary
    # Example: sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
    from ..security_processing import load_and_process_security_data
    from ..utils import parse_fund_list
    from ..config import DATA_FOLDER, COLOR_PALETTE
duration_comparison_bp = Blueprint('duration_comparison_bp', __name__,
                        template_folder='../templates',
                        static_folder='../static')
# Configure logging
log = logging.getLogger(__name__)
PER_PAGE_COMPARISON = 50 # Items per page for comparison summary
# --- Data Loading and Processing ---
def load_comparison_data(file1='sec_duration.csv', file2='sec_durationSP.csv'): # Updated filenames
    """Loads, processes, and merges data from two security duration files.
    Returns:
        tuple: (merged_df, static_data, common_static_cols, id_col_name)
               Returns (pd.DataFrame(), pd.DataFrame(), [], None) on error.
    """
    log.info(f"Loading duration comparison data: {file1} and {file2}")
    # Pass only the filename, as load_and_process_security_data prepends DATA_FOLDER internally
    df1, static_cols1 = load_and_process_security_data(file1)
    df2, static_cols2 = load_and_process_security_data(file2)
    if df1.empty or df2.empty:
        log.warning(f"One or both duration dataframes are empty. File1 empty: {df1.empty}, File2 empty: {df2.empty}")
        return pd.DataFrame(), pd.DataFrame(), [], None # Return None for id_col_name
    # Identify common static columns (excluding the ID column used for merging)
    common_static_cols = list(set(static_cols1) & set(static_cols2))
    # Get the actual ID column name (should be the same for both, use df1)
    if df1.index.nlevels == 2:
        id_col_name = df1.index.names[1] # Assuming 'Security ID'/Name is the second level
        log.info(f"Identified ID column from index: {id_col_name}")
    else:
        log.error("Processed Duration DataFrame df1 does not have the expected 2-level MultiIndex.")
        return pd.DataFrame(), pd.DataFrame(), [], None # Return None for id_col_name
    # Prepare for merge - keep only necessary columns and rename Value columns
    df1_merge = df1.reset_index()[[id_col_name, 'Date', 'Value'] + common_static_cols].rename(columns={'Value': 'Value_Orig'})
    df2_merge = df2.reset_index()[[id_col_name, 'Date', 'Value']].rename(columns={'Value': 'Value_New'}) # Don't need static cols twice
    # Perform an outer merge to keep all dates and securities from both files
    merged_df = pd.merge(df1_merge, df2_merge, on=[id_col_name, 'Date'], how='outer')
    # Calculate daily changes
    merged_df = merged_df.sort_values(by=[id_col_name, 'Date'])
    merged_df['Change_Orig'] = merged_df.groupby(id_col_name)['Value_Orig'].diff()
    merged_df['Change_New'] = merged_df.groupby(id_col_name)['Value_New'].diff()
    # Store static data separately - get the latest version per security
    static_data = merged_df.groupby(id_col_name)[common_static_cols].last().reset_index()
    log.info(f"Successfully merged duration data. Shape: {merged_df.shape}")
    return merged_df, static_data, common_static_cols, id_col_name # Return the identified ID column name
def calculate_comparison_stats(merged_df, static_data, id_col):
    """Calculates comparison statistics for each security's duration.
    Args:
        merged_df (pd.DataFrame): The merged dataframe of original and new duration values.
        static_data (pd.DataFrame): DataFrame with static info per security.
        id_col (str): The name of the column containing the Security ID/Name.
    """
    if merged_df.empty:
        return pd.DataFrame()
    if id_col not in merged_df.columns:
        log.error(f"Specified id_col '{id_col}' not found in merged_df columns: {merged_df.columns.tolist()}")
        return pd.DataFrame() # Cannot group without the ID column
    log.info(f"Calculating duration comparison statistics using ID column: {id_col}...")
    stats_list = []
    # Use the passed id_col here
    for sec_id, group in merged_df.groupby(id_col):
        sec_stats = {id_col: sec_id} # Use actual id_col name
        # Filter out rows where both values are NaN for overall analysis period
        group_valid_overall = group.dropna(subset=['Value_Orig', 'Value_New'], how='all')
        overall_min_date = group_valid_overall['Date'].min()
        overall_max_date = group_valid_overall['Date'].max()
        # Filter out rows where EITHER value is NaN for correlation/diff calculations
        valid_comparison = group.dropna(subset=['Value_Orig', 'Value_New'])
        # 1. Correlation of Levels
        if len(valid_comparison) >= 2: # Need at least 2 points for correlation
            # Use the NaN-dropped dataframe for correlation
            level_corr = valid_comparison['Value_Orig'].corr(valid_comparison['Value_New'])
            sec_stats['Level_Correlation'] = level_corr if pd.notna(level_corr) else None
        else:
             sec_stats['Level_Correlation'] = None
        # 2. Max / Min (use original group to get true max/min including non-overlapping points)
        sec_stats['Max_Orig'] = group['Value_Orig'].max()
        sec_stats['Min_Orig'] = group['Value_Orig'].min()
        sec_stats['Max_New'] = group['Value_New'].max()
        sec_stats['Min_New'] = group['Value_New'].min()
        # 3. Date Range Comparison - Refined Logic
        # Find min/max dates within the MERGED data where each series is individually valid
        min_date_orig_idx = group['Value_Orig'].first_valid_index()
        max_date_orig_idx = group['Value_Orig'].last_valid_index()
        min_date_new_idx = group['Value_New'].first_valid_index()
        max_date_new_idx = group['Value_New'].last_valid_index()
        sec_stats['Start_Date_Orig'] = group.loc[min_date_orig_idx, 'Date'] if min_date_orig_idx is not None else None
        sec_stats['End_Date_Orig'] = group.loc[max_date_orig_idx, 'Date'] if max_date_orig_idx is not None else None
        sec_stats['Start_Date_New'] = group.loc[min_date_new_idx, 'Date'] if min_date_new_idx is not None else None
        sec_stats['End_Date_New'] = group.loc[max_date_new_idx, 'Date'] if max_date_new_idx is not None else None
        # Check if the start and end dates MATCH for the valid periods of EACH series
        same_start = pd.Timestamp(sec_stats['Start_Date_Orig']) == pd.Timestamp(sec_stats['Start_Date_New']) if sec_stats['Start_Date_Orig'] and sec_stats['Start_Date_New'] else False
        same_end = pd.Timestamp(sec_stats['End_Date_Orig']) == pd.Timestamp(sec_stats['End_Date_New']) if sec_stats['End_Date_Orig'] and sec_stats['End_Date_New'] else False
        sec_stats['Same_Date_Range'] = same_start and same_end
        # Add overall date range for info
        sec_stats['Overall_Start_Date'] = overall_min_date
        sec_stats['Overall_End_Date'] = overall_max_date
        # 4. Correlation of Daily Changes (Volatility Alignment)
        # Use the dataframe where BOTH values are non-NaN to calculate changes for correlation
        valid_comparison = valid_comparison.copy() # Avoid SettingWithCopyWarning
        valid_comparison['Change_Orig_Corr'] = valid_comparison['Value_Orig'].diff()
        valid_comparison['Change_New_Corr'] = valid_comparison['Value_New'].diff()
        # Drop NaNs created by the diff() itself (first row)
        valid_changes = valid_comparison.dropna(subset=['Change_Orig_Corr', 'Change_New_Corr'])
        if len(valid_changes) >= 2:
            change_corr = valid_changes['Change_Orig_Corr'].corr(valid_changes['Change_New_Corr'])
            sec_stats['Change_Correlation'] = change_corr if pd.notna(change_corr) else None
        else:
            sec_stats['Change_Correlation'] = None
            log.debug(f"Cannot calculate Duration Change_Correlation for {sec_id}. Need >= 2 valid change pairs, found {len(valid_changes)}.")
        # 5. Difference Statistics (use the valid_comparison df where both values exist)
        valid_comparison['Abs_Diff'] = (valid_comparison['Value_Orig'] - valid_comparison['Value_New']).abs()
        sec_stats['Mean_Abs_Diff'] = valid_comparison['Abs_Diff'].mean() # Mean diff where both values exist
        sec_stats['Max_Abs_Diff'] = valid_comparison['Abs_Diff'].max() # Max diff where both values exist
        # Count NaNs - use original group
        sec_stats['NaN_Count_Orig'] = group['Value_Orig'].isna().sum()
        sec_stats['NaN_Count_New'] = group['Value_New'].isna().sum()
        sec_stats['Total_Points'] = len(group)
        stats_list.append(sec_stats)
    summary_df = pd.DataFrame(stats_list)
    # Merge static data back
    if not static_data.empty and id_col in static_data.columns and id_col in summary_df.columns:
        summary_df = pd.merge(summary_df, static_data, on=id_col, how='left')
    elif not static_data.empty:
         log.warning(f"Could not merge static data back for duration comparison. ID column '{id_col}' missing from static_data ({id_col in static_data.columns}) or summary_df ({id_col in summary_df.columns}).")
    log.info(f"Finished calculating duration stats. Summary shape: {summary_df.shape}")
    return summary_df
# --- Routes ---
@duration_comparison_bp.route('/duration_comparison/summary') # Updated route
def summary():
    """Displays the duration comparison summary page with server-side filtering, sorting, and pagination."""
    log.info("--- Starting Duration Comparison Summary Request ---")
    try:
        # --- Get Request Parameters ---
        page = request.args.get('page', 1, type=int)
        sort_by = request.args.get('sort_by', 'Change_Correlation') # Default sort
        sort_order = request.args.get('sort_order', 'desc').lower()
        if sort_order not in ['asc', 'desc']:
            sort_order = 'desc'
        ascending = sort_order == 'asc'
        # Get active filters (ensuring keys are correct)
        active_filters = {k.replace('filter_', ''): v
                          for k, v in request.args.items()
                          if k.startswith('filter_') and v}
        log.info(f"Request Params: Page={page}, SortBy={sort_by}, Order={sort_order}, Filters={active_filters}")
        # --- Load and Prepare Data ---
        # Capture the actual ID column name returned by the load function
        merged_data, static_data, static_cols, actual_id_col = load_comparison_data()
        if actual_id_col is None:
            log.error("Failed to get ID column name during duration data loading.")
            return "Error loading duration comparison data: Could not determine ID column.", 500
        # Pass the actual ID column name to the stats calculation function
        summary_stats = calculate_comparison_stats(merged_data, static_data, id_col=actual_id_col)
        if summary_stats.empty and not merged_data.empty:
             log.warning("Duration calculation resulted in empty stats DataFrame, but merged data was present.")
        elif summary_stats.empty:
             log.info("No duration summary statistics could be calculated.")
             # Render with message if empty even before filtering
             return render_template('duration_comparison_page.html', # Updated template
                                    table_data=[],
                                    columns_to_display=[],
                                    id_column_name=actual_id_col,
                                    filter_options={},
                                    active_filters={},
                                    current_sort_by=sort_by,
                                    current_sort_order=sort_order,
                                    pagination=None,
                                    message="No duration comparison data available.")
        # --- Collect Filter Options (From Full Dataset Before Filtering) ---
        filter_options = {}
        potential_filter_cols = static_cols # Add other potential categorical columns from summary_stats if needed
        for col in potential_filter_cols:
            if col in summary_stats.columns:
                unique_vals = summary_stats[col].dropna().unique().tolist()
                # Basic type check and sort if possible - Improved Robust Sorting Key
                try:
                    # Attempt numerical sort first if applicable (handles ints/floats mixed with strings gracefully)
                    sorted_vals = sorted(unique_vals, key=lambda x: (isinstance(x, (int, float)), x))
                except TypeError:
                     # Fallback to string sort if mixed types cause issues
                    sorted_vals = sorted(unique_vals, key=str)
                filter_options[col] = sorted_vals
        log.info(f"Filter options generated: {list(filter_options.keys())}")
        # --- Apply Filters ---
        filtered_data = summary_stats.copy()
        if active_filters:
            log.info(f"Applying filters: {active_filters}")
            for col, value in active_filters.items():
                if col in filtered_data.columns:
                    # Handle potential type mismatches (e.g., filter value is string, column is number)
                    try:
                         # Convert filter value to column type if possible
                        col_type = filtered_data[col].dtype
                        if pd.api.types.is_numeric_dtype(col_type):
                            value = pd.to_numeric(value, errors='ignore') # Coerce to numeric if possible
                        elif pd.api.types.is_datetime64_any_dtype(col_type):
                             value = pd.to_datetime(value, errors='ignore') # Coerce to datetime if possible
                        # Apply filter (handle NaN explicitly if needed)
                        if pd.isna(value):
                            filtered_data = filtered_data[filtered_data[col].isna()]
                        else:
                            filtered_data = filtered_data[filtered_data[col] == value]
                    except Exception as e:
                        log.warning(f"Could not apply filter for column '{col}' with value '{value}'. Error: {e}. Skipping filter.")
                else:
                    log.warning(f"Filter column '{col}' not found in data. Skipping filter.")
            log.info(f"Data shape after filtering: {filtered_data.shape}")
        else:
            log.info("No active filters.")
        # --- Apply Sorting ---
        if sort_by in filtered_data.columns:
            log.info(f"Sorting by '{sort_by}' ({'Ascending' if ascending else 'Descending'})")
            # Handle NaNs during sorting - place them appropriately
            na_position = 'last' # Default, can be 'first' if preferred
            try:
                filtered_data = filtered_data.sort_values(by=sort_by, ascending=ascending, na_position=na_position)
            except Exception as e:
                log.error(f"Error during sorting by '{sort_by}': {e}. Falling back to default sort.")
                sort_by = 'Change_Correlation' # Revert to default if error
                ascending = False
                filtered_data = filtered_data.sort_values(by=sort_by, ascending=ascending, na_position=na_position)
        else:
            log.warning(f"Sort column '{sort_by}' not found. Using default 'Change_Correlation'.")
            sort_by = 'Change_Correlation' # Ensure default is used if provided key is invalid
            ascending = False
            filtered_data = filtered_data.sort_values(by=sort_by, ascending=ascending, na_position='last')
        # --- Pagination ---
        total_items = len(filtered_data)
        total_pages = math.ceil(total_items / PER_PAGE_COMPARISON)
        start_index = (page - 1) * PER_PAGE_COMPARISON
        end_index = start_index + PER_PAGE_COMPARISON
        paginated_data = filtered_data.iloc[start_index:end_index]
        log.info(f"Pagination: Total items={total_items}, Total pages={total_pages}, Current page={page}, Displaying items {start_index}-{end_index-1}")
        # --- Prepare for Template ---
        # Define columns to display (ensure actual_id_col is first)
        # Base columns - adjust as needed for duration comparison specifics
        base_cols = [
            'Level_Correlation', 'Change_Correlation',
            'Mean_Abs_Diff', 'Max_Abs_Diff',
            'NaN_Count_Orig', 'NaN_Count_New', 'Total_Points',
            'Same_Date_Range',
            'Start_Date_Orig', 'End_Date_Orig',
            'Start_Date_New', 'End_Date_New',
            'Max_Orig', 'Min_Orig', 'Max_New', 'Min_New'
            # Add/remove columns as needed
        ]
        # Ensure static columns come after the ID and before the calculated stats
        columns_to_display = [actual_id_col] + \
                             [col for col in static_cols if col != actual_id_col and col in paginated_data.columns] + \
                             [col for col in base_cols if col in paginated_data.columns]
        # Convert DataFrame to list of dictionaries for easy template iteration
        table_data = paginated_data.to_dict(orient='records')
        # Format specific columns (like correlations, dates)
        for row in table_data:
            for col in ['Level_Correlation', 'Change_Correlation']:
                 if col in row and pd.notna(row[col]):
                    row[col] = f"{row[col]:.4f}" # Format correlation
            for col in ['Start_Date_Orig', 'End_Date_Orig', 'Start_Date_New', 'End_Date_New']:
                 if col in row and pd.notna(row[col]):
                    try:
                        # Ensure it's a Timestamp before formatting
                        if isinstance(row[col], pd.Timestamp):
                             row[col] = row[col].strftime('%Y-%m-%d')
                        # If already string, assume correct format or skip
                    except AttributeError:
                        log.debug(f"Could not format date column '{col}' with value '{row[col]}'. Type: {type(row[col])}")
                        pass # Keep original value if formatting fails
        # Create pagination object
        pagination = {
            'page': page,
            'per_page': PER_PAGE_COMPARISON,
            'total_items': total_items,
            'total_pages': total_pages,
            'has_prev': page > 1,
            'has_next': page < total_pages,
            'prev_num': page - 1 if page > 1 else None,
            'next_num': page + 1 if page < total_pages else None,
        }
        log.info("--- Successfully Prepared Data for Duration Comparison Template ---")
        return render_template('duration_comparison_page.html', # Updated template
                               table_data=table_data,
                               columns_to_display=columns_to_display,
                               id_column_name=actual_id_col, # Pass the ID column name
                               filter_options=filter_options,
                               active_filters=active_filters,
                               current_sort_by=sort_by,
                               current_sort_order=sort_order,
                               pagination=pagination,
                               message=None) # No message if data is present
    except FileNotFoundError as e:
        log.error(f"Duration comparison file not found: {e}")
        return f"Error: Required duration comparison file not found ({e.filename}). Check the Data folder.", 404
    except Exception as e:
        log.exception("An unexpected error occurred in the duration comparison summary view.") # Log full traceback
        return f"An internal error occurred: {e}", 500
@duration_comparison_bp.route('/duration_comparison/details/<path:security_id>') # Updated route
def details(security_id):
    """Displays detailed comparison charts for a single security's duration."""
    log.info(f"--- Starting Duration Comparison Details Request for Security ID: {security_id} ---")
    try:
        # Load the merged data again (could potentially cache this)
        # Specify filenames explicitly
        merged_df, _, common_static_cols, id_col_name = load_comparison_data(file1='sec_duration.csv', file2='sec_durationSP.csv')
        if id_col_name is None:
             log.error(f"Failed to get ID column name for details view (Security: {security_id}).")
             return "Error loading duration comparison data: Could not determine ID column.", 500
        if merged_df.empty:
            log.warning(f"Merged duration data is empty for details view (Security: {security_id}).")
            return f"No merged duration data found for Security ID: {security_id}", 404
        # Filter data for the specific security using the correct ID column name
        security_data = merged_df[merged_df[id_col_name] == security_id].copy() # Use .copy()
        if security_data.empty:
            log.warning(f"No duration data found for the specific Security ID: {security_id}")
            # Consider checking if the ID exists in the original files?
            return f"Duration data not found for Security ID: {security_id}", 404
        # Get static info for this security (handle potential multiple rows if ID isn't unique, take first)
        static_info = security_data[[id_col_name] + common_static_cols].iloc[0].to_dict() if not security_data.empty else {}
        # Sort by date for charting
        security_data = security_data.sort_values(by='Date')
        # Prepare data for Chart.js
        # Ensure 'Date' is in the correct string format for JSON/JS
        security_data['Date_Str'] = security_data['Date'].dt.strftime('%Y-%m-%d')
        chart_data = {
            'labels': security_data['Date_Str'].tolist(),
            'datasets': [
                {
                    'label': 'Original Duration',
                    'data': security_data['Value_Orig'].where(pd.notna(security_data['Value_Orig']), None).tolist(), # Replace NaN with None for JSON
                    'borderColor': COLOR_PALETTE[0 % len(COLOR_PALETTE)],
                    'fill': False,
                    'tension': 0.1
                },
                {
                    'label': 'New Duration',
                    'data': security_data['Value_New'].where(pd.notna(security_data['Value_New']), None).tolist(), # Replace NaN with None for JSON
                    'borderColor': COLOR_PALETTE[1 % len(COLOR_PALETTE)],
                    'fill': False,
                    'tension': 0.1
                }
            ]
        }
        # Calculate overall statistics for this security
        stats_summary = calculate_comparison_stats(security_data, pd.DataFrame([static_info]), id_col=id_col_name) # Pass single security data
        stats_dict = stats_summary.iloc[0].to_dict() if not stats_summary.empty else {}
         # Format dates and numbers in stats_dict before passing
        for key, value in stats_dict.items():
            if isinstance(value, pd.Timestamp):
                stats_dict[key] = value.strftime('%Y-%m-%d')
            elif isinstance(value, (int, float)):
                 if 'Correlation' in key and pd.notna(value):
                     stats_dict[key] = f"{value:.4f}"
                 elif 'Diff' in key and pd.notna(value):
                      stats_dict[key] = f"{value:.2f}" # Adjust formatting as needed
        log.info(f"Successfully prepared data for duration details template (Security: {security_id})")
        return render_template('duration_comparison_details_page.html', # Updated template
                               security_id=security_id,
                               static_info=static_info, # Pass static info
                               chart_data=chart_data,
                               stats_summary=stats_dict) # Pass calculated stats
    except FileNotFoundError as e:
        log.error(f"Duration comparison file not found for details view: {e} (Security: {security_id})")
        return f"Error: Required duration comparison file not found ({e.filename}). Check the Data folder.", 404
    except KeyError as e:
         log.error(f"KeyError accessing data for security '{security_id}': {e}. ID column used: '{id_col_name}'")
         return f"Error accessing data for security '{security_id}'. It might be missing required columns or have unexpected formatting.", 500
    except Exception as e:
        log.exception(f"An unexpected error occurred in the duration comparison details view for security '{security_id}'.") # Log full traceback
        return f"An internal error occurred while processing details for security '{security_id}': {e}", 500
</file>

<file path="views/exclusion_views.py">
"""
This module defines the Flask Blueprint for handling security exclusion management.
It provides routes to view the current exclusion list and add new securities
to the list.
"""
import os
import pandas as pd
from flask import Blueprint, render_template, request, redirect, url_for, current_app
from datetime import datetime
import logging
# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
# Define the Blueprint
exclusion_bp = Blueprint('exclusion_bp', __name__, template_folder='../templates')
EXCLUSIONS_FILE = 'exclusions.csv'
# Assuming sec_spread.csv contains the list of all possible securities
# We need to determine the correct file and column name for security IDs
# Let's tentatively use sec_spread.csv and 'Security ID'
SECURITIES_SOURCE_FILE = 'sec_spread.csv' # Adjust if needed
SECURITY_ID_COLUMN = 'Security Name' # Corrected column name
def get_data_path(filename):
    """Constructs the full path to a data file within the DATA_FOLDER."""
    return os.path.join(current_app.config['DATA_FOLDER'], filename)
def load_exclusions():
    """Loads the current list of exclusions from the CSV file."""
    exclusions_path = get_data_path(EXCLUSIONS_FILE)
    try:
        if os.path.exists(exclusions_path) and os.path.getsize(exclusions_path) > 0:
            df = pd.read_csv(exclusions_path, parse_dates=['AddDate', 'EndDate'], dayfirst=False) # Specify date format if needed
            # Ensure correct types after loading
            df['AddDate'] = pd.to_datetime(df['AddDate'], errors='coerce')
            df['EndDate'] = pd.to_datetime(df['EndDate'], errors='coerce')
            df['SecurityID'] = df['SecurityID'].astype(str)
            df['Comment'] = df['Comment'].astype(str)
            df = df.sort_values(by='AddDate', ascending=False)
            return df.to_dict('records')
        else:
            logging.info(f"'{EXCLUSIONS_FILE}' is empty or does not exist. Returning empty list.")
            return []
    except Exception as e:
        logging.error(f"Error loading exclusions from {exclusions_path}: {e}")
        return [] # Return empty list on error
def load_available_securities():
    """Loads the list of available security IDs from the source file."""
    securities_file_path = get_data_path(SECURITIES_SOURCE_FILE)
    try:
        if os.path.exists(securities_file_path):
            # Load only the necessary column
            # Use security_processing logic if more complex loading is needed
            df_securities = pd.read_csv(securities_file_path, usecols=[SECURITY_ID_COLUMN], encoding_errors='replace', on_bad_lines='skip')
            df_securities.dropna(subset=[SECURITY_ID_COLUMN], inplace=True)
            security_ids = df_securities[SECURITY_ID_COLUMN].astype(str).unique().tolist()
            security_ids.sort() # Sort for dropdown consistency
            return security_ids
        else:
            logging.warning(f"Securities source file '{SECURITIES_SOURCE_FILE}' not found at {securities_file_path}.")
            return []
    except KeyError:
        logging.error(f"Column '{SECURITY_ID_COLUMN}' not found in '{SECURITIES_SOURCE_FILE}'. Cannot load available securities.")
        return []
    except Exception as e:
        logging.error(f"Error loading available securities from {securities_file_path}: {e}")
        return []
def add_exclusion(security_id, end_date_str, comment):
    """Adds a new exclusion to the CSV file."""
    exclusions_path = get_data_path(EXCLUSIONS_FILE)
    try:
        # Basic validation
        if not security_id or not comment:
            logging.warning("Attempted to add exclusion with missing SecurityID or Comment.")
            return False, "Security ID and Comment are required."
        add_date = datetime.now().strftime('%Y-%m-%d')
        # Parse end_date, allow it to be empty
        end_date = pd.to_datetime(end_date_str, errors='coerce').strftime('%Y-%m-%d') if end_date_str else ''
        new_exclusion = pd.DataFrame({
            'SecurityID': [str(security_id)],
            'AddDate': [add_date],
            'EndDate': [end_date],
            'Comment': [str(comment)]
        })
        # Append to CSV, create header if file doesn't exist or is empty
        file_exists = os.path.exists(exclusions_path)
        is_empty = file_exists and os.path.getsize(exclusions_path) == 0
        write_header = not file_exists or is_empty
        new_exclusion.to_csv(exclusions_path, mode='a', header=write_header, index=False)
        logging.info(f"Added exclusion for SecurityID: {security_id}")
        return True, "Exclusion added successfully."
    except Exception as e:
        logging.error(f"Error adding exclusion to {exclusions_path}: {e}")
        return False, "An error occurred while saving the exclusion."
def remove_exclusion(security_id_to_remove, add_date_str_to_remove):
    """Removes a specific exclusion entry from the CSV file based on SecurityID and AddDate."""
    exclusions_path = get_data_path(EXCLUSIONS_FILE)
    try:
        if not os.path.exists(exclusions_path) or os.path.getsize(exclusions_path) == 0:
            logging.warning(f"Attempted to remove exclusion, but '{EXCLUSIONS_FILE}' is empty or does not exist.")
            return False, "Exclusion file is empty or missing."
        df = pd.read_csv(exclusions_path)
        # Ensure columns used for matching are strings
        df['SecurityID'] = df['SecurityID'].astype(str)
        # Keep AddDate as string for direct comparison with the string from the form
        df['AddDate'] = df['AddDate'].astype(str)
        security_id_to_remove = str(security_id_to_remove)
        original_count = len(df)
        # Filter out the row(s) to remove
        # Match both SecurityID and the AddDate string
        df_filtered = df[~((df['SecurityID'] == security_id_to_remove) & (df['AddDate'] == add_date_str_to_remove))]
        if len(df_filtered) == original_count:
            logging.warning(f"Exclusion entry for SecurityID '{security_id_to_remove}' with AddDate '{add_date_str_to_remove}' not found for removal.")
            return False, "Exclusion entry not found."
        # Overwrite the CSV with the filtered data
        df_filtered.to_csv(exclusions_path, index=False)
        logging.info(f"Removed exclusion entry for SecurityID: {security_id_to_remove}, AddDate: {add_date_str_to_remove}")
        return True, "Exclusion removed successfully."
    except Exception as e:
        logging.error(f"Error removing exclusion from {exclusions_path}: {e}")
        return False, "An error occurred while removing the exclusion."
@exclusion_bp.route('/exclusions', methods=['GET', 'POST'])
def manage_exclusions():
    """
    Handles viewing and adding security exclusions.
    GET: Displays the list of current exclusions and the form to add new ones.
    POST: Processes the form submission to add a new exclusion.
    """
    message = None
    message_type = 'info' # Can be 'success' or 'error'
    if request.method == 'POST':
        security_id = request.form.get('security_id')
        end_date_str = request.form.get('end_date')
        comment = request.form.get('comment')
        success, msg = add_exclusion(security_id, end_date_str, comment)
        if success:
            # Redirect to the same page using GET to prevent form resubmission
            return redirect(url_for('exclusion_bp.manage_exclusions', _external=True))
        else:
            message = msg
            message_type = 'error'
            # Fall through to render the page again with the error message
    # For both GET requests and POST failures, load data and render template
    current_exclusions = load_exclusions()
    available_securities = load_available_securities()
    return render_template('exclusions_page.html',
                           exclusions=current_exclusions,
                           available_securities=available_securities,
                           message=message,
                           message_type=message_type)
@exclusion_bp.route('/exclusions/remove', methods=['POST'])
def remove_exclusion_route():
    """Handles the POST request to remove an exclusion."""
    security_id = request.form.get('security_id')
    add_date_str = request.form.get('add_date') # Get AddDate as string
    if not security_id or not add_date_str:
        # Handle missing identifiers (shouldn't happen with hidden fields but good practice)
        # Redirect back with an error message (using flash or query params)
        logging.warning("Remove exclusion attempt missing SecurityID or AddDate.")
        # For simplicity, redirect back to the main page; flash messages would be better
        return redirect(url_for('exclusion_bp.manage_exclusions'))
    success, msg = remove_exclusion(security_id, add_date_str)
    # Regardless of success/failure, redirect back to the main exclusions page.
    # Consider using flash messages to display the success/error message after redirect.
    # For now, the message `msg` is logged but not shown to the user on redirect.
    return redirect(url_for('exclusion_bp.manage_exclusions'))
</file>

<file path="views/fund_views.py">
"""Blueprint for fund-specific routes, including duration details and a general fund overview page."""
from flask import Blueprint, render_template, current_app, jsonify
import os
import pandas as pd
import traceback
import logging # Added for logging
import glob # Added for finding files
import re # Added for extracting metric name
# Import necessary functions from other modules
from config import DATA_FOLDER
from utils import _is_date_like, parse_fund_list # Import required utils
# Updated import to include data loader
from data_loader import load_and_process_data
# Define the blueprint
fund_bp = Blueprint('fund', __name__, url_prefix='/fund')
@fund_bp.route('/duration_details/<fund_code>') # Corresponds to /fund/duration_details/...
def fund_duration_details(fund_code):
    """Renders a page showing duration changes for securities held by a specific fund."""
    duration_filename = "sec_duration.csv"
    data_filepath = os.path.join(DATA_FOLDER, duration_filename)
    print(f"--- Requesting Duration Details for Fund: {fund_code} --- File: {duration_filename}")
    if not os.path.exists(data_filepath):
        print(f"Error: Duration file '{duration_filename}' not found.")
        return f"Error: Data file '{duration_filename}' not found.", 404
    try:
        # 1. Load the duration data (only header first for column identification)
        header_df = pd.read_csv(data_filepath, nrows=0, encoding='utf-8')
        all_cols = [col.strip() for col in header_df.columns.tolist()]
        # Define ID column (specific to this file/route)
        id_col_name = 'Security Name' # Assuming this remains the ID for this specific file
        if id_col_name not in all_cols:
            print(f"Error: Expected ID column '{id_col_name}' not found in {duration_filename}.")
            return f"Error: Required ID column '{id_col_name}' not found in '{duration_filename}'.", 500
        # 2. Identify static and date columns dynamically
        date_cols = []
        static_cols = []
        for col in all_cols:
            if col == id_col_name:
                continue # Skip the ID column
            if _is_date_like(col): # Use the helper function from utils
                date_cols.append(col)
            else:
                static_cols.append(col) # Treat others as static
        print(f"Dynamically identified Static Cols: {static_cols}")
        print(f"Dynamically identified Date Cols (first 5): {date_cols[:5]}...")
        if not date_cols or len(date_cols) < 2:
             print("Error: Not enough date columns found in duration file to calculate change.")
             return f"Error: Insufficient date columns in '{duration_filename}' to calculate change.", 500
        # Now read the full data
        df = pd.read_csv(data_filepath, encoding='utf-8')
        df.columns = df.columns.str.strip() # Strip again after full read
        # Ensure the Funds column exists (still needed for filtering)
        funds_col = 'Funds' # Keep this assumption for now as it's key to filtering
        if funds_col not in static_cols:
             print(f"Warning: Expected column '{funds_col}' for filtering not found among static columns.")
             # Decide how to handle this - error or proceed without fund filtering? Let's error for now.
             return f"Error: Required column '{funds_col}' for fund filtering not found.", 500
        # Ensure date columns are sortable (attempt conversion if needed, basic check)
        try:
            # Check and sort date columns using the correct YYYY-MM-DD format
            pd.to_datetime(date_cols, format='%Y-%m-%d', errors='raise')
            date_cols = sorted(date_cols, key=lambda d: pd.to_datetime(d, format='%Y-%m-%d'))
            print(f"Identified and sorted date columns (YYYY-MM-DD): {date_cols[-5:]} (last 5 shown)")
        except ValueError:
            print("Warning: Could not parse all date columns using YYYY-MM-DD format. Using original order.")
            # Fallback remains, but hopefully won't be needed as often
        # Identify last two date columns based on sorted list (or original if parsing failed)
        if len(date_cols) < 2: # Double check after potential parsing failure
            return f"Error: Insufficient valid date columns in '{duration_filename}' to calculate change after sorting attempt.", 500
        last_date_col = date_cols[-1]
        second_last_date_col = date_cols[-2]
        print(f"Using dates for change calculation: {second_last_date_col} and {last_date_col}")
        # Ensure the relevant date columns are numeric for calculation
        df[last_date_col] = pd.to_numeric(df[last_date_col], errors='coerce')
        df[second_last_date_col] = pd.to_numeric(df[second_last_date_col], errors='coerce')
        # 3. Filter by Fund Code
        # Apply the parsing function from utils to the 'Funds' column
        fund_lists = df['Funds'].apply(parse_fund_list)
        # Create a boolean mask to filter rows where the fund_code is in the parsed list
        mask = fund_lists.apply(lambda funds: fund_code in funds)
        filtered_df = df[mask].copy() # Use copy to avoid SettingWithCopyWarning
        if filtered_df.empty:
            print(f"No securities found for fund '{fund_code}' in {duration_filename}.")
            # Render a template indicating no data found for this fund
            return render_template('fund_duration_details.html',
                                   fund_code=fund_code,
                                   securities_data=[],
                                   column_order=[],
                                   id_col_name=None,
                                   message=f"No securities found held by fund '{fund_code}' in {duration_filename}.")
        print(f"Found {len(filtered_df)} securities for fund '{fund_code}'. Calculating changes...")
        # 4. Calculate 1-day Change
        change_col_name = '1 Day Duration Change'
        filtered_df[change_col_name] = filtered_df[last_date_col] - filtered_df[second_last_date_col]
        # 5. Sort by Change (descending, NaN last)
        filtered_df.sort_values(by=change_col_name, ascending=False, na_position='last', inplace=True)
        print(f"Sorted securities by {change_col_name}.")
        # 6. Prepare data for template
        # Select columns for display - use the dynamically identified static_cols
        # ID column is already defined as id_col_name
        # Filter static_cols to ensure they exist in the filtered_df after operations
        existing_static_cols = [col for col in static_cols if col in filtered_df.columns]
        display_cols = [id_col_name] + existing_static_cols + [second_last_date_col, last_date_col, change_col_name]
        final_col_order = [col for col in display_cols if col in filtered_df.columns] # Ensure only existing columns are kept
        securities_data_list = filtered_df[final_col_order].round(3).to_dict(orient='records')
        # Handle potential NaN values for template rendering
        for row in securities_data_list:
             for key, value in row.items():
                 if pd.isna(value):
                     row[key] = None
        print(f"Final column order for display: {final_col_order}")
        return render_template('fund_duration_details.html',
                               fund_code=fund_code,
                               securities_data=securities_data_list,
                               column_order=final_col_order,
                               id_col_name=id_col_name,
                               message=None)
    except FileNotFoundError:
         return f"Error: Data file '{duration_filename}' not found.", 404
    except Exception as e:
        print(f"Error processing duration details for fund {fund_code}: {e}")
        traceback.print_exc()
        return f"An error occurred processing duration details for fund {fund_code}: {e}", 500
# --- New Route for Fund Detail Page ---
@fund_bp.route('/<fund_code>')
def fund_detail(fund_code):
    """Renders a page displaying all available time-series charts for a specific fund."""
    print(f"--- Requesting Detail Page for Fund: {fund_code} ---")
    all_chart_data = []
    available_metrics = []
    processed_files = 0
    skipped_files = 0
    try:
        # Find all time-series files
        ts_files_pattern = os.path.join(DATA_FOLDER, 'ts_*.csv')
        ts_files = glob.glob(ts_files_pattern)
        print(f"Found {len(ts_files)} potential ts_ files: {ts_files}")
        if not ts_files:
            print("No ts_*.csv files found in Data folder.")
            return render_template('fund_detail_page.html',
                                   fund_code=fund_code,
                                   chart_data_json='[]', # Empty JSON array
                                   available_metrics=[],
                                   message="No time-series data files (ts_*.csv) found.")
        # Process each file
        for file_path in ts_files:
            filename = os.path.basename(file_path)
            # Extract metric name from filename (e.g., ts_Yield.csv -> Yield)
            match = re.match(r'ts_(.+?)(?:_processed)?\.csv', filename, re.IGNORECASE)
            if not match:
                print(f"Could not extract metric name from filename: {filename}. Skipping.")
                skipped_files += 1
                continue
            metric_name = match.group(1).replace('_', ' ').title() # Format nicely
            print(f"\nProcessing {filename} for metric: {metric_name}")
            try:
                # Corrected unpacking: Expecting 3 values, not 4
                df, fund_cols, benchmark_col = load_and_process_data(filename, DATA_FOLDER)
                if df is None or df.empty:
                    print(f"No data loaded from {filename}. Skipping.")
                    skipped_files += 1
                    continue
                # Check if the fund_code exists in this metric's data
                if fund_code not in df.index.get_level_values('Code'):
                    print(f"Fund code '{fund_code}' not found in {filename}. Skipping metric.")
                    skipped_files += 1
                    continue
                print(f"Fund code '{fund_code}' found. Preparing chart data for '{metric_name}'...")
                available_metrics.append(metric_name)
                # Filter data for the specific fund
                fund_df = df.loc[pd.IndexSlice[:, fund_code], :]
                fund_df = fund_df.droplevel('Code') # Remove fund code level for easier plotting
                # Prepare chart data structure
                chart_data = {
                    'metricName': metric_name,
                    'labels': fund_df.index.strftime('%Y-%m-%d').tolist(), # X-axis labels (Dates)
                    'datasets': []
                }
                # Add fund dataset (use original column name if available)
                fund_col_name = next((col for col in fund_cols if col in fund_df.columns), None)
                if fund_col_name:
                    fund_values = fund_df[fund_col_name].where(pd.notna(fund_df[fund_col_name]), None).tolist() # Handle NaN for JSON
                    chart_data['datasets'].append({
                        'label': f"{fund_code} {metric_name}",
                        'data': fund_values,
                        'borderColor': current_app.config['COLOR_PALETTE'][0 % len(current_app.config['COLOR_PALETTE'])], # Use first color
                        'tension': 0.1,
                        'pointRadius': 1,
                        'borderWidth': 1.5
                    })
                else:
                    print(f"Warning: Could not find primary fund data column in {filename} for fund {fund_code}")
                # Add benchmark dataset if it exists for this fund
                if benchmark_col and benchmark_col in fund_df.columns:
                    bench_values = fund_df[benchmark_col].where(pd.notna(fund_df[benchmark_col]), None).tolist() # Handle NaN for JSON
                    chart_data['datasets'].append({
                        'label': f"Benchmark ({benchmark_col})",
                        'data': bench_values,
                        'borderColor': current_app.config['COLOR_PALETTE'][1 % len(current_app.config['COLOR_PALETTE'])], # Use second color
                        'tension': 0.1,
                        'pointRadius': 1,
                        'borderDash': [5, 5], # Dashed line for benchmark
                        'borderWidth': 1
                    })
                # Only add chart if we have at least one dataset
                if chart_data['datasets']:
                    all_chart_data.append(chart_data)
                    processed_files += 1
                else:
                     print(f"No valid datasets generated for metric '{metric_name}' from {filename}. Skipping chart.")
                     skipped_files += 1
            except Exception as e:
                print(f"Error processing file {filename} for fund {fund_code}: {e}")
                logging.exception(f"Error processing file {filename} for fund {fund_code}") # Log detailed traceback
                skipped_files += 1
                continue # Skip this file on error
        print(f"\nFinished processing. Processed {processed_files} metrics, Skipped {skipped_files} files/metrics for fund {fund_code}.")
        # Safely convert chart data to JSON for embedding in the template
        # Replace NaN/inf with None (handled above in .where())
        chart_data_json = jsonify(all_chart_data).get_data(as_text=True)
        return render_template('fund_detail_page.html',
                               fund_code=fund_code,
                               chart_data_json=chart_data_json,
                               available_metrics=available_metrics,
                               message=f"Displaying {len(all_chart_data)} charts for fund {fund_code}." if all_chart_data else f"No metrics found with data for fund '{fund_code}'.")
    except Exception as e:
        print(f"General error rendering detail page for fund {fund_code}: {e}")
        traceback.print_exc()
        logging.exception(f"General error rendering detail page for fund {fund_code}") # Log detailed traceback
        # Render the template with an error message
        return render_template('fund_detail_page.html',
                               fund_code=fund_code,
                               chart_data_json='[]',
                               available_metrics=[],
                               message=f"An unexpected error occurred while generating the page for fund {fund_code}.")
</file>

<file path="views/main_views.py">
# This file defines the routes related to the main, top-level views of the application.
# It primarily handles the dashboard or index page.
"""
Blueprint for main application routes, like the index page.
"""
from flask import Blueprint, render_template
import os
import pandas as pd
import traceback
# Import necessary functions/constants from other modules
from config import DATA_FOLDER
from data_loader import load_and_process_data
from metric_calculator import calculate_latest_metrics
# Define the blueprint for main routes
main_bp = Blueprint('main', __name__)
@main_bp.route('/')
def index():
    """Renders the main dashboard page (`index.html`).
    This view performs the following steps:
    1. Scans the `DATA_FOLDER` for time-series metric files (prefixed with `ts_`).
    2. For each `ts_` file found:
        a. Loads and processes the data using `data_loader.load_and_process_data`.
        b. Calculates metrics (including Z-scores) using `metric_calculator.calculate_latest_metrics`.
        c. Extracts the 'Change Z-Score' columns for both the benchmark and any specific fund columns.
    3. Aggregates all extracted 'Change Z-Score' columns from all files into a single pandas DataFrame (`summary_df`).
    4. Creates unique column names for the summary table by combining the original column name and the metric file name
       (e.g., 'Benchmark - Yield', 'FUND_A - Duration').
    5. Passes the list of available metric display names (filenames without `ts_`) and the aggregated Z-score
       DataFrame (`summary_df`) along with its corresponding column headers (`summary_metrics`) to the `index.html` template.
    This allows the dashboard to display a consolidated view of the most recent significant changes across all metrics.
    """
    # Find only files starting with ts_ and ending with .csv
    files = [f for f in os.listdir(DATA_FOLDER) if f.startswith('ts_') and f.endswith('.csv')]
    # Create two lists: one for filenames (with ts_), one for display (without ts_)
    metric_filenames = sorted([os.path.splitext(f)[0] for f in files])
    metric_display_names = sorted([name[3:] for name in metric_filenames]) # Remove 'ts_' prefix
    all_z_scores_list = []
    # Store the unique combined column names for the summary table header
    processed_summary_columns = []
    print("Starting Change Z-score aggregation for dashboard (ts_ files only)...")
    # Iterate using the filenames with prefix
    for metric_filename in metric_filenames:
        filename = f"{metric_filename}.csv"
        # Get the corresponding display name for this file
        display_name = metric_filename[3:]
        try:
            print(f"Processing {filename}...")
            # Unpack all 6 values, but only use the primary ones for the dashboard summary
            df, fund_cols, benchmark_col, _sec_df, _sec_fund_cols, _sec_bench_col = load_and_process_data(filename)
            # Skip if no benchmark AND no fund columns identified
            if not benchmark_col and not fund_cols:
                 print(f"Warning: No benchmark or fund columns identified in {filename}. Skipping.")
                 continue
            # Calculate metrics using the current function
            latest_metrics = calculate_latest_metrics(df, fund_cols, benchmark_col)
            # --- Extract Change Z-score for ALL columns (benchmark + funds) --- 
            if not latest_metrics.empty:
                columns_to_check = []
                if benchmark_col:
                    columns_to_check.append(benchmark_col)
                if fund_cols:
                    columns_to_check.extend(fund_cols)
                if not columns_to_check:
                    print(f"Warning: No columns to check for Z-scores in {filename} despite loading data.")
                    continue
                print(f"Checking for Z-scores for columns: {columns_to_check} in metric {display_name}")
                found_z_for_metric = False
                for original_col_name in columns_to_check:
                    z_score_col_name = f'{original_col_name} Change Z-Score'
                    if z_score_col_name in latest_metrics.columns:
                        # Create a unique name for the summary table column
                        summary_col_name = f"{original_col_name} - {display_name}"
                        # Extract and rename
                        metric_z_scores = latest_metrics[[z_score_col_name]].rename(columns={z_score_col_name: summary_col_name})
                        all_z_scores_list.append(metric_z_scores)
                        # Add the unique column name to our list if not already present (preserves order of discovery)
                        if summary_col_name not in processed_summary_columns:
                             processed_summary_columns.append(summary_col_name)
                        found_z_for_metric = True
                        print(f"  -> Extracted: {summary_col_name}")
                    else:
                        print(f"  -> Z-score column '{z_score_col_name}' not found.")
                if not found_z_for_metric:
                    print(f"Warning: No Z-score columns found for any checked column in metric {display_name} (from {filename}).")
            else:
                 print(f"Warning: Could not calculate latest_metrics for {filename}. Skipping Z-score extraction.")
        except FileNotFoundError:
            print(f"Error: Data file '{filename}' not found.")
        except ValueError as ve:
            print(f"Value Error processing {metric_filename}: {ve}") # Log with filename
        except Exception as e:
            print(f"Error processing {metric_filename} during dashboard aggregation: {e}") # Log with filename
            traceback.print_exc()
    # Combine all Z-score Series/DataFrames into one
    summary_df = pd.DataFrame()
    if all_z_scores_list:
        summary_df = pd.concat(all_z_scores_list, axis=1)
        # Ensure the columns are in the order they were discovered
        if processed_summary_columns:
             # Handle potential missing columns if a file failed processing midway
             cols_available_in_summary = [col for col in processed_summary_columns if col in summary_df.columns]
             summary_df = summary_df[cols_available_in_summary]
             # Update the list of columns to only those actually present
             processed_summary_columns = cols_available_in_summary
        print("Successfully combined Change Z-scores.")
        print(f"Summary DF columns: {summary_df.columns.tolist()}")
    else:
        print("No Change Z-scores could be extracted for the summary.")
    return render_template('index.html',
                           metrics=metric_display_names, # Still used for top-level metric links
                           summary_data=summary_df,
                           summary_metrics=processed_summary_columns) # Pass the NEW list of combined column names
</file>

<file path="views/metric_views.py">
# This file defines the routes for displaying detailed views of specific time-series metrics.
# It handles requests where the user wants to see the data and charts for a single metric
# (like 'Yield' or 'Spread Duration') across all applicable funds.
# Updated to optionally load and display a secondary data source (prefixed with 'sp_').
"""
Blueprint for metric-specific routes (e.g., displaying individual metric charts).
"""
from flask import Blueprint, render_template, jsonify
import os
import pandas as pd
import numpy as np
import traceback
# Import necessary functions/constants from other modules
from config import DATA_FOLDER, COLOR_PALETTE
from data_loader import load_and_process_data, LoadResult # Import LoadResult type
from metric_calculator import calculate_latest_metrics
# Define the blueprint for metric routes, using '/metric' as the URL prefix
metric_bp = Blueprint('metric', __name__, url_prefix='/metric')
@metric_bp.route('/<string:metric_name>')
def metric_page(metric_name):
    """Renders the detailed page (`metric_page_js.html`) for a specific metric.
    Loads primary data (e.g., 'ts_Yield.csv') and optionally secondary data ('sp_ts_Yield.csv').
    Calculates metrics for both, prepares data for Chart.js, and passes it to the template.
    Includes a flag to indicate if secondary data is available.
    """
    primary_filename = f"ts_{metric_name}.csv"
    secondary_filename = f"sp_{primary_filename}"
    fund_code = 'N/A' # Default for logging fallback in case of early error
    latest_date_overall = pd.Timestamp.min # Initialize
    try:
        print(f"--- Processing metric: {metric_name} ---")
        print(f"Primary file: {primary_filename}, Secondary file: {secondary_filename}")
        # Load Data (Primary and Secondary)
        load_result: LoadResult = load_and_process_data(primary_filename, secondary_filename)
        primary_df, pri_fund_cols, pri_bench_col, secondary_df, sec_fund_cols, sec_bench_col = load_result
        # --- Validate Primary Data --- 
        if primary_df is None or primary_df.empty or pri_fund_cols is None:
            # Check if the file exists before saying it couldn't be processed
            primary_filepath = os.path.join(DATA_FOLDER, primary_filename)
            if not os.path.exists(primary_filepath):
                 print(f"Error: Primary data file not found: {primary_filepath}")
                 return f"Error: Data file for metric '{metric_name}' (expected: '{primary_filename}') not found.", 404
            else:
                 print(f"Error: Failed to process primary data file: {primary_filename}")
                 return f"Error: Could not process required data for metric '{metric_name}' (file: {primary_filename}). Check file format or logs.", 500
        # --- Determine Combined Metadata --- 
        all_dfs = [df for df in [primary_df, secondary_df] if df is not None and not df.empty]
        if not all_dfs:
             # Should be caught by primary check, but safeguard
            print(f"Error: No valid data loaded for {metric_name}")
            return f"Error: No data found for metric '{metric_name}'.", 404
        try:
            combined_index = pd.concat(all_dfs).index
            latest_date_overall = combined_index.get_level_values(0).max()
            latest_date_str = latest_date_overall.strftime('%Y-%m-%d')
        except Exception as idx_err:
            print(f"Error combining indices or getting latest date for {metric_name}: {idx_err}")
            # Fallback or re-raise? Let's try to proceed if possible, using primary date
            latest_date_overall = primary_df.index.get_level_values(0).max()
            latest_date_str = latest_date_overall.strftime('%Y-%m-%d')
            print(f"Warning: Using latest date from primary data only: {latest_date_str}")
        secondary_data_available = secondary_df is not None and not secondary_df.empty and sec_fund_cols is not None
        print(f"Secondary data available for {metric_name}: {secondary_data_available}")
        # --- Calculate Metrics --- 
        print(f"Calculating metrics for {metric_name}...")
        latest_metrics = calculate_latest_metrics(
            primary_df=primary_df,
            primary_fund_cols=pri_fund_cols,
            primary_benchmark_col=pri_bench_col,
            secondary_df=secondary_df if secondary_data_available else None,
            secondary_fund_cols=sec_fund_cols if secondary_data_available else None,
            secondary_benchmark_col=sec_bench_col if secondary_data_available else None,
            secondary_prefix="S&P "
        )
        # --- Handle Empty Metrics Result --- 
        if latest_metrics.empty:
            print(f"Warning: Metric calculation returned empty DataFrame for {metric_name}. Rendering page with no fund data.")
            missing_latest = pd.DataFrame()
            json_payload = {
                "metadata": {
                    "metric_name": metric_name,
                    "latest_date": latest_date_str,
                    "fund_col_names": pri_fund_cols or [],
                    "benchmark_col_name": pri_bench_col,
                    "secondary_fund_col_names": sec_fund_cols if secondary_data_available else [],
                    "secondary_benchmark_col_name": sec_bench_col if secondary_data_available else None,
                    "secondary_data_available": secondary_data_available
                },
                "funds": {}
            }
            return render_template('metric_page_js.html',
                           metric_name=metric_name,
                           charts_data_json=jsonify(json_payload).get_data(as_text=True),
                           latest_date=latest_date_overall.strftime('%d/%m/%Y'), 
                           missing_funds=missing_latest)
        # --- Identify Missing Funds (based on primary data) --- 
        print(f"Identifying potentially missing latest data for {metric_name}...")
        primary_cols_for_check = []
        if pri_bench_col:
            primary_cols_for_check.append(pri_bench_col)
        if pri_fund_cols:
            primary_cols_for_check.extend(pri_fund_cols)
        # Prefer checking Z-Score, fallback to Latest Value
        primary_z_score_cols = [f'{col} Change Z-Score' for col in primary_cols_for_check 
                                if f'{col} Change Z-Score' in latest_metrics.columns]
        primary_latest_val_cols = [f'{col} Latest Value' for col in primary_cols_for_check
                                   if f'{col} Latest Value' in latest_metrics.columns]
        check_cols_for_missing = primary_z_score_cols if primary_z_score_cols else primary_latest_val_cols
        if check_cols_for_missing:
            missing_latest = latest_metrics[latest_metrics[check_cols_for_missing].isna().any(axis=1)]
        else:
            print(f"Warning: No primary Z-Score or Latest Value columns found for {metric_name} to check for missing data.")
            missing_latest = pd.DataFrame(index=latest_metrics.index) # Assume none are missing if no check cols
        # --- Prepare Data Structure for JavaScript --- 
        print(f"Preparing chart and metric data for JavaScript for {metric_name}...")
        funds_data_for_js = {}
        fund_codes_in_metrics = latest_metrics.index
        primary_df_index = primary_df.index
        secondary_df_index = secondary_df.index if secondary_data_available and secondary_df is not None else None
        # Loop through funds present in the calculated metrics
        for fund_code in fund_codes_in_metrics:
            fund_latest_metrics_row = latest_metrics.loc[fund_code]
            is_missing_latest = fund_code in missing_latest.index
            primary_labels = []
            primary_datasets = []
            fund_hist_primary = None
            primary_dt_index = None
            # Get Primary Historical Data
            if fund_code in primary_df_index.get_level_values(1):
                fund_hist_primary = primary_df.xs(fund_code, level=1).sort_index()
                if isinstance(fund_hist_primary.index, pd.DatetimeIndex):
                    primary_dt_index = fund_hist_primary.index # Store before filtering
                    fund_hist_primary = fund_hist_primary[primary_dt_index.dayofweek < 5]
                    primary_dt_index = fund_hist_primary.index # Update after filtering
                    primary_labels = primary_dt_index.strftime('%Y-%m-%d').tolist()
                else:
                    primary_labels = fund_hist_primary.index.astype(str).tolist()
                    print(f"Warning: Primary index for {fund_code} is not DatetimeIndex.")
                # Create Primary Datasets
                if pri_bench_col and pri_bench_col in fund_hist_primary.columns:
                    bench_values = fund_hist_primary[pri_bench_col].round(3).replace([np.inf, -np.inf], np.nan).where(pd.notnull, None).tolist()
                    primary_datasets.append({
                        'label': pri_bench_col,
                        'data': bench_values,
                        'borderColor': 'black', 'backgroundColor': 'grey',
                        'borderDash': [5, 5], 'tension': 0.1,
                        'source': 'primary'
                    })
                if pri_fund_cols:
                    for i, fund_col in enumerate(pri_fund_cols):
                        if fund_col in fund_hist_primary.columns:
                            fund_values = fund_hist_primary[fund_col].round(3).replace([np.inf, -np.inf], np.nan).where(pd.notnull, None).tolist()
                            color = COLOR_PALETTE[i % len(COLOR_PALETTE)]
                            primary_datasets.append({
                                'label': fund_col,
                                'data': fund_values,
                                'borderColor': color, 'backgroundColor': color + '40',
                                'tension': 0.1,
                                'source': 'primary'
                            })
            else:
                print(f"Warning: Fund {fund_code} not found in primary source for historical data.")
            # Get Secondary Historical Data (and align to primary labels)
            secondary_datasets = []
            if secondary_data_available and secondary_df_index is not None and fund_code in secondary_df_index.get_level_values(1):
                fund_hist_secondary = secondary_df.xs(fund_code, level=1).sort_index()
                if isinstance(fund_hist_secondary.index, pd.DatetimeIndex):
                    fund_hist_secondary = fund_hist_secondary[fund_hist_secondary.index.dayofweek < 5]
                    # Reindex to primary date index if possible
                    if primary_dt_index is not None and not primary_dt_index.empty:
                         try:
                             # Ensure secondary index is also datetime before reindexing
                             if isinstance(fund_hist_secondary.index, pd.DatetimeIndex):
                                 fund_hist_secondary_aligned = fund_hist_secondary.reindex(primary_dt_index)
                                 # Replace fund_hist_secondary with aligned version for dataset creation
                                 fund_hist_secondary = fund_hist_secondary_aligned
                                 print(f"Successfully reindexed secondary data for {fund_code}.")
                             else:
                                 print(f"Warning: Cannot reindex - Secondary index for {fund_code} is not DatetimeIndex after filtering.")
                         except Exception as reindex_err:
                             print(f"Warning: Reindexing secondary data for {fund_code} failed: {reindex_err}. Chart may be misaligned.")
                    else:
                         print(f"Warning: Cannot reindex secondary for {fund_code} - Primary DatetimeIndex unavailable.")
                else:
                    print(f"Warning: Secondary index for {fund_code} is not DatetimeIndex.")
                # Create Secondary Datasets (using potentially reindexed data)
                if sec_bench_col and sec_bench_col in fund_hist_secondary.columns:
                    bench_values_sec = fund_hist_secondary[sec_bench_col].round(3).replace([np.inf, -np.inf], np.nan).where(pd.notnull, None).tolist()
                    secondary_datasets.append({
                        'label': f"S&P {sec_bench_col}",
                        'data': bench_values_sec,
                        'xAxisID': 'x',
                        'borderColor': '#FFA500',
                        'backgroundColor': '#FFDAB9',
                        'borderDash': [2, 2], 'tension': 0.1,
                        'source': 'secondary',
                        'hidden': True
                    })
                if sec_fund_cols:
                    for i, fund_col in enumerate(sec_fund_cols):
                        if fund_col in fund_hist_secondary.columns:
                            fund_values_sec = fund_hist_secondary[fund_col].round(3).replace([np.inf, -np.inf], np.nan).where(pd.notnull, None).tolist()
                            color_index = i # Default index
                            if pri_fund_cols: # Try to match color with primary
                                try:
                                    color_index = pri_fund_cols.index(fund_col)
                                except ValueError:
                                    pass # Keep default index if no match
                            base_color = COLOR_PALETTE[color_index % len(COLOR_PALETTE)]
                            secondary_datasets.append({
                                'label': f"S&P {fund_col}",
                                'data': fund_values_sec,
                                'xAxisID': 'x',
                                'borderColor': base_color,
                                'backgroundColor': base_color + '20',
                                'borderDash': [2, 2],
                                'tension': 0.1,
                                'source': 'secondary',
                                'hidden': True
                            })
            elif secondary_data_available:
                 print(f"Info: Fund {fund_code} not found in secondary source for historical data.")
            # Combine metrics and chart data for the fund
            fund_latest_metrics_dict = fund_latest_metrics_row.round(3).replace([np.inf, -np.inf], np.nan).where(pd.notnull, None).to_dict()
            funds_data_for_js[fund_code] = {
                'labels': primary_labels, # Use primary labels for the chart axis
                'datasets': primary_datasets + secondary_datasets,
                'metrics': fund_latest_metrics_dict,
                'is_missing_latest': is_missing_latest
            }
        # --- Prepare Final JSON Payload --- 
        json_payload = {
            "metadata": {
                "metric_name": metric_name,
                "latest_date": latest_date_str,
                "fund_col_names": pri_fund_cols or [],
                "benchmark_col_name": pri_bench_col,
                "secondary_fund_col_names": sec_fund_cols if secondary_data_available else [],
                "secondary_benchmark_col_name": sec_bench_col if secondary_data_available else None,
                "secondary_data_available": secondary_data_available
            },
            "funds": funds_data_for_js
        }
        print(f"Finished preparing data for {metric_name}. Sending to template.")
        # --- Render Template --- 
        return render_template('metric_page_js.html',
                               metric_name=metric_name,
                               charts_data_json=jsonify(json_payload).get_data(as_text=True),
                               latest_date=latest_date_overall.strftime('%d/%m/%Y') if pd.notna(latest_date_overall) else 'N/A', 
                               missing_funds=missing_latest)
    # --- Exception Handling --- 
    except FileNotFoundError:
        # Specific handling for primary file not found is done within the try block
        # This except block catches potential FileNotFoundError from dependencies (less likely)
        print(f"Unexpected FileNotFoundError during processing of {metric_name}: {primary_filename} or {secondary_filename}")
        traceback.print_exc()
        return f"An unexpected file error occurred while processing {metric_name}.", 500
    except ValueError as ve:
        print(f"Value Error processing {metric_name}: {ve}")
        traceback.print_exc()
        return f"Error processing data for metric '{metric_name}'. Details: {ve}", 400
    except Exception as e:
        print(f"Unhandled Error processing {metric_name} for fund {fund_code}: {e}")
        traceback.print_exc()
        return f"An unexpected server error occurred while processing metric '{metric_name}'. Details: {e}", 500
</file>

<file path="views/security_views.py">
"""
Blueprint for security-related routes (e.g., summary page and individual details).
"""
from flask import Blueprint, render_template, jsonify, send_from_directory, url_for
import os
import pandas as pd
import numpy as np
import traceback
from urllib.parse import unquote
from datetime import datetime
from flask import request # Import request
import math
# Import necessary functions/constants from other modules
from config import DATA_FOLDER, COLOR_PALETTE
from security_processing import load_and_process_security_data, calculate_security_latest_metrics
# Import the exclusion loading function
from views.exclusion_views import load_exclusions, get_data_path
# Define the blueprint
security_bp = Blueprint('security', __name__, url_prefix='/security')
PER_PAGE = 50 # Define how many items per page
def get_active_exclusions():
    """Loads exclusions and returns a set of SecurityIDs that are currently active."""
    exclusions = load_exclusions() # This returns a list of dicts
    active_exclusions = set()
    today = datetime.now().date()
    for ex in exclusions:
        try:
            add_date = ex['AddDate'].date() if pd.notna(ex['AddDate']) else None
            end_date = ex['EndDate'].date() if pd.notna(ex['EndDate']) else None
            security_id = str(ex['SecurityID']) # Ensure it's string for comparison
            if add_date and add_date <= today:
                if end_date is None or end_date >= today:
                    active_exclusions.add(security_id)
        except Exception as e:
            print(f"Error processing exclusion record {ex}: {e}") # Use logging in production
    print(f"Found {len(active_exclusions)} active exclusions: {active_exclusions}")
    return active_exclusions
@security_bp.route('/summary')
def securities_page():
    """Renders a page summarizing potential issues in security-level data, with server-side pagination, filtering, and sorting."""
    print("\n--- Starting Security Data Processing (Paginated) ---")
    # --- Get Request Parameters ---
    page = request.args.get('page', 1, type=int)
    search_term = request.args.get('search_term', '', type=str).strip()
    sort_by = request.args.get('sort_by', None, type=str)
    # Default sort: Abs Change Z-Score Descending
    sort_order = request.args.get('sort_order', 'desc', type=str).lower() 
    # Ensure sort_order is either 'asc' or 'desc'
    if sort_order not in ['asc', 'desc']:
        sort_order = 'desc'
    # Collect active filters from request args (e.g., ?filter_Country=USA&filter_Sector=Tech)
    active_filters = {
        key.replace('filter_', ''): value 
        for key, value in request.args.items() 
        if key.startswith('filter_') and value # Ensure value is not empty
    }
    print(f"Request Params: Page={page}, Search='{search_term}', SortBy='{sort_by}', SortOrder='{sort_order}', Filters={active_filters}")
    # --- Load Base Data ---
    spread_filename = "sec_Spread.csv"
    data_filepath = os.path.join(DATA_FOLDER, spread_filename)
    filter_options = {} # To store all possible options for filter dropdowns
    if not os.path.exists(data_filepath):
        print(f"Error: The required file '{spread_filename}' not found.")
        return render_template('securities_page.html', message=f"Error: Required data file '{spread_filename}' not found.", securities_data=[], pagination=None)
    try:
        print(f"Loading and processing file: {spread_filename}")
        df_long, static_cols = load_and_process_security_data(spread_filename)
        if df_long is None or df_long.empty:
            print(f"Skipping {spread_filename} due to load/process errors or empty data.")
            return render_template('securities_page.html', message=f"Error loading or processing '{spread_filename}'.", securities_data=[], pagination=None)
        print("Calculating latest metrics...")
        combined_metrics_df = calculate_security_latest_metrics(df_long, static_cols)
        if combined_metrics_df.empty:
            print(f"No metrics calculated for {spread_filename}.")
            return render_template('securities_page.html', message=f"Could not calculate metrics from '{spread_filename}'.", securities_data=[], pagination=None)
        # Store the original unfiltered dataframe's columns and index name
        original_columns = combined_metrics_df.columns.tolist()
        id_col_name = combined_metrics_df.index.name or 'Security ID' # Get index name before reset
        combined_metrics_df.reset_index(inplace=True) # Reset index to make ID a regular column
        # --- Collect Filter Options (from the full dataset BEFORE filtering) ---
        print("Collecting filter options...")
        current_static_in_df = [col for col in static_cols if col in combined_metrics_df.columns]
        for col in current_static_in_df:
            unique_vals = combined_metrics_df[col].unique().tolist()
            unique_vals = [item.item() if isinstance(item, np.generic) else item for item in unique_vals]
            unique_vals = sorted([val for val in unique_vals if pd.notna(val) and val != '']) # Remove NaN/empty and sort
            if unique_vals: # Only add if there are valid options
                 filter_options[col] = unique_vals
        # Sort filter options dictionary by key for consistent display order
        final_filter_options = dict(sorted(filter_options.items()))
        # --- Apply Filtering Steps Sequentially ---
        print("Applying filters...")
        # 1. Search Term Filter (on ID column)
        if search_term:
            combined_metrics_df = combined_metrics_df[combined_metrics_df[id_col_name].astype(str).str.contains(search_term, case=False, na=False)]
            print(f"Applied search term '{search_term}'. Rows remaining: {len(combined_metrics_df)}")
        # 2. Active Exclusions Filter
        try:
            active_exclusion_ids = get_active_exclusions()
            if active_exclusion_ids:
                 combined_metrics_df = combined_metrics_df[~combined_metrics_df[id_col_name].astype(str).isin(active_exclusion_ids)]
                 print(f"Applied {len(active_exclusion_ids)} exclusions. Rows remaining: {len(combined_metrics_df)}")
        except Exception as e:
            print(f"Warning: Error loading or applying exclusions: {e}")
            # Continue without exclusions if loading fails
        # 3. Dynamic Filters (from request args)
        if active_filters:
            for col, value in active_filters.items():
                if col in combined_metrics_df.columns:
                    # Ensure consistent type for comparison, handle NaNs
                    combined_metrics_df = combined_metrics_df[combined_metrics_df[col].astype(str) == str(value)]
                    print(f"Applied filter '{col}={value}'. Rows remaining: {len(combined_metrics_df)}")
                else:
                     print(f"Warning: Filter column '{col}' not found in DataFrame.")
        # --- Handle Empty DataFrame After Filtering ---
        if combined_metrics_df.empty:
            print("No data matches the specified filters.")
            message = "No securities found matching the current criteria."
            if search_term:
                message += f" Search term: '{search_term}'."
            if active_filters:
                 message += f" Active filters: {active_filters}."
            return render_template('securities_page.html',
                                   message=message,
                                   securities_data=[],
                                   filter_options=final_filter_options,
                                   column_order=[],
                                   id_col_name=id_col_name,
                                   search_term=search_term,
                                   active_filters=active_filters,
                                   pagination=None,
                                   current_sort_by=sort_by,
                                   current_sort_order=sort_order)
        # --- Apply Sorting ---
        print(f"Applying sort: By='{sort_by}', Order='{sort_order}'")
        # Default sort column if not provided or invalid
        effective_sort_by = sort_by
        is_default_sort = False
        if sort_by not in combined_metrics_df.columns:
             # Default to sorting by absolute Z-score if 'sort_by' is invalid or not provided
             if 'Change Z-Score' in combined_metrics_df.columns:
                 print(f"'{sort_by}' not valid or not provided. Defaulting sort to 'Abs Change Z-Score' {sort_order}")
                 # Calculate Abs Z-Score temporarily for sorting
                 combined_metrics_df['_abs_z_score_'] = combined_metrics_df['Change Z-Score'].fillna(0).abs()
                 effective_sort_by = '_abs_z_score_'
                 # Default Z-score sort is always descending unless explicitly requested otherwise for Z-score itself
                 if sort_by != 'Change Z-Score': 
                      sort_order = 'desc' 
                 is_default_sort = True
             else:
                  print("Warning: Cannot apply default sort, 'Change Z-Score' missing.")
                  effective_sort_by = id_col_name # Fallback sort
                  sort_order = 'asc'
        ascending_order = (sort_order == 'asc')
        try:
            # Use na_position='last' to handle NaNs consistently
            combined_metrics_df.sort_values(by=effective_sort_by, ascending=ascending_order, inplace=True, na_position='last', key=lambda col: col.astype(str).str.lower() if col.dtype == 'object' else col)
            print(f"Sorted by '{effective_sort_by}', {sort_order}.")
        except Exception as e:
             print(f"Error during sorting by {effective_sort_by}: {e}. Falling back to sorting by ID.")
             combined_metrics_df.sort_values(by=id_col_name, ascending=True, inplace=True, na_position='last')
             sort_by = id_col_name # Update sort_by to reflect fallback
             sort_order = 'asc'
        # Remove temporary sort column if added
        if is_default_sort and '_abs_z_score_' in combined_metrics_df.columns:
            combined_metrics_df.drop(columns=['_abs_z_score_'], inplace=True)
            # Set sort_by for template correctly if default was used
            sort_by = 'Change Z-Score' # Reflect the conceptual sort column
        # --- Pagination ---
        total_items = len(combined_metrics_df)
        # Ensure PER_PAGE is positive to avoid division by zero or negative pages
        safe_per_page = max(1, PER_PAGE)
        total_pages = math.ceil(total_items / safe_per_page)
        total_pages = max(1, total_pages) # Ensure at least 1 page, even if total_items is 0
        page = max(1, min(page, total_pages)) # Ensure page is within valid range [1, total_pages]
        start_index = (page - 1) * safe_per_page
        end_index = start_index + safe_per_page
        print(f"Pagination: Total items={total_items}, Total pages={total_pages}, Current page={page}, Per page={safe_per_page}")
        # Calculate page numbers to display in pagination controls (e.g., show 2 pages before and after current)
        page_window = 2 # Number of pages to show before/after current page
        start_page_display = max(1, page - page_window)
        end_page_display = min(total_pages, page + page_window)
        paginated_df = combined_metrics_df.iloc[start_index:end_index]
        # --- Prepare Data for Template ---
        securities_data_list = paginated_df.round(3).to_dict(orient='records')
        # Replace NaN with None for JSON compatibility / template rendering
        for row in securities_data_list:
            for key, value in row.items():
                if pd.isna(value):
                    row[key] = None
        # Define column order (ID first, then Static, then Metrics)
        ordered_static_cols = sorted([col for col in static_cols if col in paginated_df.columns])
        metric_cols_ordered = ['Latest Value', 'Change', 'Change Z-Score', 'Mean', 'Max', 'Min']
        # Ensure only existing columns are included and ID col is first
        final_col_order = [id_col_name] + \
                          [col for col in ordered_static_cols if col in paginated_df.columns] + \
                          [col for col in metric_cols_ordered if col in paginated_df.columns]
        # Ensure all original columns are considered if they aren't static or metric
        other_cols = [col for col in paginated_df.columns if col not in final_col_order and col != id_col_name]
        final_col_order.extend(other_cols) # Add any remaining columns
        print(f"Final column order for display: {final_col_order}")
        # Create pagination context for the template
        pagination_context = {
            'page': page,
            'per_page': safe_per_page,
            'total_pages': total_pages,
            'total_items': total_items,
            'has_prev': page > 1,
            'has_next': page < total_pages,
            'prev_num': page - 1,
            'next_num': page + 1,
            'start_page_display': start_page_display, # Pass calculated start page
            'end_page_display': end_page_display,     # Pass calculated end page
            # Function to generate URLs for pagination links, preserving state
            'url_for_page': lambda p: url_for('security.securities_page', 
                                              page=p, 
                                              search_term=search_term, 
                                              sort_by=sort_by, 
                                              sort_order=sort_order, 
                                              **{f'filter_{k}': v for k, v in active_filters.items()}) 
        }
    except Exception as e:
        print(f"!!! Unexpected error during security page processing: {e}")
        traceback.print_exc()
        return render_template('securities_page.html', 
                               message=f"An unexpected error occurred: {e}", 
                               securities_data=[], 
                               pagination=None,
                               filter_options=final_filter_options if 'final_filter_options' in locals() else {},
                               active_filters=active_filters)
    # --- Render Template ---
    return render_template('securities_page.html',
                           securities_data=securities_data_list,
                           filter_options=final_filter_options,
                           column_order=final_col_order,
                           id_col_name=id_col_name,
                           search_term=search_term,
                           active_filters=active_filters, # Pass active filters for form state
                           pagination=pagination_context, # Pass pagination object
                           current_sort_by=sort_by,
                           current_sort_order=sort_order,
                           message=None) # Clear any previous error message if successful
@security_bp.route('/details/<metric_name>/<path:security_id>') # Corresponds to /security/details/..., use path converter
def security_details_page(metric_name, security_id):
    """Renders a page showing the time series chart for a specific security from a specific metric file."""
    # Decode the security_id using standard library
    decoded_security_id = unquote(security_id)
    filename = f"sec_{metric_name}.csv"
    price_filename = "sec_Price.csv" # Define price filename
    duration_filename = "sec_Duration.csv" # Define duration filename
    print(f"--- Requesting Security Details --- Metric: {metric_name}, Security ID (Encoded): {security_id}, Security ID (Decoded): {decoded_security_id}, File: {filename}")
    try:
        # 1. Load and process the specific security data file (for the primary metric)
        df_long, static_cols = load_and_process_security_data(filename)
        if df_long is None or df_long.empty:
            return f"Error: Could not load or process data for file '{filename}'", 404
        # Check if the requested security ID exists in the data using the DECODED ID
        actual_id_col_name = df_long.index.names[1]
        if decoded_security_id not in df_long.index.get_level_values(actual_id_col_name):
             return f"Error: Security ID '{decoded_security_id}' not found in file '{filename}'.", 404
        # 2. Extract historical data for the specific security using the DECODED ID
        security_data = df_long.xs(decoded_security_id, level=actual_id_col_name).sort_index().copy()
        # Filter to include only business days (Mon-Fri)
        if isinstance(security_data.index, pd.DatetimeIndex):
            security_data = security_data[security_data.index.dayofweek < 5]
        else:
            print(f"Warning: Index for primary metric data ({metric_name}) for {decoded_security_id} is not DatetimeIndex, skipping business day filter.")
        if security_data.empty:
            # Check if empty *after* filtering too
            return f"Error: No historical data found for Security ID '{decoded_security_id}' in file '{filename}' (or only weekend data).", 404
        # Extract static dimension values for display
        static_info = {}
        if not security_data.empty:
            first_row = security_data.iloc[0]
            for col in static_cols:
                if col in first_row.index:
                    static_info[col] = first_row[col]
        # 3. Load Price and Duration data (if files exist)
        price_data = pd.DataFrame()
        duration_data = pd.DataFrame()
        # Load Price Data
        try:
            price_df_long, _ = load_and_process_security_data(price_filename)
            if price_df_long is not None and not price_df_long.empty:
                price_id_col = price_df_long.index.names[1]
                if decoded_security_id in price_df_long.index.get_level_values(price_id_col):
                    price_data = price_df_long.xs(decoded_security_id, level=price_id_col)[['Value']].sort_index().copy()
                    price_data.rename(columns={'Value': 'Price'}, inplace=True)
                    # Filter price data to business days
                    if isinstance(price_data.index, pd.DatetimeIndex):
                        price_data = price_data[price_data.index.dayofweek < 5]
                    else:
                        print(f"Warning: Index for price data for {decoded_security_id} is not DatetimeIndex, skipping business day filter.")
        except FileNotFoundError:
            print(f"Price file '{price_filename}' not found. Price chart will not be available.")
        except Exception as e:
            print(f"Error loading price data for {decoded_security_id}: {e}")
            traceback.print_exc()
        # Load Duration Data
        try:
            duration_df_long, _ = load_and_process_security_data(duration_filename)
            if duration_df_long is not None and not duration_df_long.empty:
                duration_id_col = duration_df_long.index.names[1]
                if decoded_security_id in duration_df_long.index.get_level_values(duration_id_col):
                    duration_data = duration_df_long.xs(decoded_security_id, level=duration_id_col)[['Value']].sort_index().copy()
                    duration_data.rename(columns={'Value': 'Duration'}, inplace=True)
                    # Filter duration data to business days
                    if isinstance(duration_data.index, pd.DatetimeIndex):
                        duration_data = duration_data[duration_data.index.dayofweek < 5]
                    else:
                        print(f"Warning: Index for duration data for {decoded_security_id} is not DatetimeIndex, skipping business day filter.")
        except FileNotFoundError:
            print(f"Duration file '{duration_filename}' not found. Duration chart will not be available.")
        except Exception as e:
            print(f"Error loading duration data for {decoded_security_id}: {e}")
            traceback.print_exc()
        # 4. Prepare data for the primary chart
        labels = security_data.index.strftime('%Y-%m-%d').tolist()
        # Initialize datasets list for the primary chart
        primary_datasets = [{
            'label': f'{decoded_security_id} - {metric_name} Value', # Use decoded ID in label
            'data': security_data['Value'].round(3).fillna(np.nan).tolist(),
            'borderColor': COLOR_PALETTE[0], # Use first color
            'backgroundColor': COLOR_PALETTE[0] + '40',
            'tension': 0.1,
            'yAxisID': 'y' # Assign primary metric to the default 'y' axis
        }]
        # Initialize chart_data_for_js with primary data
        chart_data_for_js = {
            'labels': labels,
            'primary_datasets': primary_datasets, # Use a specific key for primary chart datasets
            'duration_dataset': None # Initialize duration dataset as None
        }
        latest_date_overall = security_data.index.max()
        # --- Attempt to load and add Price data to the primary chart ---
        try:
            print(f"Attempting to load price data from {price_filename} for {decoded_security_id}...")
            price_df_long, _ = load_and_process_security_data(price_filename) # Ignore static cols from price file
            if price_df_long is not None and not price_df_long.empty:
                price_actual_id_col = price_df_long.index.names[1] # Get ID column name from price file
                if decoded_security_id in price_df_long.index.get_level_values(price_actual_id_col):
                    price_data = price_df_long.xs(decoded_security_id, level=price_actual_id_col).sort_index().copy()
                    # Reindex price data to align dates with the main metric data
                    price_data = price_data.reindex(security_data.index)
                    if not price_data.empty:
                        price_dataset = {
                            'label': f'{decoded_security_id} - Price',
                            'data': price_data['Value'].round(3).fillna(np.nan).tolist(),
                            'borderColor': COLOR_PALETTE[1 % len(COLOR_PALETTE)], # Use second color
                            'backgroundColor': COLOR_PALETTE[1 % len(COLOR_PALETTE)] + '40',
                            'tension': 0.1,
                            'yAxisID': 'y1' # Assign price data to the second y-axis
                        }
                        # Append price dataset to the primary_datasets list
                        chart_data_for_js['primary_datasets'].append(price_dataset)
                        print("Successfully added Price data overlay to primary chart.")
                    else:
                         print(f"Warning: Price data found for {decoded_security_id}, but was empty after aligning dates.")
                else:
                     print(f"Warning: Security ID {decoded_security_id} not found in {price_filename}.")
            else:
                 print(f"Warning: Could not load or process {price_filename}.")
        except FileNotFoundError:
            print(f"Warning: Price data file '{price_filename}' not found. Skipping price overlay.")
        except Exception as e_price:
            print(f"Warning: Error processing price data for {decoded_security_id} from {price_filename}: {e_price}")
            traceback.print_exc() # Log the price processing error but continue
        # --- Attempt to load and add Duration data for the second chart ---
        try:
            print(f"Attempting to load duration data from {duration_filename} for {decoded_security_id}...")
            duration_df_long, _ = load_and_process_security_data(duration_filename) # Ignore static cols
            if duration_df_long is not None and not duration_df_long.empty:
                duration_actual_id_col = duration_df_long.index.names[1] # Get ID column name
                if decoded_security_id in duration_df_long.index.get_level_values(duration_actual_id_col):
                    duration_data = duration_df_long.xs(decoded_security_id, level=duration_actual_id_col).sort_index().copy()
                    # Reindex duration data to align dates with the main metric data
                    duration_data = duration_data.reindex(security_data.index)
                    if not duration_data.empty:
                        duration_dataset = {
                            'label': f'{decoded_security_id} - Duration', # Use decoded ID
                            'data': duration_data['Value'].round(3).fillna(np.nan).tolist(),
                            'borderColor': COLOR_PALETTE[2 % len(COLOR_PALETTE)], # Use third color
                            'backgroundColor': COLOR_PALETTE[2 % len(COLOR_PALETTE)] + '40',
                            'tension': 0.1
                            # No yAxisID needed if it's a separate chart with its own scale
                        }
                        # Assign duration dataset to its specific key
                        chart_data_for_js['duration_dataset'] = duration_dataset
                        print("Successfully prepared Duration data for separate chart.")
                    else:
                         print(f"Warning: Duration data found for {decoded_security_id}, but was empty after aligning dates.")
                else:
                     print(f"Warning: Security ID {decoded_security_id} not found in {duration_filename}.")
            else:
                 print(f"Warning: Could not load or process {duration_filename}.")
        except FileNotFoundError:
            print(f"Warning: Duration data file '{duration_filename}' not found. Skipping duration chart.")
        except Exception as e_duration:
            print(f"Warning: Error processing duration data for {decoded_security_id} from {duration_filename}: {e_duration}")
            traceback.print_exc() # Log the duration processing error but continue
        # 4. Render the template
        return render_template('security_details_page.html',
                               metric_name=metric_name,
                               security_id=decoded_security_id,
                               static_info=static_info,
                               chart_data_json=jsonify(chart_data_for_js).get_data(as_text=True),
                               latest_date=latest_date_overall.strftime('%d/%m/%Y'))
    except FileNotFoundError:
        return f"Error: Data file '{filename}' not found.", 404
    except ValueError as ve:
        print(f"Value Error processing security details for {metric_name}/{decoded_security_id}: {ve}")
        return f"Error processing details for {decoded_security_id}: {ve}", 400
    except Exception as e:
        print(f"Error processing security details for {metric_name}/{decoded_security_id}: {e}")
        traceback.print_exc()
        return f"An error occurred processing details for {decoded_security_id}: {e}", 500 
# Add a route for static asset discovery (like in metric_views)
@security_bp.route('/static/<path:filename>')
def static_files(filename):
    return send_from_directory(os.path.join(security_bp.root_path, '..', 'static'), filename)
</file>

<file path="views/spread_duration_comparison_views.py">
# views/spread_duration_comparison_views.py
# This module defines the Flask Blueprint for comparing two security spread duration datasets.
# It includes routes for a summary view listing securities with comparison metrics
# and a detail view showing overlayed time-series charts and statistics for a single security.
from flask import Blueprint, render_template, request, current_app, jsonify, url_for
import pandas as pd
import os
import logging
import math # Add math for pagination calculation
# Assuming security_processing and utils are in the parent directory or configured in PYTHONPATH
try:
    from security_processing import load_and_process_security_data # May need adjustments
    from utils import parse_fund_list # Example utility
    from config import DATA_FOLDER, COLOR_PALETTE
except ImportError:
    # Handle potential import errors if the structure is different
    logging.error("Could not import required modules from parent directory.")
    # Add fallback imports or path adjustments if necessary
    # Example: sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
    from ..security_processing import load_and_process_security_data
    from ..utils import parse_fund_list
    from ..config import DATA_FOLDER, COLOR_PALETTE
spread_duration_comparison_bp = Blueprint('spread_duration_comparison_bp', __name__,
                        template_folder='../templates',
                        static_folder='../static')
# Configure logging
log = logging.getLogger(__name__)
PER_PAGE_COMPARISON = 50 # Items per page for comparison summary
# --- Data Loading and Processing ---
def load_comparison_data(file1='sec_Spread duration.csv', file2='sec_Spread durationSP.csv'): # Updated filenames
    """Loads, processes, and merges data from two security spread duration files.
    Returns:
        tuple: (merged_df, static_data, common_static_cols, id_col_name)
               Returns (pd.DataFrame(), pd.DataFrame(), [], None) on error.
    """
    log.info(f"Loading spread duration comparison data: {file1} and {file2}")
    # Pass only the filename, as load_and_process_security_data prepends DATA_FOLDER internally
    df1, static_cols1 = load_and_process_security_data(file1)
    df2, static_cols2 = load_and_process_security_data(file2)
    if df1.empty or df2.empty:
        log.warning(f"One or both spread duration dataframes are empty. File1 empty: {df1.empty}, File2 empty: {df2.empty}")
        return pd.DataFrame(), pd.DataFrame(), [], None # Return None for id_col_name
    # Identify common static columns (excluding the ID column used for merging)
    common_static_cols = list(set(static_cols1) & set(static_cols2))
    # Get the actual ID column name (should be the same for both, use df1)
    if df1.index.nlevels == 2:
        id_col_name = df1.index.names[1] # Assuming 'Security ID'/Name is the second level
        log.info(f"Identified ID column from index: {id_col_name}")
    else:
        log.error("Processed Spread Duration DataFrame df1 does not have the expected 2-level MultiIndex.")
        return pd.DataFrame(), pd.DataFrame(), [], None # Return None for id_col_name
    # Prepare for merge - keep only necessary columns and rename Value columns
    df1_merge = df1.reset_index()[[id_col_name, 'Date', 'Value'] + common_static_cols].rename(columns={'Value': 'Value_Orig'})
    df2_merge = df2.reset_index()[[id_col_name, 'Date', 'Value']].rename(columns={'Value': 'Value_New'}) # Don't need static cols twice
    # Perform an outer merge to keep all dates and securities from both files
    merged_df = pd.merge(df1_merge, df2_merge, on=[id_col_name, 'Date'], how='outer')
    # Calculate daily changes
    merged_df = merged_df.sort_values(by=[id_col_name, 'Date'])
    merged_df['Change_Orig'] = merged_df.groupby(id_col_name)['Value_Orig'].diff()
    merged_df['Change_New'] = merged_df.groupby(id_col_name)['Value_New'].diff()
    # Store static data separately - get the latest version per security
    static_data = merged_df.groupby(id_col_name)[common_static_cols].last().reset_index()
    log.info(f"Successfully merged spread duration data. Shape: {merged_df.shape}")
    return merged_df, static_data, common_static_cols, id_col_name # Return the identified ID column name
def calculate_comparison_stats(merged_df, static_data, id_col):
    """Calculates comparison statistics for each security's spread duration.
    Args:
        merged_df (pd.DataFrame): The merged dataframe of original and new spread duration values.
        static_data (pd.DataFrame): DataFrame with static info per security.
        id_col (str): The name of the column containing the Security ID/Name.
    """
    if merged_df.empty:
        return pd.DataFrame()
    if id_col not in merged_df.columns:
        log.error(f"Specified id_col '{id_col}' not found in merged_df columns: {merged_df.columns.tolist()}")
        return pd.DataFrame() # Cannot group without the ID column
    log.info(f"Calculating spread duration comparison statistics using ID column: {id_col}...")
    stats_list = []
    # Use the passed id_col here
    for sec_id, group in merged_df.groupby(id_col):
        sec_stats = {id_col: sec_id} # Use actual id_col name
        # Filter out rows where both values are NaN for overall analysis period
        group_valid_overall = group.dropna(subset=['Value_Orig', 'Value_New'], how='all')
        overall_min_date = group_valid_overall['Date'].min()
        overall_max_date = group_valid_overall['Date'].max()
        # Filter out rows where EITHER value is NaN for correlation/diff calculations
        valid_comparison = group.dropna(subset=['Value_Orig', 'Value_New'])
        # 1. Correlation of Levels
        if len(valid_comparison) >= 2: # Need at least 2 points for correlation
            # Use the NaN-dropped dataframe for correlation
            level_corr = valid_comparison['Value_Orig'].corr(valid_comparison['Value_New'])
            sec_stats['Level_Correlation'] = level_corr if pd.notna(level_corr) else None
        else:
             sec_stats['Level_Correlation'] = None
        # 2. Max / Min (use original group to get true max/min including non-overlapping points)
        sec_stats['Max_Orig'] = group['Value_Orig'].max()
        sec_stats['Min_Orig'] = group['Value_Orig'].min()
        sec_stats['Max_New'] = group['Value_New'].max()
        sec_stats['Min_New'] = group['Value_New'].min()
        # 3. Date Range Comparison - Refined Logic
        # Find min/max dates within the MERGED data where each series is individually valid
        min_date_orig_idx = group['Value_Orig'].first_valid_index()
        max_date_orig_idx = group['Value_Orig'].last_valid_index()
        min_date_new_idx = group['Value_New'].first_valid_index()
        max_date_new_idx = group['Value_New'].last_valid_index()
        sec_stats['Start_Date_Orig'] = group.loc[min_date_orig_idx, 'Date'] if min_date_orig_idx is not None else None
        sec_stats['End_Date_Orig'] = group.loc[max_date_orig_idx, 'Date'] if max_date_orig_idx is not None else None
        sec_stats['Start_Date_New'] = group.loc[min_date_new_idx, 'Date'] if min_date_new_idx is not None else None
        sec_stats['End_Date_New'] = group.loc[max_date_new_idx, 'Date'] if max_date_new_idx is not None else None
        # Check if the start and end dates MATCH for the valid periods of EACH series
        same_start = pd.Timestamp(sec_stats['Start_Date_Orig']) == pd.Timestamp(sec_stats['Start_Date_New']) if sec_stats['Start_Date_Orig'] and sec_stats['Start_Date_New'] else False
        same_end = pd.Timestamp(sec_stats['End_Date_Orig']) == pd.Timestamp(sec_stats['End_Date_New']) if sec_stats['End_Date_Orig'] and sec_stats['End_Date_New'] else False
        sec_stats['Same_Date_Range'] = same_start and same_end
        # Add overall date range for info
        sec_stats['Overall_Start_Date'] = overall_min_date
        sec_stats['Overall_End_Date'] = overall_max_date
        # 4. Correlation of Daily Changes (Volatility Alignment)
        # Use the dataframe where BOTH values are non-NaN to calculate changes for correlation
        valid_comparison = valid_comparison.copy() # Avoid SettingWithCopyWarning
        valid_comparison['Change_Orig_Corr'] = valid_comparison['Value_Orig'].diff()
        valid_comparison['Change_New_Corr'] = valid_comparison['Value_New'].diff()
        # Drop NaNs created by the diff() itself (first row)
        valid_changes = valid_comparison.dropna(subset=['Change_Orig_Corr', 'Change_New_Corr'])
        if len(valid_changes) >= 2:
            change_corr = valid_changes['Change_Orig_Corr'].corr(valid_changes['Change_New_Corr'])
            sec_stats['Change_Correlation'] = change_corr if pd.notna(change_corr) else None
        else:
            sec_stats['Change_Correlation'] = None
            log.debug(f"Cannot calculate Spread Duration Change_Correlation for {sec_id}. Need >= 2 valid change pairs, found {len(valid_changes)}.")
        # 5. Difference Statistics (use the valid_comparison df where both values exist)
        valid_comparison['Abs_Diff'] = (valid_comparison['Value_Orig'] - valid_comparison['Value_New']).abs()
        sec_stats['Mean_Abs_Diff'] = valid_comparison['Abs_Diff'].mean() # Mean diff where both values exist
        sec_stats['Max_Abs_Diff'] = valid_comparison['Abs_Diff'].max() # Max diff where both values exist
        # Count NaNs - use original group
        sec_stats['NaN_Count_Orig'] = group['Value_Orig'].isna().sum()
        sec_stats['NaN_Count_New'] = group['Value_New'].isna().sum()
        sec_stats['Total_Points'] = len(group)
        stats_list.append(sec_stats)
    summary_df = pd.DataFrame(stats_list)
    # Merge static data back
    if not static_data.empty and id_col in static_data.columns and id_col in summary_df.columns:
        summary_df = pd.merge(summary_df, static_data, on=id_col, how='left')
    elif not static_data.empty:
         log.warning(f"Could not merge static data back for spread duration comparison. ID column '{id_col}' missing from static_data ({id_col in static_data.columns}) or summary_df ({id_col in summary_df.columns}).")
    log.info(f"Finished calculating spread duration stats. Summary shape: {summary_df.shape}")
    return summary_df
# --- Routes ---
@spread_duration_comparison_bp.route('/spread_duration_comparison/summary') # Updated route
def summary():
    """Displays the spread duration comparison summary page with server-side filtering, sorting, and pagination."""
    log.info("--- Starting Spread Duration Comparison Summary Request ---")
    try:
        # --- Get Request Parameters ---
        page = request.args.get('page', 1, type=int)
        sort_by = request.args.get('sort_by', 'Change_Correlation') # Default sort
        sort_order = request.args.get('sort_order', 'desc').lower()
        if sort_order not in ['asc', 'desc']:
            sort_order = 'desc'
        ascending = sort_order == 'asc'
        # Get active filters (ensuring keys are correct)
        active_filters = {k.replace('filter_', ''): v
                          for k, v in request.args.items()
                          if k.startswith('filter_') and v}
        log.info(f"Request Params: Page={page}, SortBy={sort_by}, Order={sort_order}, Filters={active_filters}")
        # --- Load and Prepare Data ---
        # Capture the actual ID column name returned by the load function
        merged_data, static_data, static_cols, actual_id_col = load_comparison_data()
        if actual_id_col is None:
            log.error("Failed to get ID column name during spread duration data loading.")
            return "Error loading spread duration comparison data: Could not determine ID column.", 500
        # Pass the actual ID column name to the stats calculation function
        summary_stats = calculate_comparison_stats(merged_data, static_data, id_col=actual_id_col)
        if summary_stats.empty and not merged_data.empty:
             log.warning("Spread Duration calculation resulted in empty stats DataFrame, but merged data was present.")
        elif summary_stats.empty:
             log.info("No spread duration summary statistics could be calculated.")
             # Render with message if empty even before filtering
             return render_template('spread_duration_comparison_page.html', # Updated template
                                    table_data=[],
                                    columns_to_display=[],
                                    id_column_name=actual_id_col,
                                    filter_options={},
                                    active_filters={},
                                    current_sort_by=sort_by,
                                    current_sort_order=sort_order,
                                    pagination=None,
                                    message="No spread duration comparison data available.")
        # --- Collect Filter Options (From Full Dataset Before Filtering) ---
        filter_options = {}
        potential_filter_cols = static_cols # Add other potential categorical columns from summary_stats if needed
        for col in potential_filter_cols:
            if col in summary_stats.columns:
                unique_vals = summary_stats[col].dropna().unique().tolist()
                # Basic type check and sort if possible - Improved Robust Sorting Key
                try:
                    # Attempt numerical sort first if applicable (handles ints/floats mixed with strings gracefully)
                    sorted_vals = sorted(unique_vals, key=lambda x: (isinstance(x, (int, float)), x))
                except TypeError:
                     # Fallback to string sort if mixed types cause issues
                    sorted_vals = sorted(unique_vals, key=str)
                filter_options[col] = sorted_vals
        log.info(f"Filter options generated: {list(filter_options.keys())}")
        # --- Apply Filters ---
        filtered_data = summary_stats.copy()
        if active_filters:
            log.info(f"Applying filters: {active_filters}")
            for col, value in active_filters.items():
                if col in filtered_data.columns:
                    # Handle potential type mismatches (e.g., filter value is string, column is number)
                    try:
                         # Convert filter value to column type if possible
                        col_type = filtered_data[col].dtype
                        if pd.api.types.is_numeric_dtype(col_type):
                            value = pd.to_numeric(value, errors='ignore') # Coerce to numeric if possible
                        elif pd.api.types.is_datetime64_any_dtype(col_type):
                             value = pd.to_datetime(value, errors='ignore') # Coerce to datetime if possible
                        # Apply filter (handle NaN explicitly if needed)
                        if pd.isna(value):
                            filtered_data = filtered_data[filtered_data[col].isna()]
                        else:
                            filtered_data = filtered_data[filtered_data[col] == value]
                    except Exception as e:
                        log.warning(f"Could not apply filter for column '{col}' with value '{value}'. Error: {e}. Skipping filter.")
                else:
                    log.warning(f"Filter column '{col}' not found in data. Skipping filter.")
            log.info(f"Data shape after filtering: {filtered_data.shape}")
        else:
            log.info("No active filters.")
        # --- Apply Sorting ---
        if sort_by in filtered_data.columns:
            log.info(f"Sorting by '{sort_by}' ({'Ascending' if ascending else 'Descending'})")
            # Handle NaNs during sorting - place them appropriately
            na_position = 'last' # Default, can be 'first' if preferred
            try:
                filtered_data = filtered_data.sort_values(by=sort_by, ascending=ascending, na_position=na_position)
            except Exception as e:
                log.error(f"Error during sorting by '{sort_by}': {e}. Falling back to default sort.")
                sort_by = 'Change_Correlation' # Revert to default if error
                ascending = False
                filtered_data = filtered_data.sort_values(by=sort_by, ascending=ascending, na_position=na_position)
        else:
            log.warning(f"Sort column '{sort_by}' not found. Using default 'Change_Correlation'.")
            sort_by = 'Change_Correlation' # Ensure default is used if provided key is invalid
            ascending = False
            filtered_data = filtered_data.sort_values(by=sort_by, ascending=ascending, na_position='last')
        # --- Pagination ---
        total_items = len(filtered_data)
        total_pages = math.ceil(total_items / PER_PAGE_COMPARISON)
        start_index = (page - 1) * PER_PAGE_COMPARISON
        end_index = start_index + PER_PAGE_COMPARISON
        paginated_data = filtered_data.iloc[start_index:end_index]
        log.info(f"Pagination: Total items={total_items}, Total pages={total_pages}, Current page={page}, Displaying items {start_index}-{end_index-1}")
        # --- Prepare for Template ---
        # Define columns to display (ensure actual_id_col is first)
        # Base columns - adjust as needed for spread duration comparison specifics
        base_cols = [
            'Level_Correlation', 'Change_Correlation',
            'Mean_Abs_Diff', 'Max_Abs_Diff',
            'NaN_Count_Orig', 'NaN_Count_New', 'Total_Points',
            'Same_Date_Range',
            'Start_Date_Orig', 'End_Date_Orig',
            'Start_Date_New', 'End_Date_New',
            'Max_Orig', 'Min_Orig', 'Max_New', 'Min_New'
            # Add/remove columns as needed
        ]
        # Ensure static columns come after the ID and before the calculated stats
        columns_to_display = [actual_id_col] + \
                             [col for col in static_cols if col != actual_id_col and col in paginated_data.columns] + \
                             [col for col in base_cols if col in paginated_data.columns]
        # Convert DataFrame to list of dictionaries for easy template iteration
        table_data = paginated_data.to_dict(orient='records')
        # Format specific columns (like correlations, dates)
        for row in table_data:
            for col in ['Level_Correlation', 'Change_Correlation']:
                 if col in row and pd.notna(row[col]):
                    row[col] = f"{row[col]:.4f}" # Format correlation
            for col in ['Start_Date_Orig', 'End_Date_Orig', 'Start_Date_New', 'End_Date_New']:
                 if col in row and pd.notna(row[col]):
                    try:
                        # Ensure it's a Timestamp before formatting
                        if isinstance(row[col], pd.Timestamp):
                             row[col] = row[col].strftime('%Y-%m-%d')
                        # If already string, assume correct format or skip
                    except AttributeError:
                        log.debug(f"Could not format date column '{col}' with value '{row[col]}'. Type: {type(row[col])}")
                        pass # Keep original value if formatting fails
        # Create pagination object
        pagination = {
            'page': page,
            'per_page': PER_PAGE_COMPARISON,
            'total_items': total_items,
            'total_pages': total_pages,
            'has_prev': page > 1,
            'has_next': page < total_pages,
            'prev_num': page - 1 if page > 1 else None,
            'next_num': page + 1 if page < total_pages else None,
        }
        log.info("--- Successfully Prepared Data for Spread Duration Comparison Template ---")
        return render_template('spread_duration_comparison_page.html', # Updated template
                               table_data=table_data,
                               columns_to_display=columns_to_display,
                               id_column_name=actual_id_col, # Pass the ID column name
                               filter_options=filter_options,
                               active_filters=active_filters,
                               current_sort_by=sort_by,
                               current_sort_order=sort_order,
                               pagination=pagination,
                               message=None) # No message if data is present
    except FileNotFoundError as e:
        log.error(f"Spread duration comparison file not found: {e}")
        return f"Error: Required spread duration comparison file not found ({e.filename}). Check the Data folder.", 404
    except Exception as e:
        log.exception("An unexpected error occurred in the spread duration comparison summary view.") # Log full traceback
        return f"An internal error occurred: {e}", 500
@spread_duration_comparison_bp.route('/spread_duration_comparison/details/<path:security_id>') # Updated route
def details(security_id):
    """Displays detailed comparison charts for a single security's spread duration."""
    log.info(f"--- Starting Spread Duration Comparison Details Request for Security ID: {security_id} ---")
    try:
        # Load the merged data again (could potentially cache this)
        # Specify filenames explicitly
        merged_df, _, common_static_cols, id_col_name = load_comparison_data(file1='sec_Spread duration.csv', file2='sec_Spread durationSP.csv')
        if id_col_name is None:
             log.error(f"Failed to get ID column name for details view (Security: {security_id}).")
             return "Error loading spread duration comparison data: Could not determine ID column.", 500
        if merged_df.empty:
            log.warning(f"Merged spread duration data is empty for details view (Security: {security_id}).")
            return f"No merged spread duration data found for Security ID: {security_id}", 404
        # Filter data for the specific security using the correct ID column name
        security_data = merged_df[merged_df[id_col_name] == security_id].copy() # Use .copy()
        if security_data.empty:
            log.warning(f"No spread duration data found for the specific Security ID: {security_id}")
            # Consider checking if the ID exists in the original files?
            return f"Spread Duration data not found for Security ID: {security_id}", 404
        # Get static info for this security (handle potential multiple rows if ID isn't unique, take first)
        static_info = security_data[[id_col_name] + common_static_cols].iloc[0].to_dict() if not security_data.empty else {}
        # Sort by date for charting
        security_data = security_data.sort_values(by='Date')
        # Prepare data for Chart.js
        # Ensure 'Date' is in the correct string format for JSON/JS
        security_data['Date_Str'] = security_data['Date'].dt.strftime('%Y-%m-%d')
        chart_data = {
            'labels': security_data['Date_Str'].tolist(),
            'datasets': [
                {
                    'label': 'Original Spread Duration', # Updated Label
                    'data': security_data['Value_Orig'].where(pd.notna(security_data['Value_Orig']), None).tolist(), # Replace NaN with None for JSON
                    'borderColor': COLOR_PALETTE[0 % len(COLOR_PALETTE)],
                    'fill': False,
                    'tension': 0.1
                },
                {
                    'label': 'New Spread Duration', # Updated Label
                    'data': security_data['Value_New'].where(pd.notna(security_data['Value_New']), None).tolist(), # Replace NaN with None for JSON
                    'borderColor': COLOR_PALETTE[1 % len(COLOR_PALETTE)],
                    'fill': False,
                    'tension': 0.1
                }
            ]
        }
        # Calculate overall statistics for this security
        stats_summary = calculate_comparison_stats(security_data, pd.DataFrame([static_info]), id_col=id_col_name) # Pass single security data
        stats_dict = stats_summary.iloc[0].to_dict() if not stats_summary.empty else {}
         # Format dates and numbers in stats_dict before passing
        for key, value in stats_dict.items():
            if isinstance(value, pd.Timestamp):
                stats_dict[key] = value.strftime('%Y-%m-%d')
            elif isinstance(value, (int, float)):
                 if 'Correlation' in key and pd.notna(value):
                     stats_dict[key] = f"{value:.4f}"
                 elif 'Diff' in key and pd.notna(value):
                      stats_dict[key] = f"{value:.2f}" # Adjust formatting as needed
        log.info(f"Successfully prepared data for spread duration details template (Security: {security_id})")
        return render_template('spread_duration_comparison_details_page.html', # Updated template
                               security_id=security_id,
                               static_info=static_info, # Pass static info
                               chart_data=chart_data,
                               stats_summary=stats_dict) # Pass calculated stats
    except FileNotFoundError as e:
        log.error(f"Spread duration comparison file not found for details view: {e} (Security: {security_id})")
        return f"Error: Required spread duration comparison file not found ({e.filename}). Check the Data folder.", 404
    except KeyError as e:
         log.error(f"KeyError accessing data for security '{security_id}': {e}. ID column used: '{id_col_name}'")
         return f"Error accessing data for security '{security_id}'. It might be missing required columns or have unexpected formatting.", 500
    except Exception as e:
        log.exception(f"An unexpected error occurred in the spread duration comparison details view for security '{security_id}'.") # Log full traceback
        return f"An internal error occurred while processing details for security '{security_id}': {e}", 500
</file>

<file path="views/weight_views.py">
# views/weight_views.py
# Purpose: Handles routes related to weight checks (e.g., ensuring weights are 100%).
import os
import pandas as pd
import traceback
import logging
from flask import Blueprint, render_template
from config import DATA_FOLDER
# Define the blueprint
weight_bp = Blueprint('weight', __name__, url_prefix='/weights')
def _parse_percentage(value):
    """Attempts to parse a string like '99.5%' into a float 99.5."""
    if pd.isna(value) or value == '':
        return None
    try:
        # Remove '%' and convert to float
        return float(str(value).replace('%', '').strip())
    except (ValueError, TypeError):
        logging.warning(f"Could not parse percentage value: {value}")
        return None # Indicate parsing failure
def _is_iso_date_column(col_name):
    """Checks if a column name matches YYYY-MM-DDTHH:MM:SS format."""
    try:
        # Attempt to parse using pandas, strict match needed
        pd.to_datetime(col_name, format='%Y-%m-%dT%H:%M:%S', errors='raise')
        return True
    except (ValueError, TypeError):
        return False
def load_and_process_weight_data(filename):
    """Loads a wide weight file, processes percentages, checks against 100%."""
    filepath = os.path.join(DATA_FOLDER, filename)
    if not os.path.exists(filepath):
        logging.error(f"Weight file not found: {filepath}")
        return None, [] # Return None for data, empty list for headers
    try:
        df = pd.read_csv(filepath, encoding='utf-8')
        df.columns = df.columns.str.strip() # Ensure no leading/trailing spaces
        # Identify ID column (assuming 'Fund Code')
        id_col = 'Fund Code'
        if id_col not in df.columns:
            logging.error(f"Required column '{id_col}' not found in {filename}")
            return None, []
        # Identify date columns based on ISO format
        date_cols_iso = [col for col in df.columns if _is_iso_date_column(col)]
        if not date_cols_iso:
            logging.warning(f"No date columns found in expected format in {filename}")
            # Fallback: Try simple YYYY-MM-DD check? For now, let's stick to expected format.
            return None, []
        # Sort date columns chronologically
        date_cols_sorted = sorted(date_cols_iso, key=lambda d: pd.to_datetime(d, format='%Y-%m-%dT%H:%M:%S'))
        # Simplify date headers for display (YYYY-MM-DD)
        date_headers_display = [pd.to_datetime(d).strftime('%Y-%m-%d') for d in date_cols_sorted]
        processed_data = {}
        # Set index for easier access
        df.set_index(id_col, inplace=True)
        for fund_code in df.index:
            processed_data[fund_code] = {}
            for date_col_iso, date_header_display in zip(date_cols_sorted, date_headers_display):
                original_value_str = str(df.loc[fund_code, date_col_iso])
                parsed_value_float = _parse_percentage(original_value_str)
                is_100 = False
                if parsed_value_float is not None:
                    # Check for exact equality with 100.0
                    is_100 = (parsed_value_float == 100.0)
                processed_data[fund_code][date_header_display] = {
                    'value_str': original_value_str if not pd.isna(original_value_str) else 'N/A',
                    'is_100': is_100,
                    'parsed_value': parsed_value_float # Keep for potential future use/debugging
                }
        return processed_data, date_headers_display
    except Exception as e:
        logging.error(f"Error processing weight file {filename}: {e}")
        traceback.print_exc()
        return None, []
@weight_bp.route('/check')
def weight_check():
    """Displays the weight check page."""
    fund_filename = 'w_Funds.csv'
    bench_filename = 'w_Bench.csv'
    fund_data, fund_date_headers = load_and_process_weight_data(fund_filename)
    bench_data, bench_date_headers = load_and_process_weight_data(bench_filename)
    # Use the longer list of dates as the canonical header list, assuming they might differ slightly
    all_date_headers = sorted(list(set(fund_date_headers + bench_date_headers)))
    return render_template('weight_check_page.html',
                           fund_data=fund_data,
                           bench_data=bench_data,
                           date_headers=all_date_headers, # Pass combined, sorted list
                           fund_filename=fund_filename,
                           bench_filename=bench_filename)
</file>

</files>
