This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-04-12T10:48:29.562Z

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
.gitignore
app.py
config.py
curve_processing.py
data_loader.py
data_validation.py
LICENSE
metric_calculator.py
process_data.py
README.md
requirements.txt
security_processing.py
static/css/style.css
static/js/main.js
static/js/modules/charts/timeSeriesChart.js
static/js/modules/ui/chartRenderer.js
static/js/modules/ui/securityTableFilter.js
static/js/modules/ui/tableSorter.js
static/js/modules/utils/helpers.js
templates/base.html
templates/comparison_details_page.html
templates/comparison_page.html
templates/curve_details.html
templates/curve_summary.html
templates/delete_metric_page.html
templates/duration_comparison_details_page.html
templates/duration_comparison_page.html
templates/exclusions_page.html
templates/fund_detail_page.html
templates/fund_duration_details.html
templates/get_data.html
templates/index.html
templates/metric_page_js.html
templates/securities_page.html
templates/security_details_page.html
templates/spread_duration_comparison_details_page.html
templates/spread_duration_comparison_page.html
templates/weight_check_page.html
utils.py
views/__init__.py
views/api_views.py
views/comparison_views.py
views/curve_views.py
views/duration_comparison_views.py
views/exclusion_views.py
views/fund_views.py
views/main_views.py
views/metric_views.py
views/security_views.py
views/spread_duration_comparison_views.py
views/weight_views.py
weight_processing.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".gitignore">
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class
*.csv
# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
# Usually these files are written by a python script from a template
# before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
.hypothesis/
.pytest_cache/

# Translations
*.mo
*.pot
*.log

# Django stuff:
*.log
local_settings.py
db.sqlite3

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
.python-version

# celery beat schedule file
celerybeat-schedule

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/
</file>

<file path="app.py">
# This file defines the main entry point and structure for the Simple Data Checker Flask web application.
# It utilizes the Application Factory pattern (`create_app`) to initialize and configure the Flask app.
# Key responsibilities include:
# - Creating the Flask application instance.
# - Setting up basic configuration (like the secret key).
# - Ensuring necessary folders (like the instance folder) exist.
# - Determining and configuring the absolute data folder path using `utils.get_data_folder_path`.
# - Centralizing logging configuration (File and Console handlers).
# - Registering Blueprints (`main_bp`, `metric_bp`, `security_bp`, `fund_bp`, `exclusion_bp`, `comparison_bp`, `duration_comparison_bp`, `spread_duration_comparison_bp`, `api_bp`, `weight_bp`) from the `views`
#   directory, which contain the application\'s routes and view logic.
# - Providing a conditional block (`if __name__ == '__main__':`) to run the development server
#   when the script is executed directly.
# This modular structure using factories and blueprints makes the application more organized and scalable.
# This file contains the main Flask application factory.
from flask import Flask, render_template, Blueprint, jsonify
import os
import logging
from logging.handlers import RotatingFileHandler # Import handler
# --- Add imports for the new route ---
import subprocess
import sys # To get python executable path
# --- End imports ---
# Import configurations and utilities
from config import COLOR_PALETTE # Import other needed configs
from utils import get_data_folder_path # Import the path utility
def create_app():
    """Factory function to create and configure the Flask app."""
    app = Flask(__name__, instance_relative_config=True) # instance_relative_config=True allows for instance folder config
    app.logger.info(f"Application root path: {app.root_path}")
    # Basic configuration (can be expanded later, e.g., loading from config file)
    app.config.from_mapping(
        SECRET_KEY='dev', # Default secret key for development. CHANGE for production!
        # Add other default configurations if needed
    )
    # Load configuration from config.py (e.g., COLOR_PALETTE)
    app.config.from_object('config')
    app.logger.info("Loaded configuration from config.py")
    # --- Determine and set the Data Folder Path --- 
    # Use the utility function to get the absolute data path, using the app's root path as the base
    # This ensures consistency whether the path in config.py is relative or absolute
    absolute_data_path = get_data_folder_path(app_root_path=app.root_path)
    app.config['DATA_FOLDER'] = absolute_data_path
    app.logger.info(f"Data folder path set to: {app.config['DATA_FOLDER']}")
    # --- End Data Folder Path Setup ---
    # Ensure the instance folder exists (needed for logging)
    try:
        os.makedirs(app.instance_path, exist_ok=True) # exist_ok=True prevents error if exists
        app.logger.info(f"Instance folder ensured at: {app.instance_path}")
    except OSError as e:
        app.logger.error(f"Could not create instance folder at {app.instance_path}: {e}", exc_info=True)
        # Depending on severity, might want to raise an exception or exit
    # --- Centralized Logging Configuration --- 
    # Remove Flask's default handlers
    app.logger.handlers.clear()
    app.logger.setLevel(logging.DEBUG) # Set the app logger level (DEBUG captures everything)
    # Formatter
    log_formatter = logging.Formatter(
        '%(asctime)s %(levelname)s: %(message)s [in %(pathname)s:%(lineno)d]'
    )
    # File Handler (Rotating)
    log_file_path = os.path.join(app.instance_path, 'app.log')
    max_log_size = 1024 * 1024 * 10 # 10 MB
    backup_count = 5
    try:
        file_handler = RotatingFileHandler(
            log_file_path,
            maxBytes=max_log_size,
            backupCount=backup_count
        )
        file_handler.setFormatter(log_formatter)
        file_handler.setLevel(logging.INFO) # Log INFO and higher to file
        app.logger.addHandler(file_handler)
        app.logger.info(f"File logging configured to: {log_file_path}")
    except Exception as e:
        app.logger.error(f"Failed to configure file logging to {log_file_path}: {e}", exc_info=True)
    # Console Handler
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(log_formatter)
    # Set console level potentially higher for less noise (e.g., INFO) or keep DEBUG for development
    console_handler.setLevel(logging.DEBUG)
    app.logger.addHandler(console_handler)
    app.logger.info("Centralized logging configured (File & Console).")
    # --- End Logging Configuration ---
    # Configure logging (consider moving to a dedicated logging setup function)
    # Note: BasicConfig should ideally be called only once. If utils.py also calls it,
    # it might conflict or be ineffective here. A more robust setup is recommended.
    # logging.basicConfig(level=logging.INFO,
    #                     format='%(asctime)s %(levelname)s %(name)s %(threadName)s : %(message)s')
    # app.logger.info("Logging configured.") # BasicConfig might be configured in utils already
    # Serve static files (for JS, CSS, etc.)
    # Note: static_url_path defaults to /static, static_folder defaults to 'static' in root
    # No need to set app.static_folder = 'static' explicitly unless changing the folder name/path
    # --- Register Blueprints ---
    from views.main_views import main_bp
    from views.metric_views import metric_bp
    from views.security_views import security_bp
    from views.fund_views import fund_bp
    from views.api_views import api_bp
    from views.exclusion_views import exclusion_bp
    from views.comparison_views import comparison_bp
    from views.weight_views import weight_bp
    # --- Import new blueprints ---
    from views.duration_comparison_views import duration_comparison_bp
    from views.spread_duration_comparison_views import spread_duration_comparison_bp
    # --- End import new blueprints ---
    from views.curve_views import curve_bp # Import the new blueprint
    app.register_blueprint(main_bp)
    app.register_blueprint(metric_bp)
    app.register_blueprint(security_bp)
    app.register_blueprint(fund_bp)
    app.register_blueprint(api_bp)
    app.register_blueprint(exclusion_bp)
    app.register_blueprint(comparison_bp)
    app.register_blueprint(weight_bp)
    # --- Register new blueprints ---
    app.register_blueprint(duration_comparison_bp)
    app.register_blueprint(spread_duration_comparison_bp)
    # --- End register new blueprints ---
    app.register_blueprint(curve_bp) # Register the new blueprint
    app.logger.info("Registered Blueprints:")
    app.logger.info(f"- {main_bp.name} (prefix: {main_bp.url_prefix})")
    app.logger.info(f"- {metric_bp.name} (prefix: {metric_bp.url_prefix})")
    app.logger.info(f"- {security_bp.name} (prefix: {security_bp.url_prefix})")
    app.logger.info(f"- {fund_bp.name} (prefix: {fund_bp.url_prefix})")
    app.logger.info(f"- {api_bp.name} (prefix: {api_bp.url_prefix})")
    app.logger.info(f"- {exclusion_bp.name} (prefix: {exclusion_bp.url_prefix})")
    app.logger.info(f"- {comparison_bp.name} (prefix: {comparison_bp.url_prefix})")
    app.logger.info(f"- {weight_bp.name} (prefix: {weight_bp.url_prefix})")
    # --- Print new blueprints ---
    print(f"- {duration_comparison_bp.name} (prefix: {duration_comparison_bp.url_prefix})")
    print(f"- {spread_duration_comparison_bp.name} (prefix: {spread_duration_comparison_bp.url_prefix})")
    # --- End print new blueprints ---
    app.logger.info(f"- {curve_bp.name} (prefix: {curve_bp.url_prefix})") # Log registration
    # Add a simple test route to confirm app creation (optional)
    @app.route('/hello')
    def hello():
        return 'Hello, World! App factory is working.'
    # --- Add the new cleanup route ---
    @app.route('/run-cleanup', methods=['POST'])
    def run_cleanup():
        """Endpoint to trigger the process_data.py script."""
        script_path = os.path.join(os.path.dirname(__file__), 'process_data.py')
        python_executable = sys.executable # Use the same python that runs flask
        if not os.path.exists(script_path):
            app.logger.error(f"Cleanup script not found at: {script_path}")
            return jsonify({'status': 'error', 'message': 'Cleanup script not found.'}), 500
        app.logger.info(f"Attempting to run cleanup script: {script_path}")
        try:
            # Run the script using the same Python interpreter that is running Flask
            # Capture stdout and stderr, decode as UTF-8, handle potential errors
            result = subprocess.run(
                [python_executable, script_path],
                capture_output=True,
                text=True,
                check=False, # Don't raise exception on non-zero exit code
                encoding='utf-8' # Explicitly set encoding
            )
            log_output = f"STDOUT:\n{result.stdout}\nSTDERR:\n{result.stderr}"
            if result.returncode == 0:
                app.logger.info(f"Cleanup script finished successfully. Output:\n{log_output}")
                return jsonify({'status': 'success', 'output': result.stdout or "No output", 'error': result.stderr}), 200
            else:
                app.logger.error(f"Cleanup script failed with return code {result.returncode}. Output:\n{log_output}")
                return jsonify({'status': 'error', 'message': 'Cleanup script failed.', 'output': result.stdout, 'error': result.stderr}), 500
        except Exception as e:
            app.logger.error(f"Exception occurred while running cleanup script: {e}", exc_info=True)
            return jsonify({'status': 'error', 'message': f'An exception occurred: {e}'}), 500
    # --- End new route ---
    return app
# --- Application Execution ---
if __name__ == '__main__':
    app = create_app() # Create the app instance using the factory
    app.run(debug=True, host='0.0.0.0') # Run in debug mode for development, accessible on network
</file>

<file path="config.py">
# This file defines configuration variables for the Simple Data Checker application.
# It centralizes settings like file paths and visual parameters (e.g., chart colors)
# to make them easily adjustable without modifying the core application code.
"""
Configuration settings for the Flask application.
"""
# Define the primary data directory.
# This path is read by the `utils.get_data_folder_path` function during application startup.
# - If this is an absolute path (e.g., 'C:/MyApp/Data', '/var/data'), it will be used directly.
# - If this is a relative path (e.g., 'Data', '../SharedData'), it will be resolved
#   to an absolute path relative to the application's root directory (determined by Flask's app.root_path
#   or the script's location for standalone scripts).
# **Use forward slashes (/) for paths, even on Windows, for consistency.**
# If this variable is missing, empty, or the file doesn't exist, the utility function
# will fall back to using 'Data' relative to the application root.
DATA_FOLDER = 'Data'
# Define a list of distinct colors for chart lines
# Add more colors if you expect more fund columns
COLOR_PALETTE = [
    'blue', 'red', 'green', 'purple', '#FF7F50', # Coral
    '#6495ED', # CornflowerBlue
    '#DC143C', # Crimson
    '#00FFFF'  # Aqua
]
</file>

<file path="curve_processing.py">
# Purpose: Handles loading, preprocessing, and analysis of yield curve data (curves.csv).
# The `load_curve_data` function expects the absolute path to the data folder to be provided.
# Stdlib imports
import os
import re
from datetime import timedelta
# Third-party imports
import pandas as pd
import numpy as np
# Local imports
# Removed: from config import DATA_FOLDER
import logging # Import logging
# Get the logger instance. Assumes Flask app has configured logging.
logger = logging.getLogger(__name__)
# Constants for term conversion
TERM_MULTIPLIERS = {
    'D': 1,
    'W': 7,
    'M': 30,  # Approximate
    'Y': 365 # Approximate
}
def _term_to_days(term_str):
    """Converts a term string (e.g., '7D', '1M', '2Y') to an approximate number of days."""
    if not isinstance(term_str, str):
        return None # Handle non-string inputs
    term_str = term_str.upper()
    match = re.match(r"(\d+)([DWMY])", term_str)
    if match:
        num, unit = match.groups()
        multiplier = TERM_MULTIPLIERS.get(unit)
        if multiplier:
            try:
                return int(num) * multiplier
            except ValueError:
                 logger.warning(f"Could not convert number part '{num}' in term '{term_str}' to integer.")
                 return None
    try:
        # Handle simple integer strings representing days (fallback)
        return int(term_str)
    except ValueError:
        logger.warning(f"Could not parse term '{term_str}' to days.")
        return None # Indicate failure to parse
def load_curve_data(data_folder_path: str):
    """Loads and preprocesses the curve data from 'curves.csv' within the given folder.
    Args:
        data_folder_path (str): The absolute path to the folder containing 'curves.csv'.
                                The caller is responsible for providing the correct path,
                                typically obtained from `current_app.config['DATA_FOLDER']`.
    Returns:
        pd.DataFrame: Processed curve data indexed by [Currency, Date, Term],
                      or an empty DataFrame if loading/processing fails.
    """
    if not data_folder_path:
        logger.error("No data_folder_path provided to load_curve_data.")
        return pd.DataFrame()
    file_path = os.path.join(data_folder_path, 'curves.csv')
    logger.info(f"Attempting to load curve data from: {file_path}")
    if not os.path.exists(file_path):
        logger.error(f"Curve data file not found at {file_path}")
        return pd.DataFrame() # Return empty DataFrame
    try:
        # Specify dayfirst=True for DD/MM/YYYY format
        df = pd.read_csv(file_path, parse_dates=['Date'], dayfirst=True)
        logger.info(f"Successfully loaded {len(df)} rows from {file_path}")
    except Exception as e:
        logger.error(f"Error reading curve data CSV '{file_path}': {e}", exc_info=True)
        return pd.DataFrame()
    # Rename columns for consistency if necessary (adjust based on actual CSV)
    rename_map = {'Currency Code': 'Currency', 'Daily Value': 'Value'}
    df.rename(columns=rename_map, inplace=True)
    logger.debug(f"Renamed columns: {rename_map}")
    # Convert Term to days for sorting and plotting
    df['TermDays'] = df['Term'].apply(_term_to_days)
    logger.debug("Applied _term_to_days conversion.")
    # Convert Value to numeric first
    original_rows = len(df)
    df['Value'] = pd.to_numeric(df['Value'], errors='coerce')
    rows_after_numeric = len(df.dropna(subset=['Value']))
    if original_rows > rows_after_numeric:
         logger.warning(f"Dropped {original_rows - rows_after_numeric} rows due to non-numeric 'Value'.")
    df.dropna(subset=['Value'], inplace=True)
    # Drop rows where term conversion failed
    original_rows = len(df)
    rows_after_term = len(df.dropna(subset=['TermDays']))
    if original_rows > rows_after_term:
         logger.warning(f"Dropped {original_rows - rows_after_term} rows due to unparseable 'Term'.")
    df.dropna(subset=['TermDays'], inplace=True)
    if df.empty:
         logger.warning("DataFrame is empty after initial processing and dropping NaNs.")
         return df # Return empty if all rows were dropped
    # Set index and sort
    try:
        df.sort_values(by=['Currency', 'Date', 'TermDays'], inplace=True)
        # Use MultiIndex for efficient lookups
        df.set_index(['Currency', 'Date', 'Term'], inplace=True)
        logger.info(f"Curve data processed. Final shape: {df.shape}")
    except KeyError as e:
         logger.error(f"Missing expected column for sorting/indexing: {e}. Columns present: {df.columns.tolist()}")
         return pd.DataFrame() # Return empty on structure error
    except Exception as e:
         logger.error(f"Unexpected error during sorting/indexing: {e}", exc_info=True)
         return pd.DataFrame()
    return df
def get_latest_curve_date(df):
    """Gets the most recent date in the DataFrame's index."""
    if df.empty or 'Date' not in df.index.names:
        logger.warning("Cannot get latest date: DataFrame is empty or 'Date' is not in the index.")
        return None
    try:
        latest_date = df.index.get_level_values('Date').max()
        logger.debug(f"Latest date found: {latest_date}")
        return latest_date
    except Exception as e:
         logger.error(f"Error getting latest date from index: {e}", exc_info=True)
         return None
def check_curve_inconsistencies(df):
    """
    Checks for inconsistencies in yield curves compared to the previous day.
    Returns a dictionary summarizing potential issues for the latest date.
    """
    if df.empty:
        logger.warning("Skipping inconsistency check: Input DataFrame is empty.")
        return {}
    latest_date = get_latest_curve_date(df)
    if not latest_date:
        logger.warning("Skipping inconsistency check: Could not determine latest date.")
        return {}
    logger.info(f"Checking inconsistencies for latest date: {latest_date.strftime('%Y-%m-%d')}")
    # Find the previous available date
    try:
        all_dates = df.index.get_level_values('Date').unique()
        available_dates = sorted(all_dates, reverse=True)
        previous_date = None
        if len(available_dates) > 1:
            # Use get_loc for robust index finding
            latest_date_pos = available_dates.get_loc(latest_date)
            if latest_date_pos + 1 < len(available_dates):
                previous_date = available_dates[latest_date_pos + 1]
                logger.info(f"Previous date found for comparison: {previous_date.strftime('%Y-%m-%d')}")
            else:
                logger.info("Latest date is the only date available. Cannot compare change profile.")
        else:
            logger.info("Only one date available. Cannot compare change profile.")
    except Exception as e:
        logger.error(f"Error determining previous date: {e}", exc_info=True)
        previous_date = None
    summary = {}
    currencies = df.index.get_level_values('Currency').unique()
    logger.debug(f"Checking currencies: {currencies.tolist()}")
    for currency in currencies:
        try:
            # --- Get Latest Curve Data (Safer Filtering) ---
            logger.debug(f"Attempting to filter latest data for {currency} on {latest_date.strftime('%Y-%m-%d')}")
            latest_mask = (df.index.get_level_values('Currency') == currency) & \
                          (df.index.get_level_values('Date') == latest_date)
            latest_curve_filtered = df[latest_mask]
            if latest_curve_filtered.empty:
                logger.warning(f"No latest data found for {currency} on {latest_date.strftime('%Y-%m-%d')} using boolean mask.")
                summary.setdefault(currency, []).append("Missing latest data")
                continue
            latest_curve = latest_curve_filtered.reset_index().sort_values('TermDays')
            logger.debug(f"Successfully filtered latest data for {currency}. Shape: {latest_curve.shape}")
            # --- Basic Check 1: Monotonicity ---
            diffs = latest_curve['Value'].diff()
            large_drops = diffs[diffs < -0.5]
            if not large_drops.empty:
                terms = latest_curve.loc[large_drops.index, 'Term'].tolist()
                issue_msg = f"Potential non-monotonic drop(s) < -0.5 between terms near: {terms} on {latest_date.strftime('%Y-%m-%d')}"
                summary.setdefault(currency, []).append(issue_msg)
                logger.warning(f"{currency}: {issue_msg}")
            # --- Check 2: Compare change shape with previous day ---
            if previous_date:
                logger.debug(f"Attempting to filter previous data for {currency} on {previous_date.strftime('%Y-%m-%d')}")
                prev_mask = (df.index.get_level_values('Currency') == currency) & \
                            (df.index.get_level_values('Date') == previous_date)
                previous_curve_filtered = df[prev_mask]
                if previous_curve_filtered.empty:
                    logger.warning(f"No previous day data ({previous_date.strftime('%Y-%m-%d')}) found for {currency} using boolean mask.")
                    summary.setdefault(currency, []).append("Missing previous data for comparison")
                else:
                    previous_curve = previous_curve_filtered.reset_index().sort_values('TermDays')
                    logger.debug(f"Successfully filtered previous data for {currency}. Shape: {previous_curve.shape}")
                    merged = pd.merge(
                        latest_curve[['Term', 'TermDays', 'Value']],
                        previous_curve[['Term', 'TermDays', 'Value']],
                        on='TermDays',
                        suffixes=('_latest', '_prev'),
                        how='inner'
                    )
                    if merged.empty:
                        logger.warning(f"No common terms found between dates for {currency}.")
                    else:
                        merged['ValueChange'] = merged['Value_latest'] - merged['Value_prev']
                        merged.sort_values('TermDays', inplace=True)
                        merged['ChangeDiff'] = merged['ValueChange'].diff()
                        change_diff_std = merged['ChangeDiff'].std()
                        change_diff_mean = merged['ChangeDiff'].mean()
                        threshold_std = np.nan_to_num(change_diff_mean + 3 * change_diff_std)
                        threshold_abs = 0.2
                        final_threshold = max(abs(threshold_std), threshold_abs)
                        anomalous_jumps = merged[abs(merged['ChangeDiff'].fillna(0)) > final_threshold]
                        if not anomalous_jumps.empty:
                            anomalous_terms = anomalous_jumps['Term_latest'].tolist()
                            issue_msg = f"Anomalous change profile jump vs {previous_date.strftime('%Y-%m-%d')} near terms: {anomalous_terms}"
                            summary.setdefault(currency, []).append(issue_msg)
                            logger.warning(f"{currency}: {issue_msg}")
            if currency not in summary:
                summary[currency] = ["OK"]
                logger.info(f"{currency}: Checks passed.")
        except pd.errors.InvalidIndexError as e:
            logger.error(f"InvalidIndexError processing curve for {currency}: {e}", exc_info=True)
            summary.setdefault(currency, []).append(f"Processing error (InvalidIndexError)")
        except KeyError as e:
            logger.error(f"KeyError processing curve for {currency}: {e}", exc_info=True)
            summary.setdefault(currency, []).append(f"Processing error (KeyError)")
        except Exception as e:
            logger.error(f"Unexpected error processing curve for {currency}: {e}", exc_info=True)
            summary.setdefault(currency, []).append(f"Processing error: {type(e).__name__}")
    logger.info("Finished inconsistency checks.")
    return summary
if __name__ == '__main__':
    # Example usage when run directly:
    print("Running curve_processing.py directly...")
    logger.info("--- Starting Standalone Execution ---")
    print("Loading curve data...")
    curve_df = load_curve_data()
    if not curve_df.empty:
        print("\nCurve data loaded successfully.")
        latest_dt = get_latest_curve_date(curve_df)
        print(f"\nLatest Date found: {latest_dt.strftime('%Y-%m-%d') if latest_dt else 'N/A'}")
        print("\nChecking for inconsistencies on latest date...")
        inconsistency_summary = check_curve_inconsistencies(curve_df)
        print("\n--- Inconsistency Summary ---")
        if inconsistency_summary:
            for currency, issues in inconsistency_summary.items():
                print(f"  {currency}: {', '.join(issues)}")
        else:
            print("  No inconsistencies detected or data was insufficient for checks.")
        # Example: Get data for a specific currency and date
        if latest_dt:
            test_currency = 'USD'
            try:
                # Use .loc with pd.IndexSlice for clarity
                idx = pd.IndexSlice
                usd_latest = curve_df.loc[idx[test_currency, latest_dt, :]]
                # Reset index to get TermDays as a column for printing
                print(f"\n{test_currency} Curve on latest date ({latest_dt.strftime('%Y-%m-%d')}):")
                print(usd_latest.reset_index()[['Term', 'TermDays', 'Value']].sort_values('TermDays').to_string())
            except KeyError:
                print(f"\n{test_currency} data not found for the latest date.")
            except Exception as e:
                 print(f"\nError retrieving {test_currency} data: {e}")
    else:
        print("\nFailed to load or process curve data.")
    logger.info("--- Finished Standalone Execution ---")
</file>

<file path="data_loader.py">
# This file is responsible for loading and preprocessing data from CSV files.
# It includes functions to dynamically identify essential columns (Date, Code, Benchmark)
# based on patterns, handle potential naming variations, parse dates, standardize column names,
# set appropriate data types, and prepare the data in a pandas DataFrame format
# suitable for further analysis and processing within the application.
# It also supports loading a secondary file (e.g., prefixed with 'sp_') for comparison.
# data_loader.py
# This file is responsible for loading and preprocessing data from time-series CSV files (typically prefixed with `ts_`).
# It includes functions to dynamically identify essential columns (Date, Code, Benchmark)
# based on patterns, handle potential naming variations, parse dates (handling 'YYYY-MM-DD' and 'DD/MM/YYYY'),
# standardize column names, set appropriate data types, and prepare the data in a pandas DataFrame format
# suitable for further analysis within the application. It includes robust error handling and logging.
# It now also supports loading and processing a secondary comparison file (e.g., sp_*.csv).
import pandas as pd
import os
import logging
from typing import List, Tuple, Optional
import re # Import regex for pattern matching
from flask import current_app # Import current_app to access config
# Get the logger instance. Assumes Flask app has configured logging.
logger = logging.getLogger(__name__)
# --- Removed logging setup block --- 
# Logging is now handled centrally by the Flask app factory in app.py
# Define constants
# Removed DATA_FOLDER constant - path is now dynamically determined
# Standard internal column names after renaming
STD_DATE_COL = 'Date'
STD_CODE_COL = 'Code'
STD_BENCHMARK_COL = 'Benchmark'
def _find_column(pattern: str, columns: List[str], filename_for_logging: str, col_type: str) -> str:
    """Helper function to find a single column matching a pattern (case-insensitive)."""
    matches = [col for col in columns if re.search(pattern, col, re.IGNORECASE)]
    if len(matches) == 1:
        logger.info(f"Found {col_type} column in '{filename_for_logging}': '{matches[0]}'")
        return matches[0]
    elif len(matches) > 1:
        # Log error before raising
        logger.error(f"Multiple possible {col_type} columns found in '{filename_for_logging}' matching pattern '{pattern}': {matches}. Please ensure unique column names.")
        raise ValueError(f"Multiple possible {col_type} columns found in '{filename_for_logging}' matching pattern '{pattern}': {matches}. Please ensure unique column names.")
    else:
         # Log error before raising
        logger.error(f"No {col_type} column found in '{filename_for_logging}' matching pattern '{pattern}'. Found columns: {columns}")
        raise ValueError(f"No {col_type} column found in '{filename_for_logging}' matching pattern '{pattern}'. Found columns: {columns}")
def _create_empty_dataframe(original_fund_val_col_names: List[str], benchmark_col_present: bool) -> pd.DataFrame:
    """Creates an empty DataFrame with the expected structure."""
    final_benchmark_col_name = STD_BENCHMARK_COL if benchmark_col_present else None
    expected_cols = [STD_DATE_COL, STD_CODE_COL] + original_fund_val_col_names
    if final_benchmark_col_name:
        expected_cols.append(final_benchmark_col_name)
    # Create an empty df with the right index and columns
    empty_index = pd.MultiIndex(levels=[[], []], codes=[[], []], names=[STD_DATE_COL, STD_CODE_COL])
    value_cols = [col for col in expected_cols if col not in [STD_DATE_COL, STD_CODE_COL]]
    return pd.DataFrame(index=empty_index, columns=value_cols)
def _process_single_file(
    filepath: str,
    filename_for_logging: str
) -> Optional[Tuple[pd.DataFrame, List[str], Optional[str]]]:
    """Internal helper to load and process a single CSV file.
    Handles finding columns, parsing dates, renaming, indexing, and type conversion.
    Returns None if the file is not found or critical processing steps fail.
    Returns:
        Optional[Tuple[pd.DataFrame, List[str], Optional[str]]]:
               Processed DataFrame, list of original fund value column names,
               and the standardized benchmark column name if present, otherwise None.
               Returns None if processing fails critically.
    """
    if not os.path.exists(filepath):
        logger.warning(f"File not found, skipping: {filepath}")
        return None # Return None if file doesn't exist
    try:
        # Read only the header first
        header_df = pd.read_csv(filepath, nrows=0, encoding='utf-8', encoding_errors='replace', on_bad_lines='skip')
        original_cols = [col.strip() for col in header_df.columns.tolist()]
        logger.info(f"Processing file: '{filename_for_logging}'. Original columns: {original_cols}")
        # Dynamically find required columns
        date_pattern = r'\b(Position\s*)?Date\b'
        actual_date_col = _find_column(date_pattern, original_cols, filename_for_logging, 'Date')
        code_pattern = r'\b(Fund\s*)?Code\b' # Allow 'Fund Code' or 'Code'
        actual_code_col = _find_column(code_pattern, original_cols, filename_for_logging, 'Code')
        benchmark_col_present = False
        actual_benchmark_col = None
        try:
            benchmark_pattern = r'\b(Benchmark|Bench)\b' # Allow 'Benchmark' or 'Bench'
            actual_benchmark_col = _find_column(benchmark_pattern, original_cols, filename_for_logging, 'Benchmark')
            benchmark_col_present = True
        except ValueError:
            logger.info(f"No Benchmark column found in '{filename_for_logging}' matching pattern. Proceeding without benchmark.")
        # Identify original fund value columns
        excluded_cols_for_funds = {actual_date_col, actual_code_col}
        if benchmark_col_present and actual_benchmark_col:
            excluded_cols_for_funds.add(actual_benchmark_col)
        original_fund_val_col_names = [col for col in original_cols if col not in excluded_cols_for_funds]
        if not original_fund_val_col_names and not benchmark_col_present:
             logger.error(f"No fund value columns and no benchmark column identified in '{filename_for_logging}'. Cannot process.")
             return None # Cannot proceed
        # Read the full CSV
        df = pd.read_csv(filepath, encoding='utf-8', encoding_errors='replace', on_bad_lines='skip', dtype={actual_date_col: str})
        df.columns = df.columns.str.strip()
        # Rename columns
        rename_map = {
            actual_date_col: STD_DATE_COL,
            actual_code_col: STD_CODE_COL
        }
        if benchmark_col_present and actual_benchmark_col:
            rename_map[actual_benchmark_col] = STD_BENCHMARK_COL
        df.rename(columns=rename_map, inplace=True)
        logger.info(f"Renamed columns in '{filename_for_logging}': {list(rename_map.keys())} -> {list(rename_map.values())}")
        # Robust Date Parsing
        date_series = df[STD_DATE_COL]
        parsed_dates = pd.to_datetime(date_series, errors='coerce', dayfirst=None, yearfirst=None) # Let pandas infer
        # Check if all parsing failed
        if parsed_dates.isnull().all() and len(date_series) > 0:
             # Try again with dayfirst=True if initial inference failed
            logger.warning(f"Initial date parsing failed for {filename_for_logging}. Trying with dayfirst=True.")
            parsed_dates = pd.to_datetime(date_series, errors='coerce', dayfirst=True)
            if parsed_dates.isnull().all() and len(date_series) > 0:
                logger.error(f"Could not parse any dates in column '{STD_DATE_COL}' (original: '{actual_date_col}') in file {filename_for_logging} even with dayfirst=True.")
                return None # Cannot proceed without valid dates
        nat_count = parsed_dates.isnull().sum()
        total_count = len(parsed_dates)
        success_count = total_count - nat_count
        logger.info(f"Parsed {success_count}/{total_count} dates in {filename_for_logging}. ({nat_count} resulted in NaT).")
        if nat_count > 0:
             logger.warning(f"{nat_count} date values in '{STD_DATE_COL}' from {filename_for_logging} became NaT.")
        df[STD_DATE_COL] = parsed_dates
        original_row_count = len(df)
        df.dropna(subset=[STD_DATE_COL], inplace=True)
        rows_dropped = original_row_count - len(df)
        if rows_dropped > 0:
            logger.warning(f"Dropped {rows_dropped} rows from {filename_for_logging} due to failed date parsing.")
        # Set Index
        if df.empty:
            logger.warning(f"DataFrame became empty after dropping rows with unparseable dates in {filename_for_logging}.")
            # Return empty structure but indicate success in file processing up to this point
            empty_df = _create_empty_dataframe(original_fund_val_col_names, benchmark_col_present)
            final_bm_col = STD_BENCHMARK_COL if benchmark_col_present else None
            return empty_df, original_fund_val_col_names, final_bm_col
        df.set_index([STD_DATE_COL, STD_CODE_COL], inplace=True)
        # Convert value columns to numeric
        value_cols_to_convert = original_fund_val_col_names[:]
        if benchmark_col_present:
            value_cols_to_convert.append(STD_BENCHMARK_COL)
        valid_cols_for_conversion = [col for col in value_cols_to_convert if col in df.columns]
        if not valid_cols_for_conversion:
             logger.error(f"No valid fund or benchmark value columns found to convert in {filename_for_logging} after processing.")
             # Return partially processed DF but log error
             final_bm_col = STD_BENCHMARK_COL if benchmark_col_present else None
             return df, original_fund_val_col_names, final_bm_col # Return what we have
        df[valid_cols_for_conversion] = df[valid_cols_for_conversion].apply(pd.to_numeric, errors='coerce')
        nan_check_cols = [col for col in valid_cols_for_conversion if col in df.columns]
        if nan_check_cols and df[nan_check_cols].isnull().all().all():
            logger.warning(f"All values in value columns {nan_check_cols} became NaN after conversion in file {filename_for_logging}. Check data types.")
        final_benchmark_col_name = STD_BENCHMARK_COL if benchmark_col_present else None
        logger.info(f"Successfully processed file: '{filename_for_logging}'. Index: {df.index.names}. Columns: {df.columns.tolist()}")
        return df, original_fund_val_col_names, final_benchmark_col_name
    except FileNotFoundError:
        logger.error(f"File not found during processing: {filepath}")
        return None # Handled above, but belt-and-suspenders
    except ValueError as e:
        logger.error(f"Value error processing {filename_for_logging}: {e}")
        return None # Return None on critical errors like missing columns
    except Exception as e:
        logger.exception(f"Unexpected error processing {filename_for_logging}: {e}") # Log full traceback
        return None # Return None on unexpected errors
# Simplified return type: focus on the dataframes and metadata needed downstream
LoadResult = Tuple[
    Optional[pd.DataFrame],      # Primary DataFrame
    Optional[List[str]],         # Primary original value columns
    Optional[str],               # Primary benchmark column name (standardized)
    Optional[pd.DataFrame],      # Secondary DataFrame
    Optional[List[str]],         # Secondary original value columns
    Optional[str]                # Secondary benchmark column name (standardized)
]
def load_and_process_data(
    primary_filename: str,
    secondary_filename: Optional[str] = None,
    data_folder_path: Optional[str] = None # Renamed and made optional
) -> LoadResult:
    """Loads and processes a primary CSV file and optionally a secondary CSV file.
    Retrieves the data folder path from Flask's current_app.config['DATA_FOLDER']
    if data_folder_path is not provided. Assumes execution within a Flask request context
    when data_folder_path is None.
    Uses the internal _process_single_file helper for processing each file.
    Args:
        primary_filename (str): The name of the primary CSV file.
        secondary_filename (Optional[str]): The name of the secondary CSV file. Defaults to None.
        data_folder_path (Optional[str]): Explicit path to the folder containing the data files.
                                           If None, path is retrieved from current_app.config.
    Returns:
        LoadResult: A tuple containing the processed DataFrames and metadata for
                    primary and (optionally) secondary files. Elements corresponding
                    to a file will be None if the file doesn't exist or processing fails,
                    or if the data folder path cannot be determined.
    """
    data_folder: Optional[str] = None
    if data_folder_path is None:
        try:
            # Retrieve the absolute path configured during app initialization
            data_folder = current_app.config['DATA_FOLDER']
            logger.info(f"Using data folder from current_app.config: {data_folder}")
            if not data_folder:
                 logger.error("DATA_FOLDER in current_app.config is not set or empty.")
                 return None, None, None, None, None, None
        except RuntimeError:
            logger.error("Cannot access current_app.config. load_and_process_data must be called within a Flask request context or be provided with an explicit data_folder_path.")
            # Return None for all parts of the tuple if path cannot be determined
            return None, None, None, None, None, None
        except KeyError:
            logger.error("'DATA_FOLDER' key not found in current_app.config. Ensure it is set during app initialization.")
            return None, None, None, None, None, None
    else:
        # Use the explicitly provided path
        data_folder = data_folder_path
        logger.info(f"Using explicitly provided data_folder_path: {data_folder}")
    # Ensure data_folder is not None before proceeding (should be handled above, but belt-and-suspenders)
    if data_folder is None:
         logger.critical("Data folder path could not be determined. Aborting load.")
         return None, None, None, None, None, None
    # --- Process Primary File --- 
    primary_filepath = os.path.join(data_folder, primary_filename)
    logger.info(f"--- Starting data load for primary: {primary_filename} from {primary_filepath} ---")
    primary_result = _process_single_file(primary_filepath, primary_filename)
    df1, cols1, bench1 = (None, None, None)
    if primary_result:
        df1, cols1, bench1 = primary_result
        logger.info(f"Primary file '{primary_filename}' processed. Shape: {df1.shape if df1 is not None else 'N/A'}. Benchmark: {bench1}")
    else:
        logger.warning(f"Processing failed for primary file: {primary_filename}")
    # --- Process Secondary File (if provided) --- 
    df2, cols2, bench2 = (None, None, None)
    if secondary_filename:
        secondary_filepath = os.path.join(data_folder, secondary_filename)
        logger.info(f"--- Starting data load for secondary: {secondary_filename} from {secondary_filepath} ---")
        secondary_result = _process_single_file(secondary_filepath, secondary_filename)
        if secondary_result:
            df2, cols2, bench2 = secondary_result
            logger.info(f"Secondary file '{secondary_filename}' processed. Shape: {df2.shape if df2 is not None else 'N/A'}. Benchmark: {bench2}")
        else:
            logger.warning(f"Processing failed for secondary file: {secondary_filename}")
    return df1, cols1, bench1, df2, cols2, bench2
# --- Standalone Execution / Testing --- 
# Note: If run directly, this block cannot use current_app.config.
# It needs to determine the data folder path independently, potentially using get_data_folder_path from utils.
# Example (requires config.py and utils.py to be importable):
# if __name__ == '__main__':
#     try:
#         from utils import get_data_folder_path
#         # Determine the root path assuming data_loader.py is one level down from the project root
#         script_dir = os.path.dirname(os.path.abspath(__file__))
#         project_root = os.path.dirname(script_dir)
#         # Use the utility function to get the configured path
#         standalone_data_path = get_data_folder_path(app_root_path=project_root)
#         print(f"[Standalone] Determined data path: {standalone_data_path}")
#
#         # Example usage:
#         primary_file = 'ts_NAV_Report_Short.csv' # Replace with your actual test file
#         # secondary_file = 'sp_NAV_Report_Short.csv' # Optional secondary file
#         df1, cols1, bench1, df2, cols2, bench2 = load_and_process_data(
#             primary_filename=primary_file,
#             # secondary_filename=secondary_file,
#             data_folder_path=standalone_data_path # Pass the determined path explicitly
#         )
#
#         if df1 is not None:
#             print(f"\n--- Primary Data ({primary_file}) ---")
#             print(df1.head())
#             print(f"Original Value Columns: {cols1}")
#             print(f"Benchmark Column: {bench1}")
#         else:
#             print(f"\nFailed to load primary data ({primary_file}). Check logs.")
#
#         # if secondary_file and df2 is not None:
#         #     print(f"\n--- Secondary Data ({secondary_file}) ---")
#         #     print(df2.head())
#         #     print(f"Original Value Columns: {cols2}")
#         #     print(f"Benchmark Column: {bench2}")
#         # elif secondary_file:
#         #      print(f"\nFailed to load secondary data ({secondary_file}). Check logs.")
#
#     except ImportError:
#         print("Error: Could not import utils.get_data_folder_path. Ensure utils.py and config.py exist and are accessible.")
#     except Exception as e:
#         print(f"An error occurred during standalone execution: {e}")
</file>

<file path="data_validation.py">
'''
Placeholder module for validating data retrieved from the API.
This module will contain functions to check the structure, data types,
and potentially the content consistency of the DataFrames returned by the
Rex API before they are saved as CSV files.
'''
import pandas as pd
def validate_data(df: pd.DataFrame, filename: str):
    """
    Validates the structure and types of the DataFrame based on filename conventions.
    This is a placeholder function. Implement specific checks based on the
    expected format for different file types (e.g., 'ts_*.csv', 'sec_*.csv').
    Args:
        df (pd.DataFrame): The DataFrame returned by the API call.
        filename (str): The intended filename for the data (e.g., 'ts_Duration.csv').
    Returns:
        tuple[bool, list[str]]: A tuple containing:
            - bool: True if the data is valid, False otherwise.
            - list[str]: A list of validation error messages, empty if valid.
    """
    errors = []
    if df is None or not isinstance(df, pd.DataFrame):
        errors.append("Invalid input: DataFrame is None or not a pandas DataFrame.")
        return False, errors
    if df.empty:
        # It might be valid for some queries to return no data, but flag it for review.
        errors.append("Warning: DataFrame is empty.")
        # Decide if empty is truly invalid or just a warning.
        # For now, let's consider it potentially valid but issue a warning.
        # return False, errors # Uncomment if empty df is strictly invalid
    # Example checks based on filename conventions:
    if filename.startswith('ts_'):
        # Checks for time-series files
        required_cols = ['Date', 'Code'] # Assuming these are standard post-processing names
        if not all(col in df.columns for col in required_cols):
            errors.append(f"Missing required columns for time-series data: Expected {required_cols}, got {list(df.columns)}")
        # Check if 'Date' column is datetime type (or can be coerced)
        # try:
        #     pd.to_datetime(df['Date'])
        # except Exception as e:
        #     errors.append(f"'Date' column cannot be parsed as datetime: {e}")
        # Check if value columns (excluding Date, Code, Benchmark if exists) are numeric
        value_cols = [col for col in df.columns if col not in ['Date', 'Code', 'Benchmark']]
        for col in value_cols:
            if not pd.api.types.is_numeric_dtype(df[col]):
                 errors.append(f"Column '{col}' in time-series data is not numeric.")
    elif filename.startswith('sec_'):
        # Checks for security-level files
        # Example: Check for an ID column (e.g., 'Security ID', 'ISIN')
        # Example: Check if columns intended as dates are parseable
        # Example: Check if value columns are numeric
        pass # Add specific checks here
    elif filename == 'FundList.csv':
        # Example: Check required columns for FundList
        required_cols = ['Fund Code', 'Total Asset Value USD', 'Picked']
        if not all(col in df.columns for col in required_cols):
             errors.append(f"Missing required columns for FundList.csv: Expected {required_cols}, got {list(df.columns)}")
    # --- Add more specific validation rules as needed based on data specs --- 
    is_valid = len(errors) == 0
    return is_valid, errors
# Example Usage (can be run manually for testing):
if __name__ == '__main__':
    # Create dummy dataframes for testing validation logic
    print("Testing validation functions...")
    # Test case 1: Valid time-series data
    valid_ts_data = {
        'Date': pd.to_datetime(['2023-01-01', '2023-01-01', '2023-01-02', '2023-01-02']),
        'Code': ['FUNDA', 'FUNDB', 'FUNDA', 'FUNDB'],
        'Value': [10.1, 20.2, 10.5, 20.8],
        'Benchmark': [10.0, 20.0, 10.4, 20.7]
    }
    valid_ts_df = pd.DataFrame(valid_ts_data)
    is_valid, errors = validate_data(valid_ts_df, 'ts_ExampleMetric.csv')
    print(f"Valid TS Data Test: Valid={is_valid}, Errors={errors}")
    assert is_valid
    # Test case 2: Invalid time-series data (missing column)
    invalid_ts_data = {
        'Date': pd.to_datetime(['2023-01-01']),
        # 'Code': ['FUNDA'], # Missing Code column
        'Value': [10.1]
    }
    invalid_ts_df = pd.DataFrame(invalid_ts_data)
    is_valid, errors = validate_data(invalid_ts_df, 'ts_AnotherMetric.csv')
    print(f"Invalid TS Data Test (Missing Col): Valid={is_valid}, Errors={errors}")
    assert not is_valid
    assert "Missing required columns" in errors[0]
    # Test case 3: Invalid time-series data (non-numeric value)
    invalid_ts_data_type = {
        'Date': pd.to_datetime(['2023-01-01']),
        'Code': ['FUNDA'],
        'Value': ['abc'] # Non-numeric value
    }
    invalid_ts_df_type = pd.DataFrame(invalid_ts_data_type)
    is_valid, errors = validate_data(invalid_ts_df_type, 'ts_BadData.csv')
    print(f"Invalid TS Data Test (Bad Type): Valid={is_valid}, Errors={errors}")
    # Note: This specific check might depend on when type conversion happens.
    # If conversion happens *before* validation, this might pass if 'abc' becomes NaN.
    # The check here assumes the raw data from API might be non-numeric.
    assert not is_valid # Assuming the validation catches non-numeric directly
    assert "not numeric" in errors[0]
    # Test case 4: Empty DataFrame
    empty_df = pd.DataFrame()
    is_valid, errors = validate_data(empty_df, 'ts_EmptyData.csv')
    print(f"Empty DF Test: Valid={is_valid}, Errors={errors}")
    assert is_valid # Currently allows empty with warning
    assert "DataFrame is empty" in errors[0]
    # Test case 5: None DataFrame
    none_df = None
    is_valid, errors = validate_data(none_df, 'ts_NoneData.csv')
    print(f"None DF Test: Valid={is_valid}, Errors={errors}")
    assert not is_valid
    assert "DataFrame is None" in errors[0]
    print("Validation tests completed.")
</file>

<file path="LICENSE">
MIT License

Copyright (c) [2025] [Robert Clark]

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</file>

<file path="metric_calculator.py">
# This file provides functions for calculating various statistical metrics from the preprocessed data.
# Key functionalities include calculating historical statistics (mean, max, min), latest values,
# period-over-period changes, and Z-scores for changes for both benchmark and fund columns.
# It operates on a pandas DataFrame indexed by Date and Fund Code, producing a summary DataFrame
# containing these metrics for each fund, often sorted by the most significant recent changes (Z-scores).
# It now supports calculating metrics for both a primary and an optional secondary DataFrame.
# metric_calculator.py
# This file contains functions for calculating metrics from the processed data.
# Updated to handle primary and optional secondary data sources.
import pandas as pd
import numpy as np
import logging
import os # Needed for logging setup
from typing import List, Dict, Any, Tuple, Optional
# --- Logging Setup ---
# Use the same log file as data_loader
LOG_FILENAME = 'data_processing_errors.log'
LOG_FORMAT = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
# Get the logger for the current module
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
# Prevent adding handlers multiple times (especially if imported by other modules)
if not logger.handlers:
    # Console Handler (INFO and above)
    ch = logging.StreamHandler()
    ch.setLevel(logging.INFO)
    ch_formatter = logging.Formatter(LOG_FORMAT)
    ch.setFormatter(ch_formatter)
    logger.addHandler(ch)
    # File Handler (WARNING and above)
    try:
        # Create log file path relative to this file's location
        log_filepath = os.path.join(os.path.dirname(__file__), '..', LOG_FILENAME)
        fh = logging.FileHandler(log_filepath, mode='a')
        fh.setLevel(logging.WARNING)
        fh_formatter = logging.Formatter(LOG_FORMAT)
        fh.setFormatter(fh_formatter)
        logger.addHandler(fh)
    except Exception as e:
        # Log to stderr if file logging setup fails
        import sys
        print(f"Error setting up file logging for metric_calculator: {e}", file=sys.stderr)
# --- End Logging Setup ---
# Configure logging (can be configured globally elsewhere if part of a larger app)
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
def _calculate_column_stats(
    col_series: pd.Series,
    col_change_series: pd.Series,
    latest_date: pd.Timestamp,
    col_name: str,
    prefix: str = "" # Optional prefix for metric names (e.g., "S&P " )
) -> Dict[str, Any]:
    """Helper function to calculate stats for a single column series.
    Calculates historical mean/max/min, latest value, latest change, and change z-score.
    Handles potential NaN values resulting from calculations or missing data gracefully.
    Args:
        col_series (pd.Series): The historical data for the column.
        col_change_series (pd.Series): The historical changes for the column.
        latest_date (pd.Timestamp): The overall latest date in the dataset.
        col_name (str): The name of the column being processed.
        prefix (str): A prefix to add to the metric names in the output dictionary.
    Returns:
        Dict[str, Any]: A dictionary containing the calculated metrics for this column.
    """
    metrics = {}
    # Calculate base historical stats for the column level
    # Pandas functions like mean, max, min typically handle NaNs by skipping them.
    metrics[f'{prefix}{col_name} Mean'] = col_series.mean()
    metrics[f'{prefix}{col_name} Max'] = col_series.max()
    metrics[f'{prefix}{col_name} Min'] = col_series.min()
    # Calculate stats for the column change
    change_mean = col_change_series.mean()
    change_std = col_change_series.std()
    # Get latest values if data exists for the latest date
    # Check if the latest_date exists in the specific series index
    if latest_date in col_series.index:
        latest_value = col_series.loc[latest_date]
        # Use .get() for change series to handle potential index mismatch (though unlikely if derived correctly)
        latest_change = col_change_series.get(latest_date, np.nan)
        metrics[f'{prefix}{col_name} Latest Value'] = latest_value
        metrics[f'{prefix}{col_name} Change'] = latest_change
        # Calculate Change Z-Score: (latest_change - change_mean) / change_std
        change_z_score = np.nan # Default to NaN
        if pd.notna(latest_change) and pd.notna(change_mean) and pd.notna(change_std) and change_std != 0:
            change_z_score = (latest_change - change_mean) / change_std
        elif change_std == 0 and pd.notna(latest_change) and pd.notna(change_mean):
             # Handle case where std dev is zero (e.g., constant series)
             if latest_change == change_mean:
                 change_z_score = 0.0 # No deviation
             else:
                 change_z_score = np.inf if latest_change > change_mean else -np.inf # Infinite deviation
             logger.debug(f"Standard deviation of change for '{prefix}{col_name}' is zero. Z-score set to {change_z_score}.")
        else:
            # Log if Z-score calculation couldn't be performed due to NaNs
            if not (pd.notna(latest_change) and pd.notna(change_mean) and pd.notna(change_std)):
                 logger.debug(f"Cannot calculate Z-score for '{prefix}{col_name}' due to NaN inputs (latest_change={latest_change}, change_mean={change_mean}, change_std={change_std})")
        metrics[f'{prefix}{col_name} Change Z-Score'] = change_z_score
    else:
        # Data for the latest date is missing for this specific column/fund
        logger.debug(f"Latest date {latest_date} not found for column '{prefix}{col_name}'. Setting latest metrics to NaN.")
        metrics[f'{prefix}{col_name} Latest Value'] = np.nan
        metrics[f'{prefix}{col_name} Change'] = np.nan
        metrics[f'{prefix}{col_name} Change Z-Score'] = np.nan
    return metrics
def _process_dataframe_metrics(
    df: pd.DataFrame,
    fund_codes: pd.Index,
    fund_cols: List[str],
    benchmark_col: Optional[str],
    latest_date: pd.Timestamp,
    metric_prefix: str = ""
) -> Tuple[List[Dict[str, Any]], Dict[str, float]]:
    """Processes a single DataFrame (primary or secondary) to calculate metrics.
    Args:
        df (pd.DataFrame): The DataFrame to process (already sorted by date index).
        fund_codes (pd.Index): Unique fund codes from the combined data.
        fund_cols (List[str]): List of original fund value column names for this df.
        benchmark_col (Optional[str]): Standardized benchmark column name for this df, if present.
        latest_date (pd.Timestamp): The latest date across combined data.
        metric_prefix (str): Prefix to add to metric names (e.g., "S&P ").
    Returns:
        Tuple[List[Dict[str, Any]], Dict[str, float]]:
            - List of metric dictionaries, one per fund.
            - Dictionary mapping fund code to its max absolute change Z-score for sorting.
    """
    if df is None or df.empty:
        logger.warning(f"Input DataFrame for prefix '{metric_prefix}' is None or empty. Returning empty results.")
        return [], {}
    # Determine which columns to actually process based on presence in df
    cols_to_process = []
    output_col_name_map = {} # Map processed col name to output name (original/std)
    if benchmark_col and benchmark_col in df.columns:
        cols_to_process.append(benchmark_col)
        output_col_name_map[benchmark_col] = benchmark_col
    elif benchmark_col:
        logger.warning(f"Specified {metric_prefix}benchmark column '{benchmark_col}' not found in DataFrame columns: {df.columns.tolist()}")
    for f_col in fund_cols:
        if f_col in df.columns:
            cols_to_process.append(f_col)
            output_col_name_map[f_col] = f_col
        else:
            logger.warning(f"Specified {metric_prefix}fund column '{f_col}' not found in DataFrame columns: {df.columns.tolist()}")
    if not cols_to_process:
        logger.error(f"No valid columns (benchmark or funds) found in the {metric_prefix}DataFrame to calculate metrics for.")
        return [], {}
    logger.info(f"Calculating {metric_prefix}metrics for columns: {cols_to_process}")
    fund_metrics_list = []
    max_abs_z_scores: Dict[str, float] = {}
    for fund_code in fund_codes:
        fund_specific_metrics: Dict[str, Any] = {'Fund Code': fund_code} # Initialize with Fund Code
        current_fund_max_abs_z: float = -1.0
        try:
            # Check if fund exists in this specific dataframe
            if fund_code not in df.index.get_level_values(1):
                logger.debug(f"Fund code '{fund_code}' not found in {metric_prefix}DataFrame. Adding empty metrics.")
                # Add NaN placeholders for all expected metrics for this source
                for col_name_proc in cols_to_process:
                    output_name = output_col_name_map[col_name_proc]
                    fund_specific_metrics[f'{metric_prefix}{output_name} Mean'] = np.nan
                    fund_specific_metrics[f'{metric_prefix}{output_name} Max'] = np.nan
                    fund_specific_metrics[f'{metric_prefix}{output_name} Min'] = np.nan
                    fund_specific_metrics[f'{metric_prefix}{output_name} Latest Value'] = np.nan
                    fund_specific_metrics[f'{metric_prefix}{output_name} Change'] = np.nan
                    fund_specific_metrics[f'{metric_prefix}{output_name} Change Z-Score'] = np.nan
                fund_metrics_list.append(fund_specific_metrics)
                max_abs_z_scores[fund_code] = np.nan # No Z-score if fund not present
                continue # Move to the next fund code
            # Extract data for the fund
            fund_data_hist = df.loc[(slice(None), fund_code), cols_to_process]
            fund_data_hist = fund_data_hist.reset_index(level=1, drop=True).sort_index()
            for col_name in cols_to_process:
                if col_name not in fund_data_hist.columns:
                    logger.warning(f"Column '{col_name}' unexpectedly not found for fund '{fund_code}' in {metric_prefix}DF. Skipping metrics.")
                    continue
                col_hist = fund_data_hist[col_name]
                col_change_hist = pd.Series(index=col_hist.index, dtype=np.float64)
                if not col_hist.dropna().empty and len(col_hist.dropna()) > 1:
                    col_change_hist = col_hist.diff()
                else:
                    logger.debug(f"Cannot calculate difference for {metric_prefix}column '{col_name}', fund '{fund_code}' due to insufficient data.")
                # Calculate stats for this specific column
                output_name = output_col_name_map[col_name]
                col_stats = _calculate_column_stats(col_hist, col_change_hist, latest_date, output_name, prefix=metric_prefix)
                fund_specific_metrics.update(col_stats)
                # Update the fund's max absolute Z-score *for this source*
                col_z_score = col_stats.get(f'{metric_prefix}{output_name} Change Z-Score', np.nan)
                compare_z_score = col_z_score
                if np.isinf(compare_z_score):
                    compare_z_score = 1e9 * np.sign(compare_z_score)
                if pd.notna(compare_z_score):
                    current_fund_max_abs_z = max(current_fund_max_abs_z, abs(compare_z_score))
            fund_metrics_list.append(fund_specific_metrics)
            max_abs_z_scores[fund_code] = current_fund_max_abs_z if current_fund_max_abs_z >= 0 else np.nan
        except Exception as e:
            logger.error(f"Error processing {metric_prefix}metrics for fund code '{fund_code}': {e}", exc_info=True)
            # Add placeholder with NaNs if error occurs mid-fund processing
            if 'Fund Code' not in fund_specific_metrics: # Ensure Fund Code is there
                fund_specific_metrics['Fund Code'] = fund_code
            # Add NaN placeholders for potentially missing metrics
            for col_name_proc in cols_to_process:
                output_name = output_col_name_map[col_name_proc]
                if f'{metric_prefix}{output_name} Mean' not in fund_specific_metrics: fund_specific_metrics[f'{metric_prefix}{output_name} Mean'] = np.nan
                if f'{metric_prefix}{output_name} Max' not in fund_specific_metrics: fund_specific_metrics[f'{metric_prefix}{output_name} Max'] = np.nan
                # ... (add for all metrics) ...
                if f'{metric_prefix}{output_name} Change Z-Score' not in fund_specific_metrics: fund_specific_metrics[f'{metric_prefix}{output_name} Change Z-Score'] = np.nan
            fund_metrics_list.append(fund_specific_metrics)
            max_abs_z_scores[fund_code] = np.nan # Mark as NaN for sorting if error occurred
    return fund_metrics_list, max_abs_z_scores
def calculate_latest_metrics(
    primary_df: Optional[pd.DataFrame],
    primary_fund_cols: Optional[List[str]],
    primary_benchmark_col: Optional[str],
    secondary_df: Optional[pd.DataFrame] = None,
    secondary_fund_cols: Optional[List[str]] = None,
    secondary_benchmark_col: Optional[str] = None,
    secondary_prefix: str = "S&P " # Prefix for secondary metrics
) -> pd.DataFrame:
    """Calculates latest metrics for primary and optional secondary data.
    Merges metrics from both sources based on Fund Code.
    Sorts the final DataFrame by the maximum absolute Z-score from the *primary* source.
    Args:
        primary_df (Optional[pd.DataFrame]): Primary processed DataFrame.
        primary_fund_cols (Optional[List[str]]): List of primary fund value column names.
        primary_benchmark_col (Optional[str]): Standardized primary benchmark column name.
        secondary_df (Optional[pd.DataFrame]): Secondary processed DataFrame. Defaults to None.
        secondary_fund_cols (Optional[List[str]]): List of secondary fund value column names. Defaults to None.
        secondary_benchmark_col (Optional[str]): Standardized secondary benchmark column name. Defaults to None.
        secondary_prefix (str): Prefix for secondary metric columns. Defaults to "S&P ".
    Returns:
        pd.DataFrame: Combined metrics indexed by Fund Code, sorted by primary max abs Z-score.
                      Returns an empty DataFrame if primary data is missing or processing fails critically.
    """
    if primary_df is None or primary_df.empty or primary_fund_cols is None:
        logger.warning("Primary DataFrame or fund columns are missing. Cannot calculate metrics.")
        return pd.DataFrame()
    if primary_df.index.nlevels != 2:
        logger.error("Primary DataFrame must have a MultiIndex with 2 levels (Date, Fund Code).")
        return pd.DataFrame()
    # Combine fund codes and find the overall latest date
    all_dfs = [df for df in [primary_df, secondary_df] if df is not None and not df.empty]
    if not all_dfs:
        logger.warning("No valid DataFrames provided. Cannot calculate metrics.")
        return pd.DataFrame()
    try:
        combined_index = pd.concat(all_dfs).index
        latest_date = combined_index.get_level_values(0).max()
        fund_codes = combined_index.get_level_values(1).unique()
        # Ensure DataFrames are sorted by date index for diff calculation
        primary_df_sorted = primary_df.sort_index(level=0)
        secondary_df_sorted = secondary_df.sort_index(level=0) if secondary_df is not None else None
    except Exception as e:
        logger.error(f"Error preparing combined data for metric calculation: {e}", exc_info=True)
        return pd.DataFrame()
    # --- Calculate Metrics for Primary Data --- #
    primary_metrics_list, primary_max_abs_z = _process_dataframe_metrics(
        primary_df_sorted,
        fund_codes, # Use combined fund codes
        primary_fund_cols,
        primary_benchmark_col,
        latest_date,
        metric_prefix="" # No prefix for primary
    )
    # --- Calculate Metrics for Secondary Data (if present) --- #
    secondary_metrics_list = []
    if secondary_df_sorted is not None and secondary_fund_cols is not None:
        logger.info(f"Processing secondary data with prefix: '{secondary_prefix}'")
        secondary_metrics_list, _ = _process_dataframe_metrics(
            secondary_df_sorted,
            fund_codes, # Use combined fund codes
            secondary_fund_cols,
            secondary_benchmark_col,
            latest_date,
            metric_prefix=secondary_prefix
        )
    else:
        logger.info("No valid secondary data provided or fund columns missing, skipping secondary metrics.")
    # --- Combine Metrics --- #
    if not primary_metrics_list:
        logger.warning("Primary metric calculation resulted in empty list. Returning empty DataFrame.")
        return pd.DataFrame()
    # Convert lists of dicts to DataFrames
    primary_metrics_df = pd.DataFrame(primary_metrics_list).set_index('Fund Code')
    if secondary_metrics_list:
        secondary_metrics_df = pd.DataFrame(secondary_metrics_list).set_index('Fund Code')
        # Merge based on Fund Code index, keeping all funds (outer merge)
        combined_metrics_df = primary_metrics_df.merge(
            secondary_metrics_df, left_index=True, right_index=True, how='outer'
        )
    else:
        combined_metrics_df = primary_metrics_df
    # --- Sort Results --- #
    # Add the primary max abs Z-score as a temporary column for sorting
    # Use .get() on the dictionary to handle funds potentially missing from primary results
    combined_metrics_df['_sort_z'] = combined_metrics_df.index.map(lambda fc: primary_max_abs_z.get(fc, np.nan))
    # Sort by the temporary Z-score column (descending), put NaNs last
    combined_metrics_df_sorted = combined_metrics_df.sort_values(by='_sort_z', ascending=False, na_position='last')
    # Drop the temporary sorting column
    combined_metrics_df_sorted = combined_metrics_df_sorted.drop(columns=['_sort_z'])
    logger.info(f"Successfully calculated and combined metrics. Final shape: {combined_metrics_df_sorted.shape}")
    return combined_metrics_df_sorted
</file>

<file path="process_data.py">
# This script serves as a pre-processing step for specific CSV files within the configured data directory.
# It targets files prefixed with 'pre_', reads them, and performs data aggregation and cleaning.
# The core logic involves grouping rows based on identical values across most columns, excluding 'Funds' and 'Security Name'.
# For rows sharing the same 'Security Name' but differing in other data points, the 'Security Name' is suffixed
# (e.g., _1, _2) to ensure uniqueness. The 'Funds' associated with identical data rows are aggregated
# into a single list-like string representation (e.g., '[FUND1,FUND2]').
# The processed data is then saved to a new CSV file prefixed with 'new_' in the same data directory.
# It also processes a weight file (`w_Funds.csv`).
# The data directory is determined dynamically using `utils.get_data_folder_path`.
# process_data.py
# This script processes CSV files in the 'Data' directory that start with 'pre_'.
# It merges rows based on identical values in all columns except 'Funds'.
# Duplicated 'Security Name' entries with differing data are suffixed (_1, _2, etc.).
# The aggregated 'Funds' are stored as a list in the output file.
import os
import pandas as pd
import logging
# Add datetime for date parsing and sorting
from datetime import datetime
import io
# Import the new weight processing function
from weight_processing import process_weight_file
# Import the path utility
from utils import get_data_folder_path
# Get the logger instance. Assumes logging is configured elsewhere (e.g., by Flask app or calling script).
logger = logging.getLogger(__name__)
# --- Removed logging setup block --- 
# Logging is now handled centrally by the Flask app factory in app.py or by the script runner.
# Removed DATES_FILE_PATH constant - path is now determined dynamically in main()
def read_and_sort_dates(dates_file_path):
    """
    Reads dates from a CSV file, sorts them, and returns them as a list of strings.
    Args:
        dates_file_path (str): Absolute path to the CSV file containing dates.
    Returns:
        list[str] | None: A sorted list of date strings (YYYY-MM-DD) or None if an error occurs.
    """
    if not dates_file_path:
        logger.error("No dates_file_path provided to read_and_sort_dates.")
        return None
    if not os.path.exists(dates_file_path):
        logger.error(f"Dates file not found at {dates_file_path}")
        return None
    try:
        dates_df = pd.read_csv(dates_file_path, parse_dates=[0]) # Assume date is the first column
        # Handle potential parsing errors if the column isn't purely dates
        if dates_df.iloc[:, 0].isnull().any():
             logger.warning(f"Warning: Some values in {dates_file_path} could not be parsed as dates.")
             # Attempt to drop NaT values and proceed
             dates_df = dates_df.dropna(subset=[dates_df.columns[0]])
             if dates_df.empty:
                 logger.error(f"Error: No valid dates found in {dates_file_path} after handling parsing issues.")
                 return None
        # Sort dates chronologically
        sorted_dates = dates_df.iloc[:, 0].sort_values()
        # Format dates as 'YYYY-MM-DD' strings for column headers
        date_strings = sorted_dates.dt.strftime('%Y-%m-%d').tolist()
        logger.info(f"Successfully read and sorted {len(date_strings)} dates from {dates_file_path}.")
        # --- Deduplicate the date list while preserving order --- 
        unique_date_strings = []
        seen_dates = set()
        duplicates_found = False
        for date_str in date_strings:
            if date_str not in seen_dates:
                unique_date_strings.append(date_str)
                seen_dates.add(date_str)
            else:
                duplicates_found = True
        if duplicates_found:
            logger.warning(f"Duplicate dates found in {dates_file_path}. Using unique sorted dates: {len(unique_date_strings)} unique dates.")
        # --- End Deduplication ---
        return unique_date_strings # Return the deduplicated list
    except FileNotFoundError:
        # This case should ideally be caught by the initial os.path.exists check, but included for robustness
        logger.error(f"Error: Dates file not found at {dates_file_path}")
        return None
    except pd.errors.EmptyDataError:
        logger.error(f"Error: Dates file is empty - {dates_file_path}")
        return None
    except IndexError:
        logger.error(f"Error: Dates file {dates_file_path} seems to have no columns.")
        return None
    except Exception as e:
        logger.error(f"An unexpected error occurred reading dates from {dates_file_path}: {e}", exc_info=True)
        return None
# Modify process_csv_file signature to accept date_columns
def process_csv_file(input_path, output_path, date_columns):
    """
    Reads a 'pre_' CSV file, potentially replaces placeholder columns with dates,
    processes it according to the rules, and writes the result to a 'new_' CSV file.
    Args:
        input_path (str): Path to the input CSV file (e.g., 'Data/pre_sec_duration.csv').
        output_path (str): Path to the output CSV file (e.g., 'Data/new_sec_duration.csv').
        date_columns (list[str] | None): Sorted list of date strings for headers, or None if dates couldn't be read.
    """
    # If date_columns is None (due to error reading dates.csv), log and skip processing files needing date replacement.
    # We'll handle the actual replacement logic further down.
    if date_columns is None:
         logger.warning(f"Skipping {input_path} because date information is unavailable (check logs for errors reading dates.csv).")
         # A file might still be processable if its columns are *already* correct dates,
         # but the current logic requires date_columns for the check/replacement.
         # To proceed without dates.csv, the logic would need significant changes.
         return # Skip processing this file if dates aren't loaded.
    try:
        # Read the input CSV - add robustness
        # Skip bad lines, handle encoding errors
        df = pd.read_csv(input_path, on_bad_lines='skip', encoding='utf-8', encoding_errors='replace')
        logger.info(f"Processing file: {input_path}")
        # Log DataFrame info right after reading (DEBUG level)
        if logger.isEnabledFor(logging.DEBUG):
            logger.debug(f"DataFrame info after read for {input_path}:")
            buf = io.StringIO()
            df.info(verbose=True, buf=buf)
            logger.debug(buf.getvalue())
        if df.empty:
            logger.warning(f"Input file {input_path} is empty or contains only invalid lines. Skipping processing.")
            return
        # --- Column Header Replacement & Validation Logic ---
        original_cols = df.columns.tolist()
        # Define core columns, allowing for 'Fund' or 'Funds'
        fund_col_name = None
        if 'Funds' in original_cols:
            fund_col_name = 'Funds'
        elif 'Fund' in original_cols:
            fund_col_name = 'Fund'
            logger.info(f"Found 'Fund' column in {input_path}. Will rename to 'Funds' for processing.")
            # Rename the column IN PLACE for subsequent operations
            df.rename(columns={'Fund': 'Funds'}, inplace=True)
        else:
            logger.error(f"Skipping {input_path}: Missing required fund column (neither 'Funds' nor 'Fund' found). Found columns: {original_cols}")
            return
        # Now define required cols using the standardized 'Funds' name
        required_cols = ['Funds', 'Security Name']
        # Check for 'Security Name' (already checked for fund column)
        if 'Security Name' not in original_cols:
            logger.error(f"Skipping {input_path}: Missing required column 'Security Name'. Found columns: {original_cols}")
            return
        logger.debug(f"Required columns {required_cols} confirmed present (or renamed) in {input_path}.")
        # Find the index after the last required column to start searching for placeholders
        # Use the potentially renamed df.columns
        current_df_cols = df.columns.tolist()
        last_required_idx = -1
        for req_col in required_cols:
            try:
                last_required_idx = max(last_required_idx, current_df_cols.index(req_col))
            except ValueError: # Should not happen due to checks above, but safeguard
                 logger.error(f"Required column '{req_col}' unexpectedly not found after initial check/rename in {input_path}. Skipping.")
                 return
        candidate_start_index = last_required_idx + 1
        # Use current_df_cols which includes the renamed column if applicable
        candidate_cols = current_df_cols[candidate_start_index:]
        # Decide on the final columns to use for processing
        current_cols = current_df_cols # Default to current (potentially renamed) columns
        # --- Enhanced Placeholder Detection ---
        # Detect sequences like 'Col', 'Col.1', 'Col.2', ... (Pandas default)
        # AND sequences like 'Base', 'Base', 'Base', ... (Target for date replacement)
        potential_placeholder_base_patternA = None # For Base, Base.1, ...
        detected_sequence_patternA = []
        start_index_patternA = -1
        potential_placeholder_base_patternB = None # For Base, Base, Base, ...
        detected_sequence_patternB = []
        start_index_patternB = -1
        is_patternB_dominant = False # Flag if Pattern B is found and should trigger date replacement
        if not candidate_cols:
            logger.warning(f"File {input_path} has no columns after required columns {required_cols}. Cannot check for date placeholders.")
        else:
            # Check for Pattern B first: 'Base', 'Base', 'Base', ...
            first_candidate = candidate_cols[0]
            # Check if ALL candidate columns are identical to the first one
            if all(col == first_candidate for col in candidate_cols):
                potential_placeholder_base_patternB = first_candidate
                detected_sequence_patternB = candidate_cols
                start_index_patternB = 0 # Starts at the beginning of candidates
                logger.debug(f"Detected Pattern B: Repeated column name '{potential_placeholder_base_patternB}' for all {len(detected_sequence_patternB)} candidate columns.")
                # If we find Pattern B covering *all* candidates, we prioritize it for date replacement check
                is_patternB_dominant = True
            else:
                 logger.info(f"Candidate columns are not all identical (Pattern B check failed). First candidate: '{first_candidate}'. Candidates: {candidate_cols[:5]}...")
                 # If Pattern B check fails, proceed to check for Pattern A ('Base', 'Base.1', ...)
                 # Iterate through candidate columns to find the *start* of the sequence 'Base', 'Base.1', ...
                 found_sequence_A = False
                 for start_idx in range(len(candidate_cols)):
                     current_potential_base = candidate_cols[start_idx]
                     # Check if it's a potential base name (no '.' suffix)
                     if '.' not in current_potential_base:
                         logger.debug(f"Checking for Pattern A starting with '{current_potential_base}' at index {start_idx} in candidate columns.")
                         temp_sequence = [current_potential_base]
                         # Check subsequent columns for the pattern 'base.1', 'base.2', etc.
                         for i in range(1, len(candidate_cols) - start_idx):
                             expected_col = f"{current_potential_base}.{i}"
                             actual_col_index = start_idx + i
                             if candidate_cols[actual_col_index] == expected_col:
                                 temp_sequence.append(candidate_cols[actual_col_index])
                             else:
                                 logger.debug(f"Pattern A sequence broken at index {actual_col_index}. Expected '{expected_col}', found '{candidate_cols[actual_col_index]}'.")
                                 break # Stop checking for this base
                         if len(temp_sequence) > 1: # Found Base, Base.1 at minimum
                             potential_placeholder_base_patternA = current_potential_base
                             detected_sequence_patternA = temp_sequence
                             start_index_patternA = start_idx
                             logger.debug(f"Found Pattern A sequence starting with '{potential_placeholder_base_patternA}' at candidate index {start_index_patternA} with length {len(detected_sequence_patternA)}.")
                             found_sequence_A = True
                             break # Exit the outer loop for Pattern A search
                         else:
                             logger.debug(f"Only base '{current_potential_base}' found or Pattern A sequence too short. Continuing search.")
                             # Continue loop to check next candidate as potential base
                     else:
                         logger.debug(f"Column '{current_potential_base}' at index {start_idx} has '.' suffix, skipping as potential Pattern A base.")
                 if not found_sequence_A:
                     logger.info(f"No Pattern A sequence ('Base', 'Base.1', ...) found in candidate columns of {input_path}.")
        # --- Date Replacement Logic using Detected Patterns ---
        if date_columns is None:
            logger.warning(f"Date information from {dates_file_path} is unavailable. Cannot check or replace headers in {input_path}. Processing with original headers: {original_cols}")
        # --- Prioritize Pattern B for Date Replacement ---
        elif is_patternB_dominant:
             placeholder_count_B = len(detected_sequence_patternB)
             original_placeholder_start_index_B = candidate_start_index + start_index_patternB # Should be last_required_idx + 1
             logger.info(f"Processing based on detected Pattern B ('{potential_placeholder_base_patternB}' repeated {placeholder_count_B} times), starting at original index {original_placeholder_start_index_B}.")
             if len(date_columns) == placeholder_count_B:
                 logger.info(f"Replacing {placeholder_count_B} repeated '{potential_placeholder_base_patternB}' columns with dates.")
                 # Use current_cols here to respect potential prior rename ('Fund' -> 'Funds')
                 cols_before = current_cols[:original_placeholder_start_index_B]
                 cols_after = current_cols[original_placeholder_start_index_B + placeholder_count_B:]
                 new_columns = cols_before + date_columns + cols_after
                 if len(new_columns) != len(current_cols): # Compare against current_cols length
                     logger.error(f"Internal error (Pattern B): Column count mismatch after constructing new columns ({len(new_columns)} vs {len(current_cols)}). Reverting to original headers.")
                     # Revert logic might need refinement, but for now, keep current_cols as is.
                     # current_cols = original_cols # Reverting might lose the 'Fund'->'Funds' rename
                 else:
                     df.columns = new_columns
                     current_cols = new_columns
                     logger.info(f"Columns after Pattern B date replacement: {current_cols}")
             else:
                 logger.warning(f"Count mismatch for Pattern B in {input_path}: Found {placeholder_count_B} repeated '{potential_placeholder_base_patternB}' columns, but expected {len(date_columns)} dates. Skipping date replacement. Processing with original headers.")
                 # current_cols remains potentially renamed cols
        # --- Handle Pattern A or No Pattern ---
        # No Pattern B found, or it didn't match date count. Check Pattern A or if columns already match dates.
        else:
             if potential_placeholder_base_patternA:
                 # Pattern A ('Base', 'Base.1', ...) was found.
                 placeholder_count_A = len(detected_sequence_patternA)
                 original_placeholder_start_index_A = candidate_start_index + start_index_patternA
                 logger.debug(f"Detected Pattern A sequence based on '{potential_placeholder_base_patternA}' (length {placeholder_count_A}) starting at original index {original_placeholder_start_index_A}.")
                 # --- Attempt Date Replacement for Pattern A if lengths match ---
                 if len(date_columns) == placeholder_count_A:
                     logger.info(f"Replacing {placeholder_count_A} Pattern A columns ('{potential_placeholder_base_patternA}', '{potential_placeholder_base_patternA}.1', ...) with dates.")
                     # Use current_cols here to respect potential prior rename ('Fund' -> 'Funds')
                     cols_before = current_cols[:original_placeholder_start_index_A]
                     cols_after = current_cols[original_placeholder_start_index_A + placeholder_count_A:]
                     new_columns = cols_before + date_columns + cols_after
                     if len(new_columns) != len(current_cols): # Compare against current_cols length
                         logger.error(f"Internal error (Pattern A): Column count mismatch after constructing new columns ({len(new_columns)} vs {len(current_cols)}). Reverting to original headers.")
                         # Keep current_cols as is to avoid losing potential rename
                         # current_cols = original_cols
                     else:
                         df.columns = new_columns
                         current_cols = new_columns
                         logger.info(f"Columns after Pattern A date replacement using {len(date_columns)} dates for {input_path}.")
                 else:
                     # Lengths don't match, log warning and proceed with original (Pattern A) headers
                     logger.warning(f"Count mismatch for Pattern A in {input_path}: Found {placeholder_count_A} columns in sequence ('{potential_placeholder_base_patternA}', '{potential_placeholder_base_patternA}.1', ...), but expected {len(date_columns)} dates. Skipping date replacement. Processing with original headers.")
                     # current_cols remains potentially renamed cols
                 # --- End Date Replacement Logic for Pattern A ---
             elif candidate_cols == date_columns:
                 # No patterns found, but candidates already match dates
                 logger.debug(f"Columns in {input_path} (after required ones) already match the expected dates. No replacement needed.")
        # --- End Column Header Replacement Logic ---
        # Identify columns to check for identity (all except Funds and Security Name) using the CURRENT columns
        # These might be the original placeholders or the replaced dates.
        # Crucially, this now correctly includes any original static columns that were *not* replaced.
        id_cols = [col for col in current_cols if col not in required_cols]
        processed_rows = []
        # Convert 'Security Name' and 'Funds' to string first to handle potential non-string types causing issues later
        # Use 'Funds' as it has been standardized by rename operation if necessary
        df['Security Name'] = df['Security Name'].astype(str)
        df['Funds'] = df['Funds'].astype(str)
        # Group by the primary identifier 'Security Name'
        # Convert 'Security Name' to string first to handle potential non-string types causing groupby issues
        # df['Security Name'] = df['Security Name'].astype(str) # Already done above
        # Ensure 'Funds' is also string for consistent processing later
        # df['Funds'] = df['Funds'].astype(str) # Already done above
        # Use the potentially renamed DataFrame for grouping
        grouped_by_sec = df.groupby('Security Name', sort=False, dropna=False)
        for sec_name, sec_group in grouped_by_sec:
            # Within each security group, further group by all other identifying columns (which might now be dates)
            # This separates rows where the same Security Name has different associated data
            distinct_versions = []
            if id_cols: # Only subgroup if there are other identifying columns
                try:
                    # dropna=False treats NaNs in id_cols as equal for grouping
                    sub_grouped = sec_group.groupby(id_cols, dropna=False, sort=False)
                    distinct_versions = [group for _, group in sub_grouped]
                except KeyError as e:
                    logger.error(f"KeyError during sub-grouping for Security Name '{sec_name}' in {input_path}. Column: {e}. Grouping columns: {id_cols}. Skipping this security.", exc_info=True)
                    continue # Skip this security name if subgrouping fails
                except Exception as e:
                     logger.error(f"Unexpected error during sub-grouping for Security Name '{sec_name}' in {input_path}: {e}. Grouping columns: {id_cols}. Skipping this security.", exc_info=True)
                     continue
            else:
                # If only Security Name and Funds exist (after potential date column issues), treat the whole sec_group as one version
                 distinct_versions = [sec_group]
            num_versions = len(distinct_versions)
            # Iterate through each distinct version found for the current Security Name
            for i, current_version_df in enumerate(distinct_versions):
                if current_version_df.empty:
                    continue # Should not happen, but safeguard
                # Aggregate the unique 'Funds' for this specific version
                # Handle potential NaN values in Funds column before aggregation
                unique_funds = current_version_df['Funds'].dropna().unique()
                # Convert funds to string before joining
                funds_list = sorted([str(f) for f in unique_funds])
                # Take the first row of this version as the template for the output row
                # Use .iloc[0] safely as we checked current_version_df is not empty
                new_row_series = current_version_df.iloc[0].copy()
                # Assign the aggregated funds as a string formatted like a list: "[FUND1,FUND2,...]"
                new_row_series['Funds'] = f"[{','.join(funds_list)}]"
                # If there was more than one distinct version for this Security Name, suffix the name
                if num_versions > 1:
                    # Ensure sec_name is a string before formatting
                    new_row_series['Security Name'] = f"{str(sec_name)}_{i+1}"
                # Else: keep the original Security Name (already stringified and set in new_row_series)
                # Append the processed row (as a dictionary) to our results list
                processed_rows.append(new_row_series.to_dict())
        if not processed_rows:
            logger.warning(f"No data rows processed for {input_path}. Output file will not be created.")
            # Changed behavior: Do not create an empty output file if no rows are processed.
            return
            # Create an empty DataFrame with original columns if no rows processed
            # output_df = pd.DataFrame(columns=original_cols)
        else:
             # Create the final DataFrame from the list of processed rows
            output_df = pd.DataFrame(processed_rows)
             # Ensure the column order reflects the potentially updated columns (current_cols)
             # Filter current_cols to only those present in output_df to avoid KeyErrors if a column was unexpectedly dropped
            final_cols = [col for col in current_cols if col in output_df.columns]
            output_df = output_df[final_cols]
        # Fill NaN values with 0 before saving
        output_df = output_df.fillna(0)
        # Log DataFrame info just before saving (DEBUG level)
        if logger.isEnabledFor(logging.DEBUG):
            logger.debug(f"Output DataFrame info before save for {output_path} (after NaN fill):")
            buf = io.StringIO()
            output_df.info(verbose=True, buf=buf)
            logger.debug(buf.getvalue())
        # Write the processed data to the new CSV file
        # The Funds column now contains comma-separated strings, which pandas will quote if necessary.
        output_df.to_csv(output_path, index=False, encoding='utf-8')
        logger.info(f"Successfully created: {output_path} with {len(output_df)} rows.")
    except FileNotFoundError:
        logger.error(f"Error: Input file not found - {input_path}")
    except pd.errors.EmptyDataError:
         logger.warning(f"Input file is empty or contains only header - {input_path}. Skipping.")
    except pd.errors.ParserError as pe:
        logger.error(f"Error parsing CSV file {input_path}: {pe}. Check file format and integrity.", exc_info=True)
    except Exception as e:
        logger.error(f"An unexpected error occurred processing {input_path}: {e}", exc_info=True)
def main():
    """Main execution function to find and process 'pre_' files and the weight file."""
    logger.info("--- Starting pre-processing script --- ")
    # Determine the root path for this script
    script_dir = os.path.dirname(os.path.abspath(__file__))
    # Use the utility to get the configured absolute data folder path
    # Pass the script's directory as the base for resolving relative paths if necessary
    input_dir = get_data_folder_path(app_root_path=script_dir)
    logger.info(f"Using data directory: {input_dir}")
    if not os.path.isdir(input_dir):
        logger.error(f"Data directory not found or is not a directory: {input_dir}. Cannot proceed.")
        return
    # Construct absolute path for dates.csv
    dates_file_path = os.path.join(input_dir, 'dates.csv')
    # Read and prepare date columns
    date_columns = read_and_sort_dates(dates_file_path)
    if date_columns is None:
        logger.warning("Could not read or process dates.csv. Files requiring date replacement might be skipped or processed incorrectly.")
        # Continue processing other files that might not need date replacement, but log the warning.
    # Find files starting with 'pre_' in the determined data directory
    processed_count = 0
    skipped_count = 0
    for filename in os.listdir(input_dir):
        if filename.startswith('pre_') and filename.endswith('.csv'):
            input_path = os.path.join(input_dir, filename)
            # Create the output filename by replacing 'pre_' with 'new_'
            output_filename = filename.replace('pre_', 'new_', 1)
            output_path = os.path.join(input_dir, output_filename)
            logger.info(f"Found file to process: {input_path} -> {output_path}")
            try:
                process_csv_file(input_path, output_path, date_columns)
                processed_count += 1
            except Exception as e:
                logger.error(f"Error processing file {input_path}: {e}", exc_info=True)
                skipped_count += 1
    logger.info(f"Finished processing 'pre_' files. Processed: {processed_count}, Skipped due to errors: {skipped_count}")
    # --- Process the weight file --- 
    weight_input_filename = 'w_Funds.csv'
    weight_output_filename = 'w_Funds_Processed.csv' # Define an output name
    weight_input_path = os.path.join(input_dir, weight_input_filename)
    weight_output_path = os.path.join(input_dir, weight_output_filename)
    if os.path.exists(weight_input_path):
        logger.info(f"Processing weight file: {weight_input_path} -> {weight_output_path}")
        try:
            # Pass the absolute input and output paths
            process_weight_file(weight_input_path, weight_output_path)
        except Exception as e:
            logger.error(f"Error processing weight file {weight_input_path}: {e}", exc_info=True)
    else:
        logger.warning(f"Weight file not found at {weight_input_path}. Skipping weight processing.")
    logger.info("--- Pre-processing script finished --- ")
if __name__ == "__main__":
    main()
</file>

<file path="README.md">
**Important Note on Date Formats:**

Throughout this application, date processing logic (especially when identifying date columns in input files) should be flexible. Aim to handle common formats like `YYYY-MM-DD`, `DD/MM/YYYY`, and `YYYY-MM-DDTHH:MM:SS` where appropriate, particularly during initial data loading and column identification steps. While pre-processing steps might standardize dates to `YYYY-MM-DD`, initial parsing should be robust.

---

# Simple Data Checker

This application provides a web interface to load, process, and check financial data, primarily focusing on time-series metrics and security-level data. It helps identify potential data anomalies by calculating changes and Z-scores.

## Features

*   **Time-Series Metric Analysis:** Load `ts_*.csv` files, view latest changes, Z-scores, and historical data charts for various metrics per fund.
    *   Optionally loads corresponding `sp_ts_*.csv` files (if they exist) to provide comparison data (e.g., S&P data) on the same charts.
    *   Includes a toggle switch on the **Metric Detail Page** (`/metric/<metric_name>`) to show/hide this comparison data.
*   **Security-Level Analysis:** Load wide-format `sec_*.csv` files, view latest changes and Z-scores across securities, and drill down into historical charts (Value, Price, Duration) for individual securities.
    *   **Performance:** Uses server-side pagination, filtering (search, dropdowns), and sorting for improved performance with large datasets.
*   **Fund-Specific Views:** Analyze data aggregated or filtered by specific funds.
    *   General Fund Overview (`/fund/<fund_code>`): Displays all available time-series metric charts for a single fund.
        *   Optionally loads corresponding `sp_ts_*.csv` files for comparison data.
        *   Includes a toggle switch to show/hide this comparison data.
    *   Fund Duration Details (`/fund/duration_details/<fund_code>`): Shows duration changes for securities held by a specific fund.
*   **Security Exclusions:** Maintain a list of securities to temporarily exclude from the main Security Summary page (`/security/summary`). Exclusions can have start/end dates and comments.
*   **Weight Check:** Load fund (`w_Funds.csv`) and benchmark (`w_Bench.csv`) weight files and display them side-by-side, highlighting any daily weights that are not exactly 100% via `/weights/check`.
*   **Yield Curve Analysis:** Load yield curve data (`curves.csv`), check for potential inconsistencies (e.g., monotonicity, anomalous daily changes) and display curve charts per currency via `/curve/summary` and `/curve/details/<currency>`.
*   **Data Comparison:**
    *   Compare two spread files (`sec_spread.csv` vs `sec_spreadSP.csv`) via `/comparison/summary`.
    *   Compare two duration files (`sec_duration.csv` vs `sec_durationSP.csv`) via `/duration_comparison/summary`.
    *   Compare two spread duration files (`sec_Spread duration.csv` vs `sec_Spread durationSP.csv`) via `/spread_duration_comparison/summary`.
    *   All comparison pages include summary statistics and side-by-side detail charts.
    *   **Performance:** Uses server-side pagination, filtering, and sorting for summary views.
*   **Data Simulation & Management:**
    *   Simulate API calls to fetch data via the `/get_data` page.
    *   Run a data cleanup process via a button on the `/get_data` page.
*   **Handling Special Characters in IDs:** Security IDs can contain special characters, including slashes (`/`, `\\`), spaces, and symbols (`#`). The application uses the `urlencode` filter in templates to create safe URLs and the `<path:security_id>` converter in Flask routes to capture these IDs correctly. **Note:** Inside view functions receiving such IDs, especially when comparing against data (e.g., filtering a DataFrame), it may be necessary to explicitly decode the `security_id` variable using `urllib.parse.unquote(security_id)` to ensure it matches the format stored in the data source.

## File Structure Overview

```mermaid
graph TD
    A[Simple Data Checker] --> B(app.py);
    A --> C{Python Modules};
    A --> D{Views};
    A --> E{Templates};
    A --> F{Static Files};
    A --> G(Data);
    A --> H(Config/Utils);

    C --> C1(data_loader.py);
    C --> C2(metric_calculator.py);
    C --> C3(security_processing.py);
    C --> C4(process_data.py);
    C --> C5(curve_processing.py);

    D --> D1(main_views.py);
    D --> D2(metric_views.py);
    D --> D3(security_views.py);
    D --> D4(fund_views.py);
    D --> D5(exclusion_views.py);
    D --> D6(comparison_views.py);
    D --> D7(weight_views.py);
    D --> D8(api_views.py);
    D --> D9(duration_comparison_views.py);
    D --> D10(spread_duration_comparison_views.py);
    D --> D11(curve_views.py);

    E --> E1(base.html);
    E --> E2(index.html);
    E --> E3(metric_page_js.html);
    E --> E4(securities_page.html);
    E --> E5(security_details_page.html);
    E --> E6(fund_duration_details.html);
    E --> E7(exclusions_page.html);
    E --> E8(get_data.html);
    E --> E9(comparison_page.html);
    E --> E10(comparison_details_page.html);
    E --> E11(fund_detail_page.html);
    E --> E12(weight_check.html);
    E --> E13(duration_comparison_page.html);
    E --> E14(duration_comparison_details_page.html);
    E --> E15(spread_duration_comparison_page.html);
    E --> E16(spread_duration_comparison_details_page.html);
    E --> E17(curve_summary.html);
    E --> E18(curve_details.html);

    F --> F1(js);
    F1 --> F1a(main.js);
    F1 --> F1b(modules);
    F1b --> F1b1(ui);
    F1b1 --> F1b1a(chartRenderer.js);
    F1b1 --> F1b1b(securityTableFilter.js);
    F1b1 --> F1b1c(tableSorter.js);
    F1b1 --> F1b1d(toggleSwitchHandler.js);
    F1b --> F1b2(utils);
    F1b2 --> F1b2a(helpers.js);
    F1b --> F1b3(charts);
    F1b3 --> F1b3a(timeSeriesChart.js);

    G --> G1(ts_*.csv);
    G --> G2(sec_*.csv);
    G --> G3(pre_*.csv);
    G --> G4(new_*.csv);
    G --> G5(exclusions.csv);
    G --> G6(QueryMap.csv);
    G --> G7(FundList.csv);
    G --> G8(w_Funds.csv);
    G --> G9(w_Bench.csv);
    G --> G10(curves.csv);

    H --> H1(config.py);
    H --> H2(utils.py);

    B --> D;
    D --> C;
    D --> H;
    D --> E;
    E --> F;
```

## Data Files (`Data/`)

*   `ts_*.csv`: Time-series data, indexed by Date and Code (Fund/Benchmark).
*   `sp_ts_*.csv`: (Optional) Secondary/comparison time-series data, corresponding to `ts_*.csv` files. Used on Metric and Fund Detail pages.
*   `sec_*.csv`: Security-level data, typically wide format with dates as columns.
*   `pre_*.csv`: Input files for the `process_data.py` script.
*   `new_*.csv`: Output files from the `process_data.py` script.
*   **`exclusions.csv`**: Stores the list of excluded securities. Contains columns: `SecurityID`, `AddDate`, `EndDate`, `Comment`.
*   `QueryMap.csv`: Maps query IDs to filenames for the API simulation.
*   `FundList.csv`: Contains fund codes and metadata used on the API simulation page.
*   `Dates.csv`: May exist for specific configurations or helper data.
*   `w_Funds.csv`: Wide format file containing daily fund weights (expected to be 100%). Used by the Weight Check page.
*   `w_Bench.csv`: Wide format file containing daily benchmark weights (expected to be 100%). Used by the Weight Check page.
*   `curves.csv`: Contains yield curve data (Date, Currency Code, Term, Daily Value). Used by the Yield Curve Check feature.

## Python Files

### `app.py`
*   **Purpose:** Defines the main entry point and structure for the Simple Data Checker Flask web application. It utilizes the Application Factory pattern (`create_app`) to initialize and configure the Flask app.
*   **Key Responsibilities:**
    *   Creating the Flask application instance.
    *   Setting up basic configuration (like the secret key).
    *   Ensuring necessary folders (like the instance folder) exist.
    *   Registering all Blueprints (e.g., `main_bp`, `metric_bp`, `security_bp`, `fund_bp`, `exclusion_bp`, `comparison_bp`, `duration_comparison_bp`, `spread_duration_comparison_bp`, `api_bp`, `weight_bp`) from the `views` directory.
    *   Includes an endpoint (`/run-cleanup`) to trigger the `process_data.py` script.
    *   Providing a conditional block (`if __name__ == '__main__':`) to run the development server.
*   **Functions:**
    *   `create_app()`: Factory function to create and configure the Flask app.
    *   `run_cleanup()`: Endpoint to run the cleanup script.
    *   `hello()`: Simple test route (can be removed).

### `config.py`
*   **Purpose:** Defines configuration variables for the Simple Data Checker application.
*   **Variables:**
    *   `DATA_FOLDER`: Specifies the directory containing data files.
    *   `COLOR_PALETTE`: Defines a list of colors for chart lines.

### `data_loader.py`
*   **Purpose:** Responsible for loading and preprocessing data from time-series CSV files (`ts_*.csv`).
*   **Key Features:**
    *   Dynamically identifies 'Date' (using `Date` or `Position Date`), 'Code', and optional benchmark columns (using `Bench`).
    *   Parses dates using pandas `to_datetime` (handles various formats).
    *   Standardizes key column names (`Date`, `Code`, `Benchmark`).
    *   Sets a MultiIndex (`Date`, `Code`).
    *   Converts value columns to numeric using `pd.to_numeric(errors='coerce')`.
    *   Logs progress and errors to `data_processing_errors.log`.
*   **Functions:**
    *   `_find_column(...)`: Helper to find columns by regex.
    *   `load_and_process_data(...)`: Main function for loading and processing.

### `metric_calculator.py`
*   **Purpose:** Provides functions for calculating statistical metrics (mean, max, min, latest value, change, Z-score) from preprocessed time-series data.
*   **Key Features:**
    *   Operates on DataFrames indexed by Date and Fund Code.
    *   Calculates metrics for primary data (e.g., `ts_*.csv`) and optionally for secondary/comparison data (e.g., `sp_ts_*.csv`).
    *   Handles fund and benchmark columns for both primary and secondary sources.
    *   Handles `NaN` values gracefully.
*   **Functions:**
    *   `_calculate_column_stats(...)`: Helper for single-column stats.
    *   `calculate_latest_metrics(...)`: Calculates latest metrics per fund for both primary and secondary data, sorted by max absolute primary Z-score.

### `process_data.py`
*   **Purpose:** Serves as a pre-processing step for specific CSV files (usually `pre_*.csv`), aggregating rows and handling duplicates.
*   **Functions:**
    *   `process_csv_file(...)`: Processes a single input file.
    *   `main()`: Processes all `pre_*.csv` files in the `Data` directory.

### `security_processing.py`
*   **Purpose:** Handles loading, processing, and analysis of security-level data (usually `sec_*.csv`).
*   **Key Features:**
    *   Assumes wide format (dates as columns).
    *   Dynamically identifies ID, static, and date columns (using `utils._is_date_like`).
    *   Melts data into long format (Date, Security ID).
    *   Robust type conversion (`pd.to_datetime`, `pd.to_numeric`, `errors='coerce'`).
    *   Calculates latest metrics (Latest Value, Change, Mean, Max, Min, Change Z-Score) per security.
*   **Functions:**
    *   `_is_date_like(...)`: Moved to `utils.py`.
    *   `load_and_process_security_data(...)`: Loads and melts wide-format data.
    *   `calculate_security_latest_metrics(...)`: Calculates metrics on long-format data.

### `utils.py`
*   **Purpose:** Contains common utility functions.
*   **Functions:**
    *   `_is_date_like(column_name)`: Checks if a column name resembles `YYYY-MM-DD` or `DD/MM/YYYY` format.
    *   `parse_fund_list(fund_string)`: Parses a string like `'[FUND1,FUND2]'` into a list.

### `curve_processing.py`
*   **Purpose:** Handles loading, preprocessing, and analysis of yield curve data (`Data/curves.csv`).
*   **Key Features:**
    *   Loads data, parses dates.
    *   Converts term strings (e.g., '7D', '1M') into an approximate number of days (`TermDays`) for plotting and sorting.
    *   Checks for basic curve inconsistencies on the latest date:
        *   Monotonicity check (identifies significant downward slopes).
        *   Compares the shape of the daily change profile against the previous day to find anomalous jumps for specific terms.
*   **Functions:**
    *   `_term_to_days(...)`: Converts term string to days.
    *   `load_curve_data(...)`: Loads and preprocesses the `curves.csv` file.
    *   `get_latest_curve_date(...)`: Finds the most recent date in the loaded data.
    *   `check_curve_inconsistencies(...)`: Performs the inconsistency checks and returns a summary dictionary.

## View Modules (`views/`)

These modules contain the Flask Blueprints defining the application's routes.

### `views/main_views.py` (`main_bp`)
*   **Purpose:** Main dashboard/index page.
*   **Routes:**
    *   `/`: Renders `index.html`, showing links to metric pages and a summary table of latest Z-Scores across all metrics and funds. Fund codes link to the general fund detail page (`/fund/<fund_code>`).

### `views/metric_views.py` (`metric_bp`)
*   **Purpose:** Detailed views for specific time-series metrics.
*   **Routes:**
    *   `/metric/<metric_name>`: Renders `metric_page_js.html`. Loads primary (`ts_*.csv`) and optional secondary (`sp_ts_*.csv`) data. Calculates metrics for both, prepares JSON data (including an `isSpData` flag in datasets), and passes it to the template. The JavaScript (`main.js`, `chartRenderer.js`) renders charts and handles the SP data toggle switch.

### `views/security_views.py` (`security_bp`)
*   **Purpose:** Security-level data checks.
*   **Routes:**
    *   `/security/summary`: Renders `securities_page.html`.
        *   **Handles server-side pagination, filtering (search, static columns), and sorting.**
        *   Loads spread data (`sec_Spread.csv`), applies filters/search/exclusions.
        *   Calculates metrics, sorts data, and selects the current page.
        *   Passes paginated data and metadata to the template.
        *   Security IDs (now ISINs) link to the details page.
    *   `/security/details/<metric_name>/<path:security_id>`: Renders `security_details_page.html`.
        *   Shows historical charts for a specific security (identified by ISIN via `security_id`).
        *   Displays the requested base `metric_name` overlaid with Price (from `sec_Price.csv`).
        *   Additionally displays separate charts for:
            *   Duration (from `sec_Duration.csv`) overlaid with SP Duration (from `sec_DurationSP.csv`).
            *   Spread Duration (from `sec_Spread duration.csv`) overlaid with SP Spread Duration (from `sec_Spread durationSP.csv`).
            *   Spread (from `sec_Spread.csv`) overlaid with SP Spread (from `sec_SpreadSP.csv`).
        *   SP data files are loaded if they exist.

### `views/fund_views.py` (`fund_bp`)
*   **Purpose:** Fund-specific views.
*   **Routes:**
    *   `/fund/duration_details/<fund_code>`: Renders `fund_duration_details.html`. Loads `sec_duration.csv`, filters by fund, calculates recent duration changes, and displays results.
    *   `/fund/<fund_code>`: Renders `fund_detail_page.html`. Finds all primary `ts_*.csv` files and corresponding optional `sp_ts_*.csv` files. Loads data for the specified fund from both sources, adds an `isSpData` flag to datasets, and prepares data for rendering multiple time-series charts on a single page via JavaScript (`main.js`, `chartRenderer.js`), including an SP data toggle switch.

### `views/exclusion_views.py` (`exclusion_bp`)
*   **Purpose:** Managing the security exclusion list (`Data/exclusions.csv`).
*   **Routes:**
    *   `/exclusions` (GET/POST): Renders `exclusions_page.html` to view/add exclusions.
    *   `/exclusions/remove` (POST): Removes an exclusion.

### `views/comparison_views.py` (`comparison_bp`)
*   **Purpose:** Comparing two spread files (`sec_spread.csv` vs. `sec_spreadSP.csv`).
*   **Routes:**
    *   `/comparison/summary`: Renders `comparison_page.html`.
        *   **Handles server-side pagination, filtering (static columns), and sorting.**
        *   Loads both files, calculates comparison statistics (correlation, diffs, date ranges).
        *   Applies filters, sorts data, and selects the current page.
        *   Passes paginated data and metadata to the template.
    *   `/comparison/details/<path:security_id>`: Renders `comparison_details_page.html`. Shows side-by-side historical charts for a specific security.

### `views/duration_comparison_views.py` (`duration_comparison_bp`)
*   **Purpose:** Comparing two duration files (`sec_duration.csv` vs. `sec_durationSP.csv`).
*   **Routes:**
    *   `/duration_comparison/summary`: Renders `duration_comparison_page.html`.
        *   Handles server-side pagination, filtering (static columns), and sorting.
        *   Loads both files, calculates comparison statistics.
        *   Applies filters, sorts data, and selects the current page.
        *   Passes paginated data and metadata to the template.
    *   `/duration_comparison/details/<path:security_id>`: Renders `duration_comparison_details_page.html`. Shows side-by-side historical charts for a specific security.

### `views/spread_duration_comparison_views.py` (`spread_duration_comparison_bp`)
*   **Purpose:** Comparing two spread duration files (`sec_Spread duration.csv` vs. `sec_Spread durationSP.csv`).
*   **Routes:**
    *   `/spread_duration_comparison/summary`: Renders `spread_duration_comparison_page.html`.
        *   Handles server-side pagination, filtering (static columns), and sorting.
        *   Loads both files, calculates comparison statistics.
        *   Applies filters, sorts data, and selects the current page.
        *   Passes paginated data and metadata to the template.
    *   `/spread_duration_comparison/details/<path:security_id>`: Renders `spread_duration_comparison_details_page.html`. Shows side-by-side historical charts for a specific security.

### `views/api_views.py` (`api_bp`)
*   **Purpose:** Handling the API simulation page interactions.
*   **Routes:**
    *   `/get_data`: Renders `get_data.html` (GET). Shows data file statuses, fund selection, and date inputs.
    *   `/run-api-calls`: Handles the POST request from `get_data.html` to simulate API calls based on `QueryMap.csv`. Reads data, merges/overwrites based on `overwrite_mode` flag, and saves to `Data/` folder.
    *   `/rerun-api-call`: Handles POST requests to rerun a single API call for a specific fund.

### `views/weight_views.py` (`weight_bp`)
*   **Purpose:** Handles the weight checking functionality.
*   **Routes:**
    *   `/weights/check`: Renders `weight_check_page.html`. Loads data from `w_Funds.csv` and `w_Bench.csv`, processes percentage values, checks if they equal 100%, and passes the processed data to the template for display.

### `views/curve_views.py` (`curve_bp`)
*   **Purpose:** Handles the yield curve checking views.
*   **Routes:**
    *   `/curve/summary`: Renders `curve_summary.html`. Loads curve data, runs inconsistency checks using `curve_processing.check_curve_inconsistencies`, and displays a summary table for all currencies.
    *   `/curve/details/<currency>`: Renders `curve_details.html`. Displays a line chart of the yield curve for the specified `currency` on a selected date. Includes a dropdown to select and view curves for previous dates.

## HTML Templates (`templates/`)

*   **`base.html`:** Main layout, includes Bootstrap, navbar, common structure. All other templates extend this.
*   **`index.html`:** Dashboard page. Displays metric links and Z-Score summary table.
*   **`metric_page_js.html`:** Detail page for a time-series metric (rendered via JS).
    *   Includes a toggle switch (`#toggleSpData`) to show/hide secondary/SP comparison data on charts.
*   **`securities_page.html`:** Security check summary table. Includes filter/search form, sortable headers, table body, and pagination controls.
*   **`security_details_page.html`:** Detail page for a single security (charts).
*   **`fund_duration_details.html`:** Table showing security duration changes for a specific fund.
*   **`exclusions_page.html`:** UI for managing security exclusions.
*   **`get_data.html`:** UI for API simulation. Includes data status table, fund selection, date inputs, status/results area, and buttons for simulation, overwrite, and cleanup.
*   **`comparison_page.html`:** Comparison summary table. Includes filter form, sortable headers, table body, and pagination controls.
*   **`comparison_details_page.html`:** Side-by-side chart comparison for a single security (Spread).
*   **`fund_detail_page.html`:** Displays multiple charts for different metrics for a single fund.
    *   Includes a toggle switch (`#toggleSpData`) to show/hide secondary/SP comparison data on charts.
*   **`weight_check.html`:** Placeholder page for weight checks.
*   **`duration_comparison_page.html`:** Comparison summary table for Duration.
*   **`duration_comparison_details_page.html`:** Side-by-side chart comparison for a single security (Duration).
*   **`spread_duration_comparison_page.html`:** Comparison summary table for Spread Duration.
*   **`spread_duration_comparison_details_page.html`:** Side-by-side chart comparison for a single security (Spread Duration).
*   **`curve_summary.html`:** Displays a summary table of the yield curve inconsistency checks for the latest date across all currencies.
*   **`curve_details.html`:** Shows a line chart of the yield curve for a specific currency and provides a date selector to view historical curves. Includes JavaScript for Chart.js rendering.
```
</file>

<file path="requirements.txt">
Flask
pandas
plotly
</file>

<file path="security_processing.py">
# This file handles the loading, processing, and analysis of security-level data.
# It assumes input CSV files are structured with one security per row and time series data
# spread across columns where headers represent dates (e.g., YYYY-MM-DD).
# Key functions:
# - `load_and_process_security_data`: Reads a wide-format CSV (given filename and data path),
#   identifies the security ID column, static attribute columns, and date columns.
#   It then 'melts' the data into a long format, converting date strings to datetime objects.
# - `calculate_security_latest_metrics`: Takes the processed long-format DataFrame and calculates
#   various metrics for each security's 'Value' over time, including latest value, change,
#   historical stats (mean, max, min), and change Z-score. It also preserves the static attributes.
import pandas as pd
import os
import numpy as np
import re # For checking date-like column headers
import logging
import traceback
# Note: Does not import current_app, relies on caller to pass the path.
# Get the logger instance. Assumes Flask app has configured logging.
logger = logging.getLogger(__name__)
# --- Removed logging setup block --- 
# Logging is now handled centrally by the Flask app factory in app.py
# Removed DATA_FOLDER constant - path is now passed to functions
def _is_date_like(column_name):
    """Check if a column name looks like a common date format.
    Recognizes formats like YYYY-MM-DD, YYYY/MM/DD, MM/DD/YYYY, M/D/YYYY, YYYYMMDD.
    """
    col_str = str(column_name)
    # Regex to match common date patterns
    # - YYYY[-/]MM[-/]DD
    # - MM[-/]DD[-/]YYYY (allows 1-2 digits for M, D and 2 or 4 for Y)
    # - YYYYMMDD
    pattern = r'^(\d{4}[-/]\d{1,2}[-/]\d{1,2}|\d{1,2}[-/]\d{1,2}[-/](\d{4}|\d{2})|\d{8})$'
    return bool(re.match(pattern, col_str))
def load_and_process_security_data(filename: str, data_folder_path: str):
    """Loads security data, identifies static/date columns, and melts to long format.
    Args:
        filename (str): The name of the CSV file (e.g., 'sec_Spread.csv').
        data_folder_path (str): The absolute path to the folder containing the data file.
                                The caller is responsible for providing the correct path,
                                typically obtained from `current_app.config['DATA_FOLDER']`.
    Returns:
        tuple: (pandas.DataFrame, list[str])
               - Processed DataFrame in long format with 'Date', 'Value', ID, and static columns.
               - List of identified static column names (excluding Security ID).
        Returns (pd.DataFrame(), []) if a critical error occurs during loading or processing.
    """
    if not data_folder_path:
        logger.error("No data_folder_path provided to load_and_process_security_data.")
        return pd.DataFrame(), []
    filepath = os.path.join(data_folder_path, filename)
    logger.info(f"Attempting to load security data from: {filepath}")
    try:
        # Read just the header to identify column types
        # Use on_bad_lines='skip' for robustness
        header_df = pd.read_csv(filepath, nrows=0, on_bad_lines='skip', encoding='utf-8', encoding_errors='replace')
        all_cols = [str(col).strip() for col in header_df.columns.tolist()] # Ensure string type and strip
        if not all_cols:
            logger.error(f"CSV file '{filename}' appears to be empty or header is missing.")
            raise ValueError(f"CSV file '{filename}' appears to be empty or header is missing.")
        # --- Define Essential ID Columns --- 
        # We always want to keep ISIN and Security Name if they exist
        essential_id_cols = []
        if 'ISIN' in all_cols:
            essential_id_cols.append('ISIN')
        if 'Security Name' in all_cols:
            essential_id_cols.append('Security Name')
        if not essential_id_cols:
             # Fallback if neither standard ID is present - use first column
             logger.warning(f"Neither 'ISIN' nor 'Security Name' found in {filename}. Using first column '{all_cols[0]}' as potential ID.")
             essential_id_cols.append(all_cols[0])
        logger.info(f"Essential ID Columns identified: {essential_id_cols}")
        # --- Identify Static and Date Columns --- 
        static_cols = []
        date_cols = []
        for col in all_cols:
            if col in essential_id_cols: # Skip essential IDs
                continue
            if _is_date_like(col):
                date_cols.append(col)
            else:
                static_cols.append(col) # Treat others as static
        if not date_cols:
            logger.error(f"No date-like columns found in '{filename}' using flexible patterns. Cannot process as security time series.")
            raise ValueError("No date-like columns found using flexible patterns.")
        logger.info(f"Identified Static Cols: {static_cols}")
        # logger.info(f"Identified Date Cols: {date_cols[:5]}...") # Avoid excessive logging
        # --- Read Full Data --- 
        df_wide = pd.read_csv(filepath, encoding='utf-8', on_bad_lines='skip', encoding_errors='replace')
        df_wide.columns = df_wide.columns.map(lambda x: str(x).strip())
        # --- Melt Data --- 
        # Use ALL essential IDs and found static columns as id_vars
        id_vars_melt = [col for col in essential_id_cols if col in df_wide.columns] + \
                       [col for col in static_cols if col in df_wide.columns]
        value_vars = [col for col in date_cols if col in df_wide.columns] # Ensure date columns exist
        if not value_vars:
             logger.error(f"Date columns identified in header of '{filename}' not found in data frame after loading. Columns available: {df_wide.columns.tolist()}")
             raise ValueError("Date columns identified in header not found in data frame after loading.")
        if not id_vars_melt:
             logger.error(f"No ID or static columns found to use as id_vars in {filename}. Columns available: {df_wide.columns.tolist()}")
             raise ValueError("No ID or static columns found for melting.")
        df_long = pd.melt(df_wide,
                          id_vars=id_vars_melt,
                          value_vars=value_vars,
                          var_name='Date_Str',
                          value_name='Value')
        # --- Process Date and Value --- 
        # Attempt robust date parsing for multiple potential formats
        date_col_str = 'Date_Str'
        date_col_dt = 'Date'
        # 1. Try DD/MM/YYYY first
        df_long[date_col_dt] = pd.to_datetime(df_long[date_col_str], format='%d/%m/%Y', errors='coerce')
        # 2. Try YYYY-MM-DD for any remaining NaTs
        # Create a mask for rows where the first attempt failed
        nat_mask = df_long[date_col_dt].isna()
        if nat_mask.any():
            logger.info(f"Attempting fallback date parsing (YYYY-MM-DD) for {nat_mask.sum()} entries in {filename}.")
            # Apply the second format ONLY to the NaT rows
            df_long.loc[nat_mask, date_col_dt] = pd.to_datetime(df_long.loc[nat_mask, date_col_str], format='%Y-%m-%d', errors='coerce')
        # Check again for NaTs after both attempts
        final_nat_count = df_long[date_col_dt].isna().sum()
        if final_nat_count > 0:
            logger.warning(f"Could not parse {final_nat_count} date strings in {filename} using specified formats (DD/MM/YYYY, YYYY-MM-DD).")
            # Example of unparsed date strings:
            unparsed_examples = df_long.loc[df_long[date_col_dt].isna(), date_col_str].unique()[:5]
            logger.warning(f"Unparsed examples: {unparsed_examples}")
        # Convert Value column
        df_long['Value'] = pd.to_numeric(df_long['Value'], errors='coerce')
        # Drop rows where essential data is missing (Date, Value, ALL essential IDs)
        initial_rows = len(df_long)
        required_cols_for_dropna = ['Date', 'Value'] + [col for col in essential_id_cols if col in df_long.columns]
        df_long.dropna(subset=required_cols_for_dropna, inplace=True)
        rows_dropped = initial_rows - len(df_long)
        if rows_dropped > 0:
             logger.warning(f"Dropped {rows_dropped} rows from '{filename}' due to missing required values (Date, Value, or Essential IDs).")
        if df_long.empty:
             logger.warning(f"DataFrame for '{filename}' is empty after melting, conversion, and NaN drop.")
             return pd.DataFrame(), static_cols
        # Ensure the ID column name used for sorting/indexing is determined correctly
        id_col_name = None
        if 'ISIN' in df_long.columns:
            id_col_name = 'ISIN'
        elif 'Security Name' in df_long.columns:
             id_col_name = 'Security Name'
        # Add fallback if needed, based on essential_id_cols logic earlier
        elif essential_id_cols and essential_id_cols[0] in df_long.columns: 
             id_col_name = essential_id_cols[0]
             logger.warning(f"Using fallback ID '{id_col_name}' for index setting in {filename}.")
        else:
             logger.error(f"Cannot determine a valid ID column ({essential_id_cols}) to set index in {filename}. Columns: {df_long.columns.tolist()}")
             # Return empty if no valid ID for index
             return pd.DataFrame(), []
        # Sort before setting index - Use the determined ID column and Date
        # Original: df_long = df_long.sort_values(by=['ID', 'Date'])
        df_long = df_long.sort_values(by=[id_col_name, 'Date']) # Sort by ID then Date
        # --- SET THE MULTIINDEX --- 
        # Set the required MultiIndex before returning
        try:
            # Original: df_long.set_index(['ID', 'Date'], inplace=True)
            # Set the index using the specific columns 'Date' and the determined id_col_name
            df_long.set_index(['Date', id_col_name], inplace=True) # Reverted order to Date, ID
            logger.info(f"Set MultiIndex ('Date', '{id_col_name}') for {filename}.") # Reverted log message
        except KeyError as e:
             # Ensure error log reflects the intended index columns
             logger.error(f"Failed to set index using ['Date', '{id_col_name}'] for {filename}. Error: {e}. Columns: {df_long.columns.tolist()}") # Reverted log message
             return pd.DataFrame(), [] # Return empty if index setting fails
        # Original code to drop Date_Str column - ensure it happens before index setting or handle potential error
        if 'Date_Str' in df_long.columns:
            # This will fail if Date_Str is part of the index (which it shouldn't be here)
            # It's better to drop it BEFORE setting the index if possible.
            # Let's move the drop earlier.
            # df_long.drop(columns=['Date_Str'], inplace=True)
            pass # Already dropped earlier implicitly or explicitly
        logger.info(f"Successfully loaded and processed '{filename}'. Returning long format with MultiIndex. Shape: {df_long.shape}")
        # Return the identified static columns (excluding essential IDs)
        return df_long, static_cols
    except FileNotFoundError:
        logger.error(f"Error: File not found at {filepath}")
        return pd.DataFrame(), [] # Return empty dataframe and list
    except ValueError as ve:
        logger.error(f"Error processing header or columns in {filename}: {ve}")
        return pd.DataFrame(), []
    except KeyError as ke:
        logger.error(f"Error melting DataFrame for {filename}, likely due to missing column used as id_var or value_var: {ke}")
        return pd.DataFrame(), []
    except Exception as e:
        logger.error(f"An unexpected error occurred loading/processing {filename}: {e}", exc_info=True)
        # traceback.print_exc() # Logger handles traceback now
        return pd.DataFrame(), []
def calculate_security_latest_metrics(df, static_cols):
    """Calculates latest metrics for each security based on its 'Value' column.
    Args:
        df (pd.DataFrame): Processed long-format DataFrame with MultiIndex (Date, Security ID).
                           Must contain a 'Value' column.
        static_cols (list[str]): List of static column names present in the DataFrame's columns (not index).
    Returns:
        pandas.DataFrame: DataFrame indexed by Security ID, including static columns and
                          calculated metrics (Latest Value, Change, Mean, Max, Min, Change Z-Score).
                          Returns an empty DataFrame if input is empty or processing fails.
    """
    if df is None or df.empty:
        logger.warning("Input DataFrame is None or empty. Cannot calculate security metrics.")
        return pd.DataFrame()
    if 'Value' not in df.columns:
        logger.error("Input DataFrame for security metrics calculation must contain a 'Value' column.")
        return pd.DataFrame()
    # Ensure index has two levels and get their names dynamically
    if df.index.nlevels != 2:
        logger.error("Input DataFrame for security metrics must have 2 index levels (Date, Security ID).")
        return pd.DataFrame()
    date_level_name, id_level_name = df.index.names
    try:
        latest_date = df.index.get_level_values(date_level_name).max()
        security_ids = df.index.get_level_values(id_level_name).unique()
        all_metrics_list = []
        for sec_id in security_ids:
            try:
                # Extract data for the current security ID
                # Use .loc for potentially cleaner selection and ensure sorting
                sec_data_hist = df.loc[(slice(None), sec_id), :].reset_index(level=id_level_name, drop=True).sort_index()
                if sec_data_hist.empty:
                     logger.debug(f"No data found for security '{sec_id}' after extraction. Skipping.")
                     continue
                sec_metrics = {} # Dictionary to hold metrics for this security
                # Add static columns first
                # Take the first available row's values, assuming they are constant per security
                # Need to handle potential multi-index if static_cols contains index names by mistake
                valid_static_cols = [col for col in static_cols if col in sec_data_hist.columns]
                if not sec_data_hist.empty:
                    static_data_row = sec_data_hist.iloc[0]
                    for static_col in valid_static_cols:
                        sec_metrics[static_col] = static_data_row.get(static_col, np.nan)
                else: # Should not happen due to check above, but safeguard
                    for static_col in valid_static_cols:
                         sec_metrics[static_col] = np.nan 
                # Ensure all expected static cols are present in the dict, even if missing from data
                for static_col in static_cols:
                     if static_col not in sec_metrics:
                          logger.warning(f"Static column '{static_col}' not found in data for security '{sec_id}', adding as NaN.")
                          sec_metrics[static_col] = np.nan
                # Calculate metrics for the 'Value' column
                value_hist = sec_data_hist['Value']
                # Calculate diff only if series has enough data
                value_change_hist = pd.Series(index=value_hist.index, dtype=np.float64)
                if not value_hist.dropna().empty and len(value_hist.dropna()) > 1:
                    value_change_hist = value_hist.diff()
                else:
                    logger.debug(f"Cannot calculate difference for 'Value' column, security '{sec_id}' due to insufficient data.")
                # Base historical stats (level) - handle potential all-NaN series
                sec_metrics['Mean'] = value_hist.mean() if value_hist.notna().any() else np.nan
                sec_metrics['Max'] = value_hist.max() if value_hist.notna().any() else np.nan
                sec_metrics['Min'] = value_hist.min() if value_hist.notna().any() else np.nan
                # Stats for change
                change_mean = value_change_hist.mean() if value_change_hist.notna().any() else np.nan
                change_std = value_change_hist.std() if value_change_hist.notna().any() else np.nan
                # Latest values
                # Check if latest_date exists in this security's specific history
                if latest_date in sec_data_hist.index:
                    latest_value = sec_data_hist.loc[latest_date, 'Value']
                    latest_change = value_change_hist.get(latest_date, np.nan)
                    sec_metrics['Latest Value'] = latest_value
                    sec_metrics['Change'] = latest_change
                    # Calculate Change Z-Score
                    change_z_score = np.nan
                    if pd.notna(latest_change) and pd.notna(change_mean) and pd.notna(change_std) and change_std != 0:
                        change_z_score = (latest_change - change_mean) / change_std
                    elif change_std == 0 and pd.notna(latest_change) and pd.notna(change_mean):
                         # Handle zero standard deviation
                         if latest_change == change_mean:
                              change_z_score = 0.0
                         else:
                             change_z_score = np.inf if latest_change > change_mean else -np.inf
                         logger.debug(f"Std dev of change for security '{sec_id}' is zero. Z-score set to {change_z_score}.")
                    else:
                         # Log if Z-score calculation failed due to NaNs
                        if not (pd.notna(latest_change) and pd.notna(change_mean) and pd.notna(change_std)):
                             logger.debug(f"Cannot calculate Z-score for security '{sec_id}' due to NaN inputs (latest_change={latest_change}, change_mean={change_mean}, change_std={change_std})")
                    sec_metrics['Change Z-Score'] = change_z_score
                else:
                    # Security missing the overall latest date
                    logger.debug(f"Security '{sec_id}' missing data for latest date {latest_date}. Setting latest metrics to NaN.")
                    sec_metrics['Latest Value'] = np.nan
                    sec_metrics['Change'] = np.nan
                    sec_metrics['Change Z-Score'] = np.nan
                # Add the security ID itself for setting the index later
                sec_metrics[id_level_name] = sec_id 
                all_metrics_list.append(sec_metrics)
            except Exception as inner_e:
                logger.error(f"Error calculating metrics for security '{sec_id}': {inner_e}", exc_info=True)
                # Optionally add a placeholder row with NaNs? Or just skip. Let's skip.
                continue
        if not all_metrics_list:
            logger.warning("No security metrics were successfully calculated. Returning empty DataFrame.")
            return pd.DataFrame()
        # Create DataFrame and set index
        latest_metrics_df = pd.DataFrame(all_metrics_list)
        # id_col_name = df.index.names[1] # Get the actual ID column name used
        if id_level_name in latest_metrics_df.columns:
             latest_metrics_df.set_index(id_level_name, inplace=True)
        else:
             logger.error(f"Security ID column '{id_level_name}' not found in the created metrics list for setting index. Columns: {latest_metrics_df.columns.tolist()}")
             # Fallback or error? Let's return as is for now, index might be RangeIndex.
        # Reorder columns to have static columns first, then calculated metrics
        metric_cols = ['Latest Value', 'Change', 'Mean', 'Max', 'Min', 'Change Z-Score']
        # Get static cols that are actually present in the final df columns (excluding the ID index)
        present_static_cols = [col for col in static_cols if col in latest_metrics_df.columns]
        final_col_order = present_static_cols + [m_col for m_col in metric_cols if m_col in latest_metrics_df.columns]
        try:
            latest_metrics_df = latest_metrics_df[final_col_order]
        except KeyError as ke:
            logger.error(f"Error reordering columns, likely a metric column is missing: {ke}. Columns available: {latest_metrics_df.columns.tolist()}")
            # Proceed with potentially incorrect order
        # Sorting (e.g., by Z-score) should be done in the view function where it's displayed
        logger.info(f"Successfully calculated metrics for {len(latest_metrics_df)} securities.")
        return latest_metrics_df
    except Exception as e:
        logger.error(f"An unexpected error occurred during security metric calculation: {e}", exc_info=True)
        # traceback.print_exc() # Logger handles traceback
        return pd.DataFrame()
</file>

<file path="static/css/style.css">
/* Basic styling for sortable table headers */
th.sortable {
    cursor: pointer;
    position: relative; /* Needed for absolute positioning of indicator */
}
/* Hide default indicator span content */
.sort-indicator {
    display: inline-block;
    width: 1em;
    height: 1em;
    margin-left: 5px;
    vertical-align: middle;
    content: "";
}
/* Style for ascending sort indicator */
th.sortable.sort-asc .sort-indicator::before {
    content: "\25B2"; /* Up arrow ▲ */
    font-size: 0.8em;
}
/* Style for descending sort indicator */
th.sortable.sort-desc .sort-indicator::before {
    content: "\25BC"; /* Down arrow ▼ */
    font-size: 0.8em;
}
/* Hover effect for sortable headers */
th.sortable:hover {
    background-color: #e9ecef; /* Light grey background on hover */
}
</file>

<file path="static/js/main.js">
// This file acts as the main entry point for the application's JavaScript.
// It runs after the DOM is fully loaded and performs several key initializations:
// 1. Imports necessary functions from UI modules (chart rendering, table filtering).
// 2. Checks for the presence of specific elements on the page to determine the context
//    (e.g., metric details page, securities list page, single security detail page).
// 3. If on a metric details page (`metric_page_js.html`):
//    - Finds the embedded JSON data (`<script id="chartData">`).
//    - Parses the JSON data containing historical values and calculated metrics for all funds.
//    - Calls `renderChartsAndTables` from `chartRenderer.js` to dynamically create
//      the metric tables and time-series charts for each fund code.
// 4. If on a securities list page (`securities_page.html`):
//    - Finds the main securities table (`<table id="securities-table">`).
//    - Calls `initSecurityTableFilter` from `securityTableFilter.js` to add
//      interactive filtering capabilities to the table header.
// 5. If on a single security detail page (`security_details_page.html`):
//    - Finds the chart canvas (`<canvas id="securityChart">`) and its associated JSON data (`<script id="chartJsonData">`).
//    - Parses the JSON data containing the time-series for that specific security.
//    - Calls `renderSingleSecurityChart` from `chartRenderer.js` to display the chart.
// This modular approach ensures that initialization code only runs when the corresponding HTML elements are present.
// static/js/main.js
// Purpose: Main entry point for client-side JavaScript. Initializes modules based on page content.
import { renderChartsAndTables, renderSingleSecurityChart, renderFundCharts, toggleSecondaryDataVisibility } from './modules/ui/chartRenderer.js';
import { initSecurityTableFilter } from './modules/ui/securityTableFilter.js';
import { initTableSorter } from './modules/ui/tableSorter.js';
document.addEventListener('DOMContentLoaded', () => {
    console.log("DOM fully loaded and parsed");
    // --- Shared Elements --- 
    const toggleSwitch = document.getElementById('toggleSpData'); // Find toggle switch globally
    // --- Metric Page (Multiple Charts per Metric) ---    
    const metricChartDataElement = document.getElementById('chartData');
    const metricChartsArea = document.getElementById('chartsArea');
    if (metricChartDataElement && metricChartsArea) {
        console.log("Metric page detected. Initializing charts.");
        try {
            const chartDataJson = metricChartDataElement.textContent;
            console.log("Raw JSON string from script tag:", chartDataJson);
            const fullChartData = JSON.parse(chartDataJson);
            console.log('Parsed fullChartData object:', fullChartData);
            // Metadata needed for toggle logic
            const metadata = fullChartData ? fullChartData.metadata : null; 
            console.log('Checking fullChartData.metadata:', metadata);
            console.log('Checking fullChartData.funds:', fullChartData ? fullChartData.funds : 'fullChartData is null/undefined');
            if (metadata && fullChartData.funds && Object.keys(fullChartData.funds).length > 0) {
                console.log("Conditional check passed. Calling renderChartsAndTables...");
                // Render charts and tables (this now just shows/hides the container)
                renderChartsAndTables(
                    metricChartsArea,
                    fullChartData
                );
                // Now, attach the event listener if the toggle exists and data is available
                if (toggleSwitch && metadata.secondary_data_available) {
                     console.log("[main.js] Attaching toggle listener for Metric Page.");
                    toggleSwitch.addEventListener('change', (event) => {
                        const showSecondary = event.target.checked;
                        console.log(`[main.js Metric Page Toggle] Toggle changed. Show Secondary: ${showSecondary}`);
                        toggleSecondaryDataVisibility(showSecondary); // Call imported function
                    });
                } else if (toggleSwitch) {
                     console.log("[main.js] Toggle exists, but secondary data not available for Metric Page.");
                     toggleSwitch.disabled = true;
                } else {
                    console.log("[main.js] Toggle switch not found for Metric Page.");
                }
            } else {
                console.error('Parsed metric chart data is missing expected structure or funds are empty:', fullChartData);
                metricChartsArea.innerHTML = '<div class="alert alert-danger">Error: Invalid data structure or no fund data.</div>';
            }
        } catch (e) {
            console.error('Error processing metric chart data:', e);
            metricChartsArea.innerHTML = '<div class="alert alert-danger">Error loading chart data. Check console.</div>';
        }
    }
    // --- Fund Detail Page (Multiple Charts per Fund) ---    
    const fundChartDataElement = document.getElementById('fundChartData');
    const fundChartsArea = document.getElementById('fundChartsArea');
    if (fundChartDataElement && fundChartsArea) {
        console.log("Fund detail page detected. Initializing charts.");
        try {
            const fundChartDataJson = fundChartDataElement.textContent;
            const allChartData = JSON.parse(fundChartDataJson);
            console.log('Parsed fund chart data:', JSON.parse(JSON.stringify(allChartData)));
            // Check if any SP data is available *before* rendering
            const anySpDataAvailable = allChartData.some(chartInfo => 
                chartInfo.datasets && chartInfo.datasets.some(ds => ds.isSpData === true)
            );
            if (Array.isArray(allChartData)) { // Check if it's an array (even if empty)
                // Render charts first
                 renderFundCharts(fundChartsArea, allChartData);
                // Setup toggle based on data availability
                if (toggleSwitch) {
                    if (anySpDataAvailable) {
                         console.log("[main.js] Attaching toggle listener for Fund Detail Page.");
                        toggleSwitch.disabled = false;
                        toggleSwitch.parentElement.querySelector('label').textContent = 'Show SP Comparison Data';
                        toggleSwitch.addEventListener('change', (event) => {
                            const showSecondary = event.target.checked;
                            console.log(`[main.js Fund Detail Page Toggle] Toggle changed. Show SP: ${showSecondary}`);
                            toggleSecondaryDataVisibility(showSecondary); // Call imported function
                        });
                    } else {
                        console.log("[main.js] Fund Detail Page: No SP data available, disabling toggle.");
                        toggleSwitch.disabled = true;
                        toggleSwitch.checked = false;
                        toggleSwitch.parentElement.querySelector('label').textContent = 'Show SP Comparison Data (N/A)';
                    }
                } else {
                    console.log("[main.js] Toggle switch not found for Fund Detail Page.");
                }
            } else {
                 console.error('Parsed fund chart data is not an array or is invalid:', allChartData);
                fundChartsArea.innerHTML = '<div class="alert alert-danger">Error: Invalid chart data received.</div>';
            }
        } catch (e) {
            console.error('Error processing fund chart data:', e);
            fundChartsArea.innerHTML = '<div class="alert alert-danger">Error loading fund charts. Check console.</div>';
        }
    }
    // --- Securities Summary Page (Filterable & Sortable Table) ---
    const securitiesTable = document.getElementById('securities-table');
    if (securitiesTable) {
        console.log("Securities page table detected. Initializing client-side sorter (filtering is server-side).");
        // initSecurityTableFilter('securities-table'); // REMOVED: Filtering is now server-side
        initTableSorter('securities-table'); // Keep client-side sorting for instant feedback after load
    } else {
        // console.log("Securities table not found, skipping table features initialization.");
    }
    // --- Comparison Summary Page (Filterable & Sortable Table) ---
    const comparisonTable = document.getElementById('comparison-table');
    if (comparisonTable) {
        console.log("Comparison page table detected. Initializing sorter.");
        // Note: Filters are handled server-side via form submission for this table
        initTableSorter('comparison-table'); // Enable client-side sorting
    }
    // --- Fund Duration Details Page ---
    const fundDurationTable = document.getElementById('fund-duration-table');
    if (fundDurationTable) {
        console.log("Fund duration details page table detected. Initializing sorter.");
        initTableSorter('fund-duration-table'); 
    }
    // Add any other global initializations here
});
</file>

<file path="static/js/modules/charts/timeSeriesChart.js">
// This file contains the specific logic for creating and configuring time-series line charts
// using the Chart.js library. It's designed to be reusable for generating consistent charts
// across different metrics and funds.
// static/js/modules/charts/timeSeriesChart.js
// Encapsulates Chart.js configuration and rendering for multiple time series datasets
/**
 * Creates and renders a time series chart using Chart.js.
 * @param {string} canvasId - The ID of the canvas element.
 * @param {object} chartData - Data object containing labels, multiple datasets, metrics.
 * @param {string} metricName - Name of the overall metric (e.g., Duration).
 * @param {string} fundCode - Code of the specific fund.
 * @param {number | null} maxZScore - The maximum absolute Z-score for this fund (used in title).
 * @param {boolean} isMissingLatest - Flag indicating if the latest point is missing for any spread.
 */
export function createTimeSeriesChart(canvasId, chartData, metricName, fundCode, maxZScore, isMissingLatest) {
    const ctx = document.getElementById(canvasId).getContext('2d');
    if (!ctx) {
        console.error(`[createTimeSeriesChart] Failed to get 2D context for canvas ID: ${canvasId}`);
        return; // Exit if canvas context is not available
    }
    // --- Prepare Chart Title (Adjusted for different contexts) --- 
    let chartTitle = metricName; // Default to just the metric name
    if (fundCode) { // If fundCode is provided (called from metric page)
        let titleSuffix = maxZScore !== null ? `(Max Spread Z: ${maxZScore.toFixed(2)})` : '(Z-Score N/A)';
        if (isMissingLatest) {
            titleSuffix = "(MISSING LATEST DATA)";
        }
        chartTitle = `${metricName} for ${fundCode} ${titleSuffix}`; 
    }
    // If fundCode is null (called from fund page), title remains just metricName
    console.log(`[createTimeSeriesChart] Using chart title: "${chartTitle}" for canvas ${canvasId}`);
    // --- Prepare Chart Data & Styling --- 
    const datasets = chartData.datasets.map((ds, index) => {
        const isBenchmark = ds.label.includes('Benchmark'); // Basic check, refine if needed
        const isLastDataset = index === chartData.datasets.length - 1; // Check if it's the benchmark dataset based on order from app.py
        return {
            ...ds,
            // Style points - highlight last point for non-benchmark lines
            pointRadius: (context) => {
                const isLastPoint = context.dataIndex === (ds.data.length - 1);
                // Only show large radius for last point of non-benchmark datasets
                return isLastPoint && !isLastDataset ? 6 : 0;
            },
            pointHoverRadius: (context) => {
                const isLastPoint = context.dataIndex === (ds.data.length - 1);
                return isLastPoint && !isLastDataset ? 8 : 5;
            },
            pointBackgroundColor: isLastDataset ? 'darkgrey' : ds.borderColor, // Use border color for fund points, grey for benchmark
            borderWidth: isLastDataset ? 2 : 1.5, // Slightly thicker benchmark line
        };
    });
    // --- Chart Configuration --- 
    const config = {
        type: 'line',
        data: {
            labels: chartData.labels, // Dates as strings
            datasets: datasets // Now includes multiple fund series + benchmark
        },
        options: {
            responsive: true,
            maintainAspectRatio: false, 
            plugins: {
                title: { display: true, text: chartTitle, font: { size: 16 } },
                legend: { position: 'top' },
                tooltip: { 
                    mode: 'index', 
                    intersect: false, 
                }
            },
            hover: { mode: 'nearest', intersect: true },
            scales: {
                x: {
                    type: 'time',
                    time: {
                        unit: 'day',
                        tooltipFormat: 'MMM dd, yyyy',
                        displayFormats: { day: 'MMM dd', week: 'MMM dd yyyy', month: 'MMM yyyy' }
                    },
                    title: { display: true, text: 'Date' }
                },
                y: {
                    display: true,
                    title: { display: true, text: metricName },
                    // Dynamic scaling based on *all* datasets
                    suggestedMin: Math.min(...datasets.flatMap(ds => ds.data.filter(d => d !== null && !isNaN(d)))),
                    suggestedMax: Math.max(...datasets.flatMap(ds => ds.data.filter(d => d !== null && !isNaN(d))))
                }
            }
        }
    };
    // --- Create Chart Instance (with Error Handling) --- 
    try {
        // Check if a chart instance already exists on the canvas and destroy it
        let existingChart = Chart.getChart(canvasId);
        if (existingChart) {
            console.log(`[createTimeSeriesChart] Destroying existing chart on canvas ${canvasId}`);
            existingChart.destroy();
        }
        // Attempt to create the new chart
        console.log(`[createTimeSeriesChart] Attempting to create new Chart on canvas ${canvasId}`);
        const chartInstance = new Chart(ctx, config); // Store the instance
        // Log success *after* instantiation
        console.log(`[createTimeSeriesChart] Successfully created chart for "${chartTitle}" on ${canvasId}`);
        return chartInstance; // Return the created chart instance
    } catch (error) {
        // Log any error during chart instantiation
        console.error(`[createTimeSeriesChart] Error creating chart on canvas ${canvasId} for "${chartTitle}":`, error);
        // Optionally display an error message in the canvas container
        const errorP = document.createElement('p');
        errorP.textContent = `Error rendering chart: ${error.message}`;
        errorP.className = 'text-danger';
        // Attempt to add error message to parent, replacing canvas if needed
        const canvasElement = document.getElementById(canvasId);
        if (canvasElement && canvasElement.parentElement) {
            canvasElement.parentElement.appendChild(errorP);
            canvasElement.style.display = 'none'; // Hide broken canvas
        }    
    }
}
</file>

<file path="static/js/modules/ui/chartRenderer.js">
// This file is responsible for dynamically creating and rendering the user interface elements
// related to charts and associated metric tables within the application.
// It separates the logic for generating the visual components from the main application flow.
// Updated to handle optional secondary data sources and toggle visibility.
// static/js/modules/ui/chartRenderer.js
// Handles creating DOM elements for charts and tables
import { createTimeSeriesChart } from '../charts/timeSeriesChart.js';
import { formatNumber } from '../utils/helpers.js';
// Store chart instances to manage them later (e.g., for toggling)
const chartInstances = {};
/**
 * Renders charts and metric tables into the specified container.
 * Handles primary and optional secondary (S&P) data source toggle.
 * @param {HTMLElement} container - The parent element to render into.
 * @param {object} payload - The full data payload object from Flask (contains metadata and funds data).
 */
export function renderChartsAndTables(container, payload) {
    const metadata = payload.metadata;
    const fundsData = payload.funds; // Renamed from chartsData for clarity
    const metricName = metadata.metric_name;
    const latestDate = metadata.latest_date;
    // Keep original column names from metadata for table generation
    const primaryFundColsMeta = metadata.fund_col_names;
    const primaryBenchColMeta = metadata.benchmark_col_name;
    const secondaryFundColsMeta = metadata.secondary_fund_col_names;
    const secondaryBenchColMeta = metadata.secondary_benchmark_col_name;
    const secondaryDataAvailableOverall = metadata.secondary_data_available;
    console.log("[chartRenderer] Rendering charts for metric:", metricName, "Latest Date:", latestDate);
    console.log("[chartRenderer] Metadata:", metadata);
    console.log("[chartRenderer] Fund Data Keys:", Object.keys(fundsData || {}));
    // Clear previous content and chart instances
    container.innerHTML = '';
    Object.keys(chartInstances).forEach(key => {
        try {
            chartInstances[key]?.destroy(); // Properly destroy old chart instances
        } catch (e) {
            console.warn(`Error destroying chart instance ${key}:`, e);
        }
        delete chartInstances[key];
    });
    if (!fundsData || Object.keys(fundsData).length === 0) {
        console.warn("[chartRenderer] No fund data available for metric:", metricName);
        container.innerHTML = '<p>No fund data available for this metric.</p>';
        return;
    }
    // --- Setup Toggle Switch --- 
    const toggleContainer = document.getElementById('sp-toggle-container');
    // Event listener will be attached by the caller (main.js)
    if (toggleContainer) {
        // Show toggle if *any* secondary data is potentially available based on metadata
        if (secondaryDataAvailableOverall) {
            console.log("[chartRenderer] Overall secondary data available, ensuring toggle container is visible.");
            toggleContainer.style.display = 'block'; 
        } else {
            console.log("[chartRenderer] Overall secondary data not available, ensuring toggle container is hidden.");
            toggleContainer.style.display = 'none'; 
        }
    } else {
        console.warn("[chartRenderer] Toggle switch container not found in the DOM.");
    }
    // --- Render Charts and Tables for Each Fund --- 
    for (const [fundCode, fundData] of Object.entries(fundsData)) {
        console.log(`[chartRenderer] Processing fund: ${fundCode}`);
        const charts = fundData.charts || [];
        const isMissingLatest = fundData.is_missing_latest;
        // Find max absolute Z-score from PRIMARY MAIN metrics for section highlight
        let maxAbsPrimaryZScore = 0;
        let primaryZScoreForTitle = null;
        const mainChartConfig = charts.find(c => c.chart_type === 'main');
        if (mainChartConfig && mainChartConfig.latest_metrics) {
            const mainMetrics = mainChartConfig.latest_metrics;
            const primaryColsToCheck = [];
            if (primaryBenchColMeta) primaryColsToCheck.push(primaryBenchColMeta);
            if (primaryFundColsMeta && Array.isArray(primaryFundColsMeta)) primaryColsToCheck.push(...primaryFundColsMeta);
            primaryColsToCheck.forEach(colName => {
                if (!colName) return;
                const zScoreKey = `${colName} Change Z-Score`; 
                const zScore = mainMetrics[zScoreKey];
                 if (zScore !== null && typeof zScore !== 'undefined' && !isNaN(zScore)) {
                     const absZ = Math.abs(zScore);
                     if (absZ > maxAbsPrimaryZScore) {
                         maxAbsPrimaryZScore = absZ;
                         primaryZScoreForTitle = zScore; 
                     }
                 }
            });
        }
        // Determine CSS class for the wrapper based on primary Z-score
        let zClass = '';
        if (maxAbsPrimaryZScore > 3) { zClass = 'very-high-z'; }
        else if (maxAbsPrimaryZScore > 2) { zClass = 'high-z'; }
        // Create a main wrapper for the fund
        const fundWrapper = document.createElement('div');
        fundWrapper.className = `fund-wrapper ${zClass}`; // Use a different class for the outer wrapper
        fundWrapper.id = `fund-wrapper-${fundCode}`;
        // Add Duration Details Link (if applicable) - Moved to fund level
        if (metricName === 'Duration') {
            const linkDiv = document.createElement('div');
            linkDiv.className = 'mb-2 text-end';
            const link = document.createElement('a');
            link.href = `/fund/duration_details/${fundCode}`;
            link.className = 'btn btn-info btn-sm';
            link.textContent = `View Security Duration Changes for ${fundCode} →`;
            linkDiv.appendChild(link);
            fundWrapper.appendChild(linkDiv);
        }
        // Create a row container for the charts within this fund
        const chartsRow = document.createElement('div');
        chartsRow.className = 'row'; // Bootstrap row class
        // Now loop through the charts for this fund (Relative first, then Main)
        charts.forEach(chartConfig => {
            const chartType = chartConfig.chart_type;
            const chartTitle = chartConfig.title;
            const chartLabels = chartConfig.labels;
            const chartDatasets = chartConfig.datasets;
            const chartMetrics = chartConfig.latest_metrics;
            const chartId = `${fundCode}-${chartType}`;
            console.log(`[chartRenderer] Creating elements for chart: ${chartId}`);
             // --- Create DOM Elements for Each Chart --- 
            const chartWrapper = document.createElement('div');
            // Add column class for side-by-side layout on large screens
            chartWrapper.className = `chart-container-wrapper chart-type-${chartType} col-lg-6`; 
            chartWrapper.id = `chart-wrapper-${chartId}`;
        // Create Chart Canvas
        const canvas = document.createElement('canvas');
            canvas.id = `chart-${chartId}`;
        canvas.className = 'chart-canvas';
            chartWrapper.appendChild(canvas);
            // Create Metrics Table (pass specific metrics and chart type)
        const table = createMetricsTable(
                chartMetrics,
            latestDate,
                chartType, // Pass chart type to determine columns
                metadata // Pass full metadata for context
            );
            chartWrapper.appendChild(table);
            // Append chartWrapper to the row, not the fundWrapper directly
            chartsRow.appendChild(chartWrapper); 
        // --- Render Chart --- 
        setTimeout(() => {
            const chartCanvas = document.getElementById(canvas.id);
             if (chartCanvas && chartCanvas.getContext('2d')) {
                    console.log(`[chartRenderer] Rendering chart for ${chartId}`);
                    // Prepare chart data object for the charting function
                    const chartDataForFunction = {
                        labels: chartLabels,
                        datasets: chartDatasets
                    };
                    // Pass specific Z-score ONLY if it's the main chart
                    const zScoreForChartTitle = (chartType === 'main') ? primaryZScoreForTitle : null;
                    const chart = createTimeSeriesChart(
                        canvas.id, 
                        chartDataForFunction, // Pass the structured data
                        chartTitle, // Use the title from config
                        fundCode, // Keep fund code for context if needed
                        zScoreForChartTitle, // Pass main Z-score only to main chart
                        isMissingLatest // Still relevant at fund level
                    );
                 if (chart) {
                        chartInstances[chartId] = chart; // Store chart instance with unique ID
                        console.log(`[chartRenderer] Stored chart instance for ${chartId}`);
                 } else {
                        console.error(`[chartRenderer] Failed to create chart instance for ${chartId}`);
                 }
            } else {
                console.error(`[chartRenderer] Could not get 2D context for canvas ${canvas.id}`);
                const errorP = document.createElement('p');
                errorP.textContent = 'Error rendering chart.';
                errorP.className = 'text-danger';
                if (chartCanvas && chartCanvas.parentNode) {
                    chartCanvas.parentNode.replaceChild(errorP, chartCanvas);
                    } else if (chartWrapper) {
                        chartWrapper.appendChild(errorP);
                }
            }
        }, 0); 
        }); // End loop through charts for the fund
        // Append the row containing the charts to the main fund wrapper
        fundWrapper.appendChild(chartsRow);
        container.appendChild(fundWrapper); // Add the fund's wrapper to the main container
    } // End loop through funds
    console.log("[chartRenderer] Finished processing all funds.");
}
/**
 * Updates the visibility of secondary/SP data datasets across all managed charts.
 * @param {boolean} show - Whether to show or hide the secondary/SP datasets.
 */
export function toggleSecondaryDataVisibility(show) { // Make sure this is exported if used by main.js
    console.log(`[chartRenderer] Toggling SP data visibility to: ${show}`);
    // Iterate through the centrally stored chart instances
    Object.entries(chartInstances).forEach(([chartId, chart]) => {
        if (!chart || typeof chart.destroy === 'undefined') { // Check if chart instance is valid
            console.warn(`[chartRenderer] Skipping invalid chart instance for ID: ${chartId}`);
            return;
        }
        let spDatasetToggled = false;
        try {
        chart.data.datasets.forEach((dataset, index) => {
            // Check the isSpData flag added from Python
            if (dataset.isSpData === true) {
                // Use setDatasetVisibility for better control than just 'hidden' property
                chart.setDatasetVisibility(index, show);
                    console.log(`[chartRenderer] Chart ${chart.canvas.id} (${chartId}) - Setting SP dataset ${index} ('${dataset.label}') visibility to ${show}`);
                spDatasetToggled = true;
            }
        });
        // Only update if an SP dataset was actually toggled for this chart
        if (spDatasetToggled) {
            chart.update(); // Update the chart to reflect visibility changes
                console.log(`[chartRenderer] Updated chart ${chart.canvas.id} (${chartId})`);
            }
        } catch (error) {
            console.error(`[chartRenderer] Error toggling visibility for chart ${chartId}:`, error);
             // Potentially remove the instance if it's causing persistent errors?
            // delete chartInstances[chartId]; 
        }
    });
}
/**
 * Creates the HTML table element displaying metrics for a specific chart.
 *
 * @param {object | null} metrics - Metrics object specific to this chart (relative or main).
 * @param {string} latestDate - The latest date string.
 * @param {string} chartType - 'relative' or 'main'.
 * @param {object} metadata - The overall metadata object from Flask (for column names).
 * @returns {HTMLTableElement} The created table element.
 */
function createMetricsTable(
    metrics, 
    latestDate, 
    chartType, 
    metadata 
) {
    const table = document.createElement('table');
    table.className = 'table table-sm table-bordered metrics-table';
    const thead = table.createTHead();
    const headerRow = thead.insertRow();
    const tbody = table.createTBody();
    const secondaryAvailable = metadata.secondary_data_available; // Overall flag
    const primaryFundColsMeta = metadata.fund_col_names || [];
    const primaryBenchColMeta = metadata.benchmark_col_name;
    const secondaryFundColsMeta = metadata.secondary_fund_col_names || [];
    const secondaryBenchColMeta = metadata.secondary_benchmark_col_name;
    const secondaryPrefix = "S&P ";
    if (!metrics || Object.keys(metrics).length === 0) {
        console.warn(`[createMetricsTable] Metrics object is null or empty for chart type: ${chartType}.`);
        headerRow.innerHTML = '<th>Metrics</th>'; // Simple header
        const row = tbody.insertRow();
        const cell = row.insertCell();
        cell.textContent = 'Metrics not available.';
        return table;
    }
    // --- Define Headers based on Chart Type --- 
    let headers = ['Column', `Latest Value (${latestDate})`, 'Change', 'Mean', 'Max', 'Min', 'Change Z-Score'];
    let secondaryHeaders = ['S&P Latest', 'S&P Change', 'S&P Mean', 'S&P Max', 'S&P Min', 'S&P Z-Score'];
    let showSecondaryColumns = false;
    if (chartType === 'relative') {
        // Check if any S&P Relative metrics actually exist
        showSecondaryColumns = Object.keys(metrics).some(key => key.startsWith(secondaryPrefix + 'Relative '));
    } else { // chartType === 'main'
        // Check if any regular S&P metrics exist (excluding relative)
        showSecondaryColumns = Object.keys(metrics).some(key => key.startsWith(secondaryPrefix) && !key.startsWith(secondaryPrefix + 'Relative '));
    }
    headerRow.innerHTML = `<th>${headers.join('</th><th>')}</th>` + 
                         (showSecondaryColumns ? `<th class="text-muted">${secondaryHeaders.join('</th><th class="text-muted">')}</th>` : '');
    // --- Populate Rows based on Chart Type --- 
    const addRow = (displayName, baseKey, isSecondary = false) => {
        const prefix = isSecondary ? secondaryPrefix : '';
        const fullBaseKey = prefix + baseKey;
        // Check if *any* metric exists for this base key and prefix
        const latestValKey = `${fullBaseKey} Latest Value`;
        const changeKey = `${fullBaseKey} Change`;
        const meanKey = `${fullBaseKey} Mean`;
        const maxKey = `${fullBaseKey} Max`;
        const minKey = `${fullBaseKey} Min`;
        const zScoreKey = `${fullBaseKey} Change Z-Score`;
        // Only add row if at least one relevant metric is present
        if (
            metrics.hasOwnProperty(latestValKey) || metrics.hasOwnProperty(changeKey) || 
            metrics.hasOwnProperty(meanKey) || metrics.hasOwnProperty(maxKey) || 
            metrics.hasOwnProperty(minKey) || metrics.hasOwnProperty(zScoreKey)
        ) {
            const row = tbody.insertRow();
            const zScore = metrics[zScoreKey];
            let zClass = '';
            if (zScore !== null && typeof zScore !== 'undefined' && !isNaN(zScore)) {
                const absZ = Math.abs(zScore);
                if (absZ > 3) { zClass = 'very-high-z'; }
                else if (absZ > 2) { zClass = 'high-z'; }
        }
            row.className = zClass;
            row.insertCell().textContent = displayName;
            row.insertCell().textContent = formatNumber(metrics[latestValKey]);
            row.insertCell().textContent = formatNumber(metrics[changeKey]);
            row.insertCell().textContent = formatNumber(metrics[meanKey]);
            row.insertCell().textContent = formatNumber(metrics[maxKey]);
            row.insertCell().textContent = formatNumber(metrics[minKey]);
            row.insertCell().textContent = formatNumber(metrics[zScoreKey]);
            if (showSecondaryColumns && !isSecondary) {
                // Add placeholder cells if primary row but secondary columns shown
                for (let i = 0; i < secondaryHeaders.length; i++) {
                    row.insertCell().textContent = '-';
                }
            }
        } else if (isSecondary && showSecondaryColumns) {
            // Add secondary row even if primary version doesn't exist, but only if secondary columns are shown
            const row = tbody.insertRow();
            row.insertCell().textContent = displayName;
            row.insertCell().textContent = formatNumber(metrics[latestValKey]);
            row.insertCell().textContent = formatNumber(metrics[changeKey]);
            row.insertCell().textContent = formatNumber(metrics[meanKey]);
            row.insertCell().textContent = formatNumber(metrics[maxKey]);
            row.insertCell().textContent = formatNumber(metrics[minKey]);
            row.insertCell().textContent = formatNumber(metrics[zScoreKey]);
            // Add empty primary cells
             for (let i = 0; i < headers.length -1; i++) { // -1 for the name column
                 row.insertCell(1).textContent = '-'; // Insert after name
             }
             row.className = 'text-muted'; // Mute the secondary row
        }
    };
    const addPairedRow = (displayName, baseKey) => {
        const primaryExists = Object.keys(metrics).some(k => k.startsWith(baseKey) && !k.startsWith(secondaryPrefix));
        const secondaryExists = showSecondaryColumns && Object.keys(metrics).some(k => k.startsWith(secondaryPrefix + baseKey));
        if (primaryExists || secondaryExists) {
            const row = tbody.insertRow();
            const zScoreKey = `${baseKey} Change Z-Score`;
            const zScore = metrics[zScoreKey]; // Use primary Z for highlight
            let zClass = '';
            if (zScore !== null && typeof zScore !== 'undefined' && !isNaN(zScore)) {
                const absZ = Math.abs(zScore);
                if (absZ > 3) { zClass = 'very-high-z'; }
                else if (absZ > 2) { zClass = 'high-z'; }
                }
            row.className = zClass;
            row.insertCell().textContent = displayName;
            // Primary Metrics
            row.insertCell().textContent = primaryExists ? formatNumber(metrics[`${baseKey} Latest Value`]) : '-';
            row.insertCell().textContent = primaryExists ? formatNumber(metrics[`${baseKey} Change`]) : '-';
            row.insertCell().textContent = primaryExists ? formatNumber(metrics[`${baseKey} Mean`]) : '-';
            row.insertCell().textContent = primaryExists ? formatNumber(metrics[`${baseKey} Max`]) : '-';
            row.insertCell().textContent = primaryExists ? formatNumber(metrics[`${baseKey} Min`]) : '-';
            row.insertCell().textContent = primaryExists ? formatNumber(metrics[zScoreKey]) : '-';
            // Secondary Metrics (if columns are shown)
            if (showSecondaryColumns) {
                row.insertCell().textContent = secondaryExists ? formatNumber(metrics[`${secondaryPrefix}${baseKey} Latest Value`]) : '-';
                row.insertCell().textContent = secondaryExists ? formatNumber(metrics[`${secondaryPrefix}${baseKey} Change`]) : '-';
                row.insertCell().textContent = secondaryExists ? formatNumber(metrics[`${secondaryPrefix}${baseKey} Mean`]) : '-';
                row.insertCell().textContent = secondaryExists ? formatNumber(metrics[`${secondaryPrefix}${baseKey} Max`]) : '-';
                row.insertCell().textContent = secondaryExists ? formatNumber(metrics[`${secondaryPrefix}${baseKey} Min`]) : '-';
                row.insertCell().textContent = secondaryExists ? formatNumber(metrics[`${secondaryPrefix}${baseKey} Change Z-Score`]) : '-';
            }
        }
    };
    if (chartType === 'relative') {
        // Add row specifically for 'Relative' if its metrics exist
        addPairedRow('Relative (Port - Bench)', 'Relative');
    } else { // chartType === 'main'
        // Add Benchmark row first if it exists
        if (primaryBenchColMeta) {
             addPairedRow(primaryBenchColMeta, primaryBenchColMeta);
        }
        // Add rows for Fund columns
        primaryFundColsMeta.forEach(fundCol => {
            addPairedRow(fundCol, fundCol);
        });
    }
    // Ensure tbody is not empty, add placeholder if needed
    if (tbody.rows.length === 0) {
        const row = tbody.insertRow();
        const cell = row.insertCell();
        cell.colSpan = headers.length + (showSecondaryColumns ? secondaryHeaders.length : 0);
        cell.textContent = 'No relevant metrics found for this chart.';
    }
    return table;
} 
/**
 * Renders a single time series chart for a specific security.
 * @param {string} canvasId - The ID of the canvas element.
 * @param {object} chartData - The chart data (labels, datasets) from Flask.
 * @param {string} securityId - The ID of the security.
 * @param {string} metricName - The name of the metric.
 */
export function renderSingleSecurityChart(canvasId, chartData, securityId, metricName) {
    const ctx = document.getElementById(canvasId);
    if (!ctx) {
        console.error(`Canvas element with ID '${canvasId}' not found.`);
        return;
    }
    if (!chartData || !chartData.labels || !chartData.datasets) {
        console.error('Invalid or incomplete chart data provided.');
        ctx.parentElement.innerHTML = '<p class="text-danger">Error: Invalid chart data.</p>';
        return;
    }
    try {
        new Chart(ctx, {
            type: 'line',
            data: {
                labels: chartData.labels,
                datasets: chartData.datasets
            },
            options: {
                responsive: true,
                maintainAspectRatio: false,
                plugins: {
                    title: {
                        display: true,
                        text: `${securityId} - ${metricName} Time Series`,
                        font: { size: 16 }
                    },
                    legend: {
                        position: 'top',
                    }
                },
                scales: {
                    x: {
                        title: {
                            display: true,
                            text: 'Date'
                        }
                    },
                    y: {
                        type: 'linear',
                        display: true,
                        position: 'left',
                        title: {
                            display: true,
                            text: `${metricName} Value`
                        },
                        beginAtZero: false,
                        ticks: {
                            maxTicksLimit: 8
                        }
                    },
                    y1: {
                        type: 'linear',
                        display: true,
                        position: 'right',
                        title: {
                            display: true,
                            text: 'Price'
                        },
                        grid: {
                            drawOnChartArea: false,
                        },
                        beginAtZero: false
                    }
                },
                interaction: {
                    intersect: false,
                    mode: 'index',
                },
            }
        });
        console.log(`Chart rendered for ${securityId} - ${metricName}`);
    } catch (error) {
        console.error(`Error creating chart for ${securityId} - ${metricName}:`, error);
        ctx.parentElement.innerHTML = '<p class="text-danger">Error rendering chart.</p>';
    }
} 
/**
 * Renders multiple charts onto a single page (like the Fund Detail page).
 * Stores created chart instances in the module-level 'chartInstances' object.
 * @param {HTMLElement} container - The parent element to render into.
 * @param {Array<object>} allChartData - An array of chart data objects, each with metricName, labels, datasets.
 */
export function renderFundCharts(container, allChartData) {
    console.log("[chartRenderer] Rendering charts for fund detail page.");
    console.log("[chartRenderer] Received Data:", JSON.parse(JSON.stringify(allChartData))); // Deep copy for logging
    container.innerHTML = ''; // Clear previous content
    // Clear previous chart instances for this specific rendering context
    Object.keys(chartInstances).forEach(key => delete chartInstances[key]); 
    if (!allChartData || !Array.isArray(allChartData) || allChartData.length === 0) {
        console.warn("[chartRenderer] No chart data provided for the fund page.");
        // Message should be handled by the template, but log it here.
        return;
    }
    // Iterate through each metric's chart data
    allChartData.forEach((metricData, index) => {
        if (!metricData || !metricData.metricName || !metricData.labels || !metricData.datasets) {
            console.warn(`[chartRenderer] Skipping chart at index ${index} due to missing data:`, metricData);
            return;
        }
        const metricName = metricData.metricName;
        const safeMetricName = metricName.replace(/[^a-zA-Z0-9]/g, '-') || 'metric'; // Create a CSS-safe ID part
        console.log(`[chartRenderer] Processing metric: ${metricName}`);
        // Create wrapper div for each chart (using Bootstrap columns for layout)
        const wrapper = document.createElement('div');
        // Uses the col classes defined in the template's fundChartsArea (row-cols-1 row-cols-lg-2)
        wrapper.className = `chart-container-wrapper fund-chart-item`; 
        wrapper.id = `fund-chart-wrapper-${safeMetricName}-${index}`;
        // Create Chart Canvas
        const canvas = document.createElement('canvas');
        // Ensure unique ID for each canvas
        canvas.id = `fund-chart-${safeMetricName}-${index}`; 
        canvas.className = 'chart-canvas';
        wrapper.appendChild(canvas);
        console.log(`[chartRenderer] Created canvas with id: ${canvas.id} for metric: ${metricName}`);
        // Append the wrapper to the main container
        container.appendChild(wrapper);
        console.log(`[chartRenderer] Appended wrapper for ${metricName} to container.`);
        // Render Chart using the existing time series chart function
        // Use setTimeout to ensure the canvas is in the DOM and sized
        setTimeout(() => {
            console.log(`[chartRenderer] Preparing to render chart for metric: ${metricName} in setTimeout.`);
             if (canvas.getContext('2d')) {
                 console.log(`[chartRenderer] Canvas context obtained. Calling createTimeSeriesChart with:`, {
                    canvasId: canvas.id,
                    data: JSON.parse(JSON.stringify(metricData)), // Log deep copy
                    titlePrefix: metricName, // Use metric name as the main title part
                    fundCodeOrSecurityId: null, // Not needed for title here
                    zScoreForTitle: null, // No specific Z-score for the whole page/chart
                    is_missing_latest: null // Not applicable here
                 });
                 // Create the chart AND store the instance
                 const chartInstance = createTimeSeriesChart(
                     canvas.id,         // The unique canvas ID
                     metricData,        // Data object with labels and datasets
                     metricName,        // Title prefix (e.g., "Yield")
                     null,              // fundCodeOrSecurityId (not needed for title)
                     null,              // zScoreForTitle (not applicable)
                     null               // is_missing_latest (not applicable)
                 );
                 if (chartInstance) {
                     // Store the instance in the module-level object
                     chartInstances[canvas.id] = chartInstance;
                     console.log(`[chartRenderer] Stored chart instance for ${metricName} with key ${canvas.id}`);
                 } else {
                     console.error(`[chartRenderer] Failed to get chart instance for metric: ${metricName}`);
                 }
            } else {
                console.error(`[chartRenderer] Could not get 2D context for canvas ${canvas.id} (Metric: ${metricName})`);
                const errorP = document.createElement('p');
                errorP.textContent = `Error rendering chart for ${metricName}.`;
                errorP.className = 'text-danger';
                canvas.parentNode.replaceChild(errorP, canvas); // Replace canvas with error message
            }
        }, 0); 
    });
    console.log("[chartRenderer] Finished rendering all fund charts.");
} 
// Export necessary functions
// REMOVED: export { toggleSecondaryDataVisibility }; // Export toggle function
</file>

<file path="static/js/modules/ui/securityTableFilter.js">
// This file implements client-side filtering for the HTML table displaying security-level metrics.
// It enhances the user experience by allowing interactive filtering based on the values
// in specific static columns (e.g., Sector, Rating) without requiring a page reload.
// static/js/modules/ui/securityTableFilter.js
// This module handles client-side filtering for the securities table.
/**
 * Initializes the filtering functionality for the securities table.
 */
export function initSecurityTableFilter() {
    const filterSelects = document.querySelectorAll('.security-filter-select');
    const tableBody = document.getElementById('securities-table-body');
    if (!tableBody || filterSelects.length === 0) {
        console.log("Security table body or filter selects not found. Filtering disabled.");
        return; // Exit if necessary elements aren't present
    }
    // Store all original rows. Use querySelectorAll for robustness.
    const originalRows = Array.from(tableBody.querySelectorAll('tr'));
    if (originalRows.length === 0) {
        console.log("No rows found in the table body.");
        return; // Exit if no data rows
    }
    // Function to get current filter values
    const getCurrentFilters = () => {
        const filters = {};
        filterSelects.forEach(select => {
            if (select.value) { // Only add if a filter is selected (not 'All')
                filters[select.dataset.column] = select.value;
            }
        });
        return filters;
    };
    // Function to perform filtering and update the table
    const applyFilters = () => {
        const currentFilters = getCurrentFilters();
        const filterKeys = Object.keys(currentFilters);
        // Clear current table body content efficiently
        tableBody.innerHTML = ''; 
        originalRows.forEach(row => {
            let matches = true;
            // Get all cells in the current row
            const cells = row.querySelectorAll('td');
            // Assuming the order of cells matches the order of `column_order` from Python
            // We need a way to map filter column names to cell indices
            // Let's get the header names to map column names to indices
            const headerCells = document.querySelectorAll('#securities-table th');
            const columnNameToIndexMap = {};
            headerCells.forEach((th, index) => {
                columnNameToIndexMap[th.textContent.trim()] = index;
            });
            for (const column of filterKeys) {
                const columnIndex = columnNameToIndexMap[column];
                if (columnIndex !== undefined) {
                    const cellValue = cells[columnIndex]?.textContent.trim(); // Use optional chaining
                    // Strict comparison - ensure types match if needed, or use == for type coercion
                    if (cellValue !== currentFilters[column]) {
                        matches = false;
                        break; // No need to check other filters for this row
                    }
                }
                 else {
                      console.warn(`Column "${column}" not found in table header for filtering.`);
                      // Decide how to handle: skip filter, always fail match? Let's skip filter for robustness.
                 }
            }
            if (matches) {
                // Append the row if it matches all active filters
                tableBody.appendChild(row.cloneNode(true)); // Append a clone to avoid issues
            }
        });
        // Display a message if no rows match
        if (tableBody.children.length === 0) {
             const noMatchRow = tableBody.insertRow();
             const cell = noMatchRow.insertCell();
             cell.colSpan = headerCells.length; // Span across all columns
             cell.textContent = 'No securities match the current filter criteria.';
             cell.style.textAlign = 'center';
             cell.style.fontStyle = 'italic';
        }
    };
    // Add event listeners to all filter dropdowns
    filterSelects.forEach(select => {
        select.addEventListener('change', applyFilters);
    });
    console.log("Security table filtering initialized.");
}
</file>

<file path="static/js/modules/ui/tableSorter.js">
// static/js/modules/ui/tableSorter.js
// Purpose: Handles client-side sorting for HTML tables.
/**
 * Initializes sorting functionality for a specified table.
 * @param {string} tableId The ID of the table element to make sortable.
 */
export function initTableSorter(tableId) {
    const table = document.getElementById(tableId);
    if (!table) {
        console.warn(`Table sorter: Table with ID '${tableId}' not found.`);
        return;
    }
    const headers = table.querySelectorAll('thead th.sortable');
    const tbody = table.querySelector('tbody');
    if (!tbody) {
        console.warn(`Table sorter: Table with ID '${tableId}' does not have a tbody.`);
        return;
    }
    headers.forEach(header => {
        header.addEventListener('click', () => {
            // Get column name from data attribute
            const columnName = header.dataset.columnName;
            const currentIsAscending = header.classList.contains('sort-asc');
            const direction = currentIsAscending ? -1 : 1; // -1 for desc, 1 for asc
            // Find the index of the clicked column
            const columnIndex = Array.from(header.parentNode.children).indexOf(header);
            // Remove sorting indicators from other columns
            headers.forEach(h => {
                if (h !== header) {
                  h.classList.remove('sort-asc', 'sort-desc');
                }
            });
            // Set sorting indicator for the current column
            header.classList.toggle('sort-asc', !currentIsAscending);
            header.classList.toggle('sort-desc', currentIsAscending);
            // Sort the rows, passing the column name
            sortRows(tbody, columnIndex, direction, columnName);
        });
    });
    // --- Default Sort --- 
    const defaultSortHeader = table.querySelector('thead th[data-sort-default]');
    if (defaultSortHeader) {
        const defaultDirection = defaultSortHeader.dataset.sortDefault === 'asc' ? 1 : -1;
        const defaultColumnIndex = Array.from(defaultSortHeader.parentNode.children).indexOf(defaultSortHeader);
        const defaultColumnName = defaultSortHeader.dataset.columnName;
        console.log(`Table sorter: Applying default sort on column '${defaultColumnName}' (index ${defaultColumnIndex}), direction ${defaultDirection === 1 ? 'asc' : 'desc'}`);
        // Set initial visual indicators
        if (defaultDirection === 1) {
            defaultSortHeader.classList.add('sort-asc');
        } else {
            defaultSortHeader.classList.add('sort-desc');
        }
        // Perform the initial sort
        sortRows(tbody, defaultColumnIndex, defaultDirection, defaultColumnName);
    } else {
        console.log(`Table sorter: No default sort specified for table '${tableId}'.`);
    }
}
/**
 * Sorts the rows within a table body.
 * @param {HTMLElement} tbody The table body element containing the rows.
 * @param {number} columnIndex The index of the column to sort by.
 * @param {number} direction 1 for ascending, -1 for descending.
 * @param {string} columnName The name of the column being sorted.
 */
function sortRows(tbody, columnIndex, direction, columnName) {
    const rows = Array.from(tbody.querySelectorAll('tr'));
    // Get the correct comparison function, passing the column name
    const compareFunction = getCompareFunction(rows, columnIndex, columnName);
    // Sort the rows
    rows.sort((rowA, rowB) => {
        const cellA = rowA.children[columnIndex];
        const cellB = rowB.children[columnIndex];
        // Use data-value attribute primarily, fall back to textContent
        const valueA = cellA?.dataset.value ?? cellA?.textContent?.trim() ?? '';
        const valueB = cellB?.dataset.value ?? cellB?.textContent?.trim() ?? '';
        return compareFunction(valueA, valueB) * direction;
    });
    // Re-append sorted rows
    tbody.append(...rows); // More efficient way to re-append
}
/**
 * Determines the appropriate comparison function (numeric or text) based on column content.
 * @param {Array<HTMLElement>} rows Array of table row elements.
 * @param {number} columnIndex The index of the column to check.
 * @param {string} columnName The name of the column being sorted.
 * @returns {function(string, string): number} The comparison function.
 */
function getCompareFunction(rows, columnIndex, columnName) {
    // Check the first few rows (up to 5 data rows) to guess the data type
    let isNumeric = true;
    for (let i = 0; i < Math.min(rows.length, 5); i++) {
        const cell = rows[i].children[columnIndex];
        // Use data-value attribute primarily for checking type
        const value = cell?.dataset.value ?? cell?.textContent?.trim() ?? '';
        // Allow empty strings in numeric columns, but if we find something non-numeric (and not empty), switch to text sort
        if (value !== '' && isNaN(Number(value.replace(/,/g, '')))) {
            isNumeric = false;
            break;
        }
    }
    if (isNumeric) {
        // Check if it's the special column 'Change Z-Score'
        if (columnName === 'Change Z-Score') {
             // Use absolute value for comparison
            return (a, b) => {
                const numA = Math.abs(parseNumber(a));
                const numB = Math.abs(parseNumber(b));
                return numA - numB;
            };
        } else {
            // Standard numeric comparison for other numeric columns
            return (a, b) => {
                const numA = parseNumber(a);
                const numB = parseNumber(b);
                return numA - numB;
            };
        }
    } else {
        // Case-insensitive text comparison
        return (a, b) => a.toLowerCase().localeCompare(b.toLowerCase());
    }
}
/**
 * Helper to parse number, handling empty strings and NaN.
 * Returns -Infinity for values that cannot be parsed as numbers or are empty,
 * ensuring they sort consistently.
 * @param {string} val The string value to parse.
 * @returns {number}
 */
function parseNumber(val) {
    if (val === null || val === undefined || val.trim() === '') {
        return -Infinity; // Treat empty/null/undefined as very small
    }
    const num = Number(val.replace(/,/g, ''));
    // Treat non-numeric as very small. Math.abs(-Infinity) is Infinity, which might be desired
    // when sorting absolute values (non-numbers/empty go to the end when ascending by abs value).
    return isNaN(num) ? -Infinity : num;
}
</file>

<file path="static/js/modules/utils/helpers.js">
// This file contains general JavaScript utility functions that can be reused across different modules.
// It helps keep common tasks, like formatting numbers for display, consistent and DRY (Don't Repeat Yourself).
// static/js/modules/utils/helpers.js
// Utility functions
/**
 * Formats a number for display, handling null/undefined.
 * @param {number | null | undefined} value - The number to format.
 * @param {number} [digits=2] - Number of decimal places.
 * @returns {string} Formatted number or 'N/A'.
 */
export function formatNumber(value, digits = 2) {
    if (value === null || typeof value === 'undefined' || isNaN(value)) {
        return 'N/A';
    }
    return Number(value).toFixed(digits);
}
</file>

<file path="templates/base.html">
<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>{% block title %}Data Checker{% endblock %}</title>
    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/css/bootstrap.min.css" rel="stylesheet">
    <style>
        /* Basic styles - can be expanded */
        body { padding-top: 6rem; } /* Increased top padding */
        .sticky-top {
            top: 56px; /* Adjust based on navbar height */
        }
        /* Add any custom global styles here */
        .table-danger {
            background-color: #f8d7da !important; /* Red for high Z */
        }
        .table-warning {
            background-color: #fff3cd !important; /* Yellow for medium Z */
        }
        /* Give chart canvases a default aspect ratio */
        .chart-canvas {
            aspect-ratio: 16 / 9; /* Default widescreen aspect ratio */
            width: 100%; /* Ensure it fills container width */
            max-width: 100%; /* Prevent overflow */
            min-height: 250px; /* Optional: Ensure a minimum height */
        }
        /* Navbar brand adjustments */
        .navbar-brand {
            display: flex; /* Use flexbox for alignment */
            align-items: center; /* Vertically center items */
            font-size: 1.5rem; /* Increase font size */
        }
        .navbar-brand img {
            height: 50px; /* Reduced logo height */
            margin-right: 0.5rem; /* Space between logo and text */
        }
    </style>
</head>
<body>
    <nav class="navbar navbar-expand-md navbar-dark bg-dark fixed-top">
        <div class="container-fluid">
            <a class="navbar-brand" href="{{ url_for('main.index') }}">
                <img src="{{ url_for('static', filename='images/bang.jpg') }}" alt="Logo">
                Data Checker
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarCollapse">
                <ul class="navbar-nav me-auto mb-2 mb-md-0">
                    <li class="nav-item">
                        <a class="nav-link" href="{{ url_for('main.index') }}">Time Series Dashboard</a>
                    </li>
                    <li class="nav-item dropdown">
                        <a class="nav-link dropdown-toggle" href="#" id="navbarDropdownChecks" role="button" data-bs-toggle="dropdown" aria-expanded="false">
                            Checks & Comparisons
                        </a>
                        <ul class="dropdown-menu" aria-labelledby="navbarDropdownChecks">
                            <li><a class="dropdown-item" href="{{ url_for('security.securities_page') }}">Securities Check</a></li>
                            <li><a class="dropdown-item" href="{{ url_for('weight.weight_check') }}">Weight Check</a></li>
                            <li><a class="dropdown-item" href="{{ url_for('curve_bp.curve_summary') }}">Yield Curve Check</a></li>
                            <li><hr class="dropdown-divider"></li>
                            <li><h6 class="dropdown-header">Comparisons</h6></li>
                            <li><a class="dropdown-item" href="{{ url_for('comparison_bp.summary') }}">Spread vs SpreadSP</a></li>
                            <li><a class="dropdown-item" href="{{ url_for('duration_comparison_bp.summary') }}">Duration vs DurationSP</a></li>
                            <li><a class="dropdown-item" href="{{ url_for('spread_duration_comparison_bp.summary') }}">Spread Duration vs SP</a></li>
                        </ul>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="{{ url_for('exclusion_bp.manage_exclusions') }}">Exclusions</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="{{ url_for('api_bp.get_data_page') }}">Data Management</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>
    <main role="main" class="container-fluid">
        {% block content %}
        {# Page specific content will go here #}
        {% endblock %}
    </main>
    <!-- Bootstrap Bundle with Popper -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/js/bootstrap.bundle.min.js"></script>
    <!-- Load Chart.js Library -->
    <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
    <!-- Load Date Adapter (e.g., date-fns) - MUST be after Chart.js -->
    <script src="https://cdn.jsdelivr.net/npm/chartjs-adapter-date-fns@3.0.0/dist/chartjs-adapter-date-fns.bundle.min.js"></script>
    <!-- Load Main Application JS (after libraries are loaded) -->
    <script type="module" src="{{ url_for('static', filename='js/main.js') }}"></script>
    {% block scripts %}
    {# Page specific scripts can go here #}
    {% endblock %}
</body>
</html>
</file>

<file path="templates/comparison_details_page.html">
{% extends "base.html" %}
{% block title %}Spread Comparison Details: {{ security_name }}{% endblock %}
{% block content %}
<div class="container mt-4">
    <nav aria-label="breadcrumb">
        <ol class="breadcrumb">
            <li class="breadcrumb-item"><a href="{{ url_for('comparison_bp.summary') }}">Comparison Summary</a></li>
            <li class="breadcrumb-item active" aria-current="page">{{ security_name }} ({{ security_id }})</li>
        </ol>
    </nav>
    <h1>Spread Comparison Details: {{ security_name }}</h1>
    <h5 class="text-muted">Security ID: {{ security_id }}</h5>
    <div class="row mt-4 mb-4">
        <div class="col-md-6">
            <h2>Comparison Statistics</h2>
            <ul class="list-group">
                <li class="list-group-item d-flex justify-content-between align-items-center">
                    Level Correlation
                    <span class="badge bg-primary rounded-pill">{{ "%.4f"|format(stats.Level_Correlation) if stats.Level_Correlation is not none else 'N/A' }}</span>
                </li>
                <li class="list-group-item d-flex justify-content-between align-items-center">
                    Change Correlation
                    <span class="badge bg-primary rounded-pill">{{ "%.4f"|format(stats.Change_Correlation) if stats.Change_Correlation is not none else 'N/A' }}</span>
                </li>
                <li class="list-group-item d-flex justify-content-between align-items-center">
                    Mean Absolute Difference
                    <span class="badge bg-secondary rounded-pill">{{ "%.2f"|format(stats.Mean_Abs_Diff) if stats.Mean_Abs_Diff is not none else 'N/A' }}</span>
                </li>
                <li class="list-group-item d-flex justify-content-between align-items-center">
                    Max Absolute Difference
                    <span class="badge bg-secondary rounded-pill">{{ "%.2f"|format(stats.Max_Abs_Diff) if stats.Max_Abs_Diff is not none else 'N/A' }}</span>
                </li>
                 <li class="list-group-item d-flex justify-content-between align-items-center">
                    Data Points (Original)
                    <span class="badge bg-info rounded-pill">{{ stats.Total_Points - stats.NaN_Count_Orig }} / {{ stats.Total_Points }}</span>
                </li>
                 <li class="list-group-item d-flex justify-content-between align-items-center">
                    Data Points (New)
                    <span class="badge bg-info rounded-pill">{{ stats.Total_Points - stats.NaN_Count_New }} / {{ stats.Total_Points }}</span>
                </li>
                 <li class="list-group-item d-flex justify-content-between align-items-center">
                    Same Date Range?
                    <span class="badge {{ 'bg-success' if stats.Same_Date_Range else 'bg-warning' }} rounded-pill">{{ 'Yes' if stats.Same_Date_Range else 'No' }}</span>
                </li>
            </ul>
        </div>
        {# Placeholder for additional stats or info if needed #}
        {# <div class="col-md-6">
             <h2>Other Info</h2>
        </div> #}
    </div>
    <h2>Time Series Comparison</h2>
    <p class="text-muted">Overlayed credit spreads from Original (sec_spread) and New (sec_spreadSP) datasets.</p>
    <div>
        <canvas id="comparisonChart"></canvas>
    </div>
    {# Embed chart data as JSON for JavaScript #}
    <script type="application/json" id="comparisonChartData">
        {{ chart_data | tojson | safe }}
    </script>
</div>
{% endblock %}
{% block scripts %}
{{ super() }}
{# We need Chart.js - ensure it's included in base.html or here #}
<script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
{# We also need the JS to render this specific chart #}
<script>
    document.addEventListener('DOMContentLoaded', function() {
        const chartDataElement = document.getElementById('comparisonChartData');
        const comparisonChartCanvas = document.getElementById('comparisonChart');
        if (chartDataElement && comparisonChartCanvas) {
            try {
                const chartData = JSON.parse(chartDataElement.textContent);
                const ctx = comparisonChartCanvas.getContext('2d');
                // --- REVERTING TO SIMPLE CONFIG --- 
                // Remove scriptable options for now
                /*
                // Define base colors (assuming these are the intended original colors)
                const baseColors = [
                    'rgba(13, 110, 253, 1)', // Bootstrap Blue (or COLOR_PALETTE[0])
                    'rgba(220, 53, 69, 1)'  // Bootstrap Red (or COLOR_PALETTE[1])
                ];
                const gapColor = 'rgba(150, 150, 150, 0.7)'; // Semi-transparent gray for gaps
                // Prepare datasets with scriptable borderColor
                console.log("Preparing datasets with scriptable colors...");
                const datasetsWithScriptableColors = chartData.datasets.map((dataset, index) => {
                     console.log(`Mapping dataset index: ${index}`);
                     return {
                        ...dataset, // Keep original label, data, tension, spanGaps etc.
                        borderColor: context => {
                            // Check if drawing a line segment and context is valid
                            if (context.type === 'segment' && context.p0 && context.p1) {
                                // --- NEW APPROACH: Check the 'skip' property of the points (safely) --- 
                                // Ensure p0 and p1 exist before accessing skip
                                const p0skip = context.p0 ? context.p0.skip : false;
                                const p1skip = context.p1 ? context.p1.skip : false;
                                if (p0skip || p1skip) {
                                     // --- DEBUG LOGGING START ---
                                    // Log when gap is detected using the 'skip' property
                                    console.log(`>>> Gap DETECTED (via skip): p0.skip=${p0skip}, p1.skip=${p1skip}, datasetIndex=${context.datasetIndex}. Applying gapColor.`);
                                    // --- DEBUG LOGGING END ---
                                    return gapColor;
                                }
                            }
                            // Default color for non-gap segments, points, legend
                            return baseColors[context.datasetIndex % baseColors.length];
                        },
                        // Ensure point colors match the line start/end unless hovered
                        pointBorderColor: context => baseColors[context.datasetIndex % baseColors.length],
                        pointBackgroundColor: context => baseColors[context.datasetIndex % baseColors.length],
                    }
                });
                console.log("Data prepared. Initializing Chart...");
                */
                // --- END REVERT --- 
                console.log("Initializing Chart with original data..."); // Log before init
                new Chart(ctx, {
                    type: 'line',
                    // Use the ORIGINAL datasets from Python/JSON
                    data: chartData, 
                    options: {
                        responsive: true,
                        maintainAspectRatio: true, // Adjust as needed
                        plugins: {
                            legend: {
                                position: 'top',
                            },
                            title: {
                                display: true,
                                text: 'Spread Comparison: {{ security_name|tojson }}'
                            }
                        },
                        scales: {
                            x: {
                                // Assuming labels are date strings, configure time scale if needed
                                // type: 'time',
                                // time: {
                                //     unit: 'day' // or week, month, etc.
                                // },
                                title: {
                                    display: true,
                                    text: 'Date'
                                }
                            },
                            y: {
                                title: {
                                    display: true,
                                    text: 'Spread'
                                }
                            }
                        },
                        interaction: {
                             intersect: false,
                             mode: 'index',
                        },
                        // Add other options from existing charts for consistency
                    }
                });
            } catch (error) {
                console.error("Error parsing chart data or rendering chart:", error);
                comparisonChartCanvas.parentElement.innerHTML = '<p class="text-danger">Error rendering chart.</p>';
            }
        } else {
             console.warn("Chart data or canvas element not found for comparison chart.");
        }
    });
</script>
{% endblock %}
</file>

<file path="templates/comparison_page.html">
{% extends "base.html" %}
{% block title %}Spread Comparison Summary{% endblock %}
{% block content %}
<div class="container-fluid mt-4"> {# Use container-fluid for wider view #}
    <h1>Spread Comparison: Original (sec_spread) vs. New (sec_spreadSP)</h1>
    <p class="text-muted">Comparing credit spreads between the two datasets. Click on a Security ID/Name to see details. Use filters or click column headers to sort. Pagination applied.</p>
    {# Display message if any #}
    {% if message %}
    <div class="alert alert-warning alert-dismissible fade show" role="alert">
        {{ message }}
        <button type="button" class="btn-close" data-bs-dismiss="alert" aria-label="Close"></button>
    </div>
    {% endif %}
    {# --- Filter Form --- #}
    {% if filter_options %}
    <form method="GET" action="{{ url_for('comparison_bp.summary') }}" class="mb-3 p-3 border rounded bg-light" id="filter-form">
        <h5>Filters</h5>
        <div class="row g-2 align-items-end">
            {% for column, options in filter_options.items() %}
            <div class="col-md-2 mb-2">
                <label for="filter-{{ column }}" class="form-label">{{ column }}</label>
                <select id="filter-{{ column }}" name="filter_{{ column }}" class="form-select form-select-sm">
                    <option value="">All</option>
                    {% for option in options %}
                    <option value="{{ option }}" {% if active_filters.get(column) == option|string %}selected{% endif %}>{{ option }}</option>
                    {% endfor %}
                </select>
            </div>
            {% endfor %}
            <div class="col-md-3 d-flex align-items-end">
                <div class="form-check form-switch mb-1">
                    <input class="form-check-input" type="checkbox" role="switch" id="showSoldToggle" name="show_sold" value="true" {% if show_sold %}checked{% endif %}>
                    <label class="form-check-label" for="showSoldToggle"><small>Show Sold Securities</small></label>
                </div>
            </div>
            <div class="col-md-auto">
                <button type="submit" class="btn btn-primary btn-sm">Apply Filters</button>
                {# Add a clear button only if filters are active #}
                {% if active_filters %}
                <a href="{{ url_for('comparison_bp.summary') }}" class="btn btn-secondary btn-sm">Clear Filters</a>
                {% endif %}
            </div>
        </div>
        {# Hidden fields to preserve current sort order when applying filters - page is implicitly reset #}
        <input type="hidden" name="sort_by" value="{{ current_sort_by }}" id="currentSortBy">
        <input type="hidden" name="sort_order" value="{{ current_sort_order }}" id="currentSortOrder">
    </form>
    {% endif %}
    {# --- Data Table --- #}
    <div class="table-responsive">
        <table class="table table-striped table-hover table-sm caption-top" id="comparison-table">
             {# Add table caption for summary #}
             {% if pagination %}
             <caption class="pb-1">
                 Displaying {{ table_data|length }} of {{ pagination.total_items }} total securities.
                 (Page {{ pagination.page }} of {{ pagination.total_pages }})
             </caption>
             {% endif %}
            <thead class="table-light">
                <tr>
                    {# Loop through the columns passed from the view #}
                    {% for col_name in columns_to_display %}
                        {% set is_sort_col = (col_name == current_sort_by) %}
                        {% set next_sort_order = 'asc' if is_sort_col and current_sort_order == 'desc' else 'desc' %}
                        {# Base arguments, including current filters #}
                        {% set sort_args = request.args.to_dict() %}
                        {% set _ = sort_args.pop('page', None) %}
                        {% set _ = sort_args.update({'sort_by': col_name, 'sort_order': next_sort_order}) %}
                        {# Generate URL for this header #}
                        {% set sort_url = url_for('comparison_bp.summary', **sort_args) %}
                        {# Add classes for styling and JS #}
                        <th class="sortable {{ 'sorted-' + current_sort_order if is_sort_col else '' }}" 
                            data-column-name="{{ col_name }}">
                            <a href="{{ sort_url }}" class="text-decoration-none text-dark">
                                {{ col_name.replace('_', ' ') | title }} 
                                {% if is_sort_col %}
                                    <span class="sort-indicator ms-1">{{ '▲' if current_sort_order == 'asc' else '▼' }}</span>
                                {% endif %}
                            </a>
                        </th>
                    {% endfor %}
                </tr>
            </thead>
            <tbody id="comparison-table-body">
                {% set id_col = id_column_name %}
                {% for row in table_data %}
                <tr>
                    {# Loop through the same columns to ensure order matches header #}
                    {% for col_name in columns_to_display %}
                        <td>
                            {% if col_name == id_col %}
                                <a href="{{ url_for('comparison_bp.comparison_details', security_id=row[id_col]|urlencode) }}">{{ row[id_col] }}</a>
                            {% elif col_name in ['Level_Correlation', 'Change_Correlation'] and row[col_name] is not none %}
                                {{ "%.3f"|format(row[col_name]) }}
                            {% elif col_name in ['Mean_Abs_Diff', 'Max_Abs_Diff'] and row[col_name] is not none %}
                                {{ "%.2f"|format(row[col_name]) }}
                            {% elif col_name == 'Same_Date_Range' %}
                                {{ 'Yes' if row[col_name] else 'No' }}
                            {% elif row[col_name] is number %}
                                {{ row[col_name]|round(3) }} {# General numeric formatting #}
                            {% else %}
                                {{ row[col_name] if row[col_name] is not none else '' }} {# Display strings or empty #}
                            {% endif %}
                        </td>
                    {% endfor %}
                </tr>
                {% else %}
                {# This message is now shown above if filtered_stats is empty #}
                {# <tr> <td colspan="{{ columns_to_display|length }}" class="text-center">No comparison data available matching the current filters.</td> </tr> #}
                {% endfor %}
            </tbody>
        </table>
    </div>
    {# --- Pagination Controls --- #}
    {% if pagination and pagination.total_pages > 1 %}
        <nav aria-label="Comparison data navigation">
            <ul class="pagination pagination-sm justify-content-center">
                {# Previous Page Link #}
                <li class="page-item {{ 'disabled' if not pagination.has_prev }}">
                    <a class="page-link" href="{{ pagination.url_for_page(pagination.prev_num) if pagination.has_prev else '#' }}" aria-label="Previous">&laquo;</a>
                </li>
                {# Page Number Links (using context variables calculated in view) #}
                 {% set start_page = pagination.start_page_display %}
                 {% set end_page = pagination.end_page_display %}
                 {% if start_page > 1 %}
                     <li class="page-item"><a class="page-link" href="{{ pagination.url_for_page(1) }}">1</a></li>
                     {% if start_page > 2 %}
                         <li class="page-item disabled"><span class="page-link">...</span></li>
                     {% endif %}
                 {% endif %}
                 {% for p in range(start_page, end_page + 1) %}
                    <li class="page-item {{ 'active' if p == pagination.page }}">
                        <a class="page-link" href="{{ pagination.url_for_page(p) }}">{{ p }}</a>
                    </li>
                {% endfor %}
                 {% if end_page < pagination.total_pages %}
                     {% if end_page < pagination.total_pages - 1 %}
                         <li class="page-item disabled"><span class="page-link">...</span></li>
                     {% endif %}
                     <li class="page-item"><a class="page-link" href="{{ pagination.url_for_page(pagination.total_pages) }}">{{ pagination.total_pages }}</a></li>
                 {% endif %}
                {# Next Page Link #}
                <li class="page-item {{ 'disabled' if not pagination.has_next }}">
                    <a class="page-link" href="{{ pagination.url_for_page(pagination.next_num) if pagination.has_next else '#' }}" aria-label="Next">&raquo;</a>
                </li>
            </ul>
        </nav>
    {% endif %}
</div>
{% endblock %}
{% block scripts %}
{{ super() }}
<script>
    document.addEventListener('DOMContentLoaded', function() {
        const filterForm = document.getElementById('filter-form');
        const currentSortBy = document.getElementById('currentSortBy');
        const currentSortOrder = document.getElementById('currentSortOrder');
        const showSoldToggle = document.getElementById('showSoldToggle');
        // Function to get current filters from the form
        function getCurrentFilters() {
            const formData = new FormData(filterForm);
            const filters = {};
            for (const [key, value] of formData.entries()) {
                if (key.startsWith('filter_') && value) {
                    filters[key] = value;
                }
            }
            // Include show_sold status explicitly
            filters['show_sold'] = showSoldToggle.checked ? 'true' : 'false';
            return filters;
        }
        // 1. Handle Form Submission (Apply Filters button)
        // The form naturally submits with all values, including the hidden sort fields and the toggle state.
        // 2. Handle Sorting Header Clicks
        document.querySelectorAll('.sortable-header').forEach(header => {
            header.addEventListener('click', function(e) {
                e.preventDefault();
                const sortBy = this.getAttribute('data-sort-by');
                let sortOrder = 'asc';
                if (currentSortBy.value === sortBy && currentSortOrder.value === 'asc') {
                    sortOrder = 'desc';
                }
                currentSortBy.value = sortBy;
                currentSortOrder.value = sortOrder;
                filterForm.submit(); // Submit the form with updated sort fields
            });
        });
        // 3. Handle Show Sold Toggle Change
        showSoldToggle.addEventListener('change', function() {
            // When toggle changes, reset page to 1 and submit form
            const url = new URL(filterForm.action);
            const params = new URLSearchParams(url.search);
            // Get existing filters
            const filters = getCurrentFilters(); // Includes new toggle state
            Object.entries(filters).forEach(([key, value]) => {
                params.set(key, value);
            });
            // Reset page to 1
            params.delete('page');
            // Keep current sort order
            params.set('sort_by', currentSortBy.value);
            params.set('sort_order', currentSortOrder.value);
            window.location.href = `${url.pathname}?${params.toString()}`;
        });
        // 4. Update Pagination Links to include all current filters and sort state
        document.querySelectorAll('.pagination-link').forEach(link => {
            if (!link.parentElement.classList.contains('disabled') && link.getAttribute('href') !== '#') {
                const url = new URL(link.href);
                const params = new URLSearchParams(url.search); // Existing params (includes page number)
                const filters = getCurrentFilters(); // Get current filters including toggle state
                // Add filters to pagination link
                Object.entries(filters).forEach(([key, value]) => {
                    params.set(key, value);
                });
                // Add sorting to pagination link
                params.set('sort_by', currentSortBy.value);
                params.set('sort_order', currentSortOrder.value);
                link.href = `${url.pathname}?${params.toString()}`;
            }
        });
    });
</script>
{% endblock %}
</file>

<file path="templates/curve_details.html">
{# templates/curve_details.html #}
{% extends "base.html" %}
{% block title %}Yield Curve Details - {{ currency }}{% endblock %}
{% block content %}
<div class="container mt-4">
    <div class="d-flex justify-content-between align-items-center mb-3">
        <h1>Yield Curve Details: <strong>{{ currency }}</strong></h1>
        <a href="{{ url_for('curve_bp.curve_summary') }}" class="btn btn-secondary">
            <i class="fas fa-arrow-left me-1"></i> Back to Summary
        </a>
    </div>
    {# Date and History Selection Row #}
    <div class="row mb-3 align-items-end">
        <div class="col-md-4">
            <label for="dateSelector" class="form-label">Select Date:</label>
            <select id="dateSelector" class="form-select">
                {% if available_dates %}
                    {% for date_str in available_dates %}
                        <option value="{{ date_str }}" {% if date_str == selected_date %}selected{% endif %}>
                            {{ date_str }}
                        </option>
                    {% endfor %}
                {% else %}
                    <option value="">No dates available</option>
                {% endif %}
            </select>
        </div>
        <div class="col-md-4">
            <label for="prevDaysSelector" class="form-label">Show Previous Days:</label>
            <select id="prevDaysSelector" class="form-select">
                {# Options for how many previous curves to show #}
                <option value="0" {% if num_prev_days == 0 %}selected{% endif %}>0 (None)</option>
                <option value="1" {% if num_prev_days == 1 %}selected{% endif %}>1</option>
                <option value="3" {% if num_prev_days == 3 %}selected{% endif %}>3</option>
                <option value="5" {% if num_prev_days == 5 %}selected{% endif %}>5</option>
            </select>
        </div>
    </div>
    {# Chart Card #}
    <div class="card shadow-sm mb-4">
        <div class="card-header">
            Yield Curve for {{ selected_date }}{% if num_prev_days > 0 %} (with {{ num_prev_days }} previous day(s)){% endif %}
        </div>
        <div class="card-body">
            {% if chart_data and chart_data.labels and chart_data.datasets %}
                <canvas id="yieldCurveChart" style="height: 750px; width: 100%;"></canvas>
            {% else %}
                <div class="alert alert-warning" role="alert">
                    No data available to display the chart for {{ currency }} on {{ selected_date }}.
                </div>
            {% endif %}
        </div>
    </div>
    {# Data Table Card for Selected Date #}
    {% if table_data %}
    <div class="card shadow-sm">
        <div class="card-header">
            Data for {{ selected_date }} (Compared to Previous Day)
        </div>
        <div class="card-body">
             <div class="table-responsive">
                <table class="table table-sm table-striped table-hover">
                    <thead>
                        <tr>
                            <th>Term</th>
                            <th>Term (Months, Approx)</th>
                            <th>Value</th>
                            <th>Daily Change</th>
                            <th>Deviation from Avg Shift</th>
                            <th>Deviation Z-Score</th>
                        </tr>
                    </thead>
                    <tbody>
                        {% for row in table_data %}
                            {# Apply conditional highlighting based on Z-score #}
                            {% set z_score = row.DeviationZScore | default(0, true) %}
                            {% set abs_z_score = z_score | abs %}
                            {% set row_class = '' %}
                            {% if abs_z_score > 3 %}
                                {% set row_class = 'table-danger' %}
                            {% elif abs_z_score > 2 %}
                                {% set row_class = 'table-warning' %}
                            {% endif %}
                            <tr class="{{ row_class }}">
                                <td>{{ row.Term }}</td>
                                <td>{{ row.TermMonths }}</td>
                                <td>{{ row.Value_Display | round(4) }}</td>
                                {# Format new columns, handle NaN with default filter #}
                                <td>{{ row.ValueChange | default('N/A', true) | round(4) }}</td>
                                <td>{{ row.ChangeDeviation | default('N/A', true) | round(4) }}</td>
                                <td>{{ z_score | default('N/A', true) | round(2) }}</td>
                            </tr>
                        {% endfor %}
                    </tbody>
                </table>
                <small class="text-muted">Highlighting: Yellow if |Z-Score| > 2, Red if |Z-Score| > 3. Z-Score measures how many standard deviations a term's daily change deviated from the average daily change of the whole curve.</small>
            </div>
        </div>
    </div>
    {% endif %}
</div> {# End container #}
{% endblock %}
{% block scripts %}
{{ super() }} {# Include scripts from base.html #}
<script src="https://cdn.jsdelivr.net/npm/chart.js"></script> {# Include Chart.js #}
<script>
document.addEventListener('DOMContentLoaded', function() {
    const chartData = {{ chart_data | tojson | safe }};
    const ctx = document.getElementById('yieldCurveChart');
    let yieldChart = null; // Reference to the chart instance
    function renderChart() {
        if (yieldChart) {
            yieldChart.destroy(); // Destroy previous chart instance if exists
        }
        if (ctx && chartData && chartData.labels && chartData.labels.length > 0 && chartData.datasets && chartData.datasets.length > 0) {
            console.log("Rendering chart with data:", chartData);
            yieldChart = new Chart(ctx, {
                type: 'line',
                data: chartData, // Now contains multiple datasets
                options: {
                    responsive: true,
                    maintainAspectRatio: false,
                    scales: {
                        x: {
                            title: {
                                display: true,
                                text: 'Term (Months)' // Already updated
                            },
                            type: 'linear',
                            position: 'bottom'
                        },
                        y: {
                            title: {
                                display: true,
                                text: 'Yield Value'
                            },
                            beginAtZero: false
                        }
                    },
                    plugins: {
                        tooltip: {
                            mode: 'index',
                            intersect: false,
                        },
                        title: {
                             display: true,
                             // Base title on the first dataset (selected date)
                             text: chartData.datasets[0].label ?
                                   `Yield Curves - ${chartData.datasets[0].label.split('(')[0].trim()}` :
                                   'Yield Curve'
                        },
                        legend: {
                            position: 'top',
                        }
                    }
                }
            });
        } else {
             console.log("Chart canvas not found or no chart data available.");
        }
    }
    renderChart(); // Initial rendering
    // Update URL and reload page when selectors change
    const dateSelector = document.getElementById('dateSelector');
    const prevDaysSelector = document.getElementById('prevDaysSelector');
    function updateUrlAndReload() {
        const selectedDate = dateSelector.value;
        const selectedPrevDays = prevDaysSelector.value;
        if (selectedDate) {
            const currentUrl = new URL(window.location.href);
            currentUrl.searchParams.set('date', selectedDate);
            currentUrl.searchParams.set('prev_days', selectedPrevDays);
            window.location.href = currentUrl.toString();
        }
    }
    if (dateSelector) {
        dateSelector.addEventListener('change', updateUrlAndReload);
    }
    if (prevDaysSelector) {
        prevDaysSelector.addEventListener('change', updateUrlAndReload);
    }
});
</script>
{% endblock %}
</file>

<file path="templates/curve_summary.html">
{# templates/curve_summary.html #}
{% extends "base.html" %}
{% block title %}Yield Curve Check Summary{% endblock %}
{% block content %}
<div class="container mt-4">
    <h1 class="mb-4">Yield Curve Check Summary</h1>
    <p class="lead">Summary of potential inconsistencies found in the yield curve data for the latest available date: <strong>{{ latest_date }}</strong>.</p>
    <p>Checks include basic monotonicity and comparison of daily change profiles against the previous day.</p>
    {% if summary %}
        <table class="table table-striped table-hover">
            <thead>
                <tr>
                    <th>Currency</th>
                    <th>Status / Issues Found</th>
                    <th>Actions</th>
                </tr>
            </thead>
            <tbody>
                {% for currency, issues in summary.items()|sort %}
                    <tr>
                        <td><strong>{{ currency }}</strong></td>
                        <td>
                            {% if issues == ["OK"] %}
                                <span class="badge bg-success">OK</span>
                            {% elif issues == ["Missing data for comparison"] %}
                                 <span class="badge bg-warning text-dark">Missing Data</span>
                                 <small class="text-muted ms-2">{{ issues[0] }}</small>
                            {% else %}
                                <span class="badge bg-danger">Check Required</span>
                                <ul class="list-unstyled mb-0 mt-1">
                                    {% for issue in issues %}
                                        <li><small><i class="fas fa-exclamation-triangle text-danger me-1"></i>{{ issue }}</small></li>
                                    {% endfor %}
                                </ul>
                            {% endif %}
                        </td>
                        <td>
                            <a href="{{ url_for('curve_bp.curve_details', currency=currency) }}" class="btn btn-sm btn-outline-primary">
                                View Details <i class="fas fa-chart-line ms-1"></i>
                            </a>
                        </td>
                    </tr>
                {% endfor %}
            </tbody>
        </table>
    {% else %}
        <div class="alert alert-warning" role="alert">
            No curve data loaded or no summary could be generated. Please check the data file (`Data/curves.csv`) and application logs.
        </div>
    {% endif %}
</div>
{% endblock %}
</file>

<file path="templates/delete_metric_page.html">
<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <title>{{ metric_name }} Check</title>
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
    <style>
        body { padding-top: 5rem; }
        .chart-container { margin-bottom: 15px; }
        .metrics-table { margin-top: 5px; margin-bottom: 25px; font-size: 0.9em; }
        .metrics-table th, .metrics-table td { padding: 4px 8px; border: 1px solid #dee2e6; }
        .missing-warning { color: red; font-weight: bold; }
        .high-z { background-color: #fff3cd; }
        .very-high-z { background-color: #f8d7da; font-weight: bold; }
    </style>
</head>
<body>
    <nav class="navbar navbar-expand-md navbar-dark bg-dark fixed-top">
        <a class="navbar-brand" href="/">Data Verification</a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarsExampleDefault" aria-controls="navbarsExampleDefault" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="navbarsExampleDefault">
            <ul class="navbar-nav mr-auto">
                <li class="nav-item">
                    <a class="nav-link" href="/">Dashboard</a>
                </li>
            </ul>
        </div>
    </nav>
    <main role="main" class="container">
        <h1>{{ metric_name }} Check</h1>
        <p>Latest Data Date: <strong>{{ latest_date }}</strong></p>
        <p>Charts sorted by the absolute Z-score of the latest <strong>Fund - Benchmark Spread</strong> (most deviation first).</p>
        {% if not missing_funds.empty %}
            <div class="alert alert-warning" role="alert">
                <strong>Warning:</strong> The following funds are missing data for the latest date ({{ latest_date }}):
                {{ missing_funds.index.tolist() | join(', ') }}
            </div>
        {% endif %}
        {% for fund_code, data in charts_data.items() %}
            {% set metrics = data.metrics %}
            {% set z_score = metrics['Spread Z-Score'] %}
            {% set z_class = 'high-z' if z_score and z_score|abs > 2 else ('very-high-z' if z_score and z_score|abs > 3 else '') %}
            <div class="chart-container {{ z_class }}">
                {{ data.chart_html|safe }}
            </div>
            <table class="table table-sm table-bordered metrics-table {{ z_class }}">
                <thead>
                    <tr>
                        <th>Metric</th>
                        <th>Latest Value ({{ latest_date }})</th>
                        <th>Change from Previous</th>
                        <th>Historical Spread</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Fund Value</td>
                        <td>{{ metrics['Latest Fund Value']|round(2) if metrics['Latest Fund Value'] is not none else 'N/A' }}</td>
                        <td>{{ metrics['Fund Value Change']|round(2) if metrics['Fund Value Change'] is not none else 'N/A' }}</td>
                        <td rowspan="2">Mean: {{ metrics['Historical Spread Mean']|round(2) if metrics['Historical Spread Mean'] is not none else 'N/A' }}</td>
                    </tr>
                    <tr>
                        <td>Benchmark Value</td>
                        <td>{{ metrics['Latest Benchmark Value']|round(2) if metrics['Latest Benchmark Value'] is not none else 'N/A' }}</td>
                        <td>N/A</td> {# Change not calculated for benchmark #}
                    </tr>
                    <tr>
                        <td>Fund - Benchmark Spread</td>
                        <td>{{ metrics['Latest Spread']|round(2) if metrics['Latest Spread'] is not none else 'N/A' }}</td>
                        <td>{{ metrics['Spread Change']|round(2) if metrics['Spread Change'] is not none else 'N/A' }}</td>
                        <td>Std Dev: {{ metrics['Historical Spread Std Dev']|round(2) if metrics['Historical Spread Std Dev'] is not none else 'N/A' }}</td>
                    </tr>
                     <tr>
                        <td><strong>Spread Z-Score</strong></td>
                        <td colspan="3"><strong>{{ z_score|round(2) if z_score is not none else 'N/A' }}</strong></td>
                    </tr>
                </tbody>
            </table>
            {# Conditionally add link to fund duration details page #}
            {% if metric_name == 'Duration' %}
                <div class="mb-4 text-right">
                     <a href="{{ url_for('fund_duration_details', fund_code=fund_code) }}" class="btn btn-info btn-sm">View Security Duration Changes for {{ fund_code }} &rarr;</a>
                </div>
            {% endif %}
        {% else %}
            <p>No data processed for this metric.</p>
        {% endfor %}
    </main>
    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.5.4/dist/umd/popper.min.js"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"></script>
</body>
</html>
</file>

<file path="templates/duration_comparison_details_page.html">
{% extends "base.html" %}
{# Note: security_name is not explicitly passed, using security_id for title/breadcrumb #}
{% block title %}Duration Comparison Details: {{ security_id }}{% endblock %}
{% block content %}
<div class="container mt-4">
    <nav aria-label="breadcrumb">
        <ol class="breadcrumb">
            <li class="breadcrumb-item"><a href="{{ url_for('duration_comparison_bp.summary') }}">Duration Comparison Summary</a></li> {# Updated Link #}
            {# Displaying ID as primary identifier if name isn't guaranteed #}
            <li class="breadcrumb-item active" aria-current="page">{{ security_id }}</li>
        </ol>
    </nav>
    <h1>Duration Comparison Details: {{ security_id }}</h1> {# Updated Title #}
    {# Add static info if available #}
    {% if static_info %}
        {% for key, value in static_info.items() %}
             {% if key != id_column_name %} {# Avoid repeating the ID #}
                <span class="text-muted me-3"><strong>{{ key }}:</strong> {{ value }}</span>
             {% endif %}
        {% endfor %}
    {% endif %}
    <div class="row mt-4 mb-4">
        <div class="col-md-6">
            <h2>Comparison Statistics</h2>
            {% if stats_summary %}
                <ul class="list-group">
                    <li class="list-group-item d-flex justify-content-between align-items-center">
                        Level Correlation
                        <span class="badge bg-primary rounded-pill">{{ stats_summary.Level_Correlation if stats_summary.Level_Correlation is not none else 'N/A' }}</span>
                    </li>
                    <li class="list-group-item d-flex justify-content-between align-items-center">
                        Change Correlation
                        <span class="badge bg-primary rounded-pill">{{ stats_summary.Change_Correlation if stats_summary.Change_Correlation is not none else 'N/A' }}</span>
                    </li>
                    <li class="list-group-item d-flex justify-content-between align-items-center">
                        Mean Absolute Difference
                        <span class="badge bg-secondary rounded-pill">{{ stats_summary.Mean_Abs_Diff if stats_summary.Mean_Abs_Diff is not none else 'N/A' }}</span>
                    </li>
                    <li class="list-group-item d-flex justify-content-between align-items-center">
                        Max Absolute Difference
                        <span class="badge bg-secondary rounded-pill">{{ stats_summary.Max_Abs_Diff if stats_summary.Max_Abs_Diff is not none else 'N/A' }}</span>
                    </li>
                    <li class="list-group-item d-flex justify-content-between align-items-center">
                        Data Points (Original)
                        <span class="badge bg-info rounded-pill">{{ stats_summary.Total_Points - stats_summary.NaN_Count_Orig }} / {{ stats_summary.Total_Points }}</span>
                    </li>
                    <li class="list-group-item d-flex justify-content-between align-items-center">
                        Data Points (New)
                        <span class="badge bg-info rounded-pill">{{ stats_summary.Total_Points - stats_summary.NaN_Count_New }} / {{ stats_summary.Total_Points }}</span>
                    </li>
                    <li class="list-group-item d-flex justify-content-between align-items-center">
                        Same Date Range?
                        <span class="badge {{ 'bg-success' if stats_summary.Same_Date_Range else 'bg-warning' }} rounded-pill">{{ 'Yes' if stats_summary.Same_Date_Range else 'No' }}</span>
                    </li>
                    {# Add date range details #}
                    <li class="list-group-item">
                        <small>Orig Range: {{ stats_summary.Start_Date_Orig or 'N/A' }} to {{ stats_summary.End_Date_Orig or 'N/A' }}</small><br>
                        <small>New Range: {{ stats_summary.Start_Date_New or 'N/A' }} to {{ stats_summary.End_Date_New or 'N/A' }}</small>
                    </li>
                </ul>
            {% else %}
                <p>No comparison statistics could be calculated.</p>
            {% endif %}
        </div>
        {# Placeholder for additional stats or info if needed #}
    </div>
    <h2>Time Series Comparison</h2>
    <p class="text-muted">Overlayed Duration from Original (sec_duration) and New (sec_durationSP) datasets.</p> {# Updated Text #}
    <div>
        <canvas id="comparisonChart"></canvas>
    </div>
    {# Embed chart data as JSON for JavaScript #}
    <script type="application/json" id="comparisonChartData">
        {{ chart_data | tojson | safe }}
    </script>
</div>
{% endblock %}
{% block scripts %}
{{ super() }}
{# We need Chart.js - ensure it's included in base.html or here #}
<script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
<script src="https://cdn.jsdelivr.net/npm/chartjs-adapter-date-fns/dist/chartjs-adapter-date-fns.bundle.min.js"></script> {# Include Date Adapter #}
<script>
    document.addEventListener('DOMContentLoaded', function() {
        const chartDataElement = document.getElementById('comparisonChartData');
        const comparisonChartCanvas = document.getElementById('comparisonChart');
        if (chartDataElement && comparisonChartCanvas) {
            try {
                const chartData = JSON.parse(chartDataElement.textContent);
                const ctx = comparisonChartCanvas.getContext('2d');
                console.log("Initializing Duration Comparison Chart...");
                new Chart(ctx, {
                    type: 'line',
                    data: chartData, // Direct use of data from JSON
                    options: {
                        responsive: true,
                        maintainAspectRatio: true,
                        plugins: {
                            legend: {
                                position: 'top',
                            },
                            title: {
                                display: true,
                                text: 'Duration Comparison: {{ security_id|tojson }}' // Use security_id
                            },
                            tooltip: {
                                mode: 'index', // Show tooltips for all datasets at the same index
                                intersect: false
                            }
                        },
                        scales: {
                            x: {
                                type: 'time', // Use time scale
                                time: {
                                    unit: 'day',
                                    tooltipFormat: 'yyyy-MM-dd', // Format for tooltip
                                    displayFormats: { // Formats for axis labels
                                        day: 'MMM d, yyyy'
                                    }
                                },
                                title: {
                                    display: true,
                                    text: 'Date'
                                }
                            },
                            y: {
                                title: {
                                    display: true,
                                    text: 'Duration' // Updated Axis Label
                                }
                            }
                        },
                        interaction: {
                             intersect: false,
                             mode: 'index',
                        },
                         elements: {
                            point:{ // Reduce point size for potentially dense data
                                radius: 2
                            }
                        }
                        // No complex scriptable options needed for basic display
                    }
                });
            } catch (error) {
                console.error("Error parsing duration chart data or rendering chart:", error);
                if (comparisonChartCanvas.parentElement) {
                    comparisonChartCanvas.parentElement.innerHTML = '<p class="text-danger">Error rendering duration chart.</p>';
                }
            }
        } else {
             console.warn("Chart data or canvas element not found for duration comparison chart.");
        }
    });
</script>
{% endblock %}
</file>

<file path="templates/duration_comparison_page.html">
{% extends "base.html" %}
{% block title %}Duration Comparison Summary{% endblock %}
{% block content %}
<div class="container-fluid mt-4"> {# Use container-fluid for wider view #}
    <h1>Duration Comparison: Original (sec_duration) vs. New (sec_durationSP)</h1> {# Updated Title #}
    <p class="text-muted">Comparing Duration between the two datasets. Click on a Security ID/Name to see details. Use filters or click column headers to sort. Pagination applied.</p> {# Updated Text #}
    {# Display message if any #}
    {% if message %}
    <div class="alert alert-warning alert-dismissible fade show" role="alert">
        {{ message }}
        <button type="button" class="btn-close" data-bs-dismiss="alert" aria-label="Close"></button>
    </div>
    {% endif %}
    {# --- Filter Form --- #}
    {% if filter_options %}
    <form method="GET" action="{{ url_for('duration_comparison_bp.summary') }}" class="mb-3 p-3 border rounded bg-light" id="filter-form"> {# Updated Action URL #}
        <h5>Filters</h5>
        <div class="row g-2 align-items-end">
            {% for column, options in filter_options.items() %}
            <div class="col-md-2 mb-2">
                <label for="filter-{{ column }}" class="form-label">{{ column }}</label>
                <select id="filter-{{ column }}" name="filter_{{ column }}" class="form-select form-select-sm">
                    <option value="">All</option>
                    {% for option in options %}
                    <option value="{{ option }}" {% if active_filters.get(column) == option|string %}selected{% endif %}>{{ option }}</option>
                    {% endfor %}
                </select>
            </div>
            {% endfor %}
            <div class="col-md-3 d-flex align-items-end">
                <div class="form-check form-switch mb-1">
                    <input class="form-check-input" type="checkbox" role="switch" id="showSoldToggle" name="show_sold" value="true" {% if show_sold %}checked{% endif %}>
                    <label class="form-check-label" for="showSoldToggle"><small>Show Sold Securities</small></label>
                </div>
            </div>
            <div class="col-md-auto">
                <button type="submit" class="btn btn-primary btn-sm">Apply Filters</button>
                {# Add a clear button only if filters are active #}
                {% if active_filters %}
                <a href="{{ url_for('duration_comparison_bp.summary') }}" class="btn btn-secondary btn-sm">Clear Filters</a> {# Updated Clear URL #}
                {% endif %}
            </div>
        </div>
        {# Hidden fields to preserve current sort order when applying filters - page is implicitly reset #}
        <input type="hidden" name="sort_by" value="{{ current_sort_by }}">
        <input type="hidden" name="sort_order" value="{{ current_sort_order }}">
    </form>
    {% endif %}
    {# --- Data Table --- #}
    <div class="table-responsive">
        <table class="table table-striped table-hover table-sm caption-top" id="duration-comparison-table"> {# Updated Table ID #}
             {# Add table caption for summary #}
             {% if pagination %}
             <caption class="pb-1">
                 Displaying {{ table_data|length }} of {{ pagination.total_items }} total securities.
                 (Page {{ pagination.page }} of {{ pagination.total_pages }})
             </caption>
             {% endif %}
            <thead class="table-light">
                <tr>
                    {# Loop through the columns passed from the view #}
                    {% for col_name in columns_to_display %}
                        {% set is_sort_col = (col_name == current_sort_by) %}
                        {% set next_sort_order = 'asc' if is_sort_col and current_sort_order == 'desc' else 'desc' %}
                        {# Base arguments, including current filters #}
                        {% set sort_args = request.args.to_dict() %}
                        {% set _ = sort_args.pop('page', None) %}
                        {% set _ = sort_args.update({'sort_by': col_name, 'sort_order': next_sort_order}) %}
                        {# Generate URL for this header #}
                        {% set sort_url = url_for('duration_comparison_bp.summary', **sort_args) %} {# Updated Sort URL #}
                        {# Add classes for styling and JS #}
                        <th class="sortable {{ 'sorted-' + current_sort_order if is_sort_col else '' }}"
                            data-column-name="{{ col_name }}">
                            <a href="{{ sort_url }}" class="text-decoration-none text-dark">
                                {{ col_name.replace('_', ' ') | title }}
                                {% if is_sort_col %}
                                    <span class="sort-indicator ms-1">{{ '▲' if current_sort_order == 'asc' else '▼' }}</span>
                                {% endif %}
                            </a>
                        </th>
                    {% endfor %}
                </tr>
            </thead>
            <tbody id="duration-comparison-table-body"> {# Updated tbody ID #}
                {% set id_col = id_column_name %}
                {% for row in table_data %}
                <tr>
                    {# Loop through the same columns to ensure order matches header #}
                    {% for col_name in columns_to_display %}
                        <td>
                            {% if col_name == id_col %}
                                <a href="{{ url_for('duration_comparison_bp.duration_comparison_details', security_id=row[id_col]|urlencode) }}">{{ row[id_col] }}</a> {# Updated Detail URL #}
                            {% elif col_name in ['Level_Correlation', 'Change_Correlation'] and row[col_name] is not none %}
                                {# Attempt to format as float, handle potential errors gracefully #}
                                {% set formatted_val = "%.3f"|format(row[col_name]|float) if row[col_name] is number else row[col_name] %}
                                {{ formatted_val }}
                            {% elif col_name in ['Mean_Abs_Diff', 'Max_Abs_Diff'] and row[col_name] is not none %}
                                {% set formatted_val = "%.2f"|format(row[col_name]|float) if row[col_name] is number else row[col_name] %}
                                {{ formatted_val }}
                             {% elif col_name == 'Same_Date_Range' %}
                                 <span class="badge {{ 'bg-success' if row[col_name] else 'bg-warning' }}">{{ 'Yes' if row[col_name] else 'No' }}</span>
                            {% elif col_name.endswith('_Date') and row[col_name] %}
                                 {# Assume date strings are already YYYY-MM-DD from view #}
                                {{ row[col_name] }}
                            {% elif row[col_name] is number %}
                                {{ row[col_name]|round(3) }} {# General numeric formatting #}
                            {% else %}
                                {{ row[col_name] if row[col_name] is not none else '' }} {# Display strings or empty #}
                            {% endif %}
                        </td>
                    {% endfor %}
                </tr>
                {% else %}
                <tr>
                    <td colspan="{{ columns_to_display|length }}" class="text-center">No duration comparison data available matching the current filters.</td> {# Updated Message #}
                </tr>
                {% endfor %}
            </tbody>
        </table>
    </div>
    {# --- Pagination Controls --- #}
    {% if pagination and pagination.total_pages > 1 %}
    <nav aria-label="Duration comparison data navigation">
        <ul class="pagination pagination-sm justify-content-center">
            {# Helper macro for generating pagination links #}
            {% macro page_link(page_num, text=None, is_disabled=False, is_active=False) %}
                {% set link_args = request.args.to_dict() %}
                {% set _ = link_args.update({'page': page_num, 'sort_by': current_sort_by, 'sort_order': current_sort_order}) %}
                {% set url = url_for('duration_comparison_bp.summary', **link_args) if page_num else '#' %} {# Updated URL #}
                <li class="page-item {{ 'disabled' if is_disabled }} {{ 'active' if is_active }}">
                    <a class="page-link" href="{{ url }}" {% if is_active %}aria-current="page"{% endif %}>{{ text or page_num }}</a>
                </li>
            {% endmacro %}
            {{ page_link(pagination.prev_num, '&laquo;', is_disabled=not pagination.has_prev) }}
            {# Simplified pagination display logic #}
            {% set window = 2 %}
            {% set start_page = [1, pagination.page - window] | max %}
            {% set end_page = [pagination.total_pages, pagination.page + window] | min %}
            {% if start_page > 1 %}
                {{ page_link(1) }}
                {% if start_page > 2 %}
                    <li class="page-item disabled"><span class="page-link">...</span></li>
                {% endif %}
            {% endif %}
            {% for p in range(start_page, end_page + 1) %}
                {{ page_link(p, is_active=(p == pagination.page)) }}
            {% endfor %}
            {% if end_page < pagination.total_pages %}
                {% if end_page < pagination.total_pages - 1 %}
                    <li class="page-item disabled"><span class="page-link">...</span></li>
                {% endif %}
                {{ page_link(pagination.total_pages) }}
            {% endif %}
            {{ page_link(pagination.next_num, '&raquo;', is_disabled=not pagination.has_next) }}
        </ul>
    </nav>
    {% endif %}
</div>
{% endblock %}
{% block scripts %}
{{ super() }}
{# No specific JS needed for this page unless client-side sorting is added back #}
{% endblock %}
</file>

<file path="templates/exclusions_page.html">
{% extends 'base.html' %}
{% block title %}Manage Security Exclusions{% endblock %}
{% block content %}
<div class="container mt-4">
    <h2>Manage Security Exclusions</h2>
    <hr>
    {# Display messages if any #}
    {% if message %}
        <div class="alert alert-{{ message_type }} alert-dismissible fade show" role="alert">
            {{ message }}
            <button type="button" class="btn-close" data-bs-dismiss="alert" aria-label="Close"></button>
        </div>
    {% endif %}
    <div class="row">
        {# Left Column: Display Current Exclusions #}
        <div class="col-md-7">
            <h4>Current Exclusions</h4>
            {% if exclusions %}
                <table class="table table-striped table-sm">
                    <thead>
                        <tr>
                            <th>Security ID</th>
                            <th>Date Added</th>
                            <th>End Date</th>
                            <th>Comment</th>
                            <th>Action</th>
                        </tr>
                    </thead>
                    <tbody>
                        {% for exclusion in exclusions %}
                            <tr>
                                <td>{{ exclusion.SecurityID }}</td>
                                <td>{{ exclusion.AddDate.strftime('%Y-%m-%d') if exclusion.AddDate else 'N/A' }}</td>
                                <td>{{ exclusion.EndDate.strftime('%Y-%m-%d') if exclusion.EndDate else '' }}</td>
                                <td>{{ exclusion.Comment }}</td>
                                <td>
                                    <form method="POST" action="{{ url_for('exclusion_bp.remove_exclusion_route') }}" style="display: inline;">
                                        <input type="hidden" name="security_id" value="{{ exclusion.SecurityID }}">
                                        <input type="hidden" name="add_date" value="{{ exclusion.AddDate.strftime('%Y-%m-%d') if exclusion.AddDate else '' }}">
                                        <button type="submit" class="btn btn-danger btn-sm" onclick="return confirm('Are you sure you want to remove this exclusion?');">Remove</button>
                                    </form>
                                </td>
                            </tr>
                        {% endfor %}
                    </tbody>
                </table>
            {% else %}
                <p>No securities are currently excluded.</p>
            {% endif %}
        </div>
        {# Right Column: Add New Exclusion Form #}
        <div class="col-md-5">
            <h4>Add New Exclusion</h4>
            <form method="POST" action="{{ url_for('exclusion_bp.manage_exclusions') }}">
                <div class="mb-3">
                    <label for="security-search-input" class="form-label">Search & Select Security ID:</label>
                    <input type="text" id="security-search-input" class="form-control mb-2" placeholder="Type to filter securities...">
                    <select class="form-select" id="security-select" name="security_id" required>
                        <option value="" disabled selected>Select a Security ID</option>
                        {% for sec_id in available_securities %}
                            <option value="{{ sec_id }}">{{ sec_id }}</option>
                        {% endfor %}
                    </select>
                </div>
                <div class="mb-3">
                    <label for="end_date" class="form-label">End Date (Optional):</label>
                    <input type="date" class="form-control" id="end_date" name="end_date">
                </div>
                <div class="mb-3">
                    <label for="comment" class="form-label">Comment (Required):</label>
                    <textarea class="form-control" id="comment" name="comment" rows="3" required></textarea>
                </div>
                <button type="submit" class="btn btn-primary">Add Exclusion</button>
            </form>
        </div>
    </div>
</div>
{% endblock %}
{% block scripts %}
{{ super() }} {# Include scripts from base.html #}
{# We will add specific JS for the dynamic dropdown here later #}
<script>
    // Basic dynamic filtering for the dropdown
    document.getElementById('security-search-input').addEventListener('input', function() {
        let filter = this.value.toLowerCase();
        let select = document.getElementById('security-select');
        let options = select.options;
        let firstVisibleOption = null;
        for (let i = 0; i < options.length; i++) {
            let option = options[i];
            // Skip the placeholder option
            if (option.value === "") {
                option.style.display = ""; // Always show placeholder if input is empty, hide otherwise
                option.style.display = filter ? "none" : "";
                continue;
            }
            let txtValue = option.textContent || option.innerText;
            if (txtValue.toLowerCase().indexOf(filter) > -1) {
                option.style.display = "";
                if (!firstVisibleOption) {
                     firstVisibleOption = option; // Keep track of the first match
                }
            } else {
                option.style.display = "none";
            }
        }
         // Optionally, select the first visible option if the user hasn't selected one manually
        // This part can be enhanced, maybe select only if input length > N or on specific event
        // if (filter && firstVisibleOption && select.selectedIndex <= 0) {
            // select.value = firstVisibleOption.value;
        // }
    });
    // Reset filter when dropdown is clicked (to show all options again initially)
    document.getElementById('security-select').addEventListener('mousedown', function(){
       // Optional: Uncomment below to clear search on dropdown click
       // document.getElementById('security-search-input').value = '';
       // let event = new Event('input');
       // document.getElementById('security-search-input').dispatchEvent(event);
    });
</script>
{% endblock %}
</file>

<file path="templates/fund_detail_page.html">
{% extends "base.html" %}
{% block title %}Fund Details: {{ fund_code }}{% endblock %}
{% block content %}
<div class="container mt-4">
    <h1 class="mb-4">Fund Details: {{ fund_code }}</h1>
    {% if message %}
        <div class="alert alert-info" role="alert">
            {{ message }}
        </div>
    {% endif %}
    {% if chart_data_json and chart_data_json != '[]' %}
        <!-- Toggle Switch for SP Data -->
        <div class="form-check form-switch mb-3">
            <input class="form-check-input" type="checkbox" role="switch" id="toggleSpData" checked>
            <label class="form-check-label" for="toggleSpData">Show SP Comparison Data</label>
        </div>
        <!-- Embed JSON data for JavaScript -->
        <script id="fundChartData" type="application/json">
            {{ chart_data_json | safe }}
        </script>
        <!-- Area where charts will be rendered by JavaScript -->
        <div id="fundChartsArea" class="row row-cols-1 row-cols-lg-2 g-4">
            <!-- Charts will be dynamically inserted here -->
        </div>
    {% elif not message %}
         <div class="alert alert-warning" role="alert">
            No chart data available to display for this fund.
        </div>
    {% endif %}
     <div class="mt-4">
        <a href="{{ url_for('main.index') }}" class="btn btn-secondary">Back to Dashboard</a>
    </div>
</div>
{% endblock %}
{% block scripts %}
{{ super() }}
<!-- Chart.js is already included in base.html -->
<!-- Make this a module to allow imports -->
<script type="module">
    // Import necessary functions from the chart renderer module
    import { renderFundCharts, toggleSecondaryDataVisibility } from '{{ url_for('static', filename='js/modules/ui/chartRenderer.js') }}';
    document.addEventListener('DOMContentLoaded', function () {
        const chartDataElement = document.getElementById('fundChartData');
        const chartsArea = document.getElementById('fundChartsArea');
        const toggleSwitch = document.getElementById('toggleSpData');
        if (!chartDataElement || !chartsArea || !toggleSwitch) {
            console.error('[Fund Detail Page] Required elements for chart rendering or toggle not found.');
            return;
        }
        try {
            const allChartData = JSON.parse(chartDataElement.textContent);
            // Check if any SP data exists across all metrics
            const anySpDataAvailable = allChartData.some(chartInfo => 
                chartInfo.datasets && chartInfo.datasets.some(ds => ds.isSpData === true)
            );
            // Initial setup of the toggle switch based on data availability
            if (anySpDataAvailable) {
                toggleSwitch.disabled = false;
                toggleSwitch.parentElement.querySelector('label').textContent = 'Show SP Comparison Data';
            } else {
                toggleSwitch.disabled = true;
                toggleSwitch.checked = false; // Ensure it's off if disabled
                toggleSwitch.parentElement.querySelector('label').textContent = 'Show SP Comparison Data (N/A)';
            }
            // Render the charts using the imported function
            // This function will now manage the chart instances internally
            renderFundCharts(chartsArea, allChartData);
            console.log("[Fund Detail Page] Called renderFundCharts.");
            // Add event listener for the toggle switch
            // This now calls the centralized toggle function from the module
            toggleSwitch.addEventListener('change', function() {
                const showSp = this.checked;
                console.log(`[Fund Detail Page] Toggle changed: Show SP Data = ${showSp}. Calling toggleSecondaryDataVisibility.`); 
                toggleSecondaryDataVisibility(showSp); // Call the imported function
            });
        } catch (error) {
            console.error('[Fund Detail Page] Error parsing chart data or setting up charts:', error);
            chartsArea.innerHTML = '<div class="alert alert-danger">Failed to load chart data.</div>';
            if (toggleSwitch) {
                toggleSwitch.disabled = true; // Disable toggle on error
                 toggleSwitch.parentElement.querySelector('label').textContent = 'Show SP Comparison Data (Error)';
            }
        }
    });
</script>
{% endblock %}
</file>

<file path="templates/fund_duration_details.html">
{% extends 'base.html' %}
{% block title %}Duration Change Details for {{ fund_code }}{% endblock %}
{% block content %}
<div class="container mt-4">
    <h2>Duration Change Details for Fund: {{ fund_code }}</h2>
    <p>Showing securities from <code>sec_duration.csv</code> held by <strong>{{ fund_code }}</strong>, sorted by the latest 1-day change in duration (largest change first).</p>
    {% if message %}
    <div class="alert alert-warning" role="alert">
        {{ message }}
    </div>
    {% endif %}
    {# Data Table Section #}
    {% if securities_data %}
    <div class="table-responsive">
        <table class="table table-striped table-hover table-sm small" id="fund-duration-table">
            <thead class="table-light">
                <tr>
                    {# Use the column_order provided by the backend #}
                    {% for col_name in column_order %}
                        {# Add sortable class and data attribute. Add default sort attribute if it matches. #}
                        <th class="sortable"
                            data-column-name="{{ col_name }}"
                            {% if col_name == 'Duration Contribution Change' %}data-sort-default="desc"{% endif %}>
                            {{ col_name }}
                        </th>
                    {% endfor %}
                </tr>
            </thead>
            <tbody id="fund-duration-table-body">
                {% for row in securities_data %}
                     {# Optionally add row highlighting based on the change magnitude if needed #}
                     {% set change_value = row['1 Day Duration Change'] %}
                     {% set row_class = '' %} {# Add logic here if desired e.g., based on change_value sign or magnitude #}
                     {# Example highlighting:
                     {% if change_value is not none %}
                        {% if change_value > 0.5 %}
                            {% set row_class = 'table-warning' %}
                        {% elif change_value < -0.5 %}
                             {% set row_class = 'table-info' %}
                        {% endif %}
                     {% endif %}
                     #}
                    <tr class="{{ row_class }}">
                        {% for col_name in column_order %}
                            {# Add data-value for numeric columns to aid sorting #}
                            <td {% if row[col_name] is number %}data-value="{{ row[col_name] }}"{% endif %}>
                                {# Special formatting for the change column or others if needed #}
                                {% if col_name == id_col_name %}
                                     {# Make the Security Name a link #}
                                     <a href="{{ url_for('security.security_details', metric_name='Duration', security_id=row[col_name]|urlencode) }}">{{ row[col_name] }}</a>
                                {% elif row[col_name] is number %}
                                    {# Format numeric columns, maybe specific format for change #}
                                     {{ "%.4f"|format(row[col_name]) }} {# Use 4 decimal places consistent with backend rounding #}
                                {% else %}
                                     {{ row[col_name] if row[col_name] is not none else '' }}
                                {% endif %}
                            </td>
                        {% endfor %}
                    </tr>
                {% endfor %}
            </tbody>
        </table>
    </div>
    {% elif not message %}
     <div class="alert alert-info" role="alert">
        No securities data to display for fund {{ fund_code }}.
    </div>
    {% endif %}
    <div class="mt-3">
         <a href="{{ url_for('metric.metric_page', metric_name='Duration') }}" class="btn btn-secondary btn-sm">&larr; Back to Duration Metric Page</a>
    </div>
</div>
{% endblock %}
</file>

<file path="templates/get_data.html">
{% extends "base.html" %}
{% block title %}Get Data via API{% endblock %}
{% block content %}
<div class="container mt-4">
    {# --- Display Data File Statuses --- #}
    <div class="card mb-4">
        <div class="card-header">
            Current Data File Status
        </div>
        <div class="card-body">
            <p class="card-text"><small class="text-muted">Data Folder: <code>{{ data_folder }}</code></small></p>
            {% if data_file_statuses %}
            <table class="table table-sm table-striped table-bordered">
                <thead>
                    <tr>
                        <th>File Name</th>
                        <th>Latest Data Date (in file)</th>
                        <th>File Last Modified</th>
                        <th>Funds Included</th>
                    </tr>
                </thead>
                <tbody>
                    {% for status in data_file_statuses %}
                    <tr>
                        <td>{{ status.filename }}</td>
                        <td>
                            {% if status.exists %}
                                {{ status.latest_data_date }}
                            {% else %}
                                <span class="text-muted">File Not Found</span>
                            {% endif %}
                        </td>
                        <td>
                             {% if status.exists %}
                                {{ status.last_modified }}
                            {% else %}
                                <span class="text-muted">N/A</span>
                            {% endif %}
                        </td>
                        <td>
                             {% if status.exists %}
                                {{ status.funds_included }}
                            {% else %}
                                <span class="text-muted">N/A</span>
                            {% endif %}
                        </td>
                    </tr>
                    {% endfor %}
                </tbody>
            </table>
            {% else %}
            <p class="text-muted">Could not retrieve data file statuses. Check QueryMap.csv or server logs.</p>
            {% endif %}
        </div>
    </div>
    {# --- End Display Data File Statuses --- #}
    <h2>Get Data via Simulated API (Rex)</h2>
    <p>Select funds and date range to simulate retrieving data using the Rex API.</p>
    <p>The simulated API calls will be printed in the terminal where the Flask app is running.</p>
    <form id="get-data-form">
        <div class="row mb-3">
            <div class="col-md-4">
                <label for="daysBack" class="form-label">Days Back:</label>
                <input type="number" class="form-control" id="daysBack" name="days_back" value="30" required>
                <div class="form-text">Number of days of history to retrieve ending on the End Date.</div>
            </div>
            <div class="col-md-4">
                <label for="endDate" class="form-label">End Date:</label>
                <input type="date" class="form-control" id="endDate" name="end_date" value="{{ default_end_date }}" required>
                <div class="form-text">Defaults to the previous business day.</div>
            </div>
        </div>
        <div class="mb-3">
            <label class="form-label">Select Funds:</label>
             <button type="button" class="btn btn-sm btn-outline-secondary ms-2" id="select-all-funds">Select All</button>
             <button type="button" class="btn btn-sm btn-outline-secondary ms-1" id="deselect-all-funds">Deselect All</button>
            <div id="fund-list" class="border p-3" style="max-height: 300px; overflow-y: auto;">
                {% for fund in funds %}
                <div class="form-check">
                    <input class="form-check-input fund-checkbox" type="checkbox" value="{{ fund['Fund Code'] }}" id="fund-{{ fund['Fund Code'] }}" name="funds"
                           {% if fund['Picked'] %}checked{% endif %}>
                    <label class="form-check-label" for="fund-{{ fund['Fund Code'] }}">
                        {{ fund['Fund Code'] }} (AUM: {{ fund['Total Asset Value USD']|int }})
                    </label>
                </div>
                {% else %}
                <p class="text-danger">No funds found or FundList.csv could not be loaded correctly.</p>
                {% endfor %}
            </div>
             <div class="form-text text-danger d-none" id="fund-selection-error">Please select at least one fund.</div>
        </div>
        <button type="submit" class="btn btn-primary">Simulate API Calls</button>
        <button type="button" id="run-overwrite-button" class="btn btn-warning ms-2">Run and Overwrite Data</button>
        <button type="button" id="run-cleanup-button" class="btn btn-secondary ms-2">Run Data Cleanup</button>
    </form>
    <div id="status-area" class="mt-4" style="display: none;">
        <h4>Processing Status</h4>
        <div class="progress mb-2" style="height: 20px;">
            <div id="progress-bar" class="progress-bar progress-bar-striped progress-bar-animated" role="progressbar" style="width: 0%;" aria-valuenow="0" aria-valuemin="0" aria-valuemax="100">0%</div>
        </div>
        <p id="status-message"></p>
        <div id="results-summary" class="mt-3">
            <h5>Results Summary</h5>
            <table class="table table-sm table-striped">
                <thead>
                    <tr>
                        <th>Query ID</th>
                        <th>File Name</th>
                        <th>Simulated Rows Returned</th>
                        <th>Simulated File Lines</th>
                        <th>Status</th>
                        <th>Actions</th>
                    </tr>
                </thead>
                <tbody id="results-table-body">
                    <!-- Results will be populated here -->
                </tbody>
            </table>
        </div>
         <div id="error-message" class="alert alert-danger mt-3" style="display: none;">
             <!-- Errors shown here -->
         </div>
    </div>
</div>
{% endblock %}
{% block scripts %}
{{ super() }} {# Include scripts from base.html #}
<script>
document.addEventListener('DOMContentLoaded', function() {
    const form = document.getElementById('get-data-form');
    const statusArea = document.getElementById('status-area');
    const statusMessage = document.getElementById('status-message');
    const progressBar = document.getElementById('progress-bar');
    const resultsTableBody = document.getElementById('results-table-body');
    const errorMessageDiv = document.getElementById('error-message');
    const fundSelectionError = document.getElementById('fund-selection-error');
    const cleanupButton = document.getElementById('run-cleanup-button');
    const runOverwriteButton = document.getElementById('run-overwrite-button');
    const cleanupStatus = document.createElement('div');
    cleanupStatus.id = 'cleanup-status';
    cleanupStatus.className = 'mt-2';
    cleanupButton.parentNode.insertBefore(cleanupStatus, cleanupButton.nextSibling);
    const fundCheckboxes = document.querySelectorAll('.fund-checkbox');
    const selectAllButton = document.getElementById('select-all-funds');
    const deselectAllButton = document.getElementById('deselect-all-funds');
    // Select/Deselect All Funds buttons
    selectAllButton.addEventListener('click', () => {
        fundCheckboxes.forEach(checkbox => checkbox.checked = true);
    });
    deselectAllButton.addEventListener('click', () => {
        fundCheckboxes.forEach(checkbox => checkbox.checked = false);
    });
    // --- Function to handle the API call logic ---
    async function handleApiCall(overwriteMode = false) {
        // Clear previous results and errors
        statusArea.style.display = 'none';
        resultsTableBody.innerHTML = '';
        errorMessageDiv.style.display = 'none';
        errorMessageDiv.textContent = '';
        cleanupButton.style.display = 'none';
        cleanupStatus.textContent = '';
        cleanupStatus.className = 'mt-2';
        statusMessage.textContent = '';
        progressBar.style.width = '0%';
        progressBar.textContent = '0%';
        progressBar.classList.remove('bg-success', 'bg-danger');
        fundSelectionError.classList.add('d-none');
        // Get selected funds
        const selectedFunds = Array.from(document.querySelectorAll('input[name="funds"]:checked'))
                                 .map(cb => cb.value);
        // Basic client-side validation
        if (selectedFunds.length === 0) {
            fundSelectionError.classList.remove('d-none');
            return; // Stop submission
        }
        const daysBack = document.getElementById('daysBack').value;
        const endDate = document.getElementById('endDate').value;
        if (!endDate) {
             errorMessageDiv.textContent = 'Please select an End Date.';
             errorMessageDiv.style.display = 'block';
            return; // Stop submission
        }
        // Show status area and indicate processing
        statusArea.style.display = 'block';
        statusMessage.textContent = `Starting ${overwriteMode ? 'overwrite' : 'simulation/merge'}...`;
        progressBar.classList.add('progress-bar-animated');
        progressBar.classList.remove('bg-success', 'bg-danger');
        progressBar.style.width = '5%'; // Initial small progress
        progressBar.textContent = '5%';
        try {
            // Prepare request body, including the overwrite_mode flag
            const requestBody = {
                days_back: parseInt(daysBack, 10),
                end_date: endDate,
                funds: selectedFunds,
                overwrite_mode: overwriteMode // Add the flag here
            };
            const response = await fetch('{{ url_for("api_bp.run_api_calls") }}', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                },
                body: JSON.stringify(requestBody) // Send the body with the flag
            });
            const result = await response.json();
            // Stop animation
             progressBar.classList.remove('progress-bar-animated');
            if (response.ok && (result.status === 'completed' || result.status === 'completed_with_errors')) {
                statusMessage.textContent = result.message;
                progressBar.style.width = '100%';
                progressBar.textContent = '100%';
                progressBar.classList.add(result.status === 'completed_with_errors' ? 'bg-warning' : 'bg-success');
                // Populate results table
                resultsTableBody.innerHTML = ''; // Clear any potential previous entries
                if (result.summary && result.summary.length > 0) {
                    result.summary.forEach(item => {
                        // Determine row content based on real vs simulated
                        let rowsCellContent = 'N/A';
                        let linesCellContent = 'N/A';
                        if (item.actual_rows !== undefined && item.actual_rows !== null) { // Real API mode was used
                            rowsCellContent = item.actual_rows;
                            linesCellContent = item.actual_lines !== undefined ? item.actual_lines : (rowsCellContent > 0 ? rowsCellContent + 1 : 0);
                        } else if (item.simulated_rows !== undefined && item.simulated_rows !== null) { // Simulated mode was used
                            rowsCellContent = item.simulated_rows;
                            linesCellContent = item.simulated_lines !== undefined ? item.simulated_lines : (rowsCellContent > 0 ? rowsCellContent + 1 : 0);
                        }
                        // Define fundCode. Prefer 'fund_code' if present, else derive from 'query_id' if possible
                        let fundCodeForRerun = item.fund_code || null;
                        // Basic attempt to extract from query_id if needed (adjust regex/logic if format differs)
                        if (!fundCodeForRerun && item.query_id && typeof item.query_id === 'string') {
                            const match = item.query_id.match(/some_pattern_to_extract_fund_code/); // Replace with actual pattern if applicable
                            if (match && match[1]) {
                                fundCodeForRerun = match[1];
                            }
                        }
                        const rerunButtonHtml = fundCodeForRerun
                            ? `<button class="btn btn-sm btn-outline-primary rerun-button" data-fund-code="${fundCodeForRerun}">Rerun</button>`
                            : `<span class="text-muted">Rerun N/A</span>`; // No rerun if fund code unknown
                        const row = `<tr data-query-id="${item.query_id}">
                                        <td>${item.query_id}</td>
                                        <td>${item.file_name}</td>
                                        <td>${rowsCellContent}</td>
                                        <td>${linesCellContent}</td>
                                        <td><span class="badge ${item.status.includes('OK') || item.status.includes('Saved') || item.status.includes('Simulated') ? 'bg-success' : (item.status.includes('Warning') ? 'bg-warning' : 'bg-danger')}">${item.status}</span></td>
                                        <td>${rerunButtonHtml}</td>
                                     </tr>`;
                        resultsTableBody.innerHTML += row;
                    });
                } else {
                    resultsTableBody.innerHTML = '<tr><td colspan="6">No summary data returned.</td></tr>'; // colspan is 6 now
                }
                 errorMessageDiv.style.display = 'none'; // Hide error div if successful
                 cleanupButton.style.display = 'inline-block';
            } else {
                // Handle errors reported by the server (e.g., validation errors, file not found)
                statusMessage.textContent = 'Processing failed.';
                progressBar.style.width = '100%';
                progressBar.textContent = 'Error';
                progressBar.classList.add('bg-danger');
                errorMessageDiv.textContent = `Error: ${result.message || 'Unknown error'}`;
                errorMessageDiv.style.display = 'block';
            }
        } catch (error) {
            // Handle network errors or issues with the fetch itself
            console.error("Fetch error:", error);
            progressBar.classList.remove('progress-bar-animated');
            progressBar.style.width = '100%';
            progressBar.textContent = 'Error';
            progressBar.classList.add('bg-danger');
            statusMessage.textContent = 'An error occurred during the request.';
            errorMessageDiv.textContent = 'Network error or server unreachable. Check console for details.';
            errorMessageDiv.style.display = 'block';
        }
    }
    // --- End of API call handler function ---
    // --- Event Listener for the original "Simulate API Calls" button (which is type="submit") ---
    form.addEventListener('submit', function(event) {
        event.preventDefault(); // Prevent traditional form submission
        handleApiCall(false); // Call the handler function with overwriteMode = false
    });
    // --- Event Listener for the new "Run and Overwrite Data" button ---
    runOverwriteButton.addEventListener('click', function() {
        // No need for event.preventDefault() as it's not a submit button
        handleApiCall(true); // Call the handler function with overwriteMode = true
    });
    // Add event listener for the Rerun buttons using event delegation
    resultsTableBody.addEventListener('click', async function(event) {
        if (event.target.classList.contains('rerun-button')) {
            const button = event.target;
            const row = button.closest('tr');
            const queryId = row.dataset.queryId;
            const fundCode = button.dataset.fundCode;
            const daysBack = document.getElementById('daysBack').value;
            const endDate = document.getElementById('endDate').value;
            const cells = row.cells; // Define cells here so it's available in all blocks
            if (!fundCode || !queryId) {
                console.error('Missing fund code or query ID for rerun');
                // Optionally display an error to the user near the button/row
                return;
            }
            // Provide visual feedback
            button.disabled = true;
            button.textContent = 'Running...';
            // You could also add a temporary status cell or highlight the row
            try {
                const response = await fetch('/rerun-api-call', { // New endpoint needed
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                    },
                    body: JSON.stringify({
                        query_id: queryId,
                        // Send fundCode as a list in the 'funds' key
                        funds: [fundCode],
                        days_back: parseInt(daysBack, 10),
                        end_date: endDate
                    })
                });
                const result = await response.json();
                if (response.ok && result.status.includes('OK')) {
                    // Update the specific row in the table
                    cells[2].textContent = result.simulated_rows !== undefined ? result.simulated_rows : 'N/A'; // Simulated Rows
                    cells[3].textContent = result.simulated_lines !== undefined ? result.simulated_lines : 'N/A'; // Simulated Lines
                    cells[4].innerHTML = `<span class="badge bg-success">${result.status}</span>`; // Status
                    // Optional: Add a temporary success indicator
                    button.textContent = 'Rerun Success';
                    setTimeout(() => { button.textContent = 'Rerun'; }, 2000); // Reset after 2s
                } else {
                    // Handle error - update status cell, show message
                    cells[4].innerHTML = `<span class="badge bg-danger">Error</span>`;
                    console.error("Rerun failed:", result.message || 'Unknown error');
                    // Optionally display error details near the row or in the main error area
                     button.textContent = 'Rerun Failed';
                     setTimeout(() => { button.textContent = 'Rerun'; }, 3000); // Reset after 3s
                }
            } catch (error) {
                console.error("Rerun fetch error:", error);
                 cells[4].innerHTML = `<span class="badge bg-danger">Network Error</span>`;
                 button.textContent = 'Rerun Error';
                 setTimeout(() => { button.textContent = 'Rerun'; }, 3000); // Reset after 3s
                // Optionally display error details
            } finally {
                 button.disabled = false; // Re-enable button
                 // Remove any temporary status indicators if needed
            }
        }
    });
    // Add event listener for the new Cleanup button
    cleanupButton.addEventListener('click', async function() {
        cleanupStatus.textContent = 'Starting cleanup process...';
        cleanupStatus.className = 'mt-2 alert alert-info'; // Show feedback
        cleanupButton.disabled = true; // Disable button while running
        try {
            const response = await fetch('/run-cleanup', { // Call the new endpoint
                method: 'POST',
                 headers: {
                    'Content-Type': 'application/json', // Optional: Send empty JSON or adjust endpoint
                },
                // body: JSON.stringify({}) // Optional: Send empty JSON or adjust endpoint
            });
            const result = await response.json();
            if (response.ok && result.status === 'success') {
                cleanupStatus.textContent = `Cleanup process finished successfully. Output:\n${result.output}`;
                cleanupStatus.className = 'mt-2 alert alert-success';
            } else {
                 cleanupStatus.textContent = `Cleanup process failed. Error:\n${result.error || result.message || 'Unknown error'}`;
                 cleanupStatus.className = 'mt-2 alert alert-danger';
            }
        } catch (error) {
            console.error("Cleanup fetch error:", error);
            cleanupStatus.textContent = 'Failed to trigger cleanup process. Network error or server unreachable.';
            cleanupStatus.className = 'mt-2 alert alert-danger';
        } finally {
             cleanupButton.disabled = false; // Re-enable button
        }
    });
});
</script>
{% endblock %}
</file>

<file path="templates/index.html">
{% extends "base.html" %}
{% block title %}Data Verification Dashboard{% endblock %}
{% block content %}
    <div class="jumbotron mt-4"> {# Added mt-4 for spacing below fixed navbar #}
        <h1>Dashboard</h1>
        <p class="lead">Select a metric below to view the detailed checks, or see the latest Z-Score summary below.</p>
        <div class="row row-cols-1 row-cols-md-2 row-cols-lg-3 g-4">
            {% for metric in metrics %}
            <div class="col">
                <div class="card h-100 metric-card">
                    <div class="card-body d-flex flex-column">
                        <h5 class="card-title">{{ metric }}</h5>
                        <p class="card-text flex-grow-1">View details for {{ metric }}.</p>
                        {# Generate the URL using url_for #}
                        {% set metric_url = url_for('metric.metric_page', metric_name=metric) %}
                        <a href="{{ metric_url }}" class="btn btn-primary metric-link">View Details</a>
                        {# Debug: Display the generated URL #}
                        <span class="text-muted small mt-1">Debug URL: {{ metric_url }}</span>
                    </div>
                </div>
            </div>
            {% endfor %}
        </div>
    </div>
    <!-- Z-Score Summary Table -->
    {% if not summary_data.empty %}
    <h2>Latest Change Z-Score Summary</h2>
    <div class="table-responsive"> <!-- Make table scrollable on small screens -->
        <table class="table table-striped table-bordered table-hover table-sm">
            <thead class="thead-light"> {# thead-light might not be standard BS5, but harmless #}
                <tr>
                    <th>Fund Code</th>
                    {# Use the new summary_metrics list which contains combined names #}
                    {% for full_metric_name in summary_metrics %}
                    <th>{{ full_metric_name }}</th>
                    {% endfor %}
                </tr>
            </thead>
            <tbody>
                {% for fund_code, row in summary_data.iterrows() %}
                <tr>
                    {# Corrected url_for to point to the general fund detail page #}
                    <td><a href="{{ url_for('fund.fund_detail', fund_code=fund_code) }}" title="View all metrics for {{ fund_code }}">{{ fund_code }}</a></td>
                    {# Iterate through the same new list for data access #}
                    {% for full_metric_name in summary_metrics %}
                        {% set z_score = row[full_metric_name] %}
                        {% if z_score is none or z_score != z_score %}
                            {# Handle NaN/None - use base.html styling implicitly #}
                            <td class="text-muted fst-italic">N/A</td> {# Using BS5 classes #}
                        {% else %}
                            {# Apply conditional styling based on Z-score value - Use classes defined in base.html #}
                            {% set z_abs = z_score|abs %}
                            {% set cell_class = '' %}
                            {% if z_abs > 3.0 %}
                                {% set cell_class = 'table-danger' %}
                            {% elif z_abs > 2.0 %}
                                {% set cell_class = 'table-warning' %}
                            {% endif %}
                            <td class="{{ cell_class }}">{{ "%.2f"|format(z_score) }}</td>
                        {% endif %}
                    {% endfor %}
                </tr>
                {% endfor %}
            </tbody>
        </table>
    </div>
    {% else %}
    <div class="alert alert-warning" role="alert">
        No Z-score data could be generated for the summary table. Check the console logs for errors.
    </div>
    {% endif %}
    <!-- Add a link to the new API Data Retrieval page - Kept original url_for -->
    <div class="mt-4 mb-4 p-3 border rounded bg-light">
        <h5>Get Data via API (Simulated)</h5>
        <p>Select funds and dates to simulate retrieving data from the Rex API.</p>
        <a href="{{ url_for('api_bp.get_data_page') }}" class="btn btn-success btn-sm">Go to Get Data Page</a>
    </div>
    <!-- Add a link to the Securities page - url_for already matches base.html -->
    <div class="mt-4 p-3 border rounded bg-light">
        <h5>Securities Data Check</h5>
        <p>View checks for individual securities based on latest daily changes.</p>
        <a href="{{ url_for('security.securities_page') }}" class="btn btn-info btn-sm">View Securities Check</a>
    </div>
{% endblock %}
{% block scripts %}
{# Add any page-specific scripts here if needed in the future #}
{% endblock %}
</file>

<file path="templates/metric_page_js.html">
<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <title>{{ metric_name }} Check (JS)</title>
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
    <!-- Include Chart.js and the date adapter -->
    <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/chartjs-adapter-date-fns/dist/chartjs-adapter-date-fns.bundle.min.js"></script>
    <style>
        body { padding-top: 5rem; }
        .chart-container-wrapper { margin-bottom: 15px; padding: 10px; border: 1px solid #eee; }
        .chart-canvas { max-height: 400px; }
        .metrics-table { margin-top: 15px; margin-bottom: 25px; font-size: 0.9em; }
        .metrics-table th, .metrics-table td { padding: 4px 8px; border: 1px solid #dee2e6; }
        .missing-warning { color: red; font-weight: bold; }
        .high-z { background-color: #fff3cd; }
        .very-high-z { background-color: #f8d7da; font-weight: bold; }
    </style>
</head>
<body>
    <nav class="navbar navbar-expand-md navbar-dark bg-dark fixed-top">
        <a class="navbar-brand" href="/">Data Verification</a>
        <!-- Navbar content -->
    </nav>
    <main role="main" class="container-fluid">
        <h1>{{ metric_name }} Check</h1>
        <p>Latest Data Date: <strong>{{ latest_date }}</strong></p>
        <p>Charts sorted by the maximum absolute <strong>Change Z-Score</strong> across all columns for the fund (most deviation first).</p>
        <!-- Toggle Switch for S&P Data -->
        <div id="sp-toggle-container" class="form-group" style="display: none;"> <!-- Initially hidden, JS will show if needed -->
            <div class="form-check form-switch"><input class="form-check-input" type="checkbox" role="switch" id="toggleSpData">
                <label class="form-check-label" for="toggleSpData">Show S&P Comparison Data</label>
            </div>
        </div>
        <!-- End Toggle Switch -->
        {% if not missing_funds.empty %}
            <div class="alert alert-warning" role="alert">
                <strong>Warning:</strong> The following funds are missing data for the latest date ({{ latest_date }}):
                {{ missing_funds.index.tolist() | join(', ') }}
            </div>
        {% endif %}
        {% if error_message %}
        <div class="alert alert-danger" role="alert">
          {{ error_message }}
        </div>
        {% endif %}
        <!-- Data passed from Flask, embedded as JSON -->
        <script type="application/json" id="chartData">
            {{ charts_data_json | safe }}
        </script>
        <div id="chartsArea">
            <!-- Charts will be rendered here by JavaScript -->
        </div>
        <!-- END: Metrics Table -->
    </main>
    <!-- Link to the external JavaScript module -->
    <script type="module" src="{{ url_for('static', filename='js/main.js') }}"></script>
    <!-- Bootstrap JS (optional) -->
    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.5.4/dist/umd/popper.min.js"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"></script>
</body>
</html>
</file>

<file path="templates/securities_page.html">
lockdown-install.js:1 Removing unpermitted intrinsics
Alpha002:181 Initializing Chart with original data...
main.js:29 DOM fully loaded and parsed
{% extends 'base.html' %}
{% block title %}Security Data Check{% endblock %}
{% block head_extra %}
  {# Link the new CSS file #}
  <link rel="stylesheet" href="{{ url_for('static', filename='css/style.css') }}">
{% endblock %}
{% block content %}
<div class="container-fluid mt-4"> {# Use container-fluid for wider tables #}
    <div class="d-flex justify-content-between align-items-center mb-3">
        <h2>Security Data Check</h2>
        <a href="{{ url_for('exclusion_bp.manage_exclusions') }}" class="btn btn-outline-secondary btn-sm">Manage Exclusions</a>
    </div>
    <p class="text-muted">Potential data issues based on the latest daily change Z-score. Filters and sorting are applied server-side.</p>
    {% if message %}
    <div class="alert alert-warning alert-dismissible fade show" role="alert">
        {{ message }}
        <button type="button" class="btn-close" data-bs-dismiss="alert" aria-label="Close"></button>
    </div>
    {% endif %}
    {# --- Search and Filter Form --- #}
    {# Combined into one form submitting GET requests #}
    <form method="GET" action="{{ url_for('security.securities_page') }}" class="mb-3 p-3 border rounded bg-light" id="filter-form">
        <div class="row g-2 align-items-end">
            {# Search Box #}
            <div class="col-md-4">
                <label for="search_term" class="form-label">Search by {{ id_col_name }}</label>
                <input type="text" name="search_term" id="search_term" class="form-control form-control-sm" placeholder="Enter search term..." value="{{ search_term or '' }}">
            </div>
            {# Dynamic Filters #}
            {% if filter_options %}
                {% for column, options in filter_options.items() %}
                <div class="col-md-2">
                    <label for="filter-{{ column|replace(' ', '_') }}" class="form-label">{{ column }}</label>
                    <select id="filter-{{ column|replace(' ', '_') }}" name="filter_{{ column }}" class="form-select form-select-sm">
                        <option value="">All</option>
                        {% for option in options %}
                        <option value="{{ option }}" {% if active_filters.get(column) == option|string %}selected{% endif %}>{{ option }}</option>
                        {% endfor %}
                    </select>
                </div>
                {% endfor %}
            {% endif %}
            {# Buttons #}
            <div class="col-md-auto">
                <button class="btn btn-primary btn-sm" type="submit">Apply Filters</button>
                {# Clear button redirects to the base URL without filters/search #}
                 {% if search_term or active_filters %}
                    <a href="{{ url_for('security.securities_page') }}" class="btn btn-secondary btn-sm">Clear All</a>
                {% endif %}
            </div>
        </div>
        {# Hidden fields to preserve pagination/sorting state if needed (though typically sorting/filtering resets page to 1) #}
        {# <input type="hidden" name="sort_by" value="{{ current_sort_by }}"> #}
        {# <input type="hidden" name="sort_order" value="{{ current_sort_order }}"> #}
    </form>
    {# --- Data Table Section --- #}
    {% if securities_data %}
    <div class="table-responsive">
        <table class="table table-striped table-hover table-sm small caption-top" id="securities-table">
            {# Add table caption for summary #}
            {% if pagination %}
            <caption class="pb-1">
                Displaying {{ securities_data|length }} of {{ pagination.total_items }} total securities. 
                (Page {{ pagination.page }} of {{ pagination.total_pages }})
            </caption>
            {% endif %}
            <thead class="table-light sticky-top"> {# Make header sticky #}
                <tr>
                    {# Generate sortable headers #}
                    {% for col_name in column_order %}
                        {% set is_sort_col = (col_name == current_sort_by or (col_name == 'Change Z-Score' and current_sort_by is none)) %}
                        {# Determine next sort order: flip if current column, default to asc otherwise #}
                        {% set next_sort_order = 'asc' if is_sort_col and current_sort_order == 'desc' else 'desc' %}
                        {# Base arguments, including current search and filters #}
                        {% set sort_args = request.args.to_dict() %}
                        {% set _ = sort_args.pop('page', None) %}
                        {% set _ = sort_args.update({'sort_by': col_name, 'sort_order': next_sort_order}) %}
                        {# Construct the URL for the sort link #}
                        {% set sort_url = url_for('security.securities_page', **sort_args) %}
                        {# Add classes for styling and potential JS hooks #}
                        <th class="sortable {{ 'sorted-' + current_sort_order if is_sort_col else '' }}" 
                            data-column-name="{{ col_name }}">
                            <a href="{{ sort_url }}" class="text-decoration-none text-dark">
                                {{ col_name }} 
                                {% if is_sort_col %}
                                    <span class="sort-indicator ms-1">{{ '▲' if current_sort_order == 'asc' else '▼' }}</span>
                                {% endif %}
                            </a>
                        </th>
                    {% endfor %}
                </tr>
            </thead>
            <tbody id="securities-table-body">
                {% for row in securities_data %}
                    {% set z_score = row['Change Z-Score'] %}
                    {% set abs_z_score = z_score|abs if z_score is not none else 0 %} {# Calculate here #}
                    {% set row_class = 'table-danger' if abs_z_score >= 3 else ('table-warning' if abs_z_score >= 2 else '') %}
                    <tr class="{{ row_class }}">
                        {% for col_name in column_order %}
                            <td>
                                {# Link for ID column (which should now be ISIN) #}
                                {% if col_name == id_col_name %}
                                    {# The row object should contain the ISIN value under the key specified by id_col_name #}
                                    <a href="{{ url_for('security.security_details', metric_name='Spread', security_id=row[id_col_name]|urlencode) }}">
                                        {{ row[id_col_name] }}
                                    </a>
                                {# Formatting for numeric columns #}
                                {% elif col_name == 'Change Z-Score' and row[col_name] is not none %}
                                    {{ "%.2f"|format(row[col_name]) }}
                                {% elif row[col_name] is number %}
                                    {# Consider if specific formatting is needed for other numbers #}
                                    {{ row[col_name]|round(3) }}
                                {# Display other values (string, None) #}
                                {% else %}
                                    {{ row[col_name] if row[col_name] is not none else '' }} {# Display empty for None #}
                                {% endif %}
                            </td>
                        {% endfor %}
                    </tr>
                {% endfor %}
            </tbody>
        </table>
    </div>
    {# --- Pagination Controls --- #}
    {% if pagination and pagination.total_pages > 1 %}
        <nav aria-label="Security data navigation">
            <ul class="pagination pagination-sm justify-content-center">
                {# Previous Page Link #}
                <li class="page-item {{ 'disabled' if not pagination.has_prev }}">
                    <a class="page-link" href="{{ pagination.url_for_page(pagination.prev_num) if pagination.has_prev else '#' }}" aria-label="Previous">
                        <span aria-hidden="true">&laquo;</span>
                    </a>
                </li>
                {# Page Number Links (using context variables calculated in view) #}
                 {% set start_page = pagination.start_page_display %}
                 {% set end_page = pagination.end_page_display %}
                 {% if start_page > 1 %}
                     <li class="page-item"><a class="page-link" href="{{ pagination.url_for_page(1) }}">1</a></li>
                     {% if start_page > 2 %}
                         <li class="page-item disabled"><span class="page-link">...</span></li>
                     {% endif %}
                 {% endif %}
                 {% for p in range(start_page, end_page + 1) %}
                    <li class="page-item {{ 'active' if p == pagination.page }}">
                        <a class="page-link" href="{{ pagination.url_for_page(p) }}">{{ p }}</a>
                    </li>
                {% endfor %}
                 {% if end_page < pagination.total_pages %}
                     {% if end_page < pagination.total_pages - 1 %}
                         <li class="page-item disabled"><span class="page-link">...</span></li>
                     {% endif %}
                     <li class="page-item"><a class="page-link" href="{{ pagination.url_for_page(pagination.total_pages) }}">{{ pagination.total_pages }}</a></li>
                 {% endif %}
                {# Next Page Link #}
                <li class="page-item {{ 'disabled' if not pagination.has_next }}">
                    <a class="page-link" href="{{ pagination.url_for_page(pagination.next_num) if pagination.has_next else '#' }}" aria-label="Next">
                        <span aria-hidden="true">&raquo;</span>
                    </a>
                </li>
            </ul>
        </nav>
    {% endif %}
    {# Handle case where filters resulted in no data (message displayed above) #}
    {% elif not message %}
     <div class="alert alert-info mt-3" role="alert">
        No security metrics data is currently available or matches the selected criteria.
    </div>
    {% endif %}
</div>
{% endblock %}
{% block scripts %}
{{ super() }}
{# Remove client-side filter script block - All filtering/sorting is server-side #}
{# Optional: Add JS for minor enhancements like highlighting sort column, but core logic is server-side #}
{% endblock %}
</file>

<file path="templates/security_details_page.html">
{% extends 'base.html' %}
{% block title %}Security Details: {{ security_id }} - {{ metric_name }}{% endblock %}
{% block content %}
<div class="container mt-4">
    <h1>{{ security_id }} - {{ metric_name }}</h1>
    <p>Latest data as of: <strong>{{ latest_date }}</strong></p>
    {# Display Static Info #}
    {% if static_info %}
    <div class="mb-3 p-3 border rounded bg-light small">
        <strong>Static Details:</strong> 
        {% for key, value in static_info.items() %}
            <span class="me-3">{{ key }}: {{ value }}</span>
        {% endfor %}
    </div>
    {% endif %}
    {# Primary Chart Area (Metric + Price Overlay) #}
    <h2>{{ metric_name }} and Price Time Series</h2>
    <div id="primary-chart-container" class="mb-4" style="height: 450px; position: relative;">
        <canvas id="primarySecurityChart"></canvas>
    </div>
    {# Duration Chart Area #}
    <h2>Duration Time Series</h2>
    <div id="duration-chart-container" class="mb-4" style="height: 450px; position: relative;">
        <canvas id="durationSecurityChart"></canvas>
    </div>
    {# Spread Duration Chart Area #}
    <h2>Spread Duration Time Series</h2>
    <div id="spread-duration-chart-container" class="mb-4" style="height: 450px; position: relative;">
        <canvas id="spreadDurationChart"></canvas>
    </div>
    {# Spread Chart Area #}
    <h2>Spread Time Series</h2>
    <div id="spread-chart-container" class="mb-4" style="height: 450px; position: relative;">
        <canvas id="spreadChart"></canvas>
    </div>
    {# Hidden script tag for chart data #}
    <script id="chartJsonData" type="application/json">
        {{ chart_data_json|safe }}
    </script>
</div>
{% endblock %}
{% block scripts %}
{# Include Chart.js library (Likely inherited from base.html, but included here for safety/explicitness if needed) #}
{# If base.html already includes it, this line can be removed #}
<script src="https://cdn.jsdelivr.net/npm/chart.js"></script> 
<script>
    document.addEventListener('DOMContentLoaded', function() {
        // Get the chart data passed from Flask
        const chartDataElement = document.getElementById('chartJsonData');
        const chartData = JSON.parse(chartDataElement.textContent);
        /* --- REMOVED: Helper function to replace 0 with null --- 
        const processDatasetData = (dataset) => {
            if (dataset && dataset.data) {
                dataset.data = dataset.data.map(value => (value === 0 ? null : value));
            }
            return dataset; // Return the processed dataset (or original if no data)
        };
        */
        // --- REMOVED: Processing datasets before creating charts --- 
        // No longer needed as backend uses NaN
        /*
        if (chartData.primary_datasets) {
            chartData.primary_datasets = chartData.primary_datasets.map(processDatasetData);
        }
        if (chartData.duration_dataset) {
            chartData.duration_dataset = processDatasetData(chartData.duration_dataset);
        }
        // Add processing for new datasets if they were added
        if (chartData.sp_duration_dataset) { chartData.sp_duration_dataset = processDatasetData(chartData.sp_duration_dataset); }
        if (chartData.spread_duration_dataset) { chartData.spread_duration_dataset = processDatasetData(chartData.spread_duration_dataset); }
        if (chartData.sp_spread_duration_dataset) { chartData.sp_spread_duration_dataset = processDatasetData(chartData.sp_spread_duration_dataset); }
        if (chartData.spread_dataset) { chartData.spread_dataset = processDatasetData(chartData.spread_dataset); }
        if (chartData.sp_spread_dataset) { chartData.sp_spread_dataset = processDatasetData(chartData.sp_spread_dataset); }
        */
        // --- Render Primary Chart (Metric + Price) --- 
        const primaryCtx = document.getElementById('primarySecurityChart').getContext('2d');
        if (chartData.primary_datasets && chartData.primary_datasets.length > 0) {
            // Check if *any* primary dataset has actual data (nulls are ignored by this check)
            const hasPrimaryData = chartData.primary_datasets.some(dataset => 
                dataset.data && dataset.data.some(value => value !== null)
            );
            if (hasPrimaryData) {
                new Chart(primaryCtx, {
                    type: 'line',
                    data: {
                        labels: chartData.labels,
                        datasets: chartData.primary_datasets // Use datasets directly
                    },
                    options: {
                        responsive: true,
                        maintainAspectRatio: false,
                        spanGaps: true, // Let Chart.js handle gaps where data is null
                        scales: {
                            x: {
                                title: {
                                    display: true,
                                    text: 'Date'
                                }
                            },
                            y: { // Primary Y-axis (for the main metric)
                                position: 'left',
                                title: {
                                    display: true,
                                    text: {{ metric_name|tojson }} + ' Value' 
                                }
                            },
                            y1: { // Secondary Y-axis (for Price)
                                position: 'right',
                                title: {
                                    display: true,
                                    text: 'Price'
                                },
                                grid: {
                                    drawOnChartArea: false, 
                                },
                            }
                        },
                        plugins: {
                            legend: {
                                position: 'top',
                            },
                            tooltip: {
                                mode: 'index',
                                intersect: false,
                            }
                        }
                    }
                });
            } else {
                 document.getElementById('primary-chart-container').innerHTML = '<p class="text-info">No primary data available for this security.</p>';
            }
        } else {
            console.error('No primary datasets found for the primary chart.');
            document.getElementById('primary-chart-container').innerHTML = '<p class="text-danger">Could not render primary chart: No data available.</p>';
        }
        // --- Render Duration Chart (Now includes SP) ---
        const durationCtx = document.getElementById('durationSecurityChart').getContext('2d');
        const durationDatasets = [];
        if (chartData.duration_dataset) {
            // Check if there's actual data (not null)
            if (chartData.duration_dataset.data && chartData.duration_dataset.data.some(value => value !== null)) {
                durationDatasets.push(chartData.duration_dataset);
            }
        }
        if (chartData.sp_duration_dataset) {
             if (chartData.sp_duration_dataset.data && chartData.sp_duration_dataset.data.some(value => value !== null)) {
                durationDatasets.push(chartData.sp_duration_dataset);
            }
        }
        if (durationDatasets.length > 0) {
            new Chart(durationCtx, {
                type: 'line',
                data: {
                    labels: chartData.labels,
                    datasets: durationDatasets // Use combined list
                },
                options: {
                    responsive: true,
                    maintainAspectRatio: false,
                    spanGaps: true, 
                    scales: {
                        x: { title: { display: true, text: 'Date' } },
                        y: { title: { display: true, text: 'Duration Value' } }
                    },
                    plugins: { /* legend, tooltip */ }
                }
            });
        } else {
             console.log('No non-null data points found for Duration or SP Duration chart.');
             document.getElementById('duration-chart-container').innerHTML = '<p class="text-info">No duration data available for this security.</p>';
        }
        // --- Render Spread Duration Chart (Includes SP) ---
        const spreadDurationCtx = document.getElementById('spreadDurationChart').getContext('2d');
        const spreadDurationDatasets = [];
        if (chartData.spread_duration_dataset && chartData.spread_duration_dataset.data && chartData.spread_duration_dataset.data.some(v => v !== null)) {
            spreadDurationDatasets.push(chartData.spread_duration_dataset);
        }
        if (chartData.sp_spread_duration_dataset && chartData.sp_spread_duration_dataset.data && chartData.sp_spread_duration_dataset.data.some(v => v !== null)) {
            spreadDurationDatasets.push(chartData.sp_spread_duration_dataset);
        }
        if (spreadDurationDatasets.length > 0) {
            new Chart(spreadDurationCtx, {
                type: 'line',
                data: { labels: chartData.labels, datasets: spreadDurationDatasets },
                options: { 
                    responsive: true, maintainAspectRatio: false, spanGaps: true, 
                    scales: { x: { title: { display: true, text: 'Date' } }, y: { title: { display: true, text: 'Spread Duration Value' } } },
                    plugins: { /* legend, tooltip */ }
                }
            });
        } else {
            console.log('No data found for Spread Duration or SP Spread Duration chart.');
            document.getElementById('spread-duration-chart-container').innerHTML = '<p class="text-info">Spread Duration data not available for this security.</p>';
        }
        // --- Render Spread Chart (Includes SP) ---
        const spreadCtx = document.getElementById('spreadChart').getContext('2d');
        const spreadDatasets = [];
        if (chartData.spread_dataset && chartData.spread_dataset.data && chartData.spread_dataset.data.some(v => v !== null)) {
            spreadDatasets.push(chartData.spread_dataset);
        }
        if (chartData.sp_spread_dataset && chartData.sp_spread_dataset.data && chartData.sp_spread_dataset.data.some(v => v !== null)) {
            spreadDatasets.push(chartData.sp_spread_dataset);
        }
        if (spreadDatasets.length > 0) {
            new Chart(spreadCtx, {
                type: 'line',
                data: { labels: chartData.labels, datasets: spreadDatasets },
                options: {
                    responsive: true, maintainAspectRatio: false, spanGaps: true, 
                    scales: { x: { title: { display: true, text: 'Date' } }, y: { title: { display: true, text: 'Spread Value' } } },
                    plugins: { /* legend, tooltip */ }
                }
            });
        } else {
             console.log('No data found for Spread or SP Spread chart.');
             document.getElementById('spread-chart-container').innerHTML = '<p class="text-info">Spread data not available for this security.</p>';
        }
    });
</script>
{% endblock %}
</file>

<file path="templates/spread_duration_comparison_details_page.html">
{% extends "base.html" %}
{# Note: security_name is not explicitly passed, using security_id for title/breadcrumb #}
{% block title %}Spread Duration Comparison Details: {{ security_id }}{% endblock %}
{% block content %}
<div class="container mt-4">
    <nav aria-label="breadcrumb">
        <ol class="breadcrumb">
            <li class="breadcrumb-item"><a href="{{ url_for('spread_duration_comparison_bp.summary') }}">Spread Duration Comparison Summary</a></li> {# Updated Link #}
            {# Displaying ID as primary identifier if name isn't guaranteed #}
            <li class="breadcrumb-item active" aria-current="page">{{ security_id }}</li>
        </ol>
    </nav>
    <h1>Spread Duration Comparison Details: {{ security_id }}</h1> {# Updated Title #}
    {# Add static info if available #}
    {% if static_info %}
        {% for key, value in static_info.items() %}
             {% if key != id_column_name %} {# Avoid repeating the ID #}
                <span class="text-muted me-3"><strong>{{ key }}:</strong> {{ value }}</span>
             {% endif %}
        {% endfor %}
    {% endif %}
    <div class="row mt-4 mb-4">
        <div class="col-md-6">
            <h2>Comparison Statistics</h2>
            {% if stats_summary %}
                <ul class="list-group">
                    <li class="list-group-item d-flex justify-content-between align-items-center">
                        Level Correlation
                        <span class="badge bg-primary rounded-pill">{{ stats_summary.Level_Correlation if stats_summary.Level_Correlation is not none else 'N/A' }}</span>
                    </li>
                    <li class="list-group-item d-flex justify-content-between align-items-center">
                        Change Correlation
                        <span class="badge bg-primary rounded-pill">{{ stats_summary.Change_Correlation if stats_summary.Change_Correlation is not none else 'N/A' }}</span>
                    </li>
                    <li class="list-group-item d-flex justify-content-between align-items-center">
                        Mean Absolute Difference
                        <span class="badge bg-secondary rounded-pill">{{ stats_summary.Mean_Abs_Diff if stats_summary.Mean_Abs_Diff is not none else 'N/A' }}</span>
                    </li>
                    <li class="list-group-item d-flex justify-content-between align-items-center">
                        Max Absolute Difference
                        <span class="badge bg-secondary rounded-pill">{{ stats_summary.Max_Abs_Diff if stats_summary.Max_Abs_Diff is not none else 'N/A' }}</span>
                    </li>
                    <li class="list-group-item d-flex justify-content-between align-items-center">
                        Data Points (Original)
                        <span class="badge bg-info rounded-pill">{{ stats_summary.Total_Points - stats_summary.NaN_Count_Orig }} / {{ stats_summary.Total_Points }}</span>
                    </li>
                    <li class="list-group-item d-flex justify-content-between align-items-center">
                        Data Points (New)
                        <span class="badge bg-info rounded-pill">{{ stats_summary.Total_Points - stats_summary.NaN_Count_New }} / {{ stats_summary.Total_Points }}</span>
                    </li>
                    <li class="list-group-item d-flex justify-content-between align-items-center">
                        Same Date Range?
                        <span class="badge {{ 'bg-success' if stats_summary.Same_Date_Range else 'bg-warning' }} rounded-pill">{{ 'Yes' if stats_summary.Same_Date_Range else 'No' }}</span>
                    </li>
                    {# Add date range details #}
                    <li class="list-group-item">
                        <small>Orig Range: {{ stats_summary.Start_Date_Orig or 'N/A' }} to {{ stats_summary.End_Date_Orig or 'N/A' }}</small><br>
                        <small>New Range: {{ stats_summary.Start_Date_New or 'N/A' }} to {{ stats_summary.End_Date_New or 'N/A' }}</small>
                    </li>
                </ul>
            {% else %}
                <p>No comparison statistics could be calculated.</p>
            {% endif %}
        </div>
        {# Placeholder for additional stats or info if needed #}
    </div>
    <h2>Time Series Comparison</h2>
    <p class="text-muted">Overlayed Spread Duration from Original (sec_Spread duration) and New (sec_Spread durationSP) datasets.</p> {# Updated Text #}
    <div>
        <canvas id="comparisonChart"></canvas>
    </div>
    {# Embed chart data as JSON for JavaScript #}
    <script type="application/json" id="comparisonChartData">
        {{ chart_data | tojson | safe }}
    </script>
</div>
{% endblock %}
{% block scripts %}
{{ super() }}
{# We need Chart.js - ensure it's included in base.html or here #}
<script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
<script src="https://cdn.jsdelivr.net/npm/chartjs-adapter-date-fns/dist/chartjs-adapter-date-fns.bundle.min.js"></script> {# Include Date Adapter #}
<script>
    document.addEventListener('DOMContentLoaded', function() {
        const chartDataElement = document.getElementById('comparisonChartData');
        const comparisonChartCanvas = document.getElementById('comparisonChart');
        if (chartDataElement && comparisonChartCanvas) {
            try {
                const chartData = JSON.parse(chartDataElement.textContent);
                const ctx = comparisonChartCanvas.getContext('2d');
                console.log("Initializing Spread Duration Comparison Chart...");
                new Chart(ctx, {
                    type: 'line',
                    data: chartData, // Direct use of data from JSON
                    options: {
                        responsive: true,
                        maintainAspectRatio: true,
                        plugins: {
                            legend: {
                                position: 'top',
                            },
                            title: {
                                display: true,
                                text: 'Spread Duration Comparison: {{ security_id|tojson }}' // Use security_id
                            },
                            tooltip: {
                                mode: 'index', // Show tooltips for all datasets at the same index
                                intersect: false
                            }
                        },
                        scales: {
                            x: {
                                type: 'time', // Use time scale
                                time: {
                                    unit: 'day',
                                    tooltipFormat: 'yyyy-MM-dd', // Format for tooltip
                                    displayFormats: { // Formats for axis labels
                                        day: 'MMM d, yyyy'
                                    }
                                },
                                title: {
                                    display: true,
                                    text: 'Date'
                                }
                            },
                            y: {
                                title: {
                                    display: true,
                                    text: 'Spread Duration' // Updated Axis Label
                                }
                            }
                        },
                        interaction: {
                             intersect: false,
                             mode: 'index',
                        },
                         elements: {
                            point:{ // Reduce point size for potentially dense data
                                radius: 2
                            }
                        }
                        // No complex scriptable options needed for basic display
                    }
                });
            } catch (error) {
                console.error("Error parsing spread duration chart data or rendering chart:", error);
                if (comparisonChartCanvas.parentElement) {
                    comparisonChartCanvas.parentElement.innerHTML = '<p class="text-danger">Error rendering spread duration chart.</p>';
                }
            }
        } else {
             console.warn("Chart data or canvas element not found for spread duration comparison chart.");
        }
    });
</script>
{% endblock %}
</file>

<file path="templates/spread_duration_comparison_page.html">
{% extends "base.html" %}
{% block title %}Spread Duration Comparison Summary{% endblock %}
{% block content %}
<div class="container-fluid mt-4"> {# Use container-fluid for wider view #}
    <h1>Spread Duration Comparison: Original (sec_Spread duration) vs. New (sec_Spread durationSP)</h1> {# Updated Title #}
    <p class="text-muted">Comparing Spread Duration between the two datasets. Click on a Security ID/Name to see details. Use filters or click column headers to sort. Pagination applied.</p> {# Updated Text #}
    {# Display message if any #}
    {% if message %}
    <div class="alert alert-warning alert-dismissible fade show" role="alert">
        {{ message }}
        <button type="button" class="btn-close" data-bs-dismiss="alert" aria-label="Close"></button>
    </div>
    {% endif %}
    {# --- Filter Form --- #}
    {% if filter_options %}
    <form method="GET" action="{{ url_for('spread_duration_comparison_bp.summary') }}" class="mb-3 p-3 border rounded bg-light" id="filter-form"> {# Updated Action URL #}
        <h5>Filters</h5>
        <div class="row g-2 align-items-end">
            {% for column, options in filter_options.items() %}
            <div class="col-md-2 mb-2">
                <label for="filter-{{ column }}" class="form-label">{{ column }}</label>
                <select id="filter-{{ column }}" name="filter_{{ column }}" class="form-select form-select-sm">
                    <option value="">All</option>
                    {% for option in options %}
                    <option value="{{ option }}" {% if active_filters.get(column) == option|string %}selected{% endif %}>{{ option }}</option>
                    {% endfor %}
                </select>
            </div>
            {% endfor %}
            <div class="col-md-3 d-flex align-items-end">
                <div class="form-check form-switch mb-1">
                    <input class="form-check-input" type="checkbox" role="switch" id="showSoldToggle" name="show_sold" value="true" {% if show_sold %}checked{% endif %}>
                    <label class="form-check-label" for="showSoldToggle"><small>Show Sold Securities</small></label>
                </div>
            </div>
            <div class="col-md-auto">
                <button type="submit" class="btn btn-primary btn-sm">Apply Filters</button>
                {# Add a clear button only if filters are active #}
                {% if active_filters %}
                <a href="{{ url_for('spread_duration_comparison_bp.summary') }}" class="btn btn-secondary btn-sm">Clear Filters</a> {# Updated Clear URL #}
                {% endif %}
            </div>
        </div>
        {# Hidden fields to preserve current sort order when applying filters - page is implicitly reset #}
        <input type="hidden" name="sort_by" value="{{ current_sort_by }}">
        <input type="hidden" name="sort_order" value="{{ current_sort_order }}">
    </form>
    {% endif %}
    {# --- Data Table --- #}
    <div class="table-responsive">
        <table class="table table-striped table-hover table-sm caption-top" id="spread-duration-comparison-table"> {# Updated Table ID #}
             {# Add table caption for summary #}
             {% if pagination %}
             <caption class="pb-1">
                 Displaying {{ table_data|length }} of {{ pagination.total_items }} total securities.
                 (Page {{ pagination.page }} of {{ pagination.total_pages }})
             </caption>
             {% endif %}
            <thead class="table-light">
                <tr>
                    {# Loop through the columns passed from the view #}
                    {% for col_name in columns_to_display %}
                        {% set is_sort_col = (col_name == current_sort_by) %}
                        {% set next_sort_order = 'asc' if is_sort_col and current_sort_order == 'desc' else 'desc' %}
                        {# Base arguments, including current filters #}
                        {% set sort_args = request.args.to_dict() %}
                        {% set _ = sort_args.pop('page', None) %}
                        {% set _ = sort_args.update({'sort_by': col_name, 'sort_order': next_sort_order}) %}
                        {# Generate URL for this header #}
                        {% set sort_url = url_for('spread_duration_comparison_bp.summary', **sort_args) %} {# Updated Sort URL #}
                        {# Add classes for styling and JS #}
                        <th class="sortable {{ 'sorted-' + current_sort_order if is_sort_col else '' }}"
                            data-column-name="{{ col_name }}">
                            <a href="{{ sort_url }}" class="text-decoration-none text-dark">
                                {{ col_name.replace('_', ' ') | title }}
                                {% if is_sort_col %}
                                    <span class="sort-indicator ms-1">{{ '▲' if current_sort_order == 'asc' else '▼' }}</span>
                                {% endif %}
                            </a>
                        </th>
                    {% endfor %}
                </tr>
            </thead>
            <tbody id="spread-duration-comparison-table-body"> {# Updated tbody ID #}
                {% set id_col = id_column_name %}
                {% for row in table_data %}
                <tr>
                    {# Loop through the same columns to ensure order matches header #}
                    {% for col_name in columns_to_display %}
                        <td>
                            {% if col_name == id_col %}
                                <a href="{{ url_for('spread_duration_comparison_bp.spread_duration_comparison_details', security_id=row[id_col]|urlencode) }}">{{ row[id_col] }}</a> {# Updated Detail URL #}
                            {% elif col_name in ['Level_Correlation', 'Change_Correlation'] and row[col_name] is not none %}
                                {# Attempt to format as float, handle potential errors gracefully #}
                                {% set formatted_val = "%.3f"|format(row[col_name]|float) if row[col_name] is number else row[col_name] %}
                                {{ formatted_val }}
                            {% elif col_name in ['Mean_Abs_Diff', 'Max_Abs_Diff'] and row[col_name] is not none %}
                                {% set formatted_val = "%.2f"|format(row[col_name]|float) if row[col_name] is number else row[col_name] %}
                                {{ formatted_val }}
                             {% elif col_name == 'Same_Date_Range' %}
                                 <span class="badge {{ 'bg-success' if row[col_name] else 'bg-warning' }}">{{ 'Yes' if row[col_name] else 'No' }}</span>
                            {% elif col_name.endswith('_Date') and row[col_name] %}
                                 {# Assume date strings are already YYYY-MM-DD from view #}
                                {{ row[col_name] }}
                            {% elif row[col_name] is number %}
                                {{ row[col_name]|round(3) }} {# General numeric formatting #}
                            {% else %}
                                {{ row[col_name] if row[col_name] is not none else '' }} {# Display strings or empty #}
                            {% endif %}
                        </td>
                    {% endfor %}
                </tr>
                {% else %}
                <tr>
                    <td colspan="{{ columns_to_display|length }}" class="text-center">No spread duration comparison data available matching the current filters.</td> {# Updated Message #}
                </tr>
                {% endfor %}
            </tbody>
        </table>
    </div>
    {# --- Pagination Controls --- #}
    {% if pagination and pagination.total_pages > 1 %}
    <nav aria-label="Spread duration comparison data navigation">
        <ul class="pagination pagination-sm justify-content-center">
            {# Helper macro for generating pagination links #}
            {% macro page_link(page_num, text=None, is_disabled=False, is_active=False) %}
                {% set link_args = request.args.to_dict() %}
                {% set _ = link_args.update({'page': page_num, 'sort_by': current_sort_by, 'sort_order': current_sort_order}) %}
                {% set url = url_for('spread_duration_comparison_bp.summary', **link_args) if page_num else '#' %} {# Updated URL #}
                <li class="page-item {{ 'disabled' if is_disabled }} {{ 'active' if is_active }}">
                    <a class="page-link" href="{{ url }}" {% if is_active %}aria-current="page"{% endif %}>{{ text or page_num }}</a>
                </li>
            {% endmacro %}
            {{ page_link(pagination.prev_num, '&laquo;', is_disabled=not pagination.has_prev) }}
            {# Simplified pagination display logic #}
            {% set window = 2 %}
            {% set start_page = [1, pagination.page - window] | max %}
            {% set end_page = [pagination.total_pages, pagination.page + window] | min %}
            {% if start_page > 1 %}
                {{ page_link(1) }}
                {% if start_page > 2 %}
                    <li class="page-item disabled"><span class="page-link">...</span></li>
                {% endif %}
            {% endif %}
            {% for p in range(start_page, end_page + 1) %}
                {{ page_link(p, is_active=(p == pagination.page)) }}
            {% endfor %}
            {% if end_page < pagination.total_pages %}
                {% if end_page < pagination.total_pages - 1 %}
                    <li class="page-item disabled"><span class="page-link">...</span></li>
                {% endif %}
                {{ page_link(pagination.total_pages) }}
            {% endif %}
            {{ page_link(pagination.next_num, '&raquo;', is_disabled=not pagination.has_next) }}
        </ul>
    </nav>
    {% endif %}
</div>
{% endblock %}
{% block scripts %}
{{ super() }}
{# No specific JS needed for this page unless client-side sorting is added back #}
{% endblock %}
</file>

<file path="templates/weight_check_page.html">
{% extends "base.html" %}
{% block title %}Weight Check (100% Target){% endblock %}
{% block content %}
<div class="container-fluid mt-4">
    <h1 class="mb-4">Weight Check (100% Target)</h1>
    <p>The following tables show weights from <code>{{ fund_filename }}</code> and <code>{{ bench_filename }}</code>. Cells highlighted in <span class="text-danger fw-bold">red</span> indicate weights that are not exactly 100.00%.</p>
    {% macro render_weight_table(title, filename, data, date_headers) %}
        <h2 class="mt-4">{{ title }} <small class="text-muted fs-6">({{ filename }})</small></h2>
        {% if data %}
            <div class="table-responsive">
                <table class="table table-bordered table-sm table-hover weight-check-table">
                    <thead class="thead-light">
                        <tr>
                            <th>Fund Code</th>
                            {% for date in date_headers %}
                                <th class="text-center">{{ date }}</th>
                            {% endfor %}
                        </tr>
                    </thead>
                    <tbody>
                        {% for fund_code, date_values in data.items()|sort %}
                            <tr>
                                <td class="fw-bold">{{ fund_code }}</td>
                                {% for date in date_headers %}
                                    {% set cell_data = date_values.get(date) %}
                                    {% if cell_data %}
                                        {% set value_str = cell_data.value_str %}
                                        {% set is_100 = cell_data.is_100 %}
                                        <td class="text-center {{ 'table-danger' if not is_100 }}">
                                            {{ value_str }}
                                        </td>
                                    {% else %}
                                        <td class="text-center text-muted">-</td> {# Data missing for this date #}
                                    {% endif %}
                                {% endfor %}
                            </tr>
                        {% else %}
                            <tr>
                                <td colspan="{{ date_headers|length + 1 }}" class="text-center">No data found in {{ filename }}.</td>
                            </tr>
                        {% endfor %}
                    </tbody>
                </table>
            </div>
        {% else %}
            <div class="alert alert-warning" role="alert">
                Could not load or process data from <code>{{ filename }}</code>. Check server logs.
            </div>
        {% endif %}
    {% endmacro %}
    {{ render_weight_table("Fund Weights", fund_filename, fund_data, date_headers) }}
    {{ render_weight_table("Benchmark Weights", bench_filename, bench_data, date_headers) }}
</div>
<style>
/* Optional: Adjust table layout for wide tables */
.weight-check-table th,
.weight-check-table td {
    white-space: nowrap; /* Prevent wrapping in cells */
    font-size: 0.85rem; /* Slightly smaller font */
    padding: 0.3rem 0.5rem; /* Adjust padding */
}
.weight-check-table th:first-child,
.weight-check-table td:first-child {
    position: sticky;
    left: 0;
    background-color: #f8f9fa; /* Light background for sticky column */
    z-index: 1;
}
.weight-check-table thead th {
    position: sticky;
    top: 0; /* Stick headers to top */
    background-color: #e9ecef;
    z-index: 2;
}
</style>
{% endblock %}
</file>

<file path="utils.py">
# This file contains utility functions used throughout the Simple Data Checker application.
# These functions provide common helper functionalities like parsing specific string formats
# or validating data types, helping to keep the main application logic cleaner.
"""
Utility functions for the Flask application.
"""
import re
import pandas as pd
import os
import logging
# Configure logging
# Removed basicConfig - logging is now configured centrally in app.py
# logger = logging.getLogger(__name__) # Get logger for this module
DEFAULT_RELATIVE_PATH = 'Data'
def _is_date_like(column_name):
    """Check if a column name looks like a date (e.g., YYYY-MM-DD or DD/MM/YYYY).
    Updated regex to match both common formats.
    Ensures the pattern matches the entire string.
    """
    # Regex explanation:
    # ^            - Start of string
    # (\d{4}-\d{2}-\d{2}) - Group 1: YYYY-MM-DD format
    # |            - OR
    # (\d{2}/\d{2}/\d{4}) - Group 2: DD/MM/YYYY format
    # $            - End of string
    pattern = r'^((\d{4}-\d{2}-\d{2})|(\d{2}/\d{2}/\d{4}))$'
    return bool(re.match(pattern, str(column_name)))
def parse_fund_list(fund_string):
    """Safely parses the fund list string like '[FUND1,FUND2]' or '[FUND1]' into a list.
       Handles potential errors and variations in spacing.
    """
    if not isinstance(fund_string, str) or not fund_string.startswith('[') or not fund_string.endswith(']'):
        return [] # Return empty list if format is unexpected
    try:
        # Remove brackets and split by comma
        content = fund_string[1:-1]
        # Split by comma, strip whitespace from each element
        funds = [f.strip() for f in content.split(',') if f.strip()]
        return funds
    except Exception as e:
        print(f"Error parsing fund string '{fund_string}': {e}")
        return []
def get_data_folder_path(app_root_path=None):
    """
    Retrieves the data folder path, prioritizing config.py, then a default.
    Resolves the path to an absolute path relative to the provided
    app_root_path or the current working directory.
    Args:
        app_root_path (str, optional): The root path of the application or script.
                                      If None, os.getcwd() is used. Defaults to None.
    Returns:
        str: The absolute path to the data folder.
    """
    chosen_path_source = f"default ('{DEFAULT_RELATIVE_PATH}')"
    chosen_path = DEFAULT_RELATIVE_PATH
    try:
        # Attempt to import the path from config.py
        # This approach allows config.py to be optional or lack the variable
        from config import DATA_FOLDER
        if isinstance(DATA_FOLDER, str) and DATA_FOLDER.strip():
            chosen_path = DATA_FOLDER.strip()
            chosen_path_source = "config.py (DATA_FOLDER)"
        else:
            logging.warning(f"DATA_FOLDER in config.py is not a valid non-empty string. Falling back to default '{DEFAULT_RELATIVE_PATH}'.")
    except ImportError:
        logging.info("config.py not found or DATA_FOLDER not defined. Using default path.")
    except Exception as e:
        logging.error(f"An unexpected error occurred while trying to read DATA_FOLDER from config.py: {e}. Falling back to default.")
    # Determine the base path for resolving relative paths
    if app_root_path:
        base_path = app_root_path
        base_path_source = "provided app_root_path"
    else:
        # Use current working directory if no app_root_path is provided
        # This is suitable for standalone scripts run from the project root,
        # but can be unreliable otherwise. Providing app_root_path is safer.
        base_path = os.getcwd()
        base_path_source = "os.getcwd()"
        logging.warning(f"No app_root_path provided to get_data_folder_path. Using current working directory ({base_path}) as base. Ensure this is the intended behavior.")
    # Resolve the chosen path (relative or absolute) to an absolute path
    try:
        if os.path.isabs(chosen_path):
            absolute_path = chosen_path
            logging.info(f"Using absolute path from {chosen_path_source}: {absolute_path}")
        else:
            absolute_path = os.path.abspath(os.path.join(base_path, chosen_path))
            logging.info(f"Resolved relative path from {chosen_path_source} ('{chosen_path}') relative to {base_path_source} ('{base_path}') to absolute path: {absolute_path}")
        # Basic check: does the directory exist? Log a warning if not.
        # Consider adding creation logic if needed, but for now, just check.
        if not os.path.isdir(absolute_path):
             logging.warning(f"The determined data folder path does not exist or is not a directory: {absolute_path}")
        return absolute_path
    except Exception as e:
        logging.error(f"Failed to resolve data folder path '{chosen_path}' relative to '{base_path}'. Error: {e}. Falling back to default relative path '{DEFAULT_RELATIVE_PATH}' resolved against '{base_path}'.")
        # Fallback resolution in case of error during primary resolution
        try:
             fallback_path = os.path.abspath(os.path.join(base_path, DEFAULT_RELATIVE_PATH))
             logging.info(f"Using fallback absolute path: {fallback_path}")
             if not os.path.isdir(fallback_path):
                 logging.warning(f"The fallback data folder path does not exist or is not a directory: {fallback_path}")
             return fallback_path
        except Exception as final_e:
            logging.critical(f"CRITICAL: Failed even to resolve fallback path. Returning '.'. Error: {final_e}")
            # Absolute last resort
            return '.'
# Example usage (for testing purposes, typically called from app.py or scripts)
# if __name__ == '__main__':
#     # Simulate being called from an app context
#     script_dir = os.path.dirname(os.path.abspath(__file__))
#     project_root = os.path.dirname(script_dir) # Assuming utils.py is one level down from root
#     print(f"Simulating call with project root: {project_root}")
#     data_path_from_app = get_data_folder_path(app_root_path=project_root)
#     print(f"Data path (app context): {data_path_from_app}")
#
#     # Simulate being called from a standalone script without app_root_path
#     print("\nSimulating call without providing app_root_path (uses CWD):")
#     data_path_standalone = get_data_folder_path()
#     print(f"Data path (standalone context): {data_path_standalone}")
</file>

<file path="views/__init__.py">
"""
This file makes the 'views' directory a Python package.
"""
# You can leave this file empty or use it to import blueprints
# for easier registration in the app factory, though explicit imports
# in the factory as done currently are also perfectly fine.
</file>

<file path="views/api_views.py">
'''
Defines the Flask Blueprint for handling API data retrieval requests, including simulation,
fetching real data, saving data (with options for merging or overwriting),
and rerunning specific API calls.
'''
import os
import pandas as pd
from flask import Blueprint, render_template, request, current_app, jsonify
import datetime
from pandas.tseries.offsets import BDay
import time # Import the time module
#from tqs import tqs_query as tqs
# # Import the placeholder validation function
from data_validation import validate_data
# --- Feature Switch --- 
# Set to True to attempt real API calls, validation, and saving.
# Set to False to only simulate the API call (print to console).
USE_REAL_TQS_API = False
# ----------------------
# Blueprint Configuration
api_bp = Blueprint(
    'api_bp', __name__,
    template_folder='../templates',
    static_folder='../static'
)
def _simulate_and_print_tqs_call(QueryID, FundCodeList, StartDate, EndDate):
    '''Simulates calling the TQS API by printing the call signature.
    This function is used when USE_REAL_TQS_API is False.
    It does NOT interact with any external API.
    Returns:
        int: A simulated number of rows for status reporting.
    '''
    # Format the call signature exactly as requested: tqs(QueryID,[FundList],StartDate,EndDate)
    call_signature = f"tqs({QueryID}, {FundCodeList}, {StartDate}, {EndDate})"
    print(f"--- SIMULATING TQS API CALL (USE_REAL_TQS_API = False) ---")
    print(call_signature)
    print(f"--------------------------------------------------------")
    # Return a simulated row count for the summary table
    simulated_row_count = len(FundCodeList) * 10 if FundCodeList else 0 # Dummy calculation
    return simulated_row_count
def _fetch_real_tqs_data(QueryID, FundCodeList, StartDate, EndDate):
    '''Fetches real data from the TQS API.
    This function is called when USE_REAL_TQS_API is True.
    Replace the placeholder logic with the actual API interaction code.
    Args:
        QueryID: The query identifier.
        FundCodeList: List of fund codes.
        StartDate: Start date string (YYYY-MM-DD).
        EndDate: End date string (YYYY-MM-DD).
    Returns:
        pd.DataFrame or None: The DataFrame containing the fetched data,
                              or None if the API call fails or returns no data.
    '''
    current_app.logger.info(f"Attempting real TQS API call for QueryID: {QueryID}")
    print(f"--- EXECUTING REAL TQS API CALL (USE_REAL_TQS_API = True) --- ")
    print(f"tqs({QueryID}, {FundCodeList}, {StartDate}, {EndDate})")
    print(f"--------------------------------------------------------")
    dataframe = None # Initialize dataframe to None
    try:
        # --- !!! Replace this comment and the line below with the actual API call !!! ---
        # Ensure the `tqs` function/library is imported (commented out at the top)
        # dataframe = tqs.get_data(QueryID, FundCodeList, StartDate, EndDate) # Example real call
        print(dataframe.head()) if dataframe is not None else print("No data to display")
        pass # Remove this pass when uncommenting the line above
        # --- End of section to replace --- 
        # Check if the API returned valid data (e.g., a DataFrame)
        if dataframe is not None and isinstance(dataframe, pd.DataFrame):
            current_app.logger.info(f"Real TQS API call successful for QueryID: {QueryID}, Rows: {len(dataframe)}")
            return dataframe
        elif dataframe is None:
             # Explicitly handle the case where the API call itself returned None (e.g., planned failure or empty result coded as None)
             current_app.logger.warning(f"Real TQS API call for QueryID: {QueryID} returned None.")
             return None
        else:
            # Handle cases where the API returned something unexpected (not a DataFrame)
            current_app.logger.warning(f"Real TQS API call for QueryID: {QueryID} returned an unexpected data type: {type(dataframe)}.")
            return None # Treat unexpected types as failure
    except NameError as ne:
         # Specific handling if the tqs function isn't defined (import is commented out)
         current_app.logger.error(f"Real TQS API call failed for QueryID: {QueryID}. TQS function not imported/defined. Error: {ne}")
         print(f"    ERROR: TQS function not available. Ensure 'from tqs import tqs_query as tqs' is uncommented and the library is installed.")
         return None
    except Exception as e:
        # Handle API call errors (timeout, connection issues, authentication, etc.)
        current_app.logger.error(f"Real TQS API call failed for QueryID: {QueryID}. Error: {e}", exc_info=True)
        print(f"    ERROR during real API call: {e}")
        return None
# --- Helper Function to Get File Statuses ---
def get_data_file_statuses(data_folder):
    """
    Scans the data folder based on QueryMap.csv and returns status for each file.
    """
    statuses = []
    query_map_path = os.path.join(data_folder, 'QueryMap.csv')
    if not os.path.exists(query_map_path):
        current_app.logger.warning(f"QueryMap.csv not found at {query_map_path} for status check.")
        return statuses # Return empty list if map is missing
    try:
        query_map_df = pd.read_csv(query_map_path)
        if 'FileName' not in query_map_df.columns:
             current_app.logger.warning(f"QueryMap.csv at {query_map_path} is missing 'FileName' column.")
             return statuses
        date_column_candidates = ['Date', 'date', 'AsOfDate', 'ASOFDATE', 'Effective Date', 'Trade Date', 'Position Date'] # Add more candidates if needed
        for index, row in query_map_df.iterrows():
            filename = row['FileName']
            file_path = os.path.join(data_folder, filename)
            status_info = {
                'filename': filename,
                'exists': False,
                'last_modified': 'N/A',
                'latest_data_date': 'N/A',
                'funds_included': 'N/A' # Initialize new key
            }
            if os.path.exists(file_path):
                status_info['exists'] = True
                try:
                    # Get file modification time
                    mod_timestamp = os.path.getmtime(file_path)
                    status_info['last_modified'] = datetime.datetime.fromtimestamp(mod_timestamp).strftime('%Y-%m-%d %H:%M:%S')
                    # Try to read the CSV and find the latest date
                    try:
                        df = pd.read_csv(file_path, low_memory=False) # low_memory=False can help with mixed types
                        df_head = df.head()
                        # Determine the actual date column name
                        date_col = None
                        # --- Update Date Column Candidates --- 
                        date_column_candidates = ['Date', 'date', 'AsOfDate', 'ASOFDATE', 'Effective Date', 'Trade Date', 'Position Date'] 
                        found_cols = df_head.columns.str.strip()
                        current_app.logger.info(f"[{filename}] Checking for date columns: {date_column_candidates} in columns {found_cols.tolist()}") 
                        for candidate in date_column_candidates:
                            # Case-insensitive check
                            matching_cols = [col for col in found_cols if col.lower() == candidate.lower()]
                            if matching_cols:
                                date_col = matching_cols[0] # Use the actual name found
                                current_app.logger.info(f"[{filename}] Found date column: '{date_col}'") 
                                break # Found the first match
                        if date_col:
                            try:
                                # --- FIX: Use the full DataFrame's date column --- 
                                if date_col not in df.columns:
                                    # Handle case where column name from head differs slightly after full read (e.g., whitespace)
                                    # Find it again in the full df columns, case-insensitively
                                    corrected_date_col = None
                                    for col in df.columns:
                                        if col.strip().lower() == date_col.lower():
                                            corrected_date_col = col
                                            break
                                    if not corrected_date_col:
                                         raise ValueError(f"Date column '{date_col}' found in header but not in full DataFrame columns: {df.columns.tolist()}")
                                    date_col = corrected_date_col # Use the name from the full df
                                date_series = df[date_col] # Use the full series from the complete DataFrame
                                # --------------------------------------------------
                                current_app.logger.info(f"[{filename}] Attempting to parse full date column '{date_col}' (length: {len(date_series)}). Top 5 values: {date_series.head().to_list()}") 
                                # Try standard YYYY-MM-DD first
                                parsed_dates = pd.to_datetime(date_series, format='%Y-%m-%d', errors='coerce')
                                # If all are NaT, try DD/MM/YYYY
                                if parsed_dates.isnull().all():
                                    current_app.logger.info(f"[{filename}] Format YYYY-MM-DD failed, trying DD/MM/YYYY...") 
                                    parsed_dates = pd.to_datetime(date_series, format='%d/%m/%Y', errors='coerce')
                                # If still all NaT, try inferring (less reliable but fallback)
                                if parsed_dates.isnull().all():
                                     current_app.logger.warning(f"[{filename}] Both specific formats failed, trying to infer date format...") 
                                     parsed_dates = pd.to_datetime(date_series, errors='coerce', infer_datetime_format=True)
                                # Check if any dates were successfully parsed
                                if not parsed_dates.isnull().all():
                                    latest_date = parsed_dates.max()
                                    if pd.notna(latest_date):
                                        status_info['latest_data_date'] = latest_date.strftime('%Y-%m-%d')
                                        current_app.logger.info(f"[{filename}] Successfully found latest date: {status_info['latest_data_date']}") 
                                    else:
                                        status_info['latest_data_date'] = 'No Valid Dates Found'
                                        current_app.logger.warning(f"[{filename}] Parsed dates but found no valid max date (all NaT?).") 
                                else:
                                    status_info['latest_data_date'] = 'Date Parsing Failed'
                                    current_app.logger.warning(f"[{filename}] All parsing attempts failed for date column '{date_col}'.") 
                            except Exception as date_err:
                                current_app.logger.error(f"Error parsing date column '{date_col}' in {file_path}: {date_err}", exc_info=True)
                                status_info['latest_data_date'] = f'Error Parsing Date ({date_col})'
                        else:
                            status_info['latest_data_date'] = 'No Date Column Found/Parsed'
                            current_app.logger.warning(f"[{filename}] Could not find a suitable date column.") 
                        # --- Add Fund Code Extraction --- 
                        code_col = None
                        # FIX: Search for 'code' OR 'fund code' (case-insensitive)
                        code_candidates = ['code', 'fund code'] 
                        found_code_col_name = None
                        for candidate in code_candidates:
                             matches = [c for c in df.columns if c.strip().lower() == candidate]
                             if matches:
                                 found_code_col_name = matches[0] # Use the actual column name found
                                 break # Stop searching once found
                        if found_code_col_name:
                            code_col = found_code_col_name # Assign the found name to code_col
                            current_app.logger.info(f"[{filename}] Found Code column: '{code_col}'")
                            if not df.empty and code_col in df:
                                try:
                                    unique_funds = sorted([str(f) for f in df[code_col].unique() if pd.notna(f)])
                                    if unique_funds:
                                        if len(unique_funds) <= 5:
                                            status_info['funds_included'] = ', '.join(unique_funds)
                                        else:
                                            status_info['funds_included'] = ', '.join(unique_funds[:5]) + f' ... ({len(unique_funds)} total)'
                                        current_app.logger.info(f"[{filename}] Found funds: {status_info['funds_included']}")
                                    else:
                                        status_info['funds_included'] = 'No Codes Found'
                                except Exception as fund_err:
                                     current_app.logger.error(f"[{filename}] Error extracting funds from column '{code_col}': {fund_err}")
                                     status_info['funds_included'] = 'Error Extracting Funds'
                            else:
                                 status_info['funds_included'] = 'Code Column Empty?' # Should be covered by EmptyDataError usually
                        else:
                            status_info['funds_included'] = 'Code Column Missing'
                            current_app.logger.warning(f"[{filename}] Code column ('Code' or 'Fund Code') not found.")
                        # --- End Fund Code Extraction ---
                    except pd.errors.EmptyDataError:
                         status_info['latest_data_date'] = 'File is Empty'
                         status_info['funds_included'] = 'File is Empty' # Also set for funds
                         current_app.logger.warning(f"CSV file is empty: {file_path}")
                    except Exception as read_err:
                        status_info['latest_data_date'] = 'Read Error'
                        current_app.logger.error(f"Error reading CSV {file_path} for status check: {read_err}", exc_info=True)
                except Exception as file_err:
                     current_app.logger.error(f"Error accessing file properties for {file_path}: {file_err}", exc_info=True)
                     status_info['last_modified'] = 'Error Accessing File'
            statuses.append(status_info)
    except Exception as e:
        current_app.logger.error(f"Failed to process QueryMap.csv for file statuses: {e}", exc_info=True)
        # Optionally return a status indicating the map couldn't be processed
        return [{'filename': 'QueryMap Error', 'exists': False, 'last_modified': str(e), 'latest_data_date': '', 'funds_included': ''}]
    return statuses
# --- End Helper Function ---
@api_bp.route('/get_data')
def get_data_page():
    '''Renders the page for users to select parameters for API data retrieval.'''
    try:
        # Construct the path to FundList.csv relative to the app's instance path or root
        # Assuming DATA_FOLDER is configured relative to the app root
        data_folder = current_app.config.get('DATA_FOLDER', 'Data')
        fund_list_path = os.path.join(data_folder, 'FundList.csv')
        if not os.path.exists(fund_list_path):
            current_app.logger.error(f"FundList.csv not found at {fund_list_path}")
            return "Error: FundList.csv not found.", 500
        fund_df = pd.read_csv(fund_list_path)
        # Ensure required columns exist
        if not {'Fund Code', 'Total Asset Value USD', 'Picked'}.issubset(fund_df.columns):
             current_app.logger.error(f"FundList.csv is missing required columns.")
             return "Error: FundList.csv is missing required columns (Fund Code, Total Asset Value USD, Picked).", 500
        # Convert Total Asset Value to numeric, coercing errors
        fund_df['Total Asset Value USD'] = pd.to_numeric(fund_df['Total Asset Value USD'], errors='coerce')
        fund_df.dropna(subset=['Total Asset Value USD'], inplace=True) # Remove rows where conversion failed
        # Sort by Total Asset Value USD descending
        fund_df = fund_df.sort_values(by='Total Asset Value USD', ascending=False)
        # Convert Picked to boolean
        fund_df['Picked'] = fund_df['Picked'].astype(bool)
        # Prepare fund data for the template
        funds = fund_df.to_dict('records')
        # Calculate default end date (previous business day)
        default_end_date = (datetime.datetime.today() - BDay(1)).strftime('%Y-%m-%d')
        # --- Get Data File Statuses ---
        data_file_statuses = get_data_file_statuses(data_folder)
        # --- End Get Data File Statuses ---
    except Exception as e:
        current_app.logger.error(f"Error preparing get_data page: {e}", exc_info=True)
        # Provide a user-friendly error message, specific details are logged
        return f"An error occurred while preparing the data retrieval page: {e}", 500
    return render_template('get_data.html', funds=funds, default_end_date=default_end_date, data_file_statuses=data_file_statuses)
# --- Helper function to find key columns ---
def _find_key_columns(df, filename):
    """Attempts to find the date and fund/identifier columns."""
    date_col = None
    fund_col = None
    # Date column candidates (add more if needed)
    date_candidates = ['Date', 'date', 'AsOfDate', 'ASOFDATE', 'Effective Date', 'Trade Date', 'Position Date']
    # Fund/ID column candidates
    fund_candidates = ['Code', 'Fund Code', 'Fundcode', 'security id', 'SecurityID', 'Security Name'] # Broadened list
    found_cols = df.columns.str.strip().str.lower()
    for candidate in date_candidates:
        if candidate.lower() in found_cols:
            # Find the original casing
            original_cols = [col for col in df.columns if col.strip().lower() == candidate.lower()]
            if original_cols:
                date_col = original_cols[0]
                current_app.logger.info(f"[{filename}] Found date column: '{date_col}'")
                break
    for candidate in fund_candidates:
        if candidate.lower() in found_cols:
            # Find the original casing
            original_cols = [col for col in df.columns if col.strip().lower() == candidate.lower()]
            if original_cols:
                fund_col = original_cols[0]
                current_app.logger.info(f"[{filename}] Found fund/ID column: '{fund_col}'")
                break
    if not date_col:
        current_app.logger.warning(f"[{filename}] Could not reliably identify a date column from candidates: {date_candidates}")
    if not fund_col:
        current_app.logger.warning(f"[{filename}] Could not reliably identify a fund/ID column from candidates: {fund_candidates}")
    return date_col, fund_col
@api_bp.route('/run_api_calls', methods=['POST'])
def run_api_calls():
    '''Handles the form submission to trigger API calls (real or simulated).'''
    try:
        # Get data from form
        data = request.get_json()
        days_back = int(data.get('days_back', 30)) # Default to 30 days if not provided
        end_date_str = data.get('end_date')
        selected_funds = data.get('funds', [])
        overwrite_mode = data.get('overwrite_mode', False) # Get the new overwrite flag
        if not end_date_str:
            # Should have been validated client-side, but handle defensively
            return jsonify({"status": "error", "message": "End date is required."}), 400
        if not selected_funds:
             return jsonify({"status": "error", "message": "At least one fund must be selected."}), 400
        # Calculate dates
        end_date = pd.to_datetime(end_date_str)
        start_date = end_date - pd.Timedelta(days=days_back)
        # Format dates as YYYY-MM-DD for the TQS call
        start_date_tqs_str = start_date.strftime('%Y-%m-%d')
        end_date_tqs_str = end_date.strftime('%Y-%m-%d')
        # --- Get Query Map ---
        data_folder = current_app.config.get('DATA_FOLDER', 'Data')
        query_map_path = os.path.join(data_folder, 'QueryMap.csv')
        if not os.path.exists(query_map_path):
            return jsonify({"status": "error", "message": f"QueryMap.csv not found at {query_map_path}"}), 500
        query_map_df = pd.read_csv(query_map_path)
        if not {'QueryID', 'FileName'}.issubset(query_map_df.columns):
            return jsonify({"status": "error", "message": "QueryMap.csv missing required columns (QueryID, FileName)."}), 500
        # Sort queries: ts_*, pre_*, others
        def sort_key(query):
            filename = query.get('FileName', '').lower()
            if filename.startswith('ts_'):
                return 0
            elif filename.startswith('pre_'):
                return 1
            else:
                # Keep original order for non-ts/pre files relative to each other
                # Or assign a consistent rank if needed (e.g., based on original index)
                return 2 # All others get rank 2 for now
        # Add original index to preserve relative order for non-ts/pre files
        queries_with_indices = list(enumerate(query_map_df.to_dict('records')))
        def sort_key_with_index(item):
            index, query = item
            filename = query.get('FileName', '').lower()
            if filename.startswith('ts_'):
                return (0, index) # Sort by ts_ first, then original index
            elif filename.startswith('pre_'):
                return (1, index) # Sort by pre_ next, then original index
            else:
                return (2, index) # Others last, sorted by original index
        queries_with_indices.sort(key=sort_key_with_index)
        # Extract the sorted queries list
        queries = [item[1] for item in queries_with_indices]
        current_app.logger.info(f"Processing order after sorting: {[q.get('FileName', 'N/A') for q in queries]}")
        results_summary = []
        total_queries = len(queries)
        completed_queries = 0
        all_ts_files_succeeded = True # Flag to track success of ts_ files
        # Determine mode for logging/messaging
        current_mode_desc = "SIMULATED mode" if not USE_REAL_TQS_API else ("REAL API mode (Overwrite Enabled)" if overwrite_mode else "REAL API mode (Merge/Append)")
        current_app.logger.info(f"--- Starting /run_api_calls in {current_mode_desc} ---")
        # Loop through sorted queries
        for query_info in queries:
            # Extract query details safely
            query_id = query_info.get('QueryID')
            file_name = query_info.get('FileName')
            # Make sure QueryID and FileName exist
            if not query_id or not file_name:
                 current_app.logger.warning(f"Skipping entry due to missing QueryID or FileName: {query_info}")
                 # Add a summary entry indicating the skip?
                 summary = {
                    "query_id": query_id or "N/A", "file_name": file_name or "N/A",
                    "status": "Skipped (Missing QueryID/FileName)",
                    "simulated_rows": None, "simulated_lines": None,
                    "actual_rows": None, "actual_lines": None,
                    "save_action": "N/A", "validation_status": "Not Run"
                 }
                 results_summary.append(summary)
                 # Don't increment completed_queries if it fundamentally couldn't run
                 continue 
            output_path = os.path.join(data_folder, file_name)
            # Initialize summary for this query (moved after basic validation)
            summary = {
                "query_id": query_id,
                "file_name": file_name,
                "status": "Pending", # Initial status
                "simulated_rows": None, # Initialize keys
                "simulated_lines": None,
                "actual_rows": None,
                "actual_lines": None,
                "save_action": "N/A",
                "validation_status": "Not Run"
            }
            # Determine file type
            file_type = 'other'
            if file_name.lower().startswith('ts_'):
                file_type = 'ts'
            elif file_name.lower().startswith('pre_'):
                file_type = 'pre'
            current_app.logger.info(f"--- Starting Process for QueryID: {query_id}, File: {file_name} (Type: {file_type}) ---")
            # Skip pre_ files if any ts_ file failed
            if file_type == 'pre' and not all_ts_files_succeeded:
                current_app.logger.warning(f"[{file_name}] Skipping pre_ file because a previous ts_ file failed processing.")
                summary['status'] = 'Skipped (Previous TS Failure)'
                summary['validation_status'] = 'Not Run'
                summary['save_action'] = 'Skipped'
                results_summary.append(summary)
                completed_queries += 1 # It was processed (by skipping)
                continue # Move to the next query
            # --- Existing try block for processing a single query ---
            try:
                if USE_REAL_TQS_API:
                    # --- Real API Call, Validation, and Save Logic ---
                    df_new = None
                    df_to_save = None # Will hold the final DF to be saved
                    force_overwrite = overwrite_mode # Use the flag passed from frontend
                    try:
                        # 1. Fetch Real Data (Common step)
                        df_new = _fetch_real_tqs_data(query_id, selected_funds, start_date_tqs_str, end_date_tqs_str)
                        # --- Handle fetch result ---
                        if df_new is None:
                            current_app.logger.warning(f"[{file_name}] No data returned from API call for QueryID {query_id}.")
                            summary['status'] = 'Warning - No data returned from API'
                            summary['validation_status'] = 'Skipped (API Returned None)'
                            if file_type == 'ts': all_ts_files_succeeded = False # Mark failure for ts_ files
                        elif df_new.empty:
                            current_app.logger.warning(f"[{file_name}] Empty DataFrame returned from API call for QueryID {query_id}.")
                            summary['status'] = 'Warning - Empty data returned from API'
                            # Don't skip validation if empty, allow saving empty file
                            summary['validation_status'] = 'OK (Empty Data)'
                            # An empty dataframe is still data, proceed to save/overwrite logic below
                            df_to_save = df_new # Allow overwriting with empty data if needed
                            summary['actual_rows'] = 0 # Explicitly set 0 rows
                        else: # Data fetched successfully (and not empty)
                            current_app.logger.info(f"[{file_name}] Fetched {len(df_new)} new rows.")
                            summary['actual_rows'] = len(df_new)
                            df_to_save = df_new # Prepare to save this new data (might be modified below)
                        # --- Type-Specific Processing (only if df_new is not None) ---
                        if df_new is not None: # Includes empty DataFrame case
                            # == TS File Processing ==
                            if file_type == 'ts':
                                current_app.logger.info(f"[{file_name}] Processing as ts_ file (Overwrite Mode: {force_overwrite}).")
                                try:
                                    # 2. Identify Key Columns in New Data (TS specific)
                                    if not df_new.empty: # Only check non-empty DFs
                                        date_col_new, fund_col_new = _find_key_columns(df_new, f"{file_name} (New TS Data)")
                                        if not date_col_new or not fund_col_new:
                                            err_msg = f"Could not find essential date/fund columns in fetched ts_ data for {file_name}. Cannot proceed."
                                            current_app.logger.error(f"[{file_name}] {err_msg}")
                                            raise ValueError(err_msg) # Caught below
                                    else:
                                        date_col_new, fund_col_new = None, None # Cannot find cols in empty df
                                        current_app.logger.info(f"[{file_name}] Skipping key column check for empty TS data.")
                                    # 3. Handle Existing File (TS specific - merge/append logic)
                                    if force_overwrite:
                                        current_app.logger.info(f"[{file_name}] Overwrite Mode enabled. Skipping check for existing file.")
                                        if os.path.exists(output_path):
                                            summary['save_action'] = 'Overwritten (User Request)'
                                        else:
                                            summary['save_action'] = 'Created (Overwrite Mode)'
                                        # df_to_save is already df_new
                                    elif os.path.exists(output_path):
                                        current_app.logger.info(f"[{file_name}] TS file exists. Reading existing data for merge/append.")
                                        try:
                                            df_existing = pd.read_csv(output_path, low_memory=False)
                                            if not df_existing.empty:
                                                # --- Existing TS merge/append logic ---
                                                if date_col_new and fund_col_new: # Requires new data cols to be found
                                                    date_col_existing, fund_col_existing = _find_key_columns(df_existing, f"{file_name} (Existing TS)")
                                                    if date_col_existing == date_col_new and fund_col_existing == fund_col_new:
                                                        date_col = date_col_new # Use consistent names
                                                        fund_col = fund_col_new
                                                        # Date range comparison (optional check)
                                                        # ... (keep existing date comparison logic if desired) ...
                                                        # Combine Data (Append/Overwrite) logic
                                                        current_app.logger.info(f"[{file_name}] Combining new data for funds/IDs {df_new[fund_col].unique()} with existing data.")
                                                        funds_in_new_data = df_new[fund_col].unique()
                                                        # Ensure fund columns have compatible types (e.g., strings)
                                                        try:
                                                            df_existing[fund_col] = df_existing[fund_col].astype(str)
                                                            df_new[fund_col] = df_new[fund_col].astype(str) # Ensure new is also str
                                                            funds_in_new_data = [str(f) for f in funds_in_new_data]
                                                        except Exception as type_err:
                                                             current_app.logger.warning(f"[{file_name}] Potential type mismatch in fund column \'{fund_col}\' during filtering: {type_err}. Filtering might be incomplete.")
                                                        df_existing_filtered = df_existing[~df_existing[fund_col].isin(funds_in_new_data)]
                                                        current_app.logger.info(f"[{file_name}] Kept {len(df_existing_filtered)} rows from existing file (other funds).")
                                                        # Concatenate
                                                        df_combined = pd.concat([df_existing_filtered, df_new], ignore_index=True)
                                                        # Optional Sort
                                                        try:
                                                            df_combined = df_combined.sort_values(by=[date_col, fund_col])
                                                        except Exception as sort_err:
                                                             current_app.logger.warning(f"[{file_name}] Could not sort combined data: {sort_err}")
                                                        df_to_save = df_combined # Update the df to save
                                                        summary['save_action'] = 'Combined (Append/Overwrite)'
                                                        current_app.logger.info(f"[{file_name}] Prepared combined data ({len(df_to_save)} rows).")
                                                    else: # Key columns mismatch
                                                        current_app.logger.warning(f"[{file_name}] Key columns mismatch between existing ({date_col_existing}, {fund_col_existing}) and new ({date_col_new}, {fund_col_new}). Overwriting entire file.")
                                                        # df_to_save is already df_new
                                                        summary['save_action'] = 'Overwritten (Column Mismatch)'
                                                else: # Cannot proceed with merge if new cols weren't found (e.g., new data was empty)
                                                    current_app.logger.warning(f"[{file_name}] Cannot merge TS data as key columns were not identified in new data. Overwriting.")
                                                    # df_to_save is already df_new
                                                    summary['save_action'] = 'Overwritten (Merge Skipped)'
                                            else: # Existing file is empty
                                                current_app.logger.warning(f"[{file_name}] Existing TS file is empty. Overwriting.")
                                                # df_to_save is already df_new
                                                summary['save_action'] = 'Overwritten (Existing Empty)'
                                        except pd.errors.EmptyDataError:
                                            current_app.logger.warning(f"[{file_name}] Existing TS file is empty (EmptyDataError). Overwriting.")
                                            # df_to_save is already df_new
                                            summary['save_action'] = 'Overwritten (Existing Empty)'
                                        except Exception as read_err:
                                            current_app.logger.error(f"[{file_name}] Error reading existing TS file: {read_err}. Overwriting.", exc_info=True)
                                            # df_to_save is already df_new
                                            summary['save_action'] = 'Overwritten (Read Error)'
                                            all_ts_files_succeeded = False # Failed to read existing TS file properly
                                    else: # No existing file (and not forcing overwrite)
                                        current_app.logger.info(f"[{file_name}] TS file does not exist (or overwrite mode is on and file was absent). Creating new file.")
                                        # df_to_save is already df_new
                                        summary['save_action'] = 'Created'
                                except ValueError as ve: # Catch _find_key_columns error
                                    current_app.logger.error(f"[{file_name}] TS validation failed: {ve}")
                                    summary['status'] = f'Error - TS Validation Failed: {ve}'
                                    summary['validation_status'] = 'Failed (Missing Columns)'
                                    all_ts_files_succeeded = False # TS validation failed
                                    df_to_save = None # Don't save if validation fails
                            # == PRE File Processing ==
                            elif file_type == 'pre':
                                current_app.logger.info(f"[{file_name}] Processing as pre_ file (checking column count).")
                                # df_to_save is already df_new (or empty df) - Always overwritten
                                if os.path.exists(output_path):
                                    try:
                                        # Check column count consistency (only if new data is not empty)
                                        if not df_new.empty:
                                            existing_header_df = pd.read_csv(output_path, nrows=0, low_memory=False) # Read only header
                                            existing_cols = existing_header_df.columns.tolist()
                                            new_cols = df_new.columns.tolist()
                                            if force_overwrite or len(existing_cols) != len(new_cols) or set(existing_cols) != set(new_cols):
                                                if force_overwrite:
                                                    current_app.logger.info(f"[{file_name}] Overwriting pre_ file as requested by user.")
                                                    summary['save_action'] = 'Overwritten (User Request)'
                                                else:
                                                    current_app.logger.warning(f"[{file_name}] Column count/names mismatch between existing pre_ file ({len(existing_cols)} cols: {existing_cols}) and new data ({len(new_cols)} cols: {new_cols}). Overwriting.")
                                                    summary['save_action'] = 'Overwritten (Column Mismatch)'
                                            else:
                                                current_app.logger.info(f"[{file_name}] Existing pre_ file found with matching columns. Overwriting.")
                                                summary['save_action'] = 'Overwritten'
                                        else: # New data is empty, just overwrite
                                            current_app.logger.info(f"[{file_name}] New data for pre_ file is empty. Overwriting existing file.")
                                            summary['save_action'] = 'Overwritten (New Data Empty)'
                                    except pd.errors.EmptyDataError:
                                         current_app.logger.warning(f"[{file_name}] Existing pre_ file is empty (EmptyDataError). Overwriting.")
                                         summary['save_action'] = 'Overwritten (Existing Empty)'
                                    except Exception as read_err:
                                         current_app.logger.error(f"[{file_name}] Error reading existing pre_ file header: {read_err}. Overwriting.", exc_info=True)
                                         summary['save_action'] = 'Overwritten (Read Error)'
                                else: # No existing pre_ file
                                    current_app.logger.info(f"[{file_name}] Pre_ file does not exist. Creating new file.")
                                    summary['save_action'] = 'Created'
                                # Note: df_to_save remains df_new for pre_ files.
                            # == Other File Processing ==
                            else: # Handle other files (e.g., sec_*)
                                # For now, treat 'other' files like 'pre_' files (overwrite, check column counts)
                                # This avoids the date/fund column check which might fail for sec_* files too
                                current_app.logger.info(f"[{file_name}] Processing as 'other' file type (using column count check).")
                                # df_to_save is already df_new (or empty df)
                                if os.path.exists(output_path):
                                    try:
                                        if not df_new.empty:
                                            existing_header_df = pd.read_csv(output_path, nrows=0, low_memory=False)
                                            existing_cols = existing_header_df.columns.tolist()
                                            new_cols = df_new.columns.tolist()
                                            if force_overwrite or len(existing_cols) != len(new_cols) or set(existing_cols) != set(new_cols):
                                                if force_overwrite:
                                                    current_app.logger.info(f"[{file_name}] Overwriting 'other' file as requested by user.")
                                                    summary['save_action'] = 'Overwritten (User Request)'
                                                else:
                                                    current_app.logger.warning(f"[{file_name}] Column count/names mismatch for 'other' file. Overwriting.")
                                                    summary['save_action'] = 'Overwritten (Column Mismatch)'
                                            else:
                                                current_app.logger.info(f"[{file_name}] Existing 'other' file found with matching columns. Overwriting.")
                                                summary['save_action'] = 'Overwritten'
                                        else:
                                            current_app.logger.info(f"[{file_name}] New data for 'other' file is empty. Overwriting existing file.")
                                            summary['save_action'] = 'Overwritten (New Data Empty)'
                                    except pd.errors.EmptyDataError:
                                         current_app.logger.warning(f"[{file_name}] Existing 'other' file is empty (EmptyDataError). Overwriting.")
                                         summary['save_action'] = 'Overwritten (Existing Empty)'
                                    except Exception as read_err:
                                         current_app.logger.error(f"[{file_name}] Error reading existing 'other' file header: {read_err}. Overwriting.", exc_info=True)
                                         summary['save_action'] = 'Overwritten (Read Error)'
                                else:
                                    current_app.logger.info(f"[{file_name}] 'Other' file does not exist. Creating new file.")
                                    summary['save_action'] = 'Created'
                            # 4. Save the Final DataFrame (Common step, if df_to_save is valid)
                            if df_to_save is not None: # Allow saving empty dataframe to overwrite/create
                                current_app.logger.info(f"[{file_name}] Attempting to save {len(df_to_save)} rows to {output_path} (Action: {summary['save_action']})")
                                # ... existing warning log ...
                                try:
                                    df_to_save.to_csv(output_path, index=False, header=True)
                                    current_app.logger.info(f"[{file_name}] Successfully saved data to {output_path}")
                                    summary['status'] = 'OK - Data Saved'
                                    # Update lines_in_file count after successful save
                                    try:
                                         with open(output_path, 'r', encoding='utf-8') as f:
                                              # Store in actual_lines as this is real API mode
                                              summary['actual_lines'] = sum(1 for line in f)
                                    except Exception:
                                         summary['actual_lines'] = 'N/A' # Or len(df_to_save)+1?
                                    # Validation step (consider if validate_data needs adjustment for pre_/other files)
                                    summary['validation_status'] = validate_data(df_to_save, file_name)
                                    current_app.logger.info(f"[{file_name}] Validation status: {summary['validation_status']})")
                                    # If validation fails for a TS file, should it mark all_ts_files_succeeded = False? Maybe.
                                    # if file_type == 'ts' and 'Error' in summary['validation_status']:
                                    #    all_ts_files_succeeded = False
                                    #    current_app.logger.warning(f"[{file_name}] TS file validation failed, marking overall TS process as failed.")
                                except Exception as write_err:
                                    current_app.logger.error(f"[{file_name}] Error writing final data to {output_path}: {write_err}", exc_info=True)
                                    summary['status'] = f'Error - Failed to save file: {write_err}'
                                    summary['validation_status'] = 'Failed (Save Error)'
                                    if file_type == 'ts': all_ts_files_succeeded = False # Save failed for ts_ file
                            # This case handles if df_new was None initially, or if df_to_save was set to None due to TS validation error
                            elif df_new is None: 
                                pass # Status already set when df_new was None
                            elif df_to_save is None and file_type == 'ts':
                                pass # Status already set from TS validation error
                            else: # Should not happen? Log if it does.
                                 current_app.logger.error(f"[{file_name}] Reached unexpected state where df_to_save is None but no prior error logged.")
                                 summary['status'] = 'Error - Internal Logic Error (df_to_save is None)'
                    except Exception as proc_err: # Catch errors during the fetch/process stage for one file
                        current_app.logger.error(f"Error processing real data for QueryID {query_id}, File {file_name}: {proc_err}", exc_info=True)
                        summary['status'] = f'Error - Processing failed: {proc_err}'
                        summary['validation_status'] = 'Failed (Processing Error)'
                        if file_type == 'ts': all_ts_files_succeeded = False # Processing failed for ts_ file
                else: # Simulate API Call (keep existing)
                    # ... existing simulation logic ...
                    # status = "Simulated OK" # Update summary status if needed
                    simulated_rows = _simulate_and_print_tqs_call(query_id, selected_funds, start_date_tqs_str, end_date_tqs_str)
                    summary['status'] = "Simulated OK"
                    summary['simulated_rows'] = simulated_rows
                    summary['simulated_lines'] = simulated_rows + 1 if simulated_rows > 0 else 0
                    summary['actual_rows'] = None # Ensure actual keys are None in sim mode
                    summary['actual_lines'] = None
            except Exception as outer_err: # Catch unexpected errors during the processing of a single query's try block
                 current_app.logger.error(f"Unexpected outer error processing QueryID {query_id} ({file_name}): {outer_err}", exc_info=True)
                 # Ensure status reflects the outer error if not already set
                 if summary['status'] == 'Pending' or summary['status'].startswith('Warning'):
                      summary['status'] = f"Outer Processing Error: {outer_err}"
                 # Mark TS as failed if outer error occurs
                 if file_type == 'ts': all_ts_files_succeeded = False 
            # Append results for this query
            results_summary.append(summary)
            completed_queries += 1
            # Pause between real API calls (keep existing)
            if USE_REAL_TQS_API and completed_queries < total_queries: # Avoid pause after last call
                print(f"Pausing for 3 seconds before next real API call ({completed_queries}/{total_queries})...") # Optional status message
                time.sleep(3) 
        # After loop (keep existing)
        mode_message = "SIMULATED mode" if not USE_REAL_TQS_API else ("REAL API mode (Overwrite Enabled)" if overwrite_mode else "REAL API mode (Merge/Append)")
        final_status = "completed"
        if USE_REAL_TQS_API and not all_ts_files_succeeded:
             completion_message = f"Processed {completed_queries}/{total_queries} API calls ({mode_message}). WARNING: One or more ts_ files failed processing or validation."
             final_status = "completed_with_errors"
        else:
             completion_message = f"Processed {completed_queries}/{total_queries} API calls ({mode_message})."
        return jsonify({
            "status": final_status, # Provide more info on completion status
            "message": completion_message,
            "summary": results_summary
        })
    except ValueError as ve:
        # Handle potential errors like invalid integer conversion for days_back
        current_app.logger.error(f"Value error in /run_api_calls: {ve}", exc_info=True)
        return jsonify({"status": "error", "message": f"Invalid input value: {ve}"}), 400
    except FileNotFoundError as fnf:
        # Specific handling for file not found during setup (e.g., QueryMap)
        current_app.logger.error(f"File not found error in /run_api_calls: {fnf}", exc_info=True)
        return jsonify({"status": "error", "message": f"Required file not found: {fnf}"}), 500
    except Exception as e:
        # Catch-all for other unexpected errors during the process
        current_app.logger.error(f"Unexpected error in /run_api_calls: {e}", exc_info=True)
        return jsonify({"status": "error", "message": f"An unexpected error occurred: {e}"}), 500
# === NEW RERUN ROUTE ===
@api_bp.route('/rerun-api-call', methods=['POST'])
def rerun_api_call():
    '''Handles the request to rerun a single API call (real or simulated).'''
    try:
        data = request.get_json()
        query_id = data.get('query_id')
        days_back = int(data.get('days_back', 30))
        end_date_str = data.get('end_date')
        selected_funds = data.get('funds', []) # Get the list of funds
        overwrite_mode = data.get('overwrite_mode', False) # Get the new overwrite flag
        # --- Basic Input Validation ---
        if not query_id:
            return jsonify({"status": "error", "message": "Query ID is required."}), 400
        if not end_date_str:
            return jsonify({"status": "error", "message": "End date is required."}), 400
        if not selected_funds:
             # Allow rerunning even if no funds are selected? Decide based on API behavior.
             # For now, let's require funds similar to the initial run.
             return jsonify({"status": "error", "message": "At least one fund must be selected."}), 400
        # --- Calculate Dates ---
        end_date = pd.to_datetime(end_date_str)
        start_date = end_date - pd.Timedelta(days=days_back)
        start_date_tqs_str = start_date.strftime('%Y-%m-%d')
        end_date_tqs_str = end_date.strftime('%Y-%m-%d')
        # --- Find FileName from QueryMap ---
        data_folder = current_app.config.get('DATA_FOLDER', 'Data')
        query_map_path = os.path.join(data_folder, 'QueryMap.csv')
        if not os.path.exists(query_map_path):
            return jsonify({"status": "error", "message": f"QueryMap.csv not found at {query_map_path}"}), 500
        query_map_df = pd.read_csv(query_map_path)
        # Ensure comparison is string vs string
        query_map_df['QueryID'] = query_map_df['QueryID'].astype(str)
        if 'QueryID' not in query_map_df.columns or 'FileName' not in query_map_df.columns:
             return jsonify({"status": "error", "message": "QueryMap.csv missing required columns (QueryID, FileName)."}), 500
        # Compare string query_id from request with string QueryID column
        query_row = query_map_df[query_map_df['QueryID'] == query_id]
        if query_row.empty:
            # Log the types for debugging if it still fails
            current_app.logger.warning(f"QueryID '{query_id}' (type: {type(query_id)}) not found in QueryMap QueryIDs (types: {query_map_df['QueryID'].apply(type).unique()}).")
            return jsonify({"status": "error", "message": f"QueryID '{query_id}' not found in QueryMap.csv."}), 404
        file_name = query_row.iloc[0]['FileName']
        output_path = os.path.join(data_folder, file_name)
        # --- Execute Single API Call (Simulated or Real) ---
        status = "Rerun Error: Unknown"
        rows_returned = 0
        lines_in_file = 0
        actual_df = None
        simulated_rows = None # Initialize simulation keys too
        simulated_lines = None
        try:
            if USE_REAL_TQS_API:
                # --- Real API Call, Validation, and Save ---
                actual_df = _fetch_real_tqs_data(query_id, selected_funds, start_date_tqs_str, end_date_tqs_str)
                if actual_df is not None and isinstance(actual_df, pd.DataFrame):
                    rows_returned = len(actual_df)
                    if actual_df.empty:
                        current_app.logger.info(f"(Rerun) API returned empty DataFrame for {query_id} ({file_name}). Saving empty file.")
                        status = "Saved OK (Empty)"
                    else:
                        is_valid, validation_errors = validate_data(actual_df, file_name)
                        if not is_valid:
                            current_app.logger.warning(f"(Rerun) Data validation failed for {file_name}: {validation_errors}")
                            status = f"Validation Failed: {'; '.join(validation_errors)}"
                            lines_in_file = 0
                        # else: Validation passed
                    if not status.startswith("Validation Failed"):
                        try:
                            os.makedirs(os.path.dirname(output_path), exist_ok=True)
                            actual_df.to_csv(output_path, index=False)
                            current_app.logger.info(f"(Rerun) Successfully saved data to {output_path}")
                            lines_in_file = rows_returned + 1
                            if status != "Saved OK (Empty)":
                                status = "Saved OK"
                        except Exception as e:
                            current_app.logger.error(f"(Rerun) Error saving DataFrame to {output_path}: {e}", exc_info=True)
                            status = f"Save Error: {e}"
                            lines_in_file = 0
                elif actual_df is None:
                    current_app.logger.warning(f"(Rerun) Real API call/fetch for {query_id} ({file_name}) returned None.")
                    status = "No Data / API Error / TQS Missing"
                    rows_returned = 0
                    lines_in_file = 0
                else:
                    current_app.logger.error(f"(Rerun) Real API fetch for {query_id} ({file_name}) returned unexpected type: {type(actual_df)}.")
                    status = "API Returned Invalid Type"
                    rows_returned = 0
                    lines_in_file = 0
            else:
                # --- Simulate API Call ---
                simulated_rows = _simulate_and_print_tqs_call(query_id, selected_funds, start_date_tqs_str, end_date_tqs_str)
                rows_returned = simulated_rows
                lines_in_file = simulated_rows + 1 if simulated_rows > 0 else 0
                status = "Simulated OK"
        except Exception as e:
            current_app.logger.error(f"Error during single rerun for query {query_id} ({file_name}): {e}", exc_info=True)
            status = f"Processing Error: {e}"
            rows_returned = 0
            lines_in_file = 0
        # --- Return Result for the Single Query ---
        result_data = {
            "status": status,
             # Provide consistent keys for the frontend to update the table
            "simulated_rows": simulated_rows, # Value if simulated, None otherwise
            "actual_rows": rows_returned if USE_REAL_TQS_API else None, # Value if real, None otherwise
            "simulated_lines": simulated_lines, # Value if simulated, None otherwise
            "actual_lines": lines_in_file if USE_REAL_TQS_API else None # Value if real, None otherwise
        }
        return jsonify(result_data)
    except ValueError as ve:
        current_app.logger.error(f"Value error in /rerun-api-call: {ve}", exc_info=True)
        return jsonify({"status": "error", "message": f"Invalid input value: {ve}"}), 400
    except FileNotFoundError as fnf:
        current_app.logger.error(f"File not found error in /rerun-api-call: {fnf}", exc_info=True)
        return jsonify({"status": "error", "message": f"Required file not found: {fnf}"}), 500
    except Exception as e:
        current_app.logger.error(f"Unexpected error in /rerun-api-call: {e}", exc_info=True)
        return jsonify({"status": "error", "message": f"An unexpected error occurred: {e}"}), 500
# Ensure no code remains after this point in the file for this function.
</file>

<file path="views/comparison_views.py">
# views/comparison_views.py
# This module defines the Flask Blueprint for comparing two security spread datasets.
# It includes routes for a summary view listing securities with comparison metrics
# and a detail view showing overlayed time-series charts and statistics for a single security.
from flask import Blueprint, render_template, request, current_app, jsonify, url_for
import pandas as pd
import os
import logging
import math # Add math for pagination calculation
# Assuming security_processing and utils are in the parent directory or configured in PYTHONPATH
try:
    from security_processing import load_and_process_security_data, calculate_security_latest_metrics # May need adjustments
    from utils import parse_fund_list # Example utility
    from config import COLOR_PALETTE # Still need colors
except ImportError:
    # Handle potential import errors if the structure is different
    logging.error("Could not import required modules from parent directory.")
    # Add fallback imports or path adjustments if necessary
    # Example: sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
    from ..security_processing import load_and_process_security_data, calculate_security_latest_metrics
    from ..utils import parse_fund_list
    from ..config import COLOR_PALETTE
comparison_bp = Blueprint('comparison_bp', __name__,
                        template_folder='../templates',
                        static_folder='../static')
# Configure logging
log = logging.getLogger(__name__)
PER_PAGE_COMPARISON = 50 # Items per page for comparison summary
# --- Data Loading and Processing ---
def load_weights_and_held_status(data_folder_path: str, weights_filename='w_secs.csv'):
    """Loads weights data and determines the latest held status for each security ID.
    Args:
        data_folder_path (str): The absolute path to the data folder.
        weights_filename (str, optional): The name of the weights file. Defaults to 'w_secs.csv'.
    Returns:
        pd.Series: Series indexed by Security ID indicating held status (True/False).
                   Returns an empty Series on error.
    """
    if not data_folder_path:
        log.error("No data_folder_path provided to load_weights_and_held_status.")
        return pd.Series(dtype=bool)
    weights_filepath = os.path.join(data_folder_path, weights_filename)
    log.info(f"Loading weights data from: {weights_filepath}")
    # Pass the full path to the data loading function
    df_weights, _ = load_and_process_security_data(weights_filename, data_folder_path)
    if df_weights.empty:
        log.warning(f"Weights file '{weights_filepath}' is empty or failed to load.")
        return pd.Series(dtype=bool)
    # --- Check index and columns AFTER loading --- 
    if df_weights.index.nlevels != 2:
        log.error(f"Weights file '{weights_filepath}' did not have the expected 2 index levels (Date, ID) after processing.")
        return pd.Series(dtype=bool)
    # Get index names dynamically
    date_level_name, id_level_name = df_weights.index.names
    log.info(f"Weights file index levels identified: Date='{date_level_name}', ID='{id_level_name}'")
    # Reset index to access Date and ID as columns
    df_weights = df_weights.reset_index()
    # Check if required columns are present AFTER resetting index
    required_cols = [date_level_name, id_level_name, 'Value']
    missing_cols = [col for col in required_cols if col not in df_weights.columns]
    if missing_cols:
        log.error(f"Weights file '{weights_filepath}' is missing required columns after processing and index reset: {missing_cols}. Available columns: {df_weights.columns.tolist()}")
        return pd.Series(dtype=bool)
    # Find the latest date in the weights data using the dynamic date column name
    latest_date = df_weights[date_level_name].max()
    if pd.isna(latest_date):
        log.warning(f"Could not determine the latest date in '{weights_filepath}'.")
        return pd.Series(dtype=bool)
    log.info(f"Latest date in weights file '{weights_filepath}': {latest_date}")
    # Filter for the latest date using the dynamic date column name
    latest_weights = df_weights[df_weights[date_level_name] == latest_date]
    # Set index using the dynamic ID column name and check weight > 0
    held_status = latest_weights.set_index(id_level_name)['Value'] > 0
    held_status.name = 'is_held' # Name the series for easier merging later
    log.info(f"Determined held status for {len(held_status)} IDs based on weights on {latest_date}.")
    return held_status
def load_comparison_data(data_folder_path: str, file1='sec_spread.csv', file2='sec_spreadSP.csv'):
    """Loads, processes, merges data from two security spread files, and gets held status.
    Args:
        data_folder_path (str): The absolute path to the data folder.
        file1 (str, optional): Filename for the first dataset. Defaults to 'sec_spread.csv'.
        file2 (str, optional): Filename for the second dataset. Defaults to 'sec_spreadSP.csv'.
    Returns:
        tuple: (merged_df, static_data, common_static_cols, id_col_name, held_status)
               Returns (pd.DataFrame(), pd.DataFrame(), [], None, pd.Series(dtype=bool)) on error.
    """
    log.info(f"Loading comparison data: {file1} and {file2} from {data_folder_path}")
    if not data_folder_path:
        log.error("No data_folder_path provided to load_comparison_data.")
        return pd.DataFrame(), pd.DataFrame(), [], None, pd.Series(dtype=bool)
    # Pass the absolute data folder path to the loading functions
    df1, static_cols1 = load_and_process_security_data(file1, data_folder_path)
    df2, static_cols2 = load_and_process_security_data(file2, data_folder_path)
    # Reset index to make Date and ID columns accessible
    if not df1.empty:
        df1 = df1.reset_index()
    if not df2.empty:
        df2 = df2.reset_index()
    # Load held status, passing the data folder path
    held_status = load_weights_and_held_status(data_folder_path) # Uses default 'w_secs.csv'
    if df1.empty or df2.empty:
        log.warning(f"One or both dataframes are empty. File1 empty: {df1.empty}, File2 empty: {df2.empty}")
        return pd.DataFrame(), pd.DataFrame(), [], None, held_status # Still return status
    common_static_cols = list(set(static_cols1) & set(static_cols2))
    # Identify ID column - check for 'ISIN' first
    if 'ISIN' in df1.columns:
        id_col_name = 'ISIN'
        log.info(f"Identified ID column from columns: {id_col_name}")
    # Explicitly check if there are columns before accessing index 0
    elif not df1.empty and len(df1.columns) > 0:
        potential_id = df1.columns[0]
        log.warning(f"\'ISIN\' column not found in df1. Attempting to use the first column \'{potential_id}\' as ID. This might be incorrect.")
        id_col_name = potential_id
        if id_col_name not in df2.columns:
            log.error(f"Fallback ID column '{id_col_name}' from df1 not found in df2.")
            return pd.DataFrame(), pd.DataFrame(), [], None, held_status
    else:
        log.error("Failed to identify ID column. \'ISIN\' not found and DataFrame might be empty or malformed.")
        return pd.DataFrame(), pd.DataFrame(), [], None, held_status
    if id_col_name in common_static_cols:
        common_static_cols.remove(id_col_name)
        log.debug(f"Removed ID column '{id_col_name}' from common_static_cols.")
    try:
        df1_merge = df1[[id_col_name, 'Date', 'Value'] + common_static_cols].rename(columns={'Value': 'Value_Orig'})
        if id_col_name not in df2.columns:
             log.error(f"ID column '{id_col_name}' identified in df1 not found in df2 columns: {df2.columns.tolist()}")
             raise KeyError(f"ID column '{id_col_name}' not found in second dataframe")
        df2_merge = df2[[id_col_name, 'Date', 'Value']].rename(columns={'Value': 'Value_New'})
    except KeyError as e:
        log.error(f"Missing required column for merge preparation: {e}. Df1 cols: {df1.columns.tolist()}, Df2 cols: {df2.columns.tolist()}")
        return pd.DataFrame(), pd.DataFrame(), [], None, held_status
    merged_df = pd.merge(df1_merge, df2_merge, on=[id_col_name, 'Date'], how='outer')
    merged_df = merged_df.sort_values(by=[id_col_name, 'Date'])
    merged_df['Change_Orig'] = merged_df.groupby(id_col_name)['Value_Orig'].diff()
    merged_df['Change_New'] = merged_df.groupby(id_col_name)['Value_New'].diff()
    static_data = merged_df.groupby(id_col_name)[common_static_cols].last().reset_index()
    log.info(f"Successfully merged data. Shape: {merged_df.shape}")
    # Return held_status along with other data
    return merged_df, static_data, common_static_cols, id_col_name, held_status
def calculate_comparison_stats(merged_df, static_data, id_col):
    """Calculates comparison statistics for each security.
    Args:
        merged_df (pd.DataFrame): The merged dataframe of original and new values.
        static_data (pd.DataFrame): DataFrame with static info per security.
        id_col (str): The name of the column containing the Security ID/Name.
    """
    if merged_df.empty:
        return pd.DataFrame()
    if id_col not in merged_df.columns:
        log.error(f"Specified id_col '{id_col}' not found in merged_df columns: {merged_df.columns.tolist()}")
        return pd.DataFrame() # Cannot group without the ID column
    log.info(f"Calculating comparison statistics using ID column: {id_col}...")
    stats_list = []
    # Use the passed id_col here
    for sec_id, group in merged_df.groupby(id_col):
        sec_stats = {id_col: sec_id} # Use actual id_col name
        # Filter out rows where both values are NaN for overall analysis period
        group_valid_overall = group.dropna(subset=['Value_Orig', 'Value_New'], how='all')
        overall_min_date = group_valid_overall['Date'].min()
        overall_max_date = group_valid_overall['Date'].max()
        # Filter out rows where EITHER value is NaN for correlation/diff calculations
        valid_comparison = group.dropna(subset=['Value_Orig', 'Value_New'])
        # 1. Correlation of Levels
        if len(valid_comparison) >= 2: # Need at least 2 points for correlation
            # Use the NaN-dropped dataframe for correlation
            level_corr = valid_comparison['Value_Orig'].corr(valid_comparison['Value_New'])
            sec_stats['Level_Correlation'] = level_corr if pd.notna(level_corr) else None
        else:
             sec_stats['Level_Correlation'] = None
        # 2. Max / Min (use original group to get true max/min including non-overlapping points)
        sec_stats['Max_Orig'] = group['Value_Orig'].max()
        sec_stats['Min_Orig'] = group['Value_Orig'].min()
        sec_stats['Max_New'] = group['Value_New'].max()
        sec_stats['Min_New'] = group['Value_New'].min()
        # 3. Date Range Comparison - Refined Logic
        # Find min/max dates within the MERGED data where each series is individually valid
        min_date_orig_idx = group['Value_Orig'].first_valid_index()
        max_date_orig_idx = group['Value_Orig'].last_valid_index()
        min_date_new_idx = group['Value_New'].first_valid_index()
        max_date_new_idx = group['Value_New'].last_valid_index()
        sec_stats['Start_Date_Orig'] = group.loc[min_date_orig_idx, 'Date'] if min_date_orig_idx is not None else None
        sec_stats['End_Date_Orig'] = group.loc[max_date_orig_idx, 'Date'] if max_date_orig_idx is not None else None
        sec_stats['Start_Date_New'] = group.loc[min_date_new_idx, 'Date'] if min_date_new_idx is not None else None
        sec_stats['End_Date_New'] = group.loc[max_date_new_idx, 'Date'] if max_date_new_idx is not None else None
        # Check if the start and end dates MATCH for the valid periods of EACH series
        same_start = pd.Timestamp(sec_stats['Start_Date_Orig']) == pd.Timestamp(sec_stats['Start_Date_New']) if sec_stats['Start_Date_Orig'] and sec_stats['Start_Date_New'] else False
        same_end = pd.Timestamp(sec_stats['End_Date_Orig']) == pd.Timestamp(sec_stats['End_Date_New']) if sec_stats['End_Date_Orig'] and sec_stats['End_Date_New'] else False
        sec_stats['Same_Date_Range'] = same_start and same_end
        # Add overall date range for info
        sec_stats['Overall_Start_Date'] = overall_min_date
        sec_stats['Overall_End_Date'] = overall_max_date
        # 4. Correlation of Daily Changes (Volatility Alignment)
        # Use the dataframe where BOTH values are non-NaN to calculate changes for correlation
        valid_comparison = valid_comparison.copy() # Avoid SettingWithCopyWarning
        valid_comparison['Change_Orig_Corr'] = valid_comparison['Value_Orig'].diff()
        valid_comparison['Change_New_Corr'] = valid_comparison['Value_New'].diff()
        # Drop NaNs created by the diff() itself (first row)
        valid_changes = valid_comparison.dropna(subset=['Change_Orig_Corr', 'Change_New_Corr'])
        # --- Debug Logging Start ---
        # if sec_id == 'Alpha001': # Log only for a specific security to avoid flooding
        #     log.debug(f"Debug {sec_id} - valid_changes DataFrame (first 5 rows):\n{valid_changes.head()}")
        #     log.debug(f"Debug {sec_id} - valid_changes count: {len(valid_changes)}")
        # --- Debug Logging End ---
        if len(valid_changes) >= 2:
            change_corr = valid_changes['Change_Orig_Corr'].corr(valid_changes['Change_New_Corr'])
            sec_stats['Change_Correlation'] = change_corr if pd.notna(change_corr) else None
        else:
            sec_stats['Change_Correlation'] = None
            # Log why correlation is None
            log.debug(f"Cannot calculate Change_Correlation for {sec_id}. Need >= 2 valid change pairs, found {len(valid_changes)}.")
        # 5. Difference Statistics (use the valid_comparison df where both values exist)
        valid_comparison['Abs_Diff'] = (valid_comparison['Value_Orig'] - valid_comparison['Value_New']).abs()
        sec_stats['Mean_Abs_Diff'] = valid_comparison['Abs_Diff'].mean() # Mean diff where both values exist
        sec_stats['Max_Abs_Diff'] = valid_comparison['Abs_Diff'].max() # Max diff where both values exist
        # Count NaNs - use original group
        sec_stats['NaN_Count_Orig'] = group['Value_Orig'].isna().sum()
        sec_stats['NaN_Count_New'] = group['Value_New'].isna().sum()
        sec_stats['Total_Points'] = len(group)
        stats_list.append(sec_stats)
    summary_df = pd.DataFrame(stats_list)
    # Merge static data back
    if not static_data.empty and id_col in static_data.columns and id_col in summary_df.columns:
        summary_df = pd.merge(summary_df, static_data, on=id_col, how='left')
    elif not static_data.empty:
         log.warning(f"Could not merge static data back. ID column '{id_col}' missing from static_data ({id_col in static_data.columns}) or summary_df ({id_col in summary_df.columns}).")
    log.info(f"Finished calculating stats. Summary shape: {summary_df.shape}")
    return summary_df
# --- Routes ---
@comparison_bp.route('/comparison/summary')
def summary():
    """Displays the comparison summary page with server-side filtering, sorting, and pagination."""
    log.info("--- Starting Comparison Summary Request ---")
    # Retrieve the configured absolute data folder path
    data_folder = current_app.config['DATA_FOLDER']
    if not data_folder:
        current_app.logger.error("DATA_FOLDER is not configured in the application.")
        return "Internal Server Error: Data folder not configured", 500
    # --- Get Request Parameters ---
    page = request.args.get('page', 1, type=int)
    sort_by = request.args.get('sort_by', 'Change_Correlation') # Default sort
    sort_order = request.args.get('sort_order', 'desc').lower()
    if sort_order not in ['asc', 'desc']:
        sort_order = 'desc'
    ascending = sort_order == 'asc'
    # NEW: Get holding status filter (default to 'false' -> show only held)
    show_sold = request.args.get('show_sold', 'false').lower() == 'true'
    log.info(f"Show Sold filter: {show_sold}")
    # Get active filters (ensuring keys are correct)
    active_filters = {k.replace('filter_', ''): v 
                      for k, v in request.args.items() 
                      if k.startswith('filter_') and v}
    log.info(f"Request Params: Page={page}, SortBy={sort_by}, Order={sort_order}, Filters={active_filters}, ShowSold={show_sold}")
    # --- Load and Prepare Data --- 
    # Capture the actual ID column name and held status returned by the load function
    merged_data, static_data, static_cols, actual_id_col, held_status = load_comparison_data(data_folder)
    if actual_id_col is None:
        log.error("Failed to get ID column name during data loading.")
        return "Error loading comparison data: Could not determine ID column.", 500
    # Pass the actual ID column name to the stats calculation function
    summary_stats = calculate_comparison_stats(merged_data, static_data, id_col=actual_id_col)
    if summary_stats.empty:
         # Log reason if possible
         if merged_data.empty:
             log.info("Merged data was empty, no stats calculated.")
         else:
             log.warning("Merged data was present, but calculation resulted in empty stats DataFrame.")
         # Render with message if empty even before filtering
         return render_template('comparison_page.html',
                                table_data=[],
                                columns_to_display=[],
                                id_column_name=actual_id_col,
                                filter_options={},
                                active_filters={},
                                current_sort_by=sort_by,
                                current_sort_order=sort_order,
                                pagination=None,
                                show_sold=show_sold, # Pass filter status
                                message="No comparison data available.")
    # --- Merge Held Status --- 
    if not held_status.empty and actual_id_col in summary_stats.columns:
        summary_stats = pd.merge(summary_stats, held_status, left_on=actual_id_col, right_index=True, how='left')
        # Fill NaN in 'is_held' with False (assume not held if not in weights file/latest date)
        summary_stats['is_held'] = summary_stats['is_held'].fillna(False)
        log.info(f"Merged held status. Stats shape: {summary_stats.shape}")
    else:
        log.warning("Could not merge held status. Weights data might be missing or empty, or ID column mismatch.")
        summary_stats['is_held'] = False # Assume all are not held if merge fails
    # --- Apply Holding Status Filter --- 
    original_count = len(summary_stats)
    if not show_sold:
        summary_stats = summary_stats[summary_stats['is_held'] == True]
        log.info(f"Applied 'Show Held Only' filter. Kept {len(summary_stats)} out of {original_count} securities.")
    else:
        log.info("Skipping 'Show Held Only' filter (show_sold is True).")
    # If filtering by holding status resulted in empty df, render with message
    if summary_stats.empty:
         log.info("No securities remaining after applying holding status filter.")
         return render_template('comparison_page.html',
                                table_data=[],
                                columns_to_display=[actual_id_col] + static_cols, # Show basic cols
                                id_column_name=actual_id_col,
                                filter_options={}, # Filters not relevant now
                                active_filters={},
                                current_sort_by=sort_by,
                                current_sort_order=sort_order,
                                pagination=None,
                                show_sold=show_sold, # Pass filter status
                                message="No currently held securities found.")
    # --- Collect Filter Options (From Dataset *After* Holding Filter) --- 
    filter_options = {}
    # Use static_cols identified earlier
    potential_filter_cols = static_cols 
    for col in potential_filter_cols:
        if col in summary_stats.columns:
            # Get unique non-NA values from the *potentially filtered* stats df
            unique_vals = summary_stats[col].dropna().unique().tolist()
            # Basic type check and sort if possible - Improved Robust Sorting Key
            try:
                unique_vals = sorted(unique_vals, key=lambda x: \
                    (0, float(x)) if isinstance(x, bool) else \
                    (0, x) if isinstance(x, (int, float)) else \
                    (0, float(x)) if isinstance(x, str) and x.replace('.', '', 1).lstrip('-').isdigit() else \
                    (1, str(x)) if isinstance(x, str) else \
                    (2, str(x)) # Fallback for other types, sort as string
                )
            except (TypeError, ValueError) as e:
                log.warning(f"Type error or value error during sorting unique values for column '{col}': {e}. Falling back to simple string sort.")
                try:
                     unique_vals = sorted(str(x) for x in unique_vals)
                except Exception as final_sort_err:
                    log.error(f"Final string sort failed for column '{col}': {final_sort_err}")
                    # Keep original unsorted list if all else fails
            if unique_vals:
                filter_options[col] = unique_vals
    final_filter_options = dict(sorted(filter_options.items())) # Sort filter dropdowns alphabetically
    # --- Apply Static Column Filtering (on potentially pre-filtered data) ---
    filtered_stats = summary_stats.copy()
    if active_filters:
        log.info(f"Applying static column filters: {active_filters}")
        for col, value in active_filters.items():
            if col in filtered_stats.columns and value:
                # Ensure comparison is robust (e.g., string comparison)
                try:
                    filtered_stats = filtered_stats[filtered_stats[col].astype(str).str.lower() == str(value).lower()]
                except Exception as e:
                    log.warning(f"Could not apply filter on column '{col}' with value '{value}'. Error: {e}")
        log.info(f"Stats shape after static filtering: {filtered_stats.shape}")
    else:
         log.info("No active static column filters.")
    # If filtering resulted in empty df, render with message
    if filtered_stats.empty:
         log.info("No data remaining after applying static column filters.")
         return render_template('comparison_page.html',
                                table_data=[],
                                columns_to_display=[actual_id_col] + static_cols, # Show basic cols
                                id_column_name=actual_id_col,
                                filter_options=final_filter_options, # Still show filter options
                                active_filters=active_filters,
                                current_sort_by=sort_by,
                                current_sort_order=sort_order,
                                pagination=None,
                                show_sold=show_sold, # Pass filter status
                                message="No data matches the current filters.")
    # --- Apply Sorting ---
    # Sort the filtered data
    if sort_by in filtered_stats.columns:
        log.info(f"Sorting by '{sort_by}' {sort_order}")
        if pd.api.types.is_numeric_dtype(filtered_stats[sort_by]):
             filtered_stats = filtered_stats.sort_values(by=sort_by, ascending=ascending, na_position='last')
        else:
             filtered_stats = filtered_stats.sort_values(by=sort_by, ascending=ascending, na_position='last', key=lambda col: col.astype(str).str.lower())
    else:
        log.warning(f"Sort column '{sort_by}' not found in filtered data. Defaulting to ID sort.")
        sort_by = actual_id_col 
        sort_order = 'asc'
        ascending = True
        filtered_stats = filtered_stats.sort_values(by=sort_by, ascending=ascending, na_position='last')
    # --- Define Columns to Display ---
    # Identify fund columns 
    fund_cols = sorted([col for col in static_cols if 'fund' in col.lower() and col != actual_id_col])
    other_static_cols = sorted([col for col in static_cols if col != actual_id_col and col not in fund_cols])
    calculated_cols = sorted([col for col in summary_stats.columns # Use summary_stats to get all potential cols
                             if col not in static_cols and col != actual_id_col and
                             col not in ['Start_Date_Orig', 'End_Date_Orig', 'Start_Date_New', 'End_Date_New',
                                          'NaN_Count_Orig', 'NaN_Count_New', 'Total_Points',
                                          'Overall_Start_Date', 'Overall_End_Date', 'is_held']]) # Exclude is_held too
    columns_to_display = [actual_id_col] + other_static_cols + calculated_cols + fund_cols
    log.debug(f"Columns to display: {columns_to_display}")
    # --- Pagination ---
    total_items = len(filtered_stats)
    # This check is now redundant due to earlier checks, but keep for safety
    if total_items == 0:
         log.info("Pagination step: No items remain after all filtering/sorting.") 
         return render_template('comparison_page.html',
                                table_data=[],
                                columns_to_display=columns_to_display, \
                                id_column_name=actual_id_col,
                                filter_options=final_filter_options,
                                active_filters=active_filters,
                                current_sort_by=sort_by,
                                current_sort_order=sort_order,
                                pagination=None,
                                show_sold=show_sold, # Pass filter status
                                message="No data matches the current criteria.")
    safe_per_page = max(1, PER_PAGE_COMPARISON)
    total_pages = math.ceil(total_items / safe_per_page)
    total_pages = max(1, total_pages) 
    page = max(1, min(page, total_pages)) 
    start_index = (page - 1) * safe_per_page
    end_index = start_index + safe_per_page
    log.info(f"Pagination: Total items={total_items}, Total pages={total_pages}, Current page={page}, Per page={safe_per_page}")
    page_window = 2
    start_page_display = max(1, page - page_window)
    end_page_display = min(total_pages, page + page_window)
    paginated_stats = filtered_stats.iloc[start_index:end_index]
    # --- Prepare Data for Template ---
    paginated_stats = paginated_stats[[col for col in columns_to_display if col in paginated_stats.columns]]
    table_data_list = paginated_stats.to_dict(orient='records')
    for row in table_data_list:
        for key, value in row.items():
            if pd.isna(value):
                row[key] = None
    pagination_context = {
        'page': page,
        'per_page': safe_per_page,
        'total_pages': total_pages,
        'total_items': total_items,
        'has_prev': page > 1,
        'has_next': page < total_pages,
        'prev_num': page - 1,
        'next_num': page + 1,
        'start_page_display': start_page_display, \
        'end_page_display': end_page_display,     \
        # Function to generate URLs for pagination links, preserving ALL filters including show_sold
        'url_for_page': lambda p: url_for('comparison_bp.summary', 
                                          page=p, 
                                          sort_by=sort_by, 
                                          sort_order=sort_order, 
                                          show_sold=str(show_sold).lower(), # Pass holding status
                                          **{f'filter_{k}': v for k, v in active_filters.items()})
    }
    return render_template('comparison_page.html',
                           table_data=table_data_list,
                           columns_to_display=columns_to_display,
                           id_column_name=actual_id_col,
                           filter_options=final_filter_options, # Use sorted options
                           active_filters=active_filters,
                           current_sort_by=sort_by,
                           current_sort_order=sort_order,
                           pagination=pagination_context,
                           show_sold=show_sold, # Pass holding filter status
                           message=None)
@comparison_bp.route('/comparison/details/<path:security_id>')
def comparison_details(security_id):
    """Displays side-by-side historical charts for a specific security."""
    log.info(f"--- Starting Comparison Detail Request for Security ID: {security_id} ---")
    # Retrieve the configured absolute data folder path
    data_folder = current_app.config['DATA_FOLDER']
    if not data_folder:
        current_app.logger.error("DATA_FOLDER is not configured in the application.")
        return "Internal Server Error: Data folder not configured", 500
    try:
        # Pass the absolute data folder path here
        merged_df, static_data, common_static_cols, id_col, _ = load_comparison_data(data_folder)
        if merged_df.empty:
            log.warning("Merged data is empty, cannot show details.")
            return "Error: Could not load comparison data.", 404
        if id_col is None or id_col not in merged_df.columns:
             log.error(f"ID column ('{id_col}') not found in merged data for details view.")
             return "Error: Could not identify security ID column in data.", 500
        # Filter using the actual ID column name
        security_data = merged_df[merged_df[id_col] == security_id].copy()
        if security_data.empty:
            return "Security ID not found", 404
        # Get the static data for this specific security
        sec_static_data = static_data[static_data[id_col] == security_id]
        # Recalculate detailed stats for this security, passing the correct ID column
        stats_df = calculate_comparison_stats(security_data.copy(), sec_static_data, id_col=id_col)
        security_stats = stats_df.iloc[0].where(pd.notnull(stats_df.iloc[0]), None).to_dict() if not stats_df.empty else {}
        # Prepare chart data
        security_data['Date_Str'] = security_data['Date'].dt.strftime('%Y-%m-%d')
        # Convert NaN to None using list comprehension after .tolist()
        data_orig = security_data['Value_Orig'].tolist()
        data_orig_processed = [None if pd.isna(x) else x for x in data_orig]
        data_new = security_data['Value_New'].tolist()
        data_new_processed = [None if pd.isna(x) else x for x in data_new]
        chart_data = {
            'labels': security_data['Date_Str'].tolist(),
            'datasets': [
                {
                    'label': 'Original Spread (Sec_spread)',
                    'data': data_orig_processed, # Use processed list
                    'borderColor': COLOR_PALETTE[0 % len(COLOR_PALETTE)],
                    'tension': 0.1
                },
                {
                    'label': 'New Spread (Sec_spreadSP)',
                    'data': data_new_processed, # Use processed list
                    'borderColor': COLOR_PALETTE[1 % len(COLOR_PALETTE)],
                    'tension': 0.1
                }
            ]
        }
        # Get static attributes for display (use actual_id_col if it's 'Security Name')
        # Best to get from security_stats which should now include merged static data
        security_name_display = security_stats.get('Security Name', security_id) if id_col == 'Security Name' else security_id
        # If 'Security Name' is not the ID, try to get it from stats
        if id_col != 'Security Name' and 'Security Name' in security_stats:
             security_name_display = security_stats.get('Security Name', security_id)
        return render_template('comparison_details_page.html',
                               security_id=security_id,
                               security_name=security_name_display,
                               chart_data=chart_data, # Pass as JSONifiable dict
                               stats=security_stats, # Pass comparison stats
                               id_column_name=id_col) # Pass actual ID col name
    except Exception as e:
        log.exception(f"Error generating comparison details page for {security_id}.")
        return f"An error occurred: {e}", 500
</file>

<file path="views/curve_views.py">
# Purpose: Defines the Flask Blueprint for yield curve analysis views.
# Stdlib imports
from datetime import datetime
# Third-party imports
from flask import Blueprint, render_template, request, jsonify, current_app
import pandas as pd
import numpy as np
# Local imports
from curve_processing import load_curve_data, check_curve_inconsistencies, get_latest_curve_date
from config import COLOR_PALETTE
curve_bp = Blueprint('curve_bp', __name__, template_folder='../templates')
# --- Routes ---
@curve_bp.route('/curve/summary')
def curve_summary():
    """Displays a summary of yield curve checks for the latest date."""
    # Retrieve the absolute data folder path
    data_folder = current_app.config['DATA_FOLDER']
    if not data_folder:
        current_app.logger.error("DATA_FOLDER is not configured in the application.")
        return "Internal Server Error: Data folder not configured", 500
    current_app.logger.info("Loading curve data for summary...")
    curve_df = load_curve_data(data_folder_path=data_folder)
    if curve_df.empty:
        current_app.logger.warning("Curve data is empty or failed to load.")
        summary = {}
        latest_date_str = "N/A"
    else:
        current_app.logger.info("Checking curve inconsistencies...")
        summary = check_curve_inconsistencies(curve_df)
        latest_date = get_latest_curve_date(curve_df)
        latest_date_str = latest_date.strftime('%Y-%m-%d') if latest_date else "N/A"
        current_app.logger.info(f"Inconsistency summary generated for date: {latest_date_str}")
    return render_template('curve_summary.html',
                           summary=summary,
                           latest_date=latest_date_str)
@curve_bp.route('/curve/details/<currency>')
def curve_details(currency):
    """Displays the yield curve chart for a specific currency and date, with historical overlays."""
    current_app.logger.info(f"Loading curve data for currency: {currency}")
    # Retrieve the absolute data folder path
    data_folder = current_app.config['DATA_FOLDER']
    if not data_folder:
        current_app.logger.error("DATA_FOLDER is not configured in the application.")
        return "Internal Server Error: Data folder not configured", 500
    # Pass the data folder path to the loading function
    curve_df = load_curve_data(data_folder_path=data_folder)
    # Get parameters from request
    selected_date_str = request.args.get('date')
    try:
        num_prev_days = int(request.args.get('prev_days', 1))
    except ValueError:
        num_prev_days = 1
    available_dates = []
    curve_table_data = []
    chart_data = {'labels': [], 'datasets': []}
    selected_date = None
    if curve_df.empty:
        current_app.logger.warning(f"Curve data is empty for currency details: {currency}")
    else:
        # --- Get Available Dates for Currency ---
        try:
            if currency in curve_df.index.get_level_values('Currency'):
                available_dates = sorted(
                    curve_df.loc[currency].index.get_level_values('Date').unique(),
                    reverse=True
                )
            else:
                current_app.logger.warning(f"Currency '{currency}' not found in curve data index.")
                available_dates = []
        except Exception as e:
            current_app.logger.error(f"Error getting dates for {currency}: {e}", exc_info=True)
            available_dates = []
        # --- Determine Selected Date and Previous Date ---
        latest_date = available_dates[0] if available_dates else None
        previous_date = None # For daily change calculation
        if selected_date_str:
            try:
                selected_date = pd.to_datetime(selected_date_str).normalize()
                if selected_date not in available_dates:
                    current_app.logger.warning(f"Requested date {selected_date_str} not available for {currency}, falling back to latest.")
                    selected_date = latest_date
                else:
                    # Find the actual previous date in the available list
                    selected_date_index = available_dates.index(selected_date)
                    if selected_date_index + 1 < len(available_dates):
                        previous_date = available_dates[selected_date_index + 1]
                        current_app.logger.info(f"Previous date for change calc: {previous_date}")
                    else:
                        current_app.logger.info("Selected date is the oldest available, no previous date for change calc.")
            except ValueError:
                current_app.logger.warning(f"Invalid date format '{selected_date_str}', falling back to latest.")
                selected_date = latest_date
        else:
            selected_date = latest_date
            # If defaulting to latest, find the previous date
            if len(available_dates) > 1:
                 previous_date = available_dates[1]
                 current_app.logger.info(f"Defaulting to latest date. Previous date for change calc: {previous_date}")
        # Update selected_date_str after determining selected_date
        selected_date_str = selected_date.strftime('%Y-%m-%d') if selected_date else "N/A"
        # --- Prepare Data for Chart and Table ---
        if selected_date and available_dates:
            # 1. Process Selected Date for Labels, Chart, and Table Base
            curve_for_selected_date_df = pd.DataFrame() # Ensure it's defined
            try:
                mask_selected = (curve_df.index.get_level_values('Currency') == currency) & \
                                (curve_df.index.get_level_values('Date') == selected_date)
                curve_for_selected_date_df = curve_df[mask_selected].reset_index()
                if not curve_for_selected_date_df.empty:
                    curve_for_selected_date_df['TermMonths'] = (curve_for_selected_date_df['TermDays'] / 30).round(1)
                    curve_for_selected_date_df = curve_for_selected_date_df.sort_values('TermDays')
                    chart_data['labels'] = curve_for_selected_date_df['TermMonths'].tolist()
                    # Base table data on selected date
                    curve_table_df = curve_for_selected_date_df[['Term', 'TermDays', 'TermMonths', 'Value']].copy()
                else:
                    current_app.logger.warning(f"No data for selected date {selected_date_str} to generate labels/table.")
            except Exception as e:
                current_app.logger.error(f"Error processing selected date {selected_date_str} for labels/table base: {e}", exc_info=True)
            # 2. Calculate Daily Changes (if previous date exists)
            if previous_date and not curve_for_selected_date_df.empty:
                try:
                    mask_previous = (curve_df.index.get_level_values('Currency') == currency) & \
                                    (curve_df.index.get_level_values('Date') == previous_date)
                    curve_for_previous_date = curve_df[mask_previous].reset_index()
                    if not curve_for_previous_date.empty:
                        # Merge selected and previous date data on TermDays
                        merged_changes = pd.merge(
                            curve_for_selected_date_df[['TermDays', 'Value']],
                            curve_for_previous_date[['TermDays', 'Value']],
                            on='TermDays',
                            suffixes=('_selected', '_prev'),
                            how='left' # Keep all terms from selected date
                        )
                        merged_changes['ValueChange'] = merged_changes['Value_selected'] - merged_changes['Value_prev']
                        # Calculate deviation from average shift
                        average_curve_shift = merged_changes['ValueChange'].mean()
                        merged_changes['ChangeDeviation'] = merged_changes['ValueChange'] - average_curve_shift
                        # Calculate Z-score of the deviation
                        deviation_std = merged_changes['ChangeDeviation'].std()
                        if deviation_std != 0 and pd.notna(deviation_std): # Avoid division by zero
                             merged_changes['DeviationZScore'] = (merged_changes['ChangeDeviation'] - merged_changes['ChangeDeviation'].mean()) / deviation_std
                        else:
                             merged_changes['DeviationZScore'] = 0.0 # Or np.nan if preferred
                        # Merge calculated changes back into the main table DataFrame
                        curve_table_df = pd.merge(
                             curve_table_df,
                             merged_changes[['TermDays', 'ValueChange', 'ChangeDeviation', 'DeviationZScore']],
                             on='TermDays',
                             how='left' # Keep all terms from selected date
                        )
                    else:
                         current_app.logger.warning(f"No data found for previous date {previous_date.strftime('%Y-%m-%d')}")
                         # Add empty columns if previous data missing
                         curve_table_df['ValueChange'] = np.nan
                         curve_table_df['ChangeDeviation'] = np.nan
                         curve_table_df['DeviationZScore'] = np.nan
                except Exception as e:
                     current_app.logger.error(f"Error calculating daily changes: {e}", exc_info=True)
                     curve_table_df['ValueChange'] = np.nan
                     curve_table_df['ChangeDeviation'] = np.nan
                     curve_table_df['DeviationZScore'] = np.nan
            else:
                # Add empty columns if no previous date
                if 'ValueChange' not in curve_table_df.columns:
                    curve_table_df['ValueChange'] = np.nan
                    curve_table_df['ChangeDeviation'] = np.nan
                    curve_table_df['DeviationZScore'] = np.nan
            # Prepare final table data (convert df to dict)
            # Rename Value to Value_Display after calculations are done
            curve_table_df.rename(columns={'Value': 'Value_Display'}, inplace=True)
            curve_table_data = curve_table_df[['Term', 'TermMonths', 'Value_Display', 'ValueChange', 'ChangeDeviation', 'DeviationZScore']].to_dict('records')
            # 3. Fetch and prepare datasets for Chart
            # Determine the range of plot dates based on num_prev_days
            selected_date_index_for_plot = available_dates.index(selected_date)
            start_index = selected_date_index_for_plot
            end_index = min(len(available_dates), selected_date_index_for_plot + num_prev_days + 1)
            dates_to_plot = available_dates[start_index:end_index]
            current_app.logger.info(f"Plotting dates for {currency}: {dates_to_plot}")
            for i, plot_date in enumerate(dates_to_plot):
                try:
                    mask = (curve_df.index.get_level_values('Currency') == currency) & \
                           (curve_df.index.get_level_values('Date') == plot_date)
                    curve_for_plot_date_filtered = curve_df[mask]
                    if not curve_for_plot_date_filtered.empty:
                        curve_for_plot_date = curve_for_plot_date_filtered.reset_index().sort_values('TermDays')
                        # Align data points to selected date's TermDays
                        if not curve_for_selected_date_df.empty:
                             aligned_curve = curve_for_plot_date.set_index('TermDays')[['Value']].reindex(curve_for_selected_date_df['TermDays'])
                             plot_data_values = aligned_curve['Value'].fillna(np.nan).tolist()
                        else:
                             plot_data_values = []
                        # Determine color and style
                        color_index = i % len(COLOR_PALETTE)
                        border_color = COLOR_PALETTE[color_index]
                        if i > 0:
                            try:
                                r, g, b = int(border_color[1:3], 16), int(border_color[3:5], 16), int(border_color[5:7], 16)
                                border_color = f'rgba({r},{g},{b},0.4)'
                            except (IndexError, ValueError):
                                current_app.logger.warning(f"Could not parse color {border_color} for fading, using default.")
                        dataset = {
                            'label': f'{currency} ({plot_date.strftime("%Y-%m-%d")})',
                            'data': plot_data_values,
                            'borderColor': border_color,
                            'backgroundColor': border_color,
                            'fill': False,
                            'tension': 0.1,
                            'borderWidth': 2 if i == 0 else 1.5
                        }
                        chart_data['datasets'].append(dataset)
                    else:
                        current_app.logger.warning(f"No data found for {currency} on {plot_date.strftime('%Y-%m-%d')}")
                except Exception as e:
                    current_app.logger.error(f"Error processing plot data for {currency} on {plot_date.strftime('%Y-%m-%d')}: {e}", exc_info=True)
    # --- Final Rendering --- 
    return render_template('curve_details.html',
                           currency=currency,
                           chart_data=chart_data,
                           table_data=curve_table_data,
                           available_dates=[d.strftime('%Y-%m-%d') for d in available_dates],
                           selected_date=selected_date_str,
                           num_prev_days=num_prev_days,
                           color_palette=COLOR_PALETTE
                           )
</file>

<file path="views/duration_comparison_views.py">
# views/duration_comparison_views.py
# This module defines the Flask Blueprint for comparing two security duration datasets.
# It includes routes for a summary view listing securities with comparison metrics
# and a detail view showing overlayed time-series charts and statistics for a single security.
from flask import Blueprint, render_template, request, current_app, jsonify, url_for
import pandas as pd
import os
import logging
import math # Add math for pagination calculation
# Assuming security_processing and utils are in the parent directory or configured in PYTHONPATH
try:
    from security_processing import load_and_process_security_data # May need adjustments
    from utils import parse_fund_list # Example utility
    from config import COLOR_PALETTE
except ImportError:
    # Handle potential import errors if the structure is different
    logging.error("Could not import required modules from parent directory.")
    # Add fallback imports or path adjustments if necessary
    # Example: sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
    from ..security_processing import load_and_process_security_data
    from ..utils import parse_fund_list
    from ..config import COLOR_PALETTE
duration_comparison_bp = Blueprint('duration_comparison_bp', __name__,
                        template_folder='../templates',
                        static_folder='../static')
# Configure logging
log = logging.getLogger(__name__)
PER_PAGE_COMPARISON = 50 # Items per page for comparison summary
# --- Data Loading and Processing ---
def load_weights_and_held_status(data_folder_path: str, weights_filename='w_secs.csv'):
    """Loads weights data and determines the latest held status for each security ID.
    Args:
        data_folder_path (str): The absolute path to the data folder.
        weights_filename (str, optional): The name of the weights file. Defaults to 'w_secs.csv'.
    Returns:
        pd.Series: Series indexed by Security ID indicating held status (True/False).
                   Returns an empty Series on error.
    """
    # Corrected logic starts here
    if not data_folder_path:
        log.error("No data_folder_path provided to load_weights_and_held_status.")
        return pd.Series(dtype=bool)
    weights_filepath = os.path.join(data_folder_path, weights_filename)
    log.info(f"Loading weights data from: {weights_filepath}")
    # Pass the full path to the data loading function
    df_weights, _ = load_and_process_security_data(weights_filename, data_folder_path)
    if df_weights.empty:
        log.warning(f"Weights file '{weights_filepath}' is empty or failed to load.")
        return pd.Series(dtype=bool)
    # --- Check index and columns AFTER loading --- 
    if df_weights.index.nlevels != 2:
        log.error(f"Weights file '{weights_filepath}' did not have the expected 2 index levels (Date, ID) after processing.")
        return pd.Series(dtype=bool)
    # Get index names dynamically
    date_level_name, id_level_name = df_weights.index.names
    log.info(f"Weights file index levels identified: Date='{date_level_name}', ID='{id_level_name}'")
    # Reset index to access Date and ID as columns
    df_weights = df_weights.reset_index()
    # Check if required columns are present AFTER resetting index
    required_cols = [date_level_name, id_level_name, 'Value']
    missing_cols = [col for col in required_cols if col not in df_weights.columns]
    if missing_cols:
        log.error(f"Weights file '{weights_filepath}' is missing required columns after processing and index reset: {missing_cols}. Available columns: {df_weights.columns.tolist()}")
        return pd.Series(dtype=bool)
    # Find the latest date in the weights data using the dynamic date column name
    latest_date = df_weights[date_level_name].max()
    if pd.isna(latest_date):
        log.warning(f"Could not determine the latest date in '{weights_filepath}'.")
        return pd.Series(dtype=bool)
    log.info(f"Latest date in weights file '{weights_filepath}': {latest_date}")
    # Filter for the latest date using the dynamic date column name
    latest_weights = df_weights[df_weights[date_level_name] == latest_date]
    # Set index using the dynamic ID column name and check weight > 0
    held_status = latest_weights.set_index(id_level_name)['Value'] > 0
    held_status.name = 'is_held' # Name the series for easier merging later
    log.info(f"Determined held status for {len(held_status)} IDs based on weights on {latest_date}.")
    return held_status
    # Corrected logic ends here
def load_duration_comparison_data(data_folder_path: str, file1='sec_duration.csv', file2='sec_durationSP.csv'):
    """Loads, processes, merges data from two security duration files, and gets held status.
    Args:
        data_folder_path (str): The absolute path to the data folder.
        file1 (str, optional): Filename for the first dataset. Defaults to 'sec_duration.csv'.
        file2 (str, optional): Filename for the second dataset. Defaults to 'sec_durationSP.csv'.
    Returns:
        tuple: (merged_df, static_data, common_static_cols, id_col_name, held_status)
               Returns (pd.DataFrame(), pd.DataFrame(), [], None, pd.Series(dtype=bool)) on error.
    """
    log.info(f"Loading duration comparison data: {file1} and {file2} from {data_folder_path}")
    if not data_folder_path:
        log.error("No data_folder_path provided to load_duration_comparison_data.")
        return pd.DataFrame(), pd.DataFrame(), [], None, pd.Series(dtype=bool)
    # Load held status first (uses its own corrected loading logic)
    held_status = load_weights_and_held_status(data_folder_path)
    # Pass the absolute data folder path to the loading functions
    df1, static_cols1 = load_and_process_security_data(file1, data_folder_path)
    df2, static_cols2 = load_and_process_security_data(file2, data_folder_path)
    if df1.empty or df2.empty:
        log.warning(f"One or both duration dataframes are empty after loading. File1 empty: {df1.empty}, File2 empty: {df2.empty}")
        # Return held_status even if data is empty, as it might be needed
        return pd.DataFrame(), pd.DataFrame(), [], None, held_status
    # --- Verify Index and Get Actual Names ---
    if df1.index.nlevels != 2 or df2.index.nlevels != 2:
        log.error("One or both duration dataframes do not have the expected 2 index levels after loading.")
        return pd.DataFrame(), pd.DataFrame(), [], None, held_status
    # Assume index names are consistent between df1 and df2 as they use the same loader
    date_level_name, id_level_name = df1.index.names
    log.info(f"Duration data index levels identified: Date='{date_level_name}', ID='{id_level_name}'")
    # --- Reset Index ---
    df1 = df1.reset_index()
    df2 = df2.reset_index()
    log.debug(f"Duration df1 columns after reset: {df1.columns.tolist()}")
    log.debug(f"Duration df2 columns after reset: {df2.columns.tolist()}")
    # --- Check Required Columns (Post-Reset) ---
    required_cols_df1 = [id_level_name, date_level_name, 'Value']
    required_cols_df2 = [id_level_name, date_level_name, 'Value'] # Assuming Value is standard output name
    missing_cols_df1 = [col for col in required_cols_df1 if col not in df1.columns]
    missing_cols_df2 = [col for col in required_cols_df2 if col not in df2.columns]
    if missing_cols_df1 or missing_cols_df2:
        log.error(f"Missing required columns after index reset. Df1 missing: {missing_cols_df1}, Df2 missing: {missing_cols_df2}")
        return pd.DataFrame(), pd.DataFrame(), [], None, held_status
    # Common static columns (excluding the ID column which is now standard)
    common_static_cols = list(set(static_cols1) & set(static_cols2))
    if id_level_name in common_static_cols:
        common_static_cols.remove(id_level_name)
        log.debug(f"Removed ID column '{id_level_name}' from common_static_cols list.")
    # Ensure 'Value' is not accidentally in common_static_cols
    if 'Value' in common_static_cols:
        common_static_cols.remove('Value')
    # --- Merge Preparation (Using Correct Column Names) ---
    try:
        # Select using the dynamically identified date and id column names
        df1_merge = df1[[id_level_name, date_level_name, 'Value'] + common_static_cols].rename(columns={'Value': 'Value_Orig'})
        df2_merge = df2[[id_level_name, date_level_name, 'Value']].rename(columns={'Value': 'Value_New'})
    except KeyError as e:
        # This should be less likely now, but keep for safety
        log.error(f"KeyError during merge preparation using dynamic names '{id_level_name}', '{date_level_name}': {e}. Df1 cols: {df1.columns.tolist()}, Df2 cols: {df2.columns.tolist()}")
        return pd.DataFrame(), pd.DataFrame(), [], None, held_status
    # --- Perform Merge ---
    # Merge using the dynamic date and id column names
    merged_df = pd.merge(df1_merge, df2_merge, on=[id_level_name, date_level_name], how='outer')
    merged_df = merged_df.sort_values(by=[id_level_name, date_level_name])
    # Calculate changes using the dynamic ID column name for grouping
    merged_df['Change_Orig'] = merged_df.groupby(id_level_name)['Value_Orig'].diff()
    merged_df['Change_New'] = merged_df.groupby(id_level_name)['Value_New'].diff()
    # Extract static data using the dynamic ID column name
    static_data = merged_df.groupby(id_level_name)[common_static_cols].last().reset_index()
    log.info(f"Successfully merged duration data. Shape: {merged_df.shape}")
    # Return the dynamic ID column name for use in later functions
    return merged_df, static_data, common_static_cols, id_level_name, held_status
def calculate_comparison_stats(merged_df, static_data, id_col):
    """Calculates comparison statistics for each security's duration.
    Args:
        merged_df (pd.DataFrame): The merged dataframe of original and new duration values.
        static_data (pd.DataFrame): DataFrame with static info per security.
        id_col (str): The name of the column containing the Security ID/Name.
    """
    if merged_df.empty:
        return pd.DataFrame()
    if id_col not in merged_df.columns:
        log.error(f"Specified id_col '{id_col}' not found in merged_df columns: {merged_df.columns.tolist()}")
        return pd.DataFrame() # Cannot group without the ID column
    log.info(f"Calculating duration comparison statistics using ID column: {id_col}...")
    stats_list = []
    # Use the passed id_col here
    for sec_id, group in merged_df.groupby(id_col):
        sec_stats = {id_col: sec_id} # Use actual id_col name
        # Filter out rows where both values are NaN for overall analysis period
        group_valid_overall = group.dropna(subset=['Value_Orig', 'Value_New'], how='all')
        overall_min_date = group_valid_overall['Date'].min()
        overall_max_date = group_valid_overall['Date'].max()
        # Filter out rows where EITHER value is NaN for correlation/diff calculations
        valid_comparison = group.dropna(subset=['Value_Orig', 'Value_New'])
        # 1. Correlation of Levels
        if len(valid_comparison) >= 2: # Need at least 2 points for correlation
            # Use the NaN-dropped dataframe for correlation
            level_corr = valid_comparison['Value_Orig'].corr(valid_comparison['Value_New'])
            sec_stats['Level_Correlation'] = level_corr if pd.notna(level_corr) else None
        else:
             sec_stats['Level_Correlation'] = None
        # 2. Max / Min (use original group to get true max/min including non-overlapping points)
        sec_stats['Max_Orig'] = group['Value_Orig'].max()
        sec_stats['Min_Orig'] = group['Value_Orig'].min()
        sec_stats['Max_New'] = group['Value_New'].max()
        sec_stats['Min_New'] = group['Value_New'].min()
        # 3. Date Range Comparison - Refined Logic
        # Find min/max dates within the MERGED data where each series is individually valid
        min_date_orig_idx = group['Value_Orig'].first_valid_index()
        max_date_orig_idx = group['Value_Orig'].last_valid_index()
        min_date_new_idx = group['Value_New'].first_valid_index()
        max_date_new_idx = group['Value_New'].last_valid_index()
        sec_stats['Start_Date_Orig'] = group.loc[min_date_orig_idx, 'Date'] if min_date_orig_idx is not None else None
        sec_stats['End_Date_Orig'] = group.loc[max_date_orig_idx, 'Date'] if max_date_orig_idx is not None else None
        sec_stats['Start_Date_New'] = group.loc[min_date_new_idx, 'Date'] if min_date_new_idx is not None else None
        sec_stats['End_Date_New'] = group.loc[max_date_new_idx, 'Date'] if max_date_new_idx is not None else None
        # Check if the start and end dates MATCH for the valid periods of EACH series
        same_start = pd.Timestamp(sec_stats['Start_Date_Orig']) == pd.Timestamp(sec_stats['Start_Date_New']) if sec_stats['Start_Date_Orig'] and sec_stats['Start_Date_New'] else False
        same_end = pd.Timestamp(sec_stats['End_Date_Orig']) == pd.Timestamp(sec_stats['End_Date_New']) if sec_stats['End_Date_Orig'] and sec_stats['End_Date_New'] else False
        sec_stats['Same_Date_Range'] = same_start and same_end
        # Add overall date range for info
        sec_stats['Overall_Start_Date'] = overall_min_date
        sec_stats['Overall_End_Date'] = overall_max_date
        # 4. Correlation of Daily Changes (Volatility Alignment)
        # Use the dataframe where BOTH values are non-NaN to calculate changes for correlation
        valid_comparison = valid_comparison.copy() # Avoid SettingWithCopyWarning
        valid_comparison['Change_Orig_Corr'] = valid_comparison['Value_Orig'].diff()
        valid_comparison['Change_New_Corr'] = valid_comparison['Value_New'].diff()
        # Drop NaNs created by the diff() itself (first row)
        valid_changes = valid_comparison.dropna(subset=['Change_Orig_Corr', 'Change_New_Corr'])
        if len(valid_changes) >= 2:
            change_corr = valid_changes['Change_Orig_Corr'].corr(valid_changes['Change_New_Corr'])
            sec_stats['Change_Correlation'] = change_corr if pd.notna(change_corr) else None
        else:
            sec_stats['Change_Correlation'] = None
            log.debug(f"Cannot calculate Duration Change_Correlation for {sec_id}. Need >= 2 valid change pairs, found {len(valid_changes)}.")
        # 5. Difference Statistics (use the valid_comparison df where both values exist)
        valid_comparison['Abs_Diff'] = (valid_comparison['Value_Orig'] - valid_comparison['Value_New']).abs()
        sec_stats['Mean_Abs_Diff'] = valid_comparison['Abs_Diff'].mean() # Mean diff where both values exist
        sec_stats['Max_Abs_Diff'] = valid_comparison['Abs_Diff'].max() # Max diff where both values exist
        # Count NaNs - use original group
        sec_stats['NaN_Count_Orig'] = group['Value_Orig'].isna().sum()
        sec_stats['NaN_Count_New'] = group['Value_New'].isna().sum()
        sec_stats['Total_Points'] = len(group)
        stats_list.append(sec_stats)
    summary_df = pd.DataFrame(stats_list)
    # Merge static data back
    if not static_data.empty and id_col in static_data.columns and id_col in summary_df.columns:
        summary_df = pd.merge(summary_df, static_data, on=id_col, how='left')
    elif not static_data.empty:
         log.warning(f"Could not merge static data back for duration comparison. ID column '{id_col}' missing from static_data ({id_col in static_data.columns}) or summary_df ({id_col in summary_df.columns}).")
    log.info(f"Finished calculating duration stats. Summary shape: {summary_df.shape}")
    return summary_df
# --- Routes ---
@duration_comparison_bp.route('/duration_comparison/summary') # Updated route
def summary():
    """Displays the duration comparison summary page with server-side filtering, sorting, and pagination."""
    log.info("--- Starting Duration Comparison Summary Request ---")
    # Retrieve the configured absolute data folder path
    data_folder = current_app.config['DATA_FOLDER']
    if not data_folder:
        current_app.logger.error("DATA_FOLDER is not configured in the application.")
        return "Internal Server Error: Data folder not configured", 500
    try:
        # --- Get Request Parameters ---
        page = request.args.get('page', 1, type=int)
        sort_by = request.args.get('sort_by', 'Change_Correlation') # Default sort
        sort_order = request.args.get('sort_order', 'desc').lower()
        if sort_order not in ['asc', 'desc']:
            sort_order = 'desc'
        ascending = sort_order == 'asc'
        # NEW: Get holding status filter
        show_sold = request.args.get('show_sold', 'false').lower() == 'true'
        # Get active filters (ensuring keys are correct)
        active_filters = {k.replace('filter_', ''): v
                          for k, v in request.args.items()
                          if k.startswith('filter_') and v}
        log.info(f"Request Params: Page={page}, SortBy={sort_by}, Order={sort_order}, Filters={active_filters}, ShowSold={show_sold}")
        # --- Load and Prepare Data ---
        merged_data, static_data, static_cols, actual_id_col, held_status = load_duration_comparison_data(data_folder)
        if actual_id_col is None:
            log.error("Failed to get ID column name during duration data loading.")
            return "Error loading duration comparison data: Could not determine ID column.", 500
        summary_stats = calculate_comparison_stats(merged_data, static_data, id_col=actual_id_col)
        if summary_stats.empty:
            log.info("No duration summary statistics could be calculated.")
            return render_template('duration_comparison_page.html', # Updated template
                                   table_data=[],
                                   columns_to_display=[],
                                   id_column_name=actual_id_col,
                                   filter_options={},
                                   active_filters={},
                                   current_sort_by=sort_by,
                                   current_sort_order=sort_order,
                                   pagination=None,
                                   show_sold=show_sold, # Pass filter status
                                   message="No duration comparison data available.")
        # --- Merge Held Status --- 
        if not held_status.empty and actual_id_col in summary_stats.columns:
            summary_stats = pd.merge(summary_stats, held_status, left_on=actual_id_col, right_index=True, how='left')
            summary_stats['is_held'] = summary_stats['is_held'].fillna(False)
            log.info(f"Merged held status. Stats shape: {summary_stats.shape}")
        else:
            log.warning("Could not merge held status for duration data.")
            summary_stats['is_held'] = False
        # --- Apply Holding Status Filter --- 
        original_count = len(summary_stats)
        if not show_sold:
            summary_stats = summary_stats[summary_stats['is_held'] == True]
            log.info(f"Applied 'Show Held Only' filter. Kept {len(summary_stats)} out of {original_count} securities.")
        else:
            log.info("Skipping 'Show Held Only' filter (show_sold is True).")
        if summary_stats.empty:
             log.info("No securities remaining after applying holding status filter.")
             return render_template('duration_comparison_page.html', # Updated template
                                    table_data=[],
                                    columns_to_display=[actual_id_col] + static_cols, # Show basic cols
                                    id_column_name=actual_id_col,
                                    filter_options={},
                                    active_filters={},
                                    current_sort_by=sort_by,
                                    current_sort_order=sort_order,
                                    pagination=None,
                                    show_sold=show_sold, # Pass filter status
                                    message="No currently held securities found.")
        # --- Collect Filter Options (From Data *After* Holding Filter) --- 
        filter_options = {}
        potential_filter_cols = static_cols 
        for col in potential_filter_cols:
            if col in summary_stats.columns:
                unique_vals = summary_stats[col].dropna().unique().tolist()
                try:
                    sorted_vals = sorted(unique_vals, key=lambda x: (isinstance(x, (int, float)), x))
                except TypeError:
                    sorted_vals = sorted(unique_vals, key=str)
                filter_options[col] = sorted_vals
        final_filter_options = dict(sorted(filter_options.items())) # Sort filter dropdowns alphabetically
        log.info(f"Filter options generated: {list(final_filter_options.keys())}") # Use final_filter_options
        # --- Apply Static Column Filters --- 
        filtered_data = summary_stats.copy()
        if active_filters:
            log.info(f"Applying static column filters: {active_filters}")
            for col, value in active_filters.items():
                if col in filtered_data.columns and value:
                    try:
                        # Robust string comparison
                         filtered_data = filtered_data[filtered_data[col].astype(str).str.lower() == str(value).lower()]
                    except Exception as e:
                        log.warning(f"Could not apply filter for column '{col}' with value '{value}'. Error: {e}. Skipping filter.")
                else:
                    log.warning(f"Filter column '{col}' not found in data. Skipping filter.")
            log.info(f"Data shape after static filtering: {filtered_data.shape}")
        else:
            log.info("No active static column filters.")
        if filtered_data.empty:
             log.info("No data remaining after applying static column filters.")
             return render_template('duration_comparison_page.html', # Updated template
                                    table_data=[],
                                    columns_to_display=[actual_id_col] + static_cols, # Show basic cols
                                    id_column_name=actual_id_col,
                                    filter_options=final_filter_options, # Show filter options
                                    active_filters=active_filters,
                                    current_sort_by=sort_by,
                                    current_sort_order=sort_order,
                                    pagination=None,
                                    show_sold=show_sold, # Pass filter status
                                    message="No data matches the current filters.")
        # --- Apply Sorting ---
        if sort_by in filtered_data.columns:
            log.info(f"Sorting by '{sort_by}' ({'Ascending' if ascending else 'Descending'})")
            na_position = 'last' 
            try:
                filtered_data = filtered_data.sort_values(by=sort_by, ascending=ascending, na_position=na_position)
            except Exception as e:
                log.error(f"Error during sorting by '{sort_by}': {e}. Falling back to default sort.")
                sort_by = 'Change_Correlation' 
                ascending = False
                filtered_data = filtered_data.sort_values(by=sort_by, ascending=ascending, na_position=na_position)
        else:
            log.warning(f"Sort column '{sort_by}' not found. Using default ID sort.")
            sort_by = actual_id_col 
            ascending = True
            filtered_data = filtered_data.sort_values(by=actual_id_col, ascending=ascending, na_position='last')
        # --- Pagination ---
        total_items = len(filtered_data)
        safe_per_page = max(1, PER_PAGE_COMPARISON)
        total_pages = math.ceil(total_items / safe_per_page)
        total_pages = max(1, total_pages)
        page = max(1, min(page, total_pages))
        start_index = (page - 1) * safe_per_page
        end_index = start_index + safe_per_page
        paginated_data = filtered_data.iloc[start_index:end_index]
        log.info(f"Pagination: Total items={total_items}, Total pages={total_pages}, Current page={page}, Displaying items {start_index}-{end_index-1}")
        page_window = 2
        start_page_display = max(1, page - page_window)
        end_page_display = min(total_pages, page + page_window)
        # --- Prepare for Template ---
        base_cols = [
            'Level_Correlation', 'Change_Correlation',
            'Mean_Abs_Diff', 'Max_Abs_Diff',
            'NaN_Count_Orig', 'NaN_Count_New', 'Total_Points',
            'Same_Date_Range',
            # Add/remove columns as needed
        ]
        columns_to_display = [actual_id_col] + \
                             [col for col in static_cols if col != actual_id_col and col in paginated_data.columns] + \
                             [col for col in base_cols if col in paginated_data.columns]
        table_data = paginated_data.to_dict(orient='records')
        # Format specific columns 
        for row in table_data:
            for col in ['Level_Correlation', 'Change_Correlation']:
                 if col in row and pd.notna(row[col]):
                    row[col] = f"{row[col]:.4f}" 
            # Add date formatting if needed for stats cols
        # Create pagination object
        pagination_context = {
            'page': page,
            'per_page': safe_per_page,
            'total_items': total_items,
            'total_pages': total_pages,
            'has_prev': page > 1,
            'has_next': page < total_pages,
            'prev_num': page - 1,
            'next_num': page + 1,
            'start_page_display': start_page_display,
            'end_page_display': end_page_display,
            # Function to generate URLs for pagination links, preserving state
             'url_for_page': lambda p: url_for('duration_comparison_bp.summary', 
                                              page=p, 
                                              sort_by=sort_by, 
                                              sort_order=sort_order, 
                                              show_sold=str(show_sold).lower(), # Pass holding status
                                              **{f'filter_{k}': v for k, v in active_filters.items()})
        }
        log.info("--- Successfully Prepared Data for Duration Comparison Template ---")
        return render_template('duration_comparison_page.html', # Updated template
                               table_data=table_data,
                               columns_to_display=columns_to_display,
                               id_column_name=actual_id_col, # Pass the ID column name
                               filter_options=final_filter_options,
                               active_filters=active_filters,
                               current_sort_by=sort_by,
                               current_sort_order=sort_order,
                               pagination=pagination_context,
                               show_sold=show_sold, # Pass holding filter status
                               message=None) # No message if data is present
    except FileNotFoundError as e:
        log.error(f"Duration comparison file not found: {e}")
        return f"Error: Required duration comparison file not found ({e.filename}). Check the Data folder.", 404
    except Exception as e:
        log.exception("An unexpected error occurred in the duration comparison summary view.") # Log full traceback
        return render_template('duration_comparison_page.html', 
                               message=f"An unexpected error occurred: {e}",
                               table_data=[], pagination=None, filter_options={}, 
                               active_filters={}, show_sold=show_sold, columns_to_display=[], 
                               id_column_name='Security') # Include show_sold in error template
@duration_comparison_bp.route('/duration_comparison/details/<path:security_id>')
def duration_comparison_details(security_id):
    """Displays side-by-side historical duration charts for a specific security."""
    log.info(f"--- Starting Duration Comparison Detail Request for Security ID: {security_id} ---")
    # Retrieve the configured absolute data folder path
    data_folder = current_app.config['DATA_FOLDER']
    if not data_folder:
        current_app.logger.error("DATA_FOLDER is not configured in the application.")
        return "Internal Server Error: Data folder not configured", 500
    try:
        # Pass the absolute data folder path
        merged_data, static_data, common_static_cols, id_col_name, _ = load_duration_comparison_data(data_folder)
        if id_col_name is None:
             log.error(f"Failed to get ID column name for details view (Security: {security_id}).")
             return "Error loading duration comparison data: Could not determine ID column.", 500
        if merged_data.empty:
            log.warning(f"Merged duration data is empty for details view (Security: {security_id}).")
            return f"No merged duration data found for Security ID: {security_id}", 404
        # Filter data for the specific security using the correct ID column name
        security_data = merged_data[merged_data[id_col_name] == security_id].copy() # Use .copy()
        if security_data.empty:
            log.warning(f"No duration data found for the specific Security ID: {security_id}")
            # Consider checking if the ID exists in the original files?
            return f"Duration data not found for Security ID: {security_id}", 404
        # Get static info for this security (handle potential multiple rows if ID isn't unique, take first)
        static_info = security_data[[id_col_name] + common_static_cols].iloc[0].to_dict() if not security_data.empty else {}
        # Sort by date for charting
        security_data = security_data.sort_values(by='Date')
        # Prepare data for Chart.js
        # Ensure 'Date' is in the correct string format for JSON/JS
        security_data['Date_Str'] = security_data['Date'].dt.strftime('%Y-%m-%d')
        chart_data = {
            'labels': security_data['Date_Str'].tolist(),
            'datasets': [
                {
                    'label': 'Original Duration',
                    'data': security_data['Value_Orig'].where(pd.notna(security_data['Value_Orig']), None).tolist(), # Replace NaN with None for JSON
                    'borderColor': COLOR_PALETTE[0 % len(COLOR_PALETTE)],
                    'fill': False,
                    'tension': 0.1
                },
                {
                    'label': 'New Duration',
                    'data': security_data['Value_New'].where(pd.notna(security_data['Value_New']), None).tolist(), # Replace NaN with None for JSON
                    'borderColor': COLOR_PALETTE[1 % len(COLOR_PALETTE)],
                    'fill': False,
                    'tension': 0.1
                }
            ]
        }
        # Calculate overall statistics for this security
        stats_summary = calculate_comparison_stats(security_data, pd.DataFrame([static_info]), id_col=id_col_name) # Pass single security data
        stats_dict = stats_summary.iloc[0].to_dict() if not stats_summary.empty else {}
         # Format dates and numbers in stats_dict before passing
        for key, value in stats_dict.items():
            if isinstance(value, pd.Timestamp):
                stats_dict[key] = value.strftime('%Y-%m-%d')
            elif isinstance(value, (int, float)):
                 if 'Correlation' in key and pd.notna(value):
                     stats_dict[key] = f"{value:.4f}"
                 elif 'Diff' in key and pd.notna(value):
                      stats_dict[key] = f"{value:.2f}" # Adjust formatting as needed
        log.info(f"Successfully prepared data for duration details template (Security: {security_id})")
        return render_template('duration_comparison_details_page.html', # Updated template
                               security_id=security_id,
                               static_info=static_info, # Pass static info
                               chart_data=chart_data,
                               stats_summary=stats_dict) # Pass calculated stats
    except FileNotFoundError as e:
        log.error(f"Duration comparison file not found for details view: {e} (Security: {security_id})")
        return f"Error: Required duration comparison file not found ({e.filename}). Check the Data folder.", 404
    except KeyError as e:
         log.error(f"KeyError accessing data for security '{security_id}': {e}. ID column used: '{id_col_name}'")
         return f"Error accessing data for security '{security_id}'. It might be missing required columns or have unexpected formatting.", 500
    except Exception as e:
        log.exception(f"An unexpected error occurred in the duration comparison details view for security '{security_id}'.") # Log full traceback
        return f"An internal error occurred while processing details for security '{security_id}': {e}", 500
</file>

<file path="views/exclusion_views.py">
"""
This module defines the Flask Blueprint for handling security exclusion management.
It provides routes to view the current exclusion list and add new securities
to the list.
"""
import os
import pandas as pd
from flask import Blueprint, render_template, request, redirect, url_for, current_app
from datetime import datetime
import logging
# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
# Define the Blueprint
exclusion_bp = Blueprint('exclusion_bp', __name__, template_folder='../templates')
EXCLUSIONS_FILE = 'exclusions.csv'
# Assuming sec_spread.csv contains the list of all possible securities
# We need to determine the correct file and column name for security IDs
# Let's tentatively use sec_spread.csv and 'Security ID'
SECURITIES_SOURCE_FILE = 'sec_spread.csv' # Adjust if needed
SECURITY_ID_COLUMN = 'Security Name' # Corrected column name
def load_exclusions(data_folder_path: str):
    """Loads the current list of exclusions from the CSV file.
    Args:
        data_folder_path (str): The absolute path to the data folder.
    Returns:
        list[dict]: A list of dictionaries representing the exclusions, or [] if error.
    """
    if not data_folder_path:
        logging.error("No data_folder_path provided to load_exclusions.")
        return []
    exclusions_path = os.path.join(data_folder_path, EXCLUSIONS_FILE)
    try:
        if os.path.exists(exclusions_path) and os.path.getsize(exclusions_path) > 0:
            df = pd.read_csv(exclusions_path, parse_dates=['AddDate', 'EndDate'], dayfirst=False) # Specify date format if needed
            # Ensure correct types after loading
            df['AddDate'] = pd.to_datetime(df['AddDate'], errors='coerce')
            df['EndDate'] = pd.to_datetime(df['EndDate'], errors='coerce')
            df['SecurityID'] = df['SecurityID'].astype(str)
            df['Comment'] = df['Comment'].astype(str)
            df = df.sort_values(by='AddDate', ascending=False)
            return df.to_dict('records')
        else:
            logging.info(f"'{EXCLUSIONS_FILE}' is empty or does not exist. Returning empty list.")
            return []
    except Exception as e:
        logging.error(f"Error loading exclusions from {exclusions_path}: {e}")
        return [] # Return empty list on error
def load_available_securities(data_folder_path: str):
    """Loads the list of available security IDs from the source file.
    Args:
        data_folder_path (str): The absolute path to the data folder.
    Returns:
        list[str]: A sorted list of unique available security IDs, or [] if error.
    """
    if not data_folder_path:
        logging.error("No data_folder_path provided to load_available_securities.")
        return []
    securities_file_path = os.path.join(data_folder_path, SECURITIES_SOURCE_FILE)
    try:
        if os.path.exists(securities_file_path):
            # Load only the necessary column
            # Use security_processing logic if more complex loading is needed
            df_securities = pd.read_csv(securities_file_path, usecols=[SECURITY_ID_COLUMN], encoding_errors='replace', on_bad_lines='skip')
            df_securities.dropna(subset=[SECURITY_ID_COLUMN], inplace=True)
            security_ids = df_securities[SECURITY_ID_COLUMN].astype(str).unique().tolist()
            security_ids.sort() # Sort for dropdown consistency
            return security_ids
        else:
            logging.warning(f"Securities source file '{SECURITIES_SOURCE_FILE}' not found at {securities_file_path}.")
            return []
    except KeyError:
        logging.error(f"Column '{SECURITY_ID_COLUMN}' not found in '{SECURITIES_SOURCE_FILE}'. Cannot load available securities.")
        return []
    except Exception as e:
        logging.error(f"Error loading available securities from {securities_file_path}: {e}")
        return []
def add_exclusion(data_folder_path: str, security_id, end_date_str, comment):
    """Adds a new exclusion to the CSV file.
    Args:
        data_folder_path (str): The absolute path to the data folder.
        security_id:
        end_date_str:
        comment:
    Returns:
        tuple[bool, str]: (Success status, Message)
    """
    if not data_folder_path:
        logging.error("No data_folder_path provided to add_exclusion.")
        return False, "Internal Server Error: Data folder path not configured."
    exclusions_path = os.path.join(data_folder_path, EXCLUSIONS_FILE)
    try:
        # Basic validation
        if not security_id or not comment:
            logging.warning("Attempted to add exclusion with missing SecurityID or Comment.")
            return False, "Security ID and Comment are required."
        add_date = datetime.now().strftime('%Y-%m-%d')
        # Parse end_date, allow it to be empty
        end_date = pd.to_datetime(end_date_str, errors='coerce').strftime('%Y-%m-%d') if end_date_str else ''
        new_exclusion = pd.DataFrame({
            'SecurityID': [str(security_id)],
            'AddDate': [add_date],
            'EndDate': [end_date],
            'Comment': [str(comment)]
        })
        # Append to CSV, create header if file doesn't exist or is empty
        file_exists = os.path.exists(exclusions_path)
        is_empty = file_exists and os.path.getsize(exclusions_path) == 0
        write_header = not file_exists or is_empty
        new_exclusion.to_csv(exclusions_path, mode='a', header=write_header, index=False)
        logging.info(f"Added exclusion for SecurityID: {security_id}")
        return True, "Exclusion added successfully."
    except Exception as e:
        logging.error(f"Error adding exclusion to {exclusions_path}: {e}")
        return False, "An error occurred while saving the exclusion."
def remove_exclusion(data_folder_path: str, security_id_to_remove, add_date_str_to_remove):
    """Removes a specific exclusion entry from the CSV file based on SecurityID and AddDate.
    Args:
        data_folder_path (str): The absolute path to the data folder.
        security_id_to_remove:
        add_date_str_to_remove:
    Returns:
        tuple[bool, str]: (Success status, Message)
    """
    if not data_folder_path:
        logging.error("No data_folder_path provided to remove_exclusion.")
        return False, "Internal Server Error: Data folder path not configured."
    exclusions_path = os.path.join(data_folder_path, EXCLUSIONS_FILE)
    try:
        if not os.path.exists(exclusions_path) or os.path.getsize(exclusions_path) == 0:
            logging.warning(f"Attempted to remove exclusion, but '{EXCLUSIONS_FILE}' is empty or does not exist.")
            return False, "Exclusion file is empty or missing."
        df = pd.read_csv(exclusions_path)
        # Ensure columns used for matching are strings
        df['SecurityID'] = df['SecurityID'].astype(str)
        # Keep AddDate as string for direct comparison with the string from the form
        df['AddDate'] = df['AddDate'].astype(str)
        security_id_to_remove = str(security_id_to_remove)
        original_count = len(df)
        # Filter out the row(s) to remove
        # Match both SecurityID and the AddDate string
        df_filtered = df[~((df['SecurityID'] == security_id_to_remove) & (df['AddDate'] == add_date_str_to_remove))]
        if len(df_filtered) == original_count:
            logging.warning(f"Exclusion entry for SecurityID '{security_id_to_remove}' with AddDate '{add_date_str_to_remove}' not found for removal.")
            return False, "Exclusion entry not found."
        # Overwrite the CSV with the filtered data
        df_filtered.to_csv(exclusions_path, index=False)
        logging.info(f"Removed exclusion entry for SecurityID: {security_id_to_remove}, AddDate: {add_date_str_to_remove}")
        return True, "Exclusion removed successfully."
    except Exception as e:
        logging.error(f"Error removing exclusion from {exclusions_path}: {e}")
        return False, "An error occurred while removing the exclusion."
@exclusion_bp.route('/exclusions', methods=['GET', 'POST'])
def manage_exclusions():
    """
    Handles viewing and adding security exclusions.
    GET: Displays the list of current exclusions and the form to add new ones.
    POST: Processes the form submission to add a new exclusion.
    """
    # Retrieve the configured absolute data folder path
    data_folder = current_app.config['DATA_FOLDER']
    if not data_folder:
        current_app.logger.error("DATA_FOLDER is not configured in the application.")
        return "Internal Server Error: Data folder not configured", 500
    message = None
    message_type = 'info' # Can be 'success' or 'error'
    if request.method == 'POST':
        security_id = request.form.get('security_id')
        end_date_str = request.form.get('end_date')
        comment = request.form.get('comment')
        # Pass the absolute data_folder path to the helper function
        success, msg = add_exclusion(data_folder, security_id, end_date_str, comment)
        if success:
            # Redirect to the same page using GET to prevent form resubmission
            return redirect(url_for('exclusion_bp.manage_exclusions', _external=True))
        else:
            message = msg
            message_type = 'error'
            # Fall through to render the page again with the error message
    # For both GET requests and POST failures, load data and render template
    # Pass the absolute data_folder path to the helper functions
    current_exclusions = load_exclusions(data_folder)
    available_securities = load_available_securities(data_folder)
    return render_template('exclusions_page.html',
                           exclusions=current_exclusions,
                           available_securities=available_securities,
                           message=message,
                           message_type=message_type)
@exclusion_bp.route('/exclusions/remove', methods=['POST'])
def remove_exclusion_route():
    """Handles the POST request to remove an exclusion."""
    # Retrieve the configured absolute data folder path
    data_folder = current_app.config['DATA_FOLDER']
    if not data_folder:
        current_app.logger.error("DATA_FOLDER is not configured in the application.")
        # Redirect back with an error indication (flash message would be better)
        return redirect(url_for('exclusion_bp.manage_exclusions'))
    security_id = request.form.get('security_id')
    add_date_str = request.form.get('add_date') # Get AddDate as string
    if not security_id or not add_date_str:
        # Handle missing identifiers (shouldn't happen with hidden fields but good practice)
        # Redirect back with an error message (using flash or query params)
        logging.warning("Remove exclusion attempt missing SecurityID or AddDate.")
        # For simplicity, redirect back to the main page; flash messages would be better
        return redirect(url_for('exclusion_bp.manage_exclusions'))
    # Pass the absolute data_folder path to the helper function
    success, msg = remove_exclusion(data_folder, security_id, add_date_str)
    # Regardless of success/failure, redirect back to the main exclusions page.
    # Consider using flash messages to display the success/error message after redirect.
    # For now, the message `msg` is logged but not shown to the user on redirect.
    return redirect(url_for('exclusion_bp.manage_exclusions'))
</file>

<file path="views/fund_views.py">
"""Blueprint for fund-specific routes, including duration details and a general fund overview page."""
from flask import Blueprint, render_template, current_app, jsonify
import os
import pandas as pd
import traceback
import logging # Added for logging
import glob # Added for finding files
import re # Added for extracting metric name
import numpy as np
# Import necessary functions from other modules
from utils import _is_date_like, parse_fund_list # Import required utils
# Updated import to include data loader
from data_loader import load_and_process_data
# Define the blueprint
fund_bp = Blueprint('fund', __name__, url_prefix='/fund')
@fund_bp.route('/duration_details/<fund_code>') # Corresponds to /fund/duration_details/...
def fund_duration_details(fund_code):
    """Renders a page showing duration changes for securities held by a specific fund."""
    # Retrieve the absolute data folder path from the app context
    data_folder = current_app.config['DATA_FOLDER']
    if not data_folder:
        current_app.logger.error("DATA_FOLDER is not configured in the application.")
        return "Internal Server Error: Data folder not configured", 500
    duration_filename = "sec_duration.csv"
    # Construct absolute path
    data_filepath = os.path.join(data_folder, duration_filename)
    current_app.logger.info(f"--- Requesting Duration Details for Fund: {fund_code} --- File: {data_filepath}")
    if not os.path.exists(data_filepath):
        print(f"Error: Duration file '{duration_filename}' not found.")
        return f"Error: Data file '{duration_filename}' not found.", 404
    try:
        # 1. Load the duration data (only header first for column identification)
        header_df = pd.read_csv(data_filepath, nrows=0, encoding='utf-8')
        all_cols = [col.strip() for col in header_df.columns.tolist()]
        # Define ID column (specific to this file/route)
        id_col_name = 'ISIN'
        if id_col_name not in all_cols:
            print(f"Error: Expected ID column '{id_col_name}' not found in {duration_filename}.")
            return f"Error: Required ID column '{id_col_name}' not found in '{duration_filename}'.", 500
        # 2. Identify static and date columns dynamically
        date_cols = []
        static_cols = []
        for col in all_cols:
            if col == id_col_name:
                continue # Skip the ID column
            if col == 'Security Name':
                continue # Skip the old ID column if it exists
            if _is_date_like(col): # Use the helper function from utils
                date_cols.append(col)
            else:
                static_cols.append(col) # Treat others as static
        print(f"Dynamically identified Static Cols: {static_cols}")
        print(f"Dynamically identified Date Cols (first 5): {date_cols[:5]}...")
        if not date_cols or len(date_cols) < 2:
             print("Error: Not enough date columns found in duration file to calculate change.")
             return f"Error: Insufficient date columns in '{duration_filename}' to calculate change.", 500
        # Now read the full data
        df = pd.read_csv(data_filepath, encoding='utf-8')
        df.columns = df.columns.str.strip() # Strip again after full read
        # Ensure the Funds column exists (still needed for filtering)
        funds_col = 'Funds' # Keep this assumption for now as it's key to filtering
        if funds_col not in static_cols:
             print(f"Warning: Expected column '{funds_col}' for filtering not found among static columns.")
             # Decide how to handle this - error or proceed without fund filtering? Let's error for now.
             return f"Error: Required column '{funds_col}' for fund filtering not found.", 500
        # Ensure date columns are sortable (attempt conversion if needed, basic check)
        try:
            # Check and sort date columns using the correct YYYY-MM-DD format
            pd.to_datetime(date_cols, format='%Y-%m-%d', errors='raise')
            date_cols = sorted(date_cols, key=lambda d: pd.to_datetime(d, format='%Y-%m-%d'))
            print(f"Identified and sorted date columns (YYYY-MM-DD): {date_cols[-5:]} (last 5 shown)")
        except ValueError:
            print("Warning: Could not parse all date columns using YYYY-MM-DD format. Using original order.")
            # Fallback remains, but hopefully won't be needed as often
        # Identify last two date columns based on sorted list (or original if parsing failed)
        if len(date_cols) < 2: # Double check after potential parsing failure
            return f"Error: Insufficient valid date columns in '{duration_filename}' to calculate change after sorting attempt.", 500
        last_date_col = date_cols[-1]
        second_last_date_col = date_cols[-2]
        print(f"Using dates for change calculation: {second_last_date_col} and {last_date_col}")
        # Ensure the relevant date columns are numeric for calculation
        df[last_date_col] = pd.to_numeric(df[last_date_col], errors='coerce')
        df[second_last_date_col] = pd.to_numeric(df[second_last_date_col], errors='coerce')
        # 3. Filter by Fund Code
        # Apply the parsing function from utils to the 'Funds' column
        fund_lists = df['Funds'].apply(parse_fund_list)
        # Create a boolean mask to filter rows where the fund_code is in the parsed list
        mask = fund_lists.apply(lambda funds: fund_code in funds)
        filtered_df = df[mask].copy() # Use copy to avoid SettingWithCopyWarning
        # --- Load and Process Weight Data (w_secs.csv) --- 
        weights_filename = "w_secs.csv"
        # Construct absolute path for weights file
        weights_filepath = os.path.join(data_folder, weights_filename)
        contribution_calculated = False # Flag to track if calculation was successful
        new_contribution_cols = []
        if not os.path.exists(weights_filepath):
            print(f"Warning: Weight file '{weights_filename}' not found. Skipping duration contribution calculation.")
        else:
            try:
                print(f"Loading weight file: {weights_filename}")
                weights_df = pd.read_csv(weights_filepath, encoding='utf-8')
                weights_df.columns = weights_df.columns.str.strip()
                # Define expected columns in weights file
                weight_id_col = 'Security Name'
                weight_fund_col = 'Funds'
                # Check if necessary columns exist in weights_df
                # Convert the dates from duration file (YYYY-MM-DD) to the format in weights file (DD/MM/YYYY)
                try:
                    last_date_dt = pd.to_datetime(last_date_col, format='%Y-%m-%d')
                    second_last_date_dt = pd.to_datetime(second_last_date_col, format='%Y-%m-%d')
                    last_date_col_weights_fmt = last_date_dt.strftime('%d/%m/%Y')
                    second_last_date_col_weights_fmt = second_last_date_dt.strftime('%d/%m/%Y')
                    print(f"Looking for weight columns: {last_date_col_weights_fmt}, {second_last_date_col_weights_fmt}")
                except ValueError:
                     print(f"Error: Could not convert duration dates ({last_date_col}, {second_last_date_col}) to datetime objects for weight lookup. Skipping contribution.")
                     last_date_col_weights_fmt = None # Ensure it skips if conversion fails
                # Check using the formatted date strings
                required_weight_cols = [weight_id_col, weight_fund_col]
                if last_date_col_weights_fmt:
                    required_weight_cols.extend([last_date_col_weights_fmt, second_last_date_col_weights_fmt])
                else:
                    # Skip if dates couldn't be formatted
                    print(f"Skipping weight check due to date format conversion error.")
                    all_cols_exist = False
                all_cols_exist = all(col in weights_df.columns for col in required_weight_cols)
                if not all_cols_exist:
                    missing_cols = [col for col in required_weight_cols if col not in weights_df.columns]
                    print(f"Warning: Weight file '{weights_filename}' is missing required columns (needed: {required_weight_cols}, missing: {missing_cols}). Skipping contribution calculation.")
                else:
                    print(f"Filtering weights for fund: {fund_code}")
                    # Filter weights by fund code (assuming direct match in 'Funds' column)
                    fund_weights_df = weights_df[weights_df[weight_fund_col] == fund_code].copy()
                    if fund_weights_df.empty:
                        print(f"Warning: No weights found for fund '{fund_code}' in {weights_filename}. Contribution will be zero.")
                        # Create empty df with correct columns to avoid merge errors later if we still want zero cols
                        weights_to_merge = pd.DataFrame(columns=[weight_id_col, 'Weight Last Date', 'Weight Second Last Date'])
                        weights_to_merge = weights_to_merge.astype({weight_id_col: 'object', 'Weight Last Date': 'float64', 'Weight Second Last Date': 'float64'})
                    else:
                         # Select and rename relevant weight columns using the CORRECT formatted date strings
                        weights_to_merge = fund_weights_df[[weight_id_col, last_date_col_weights_fmt, second_last_date_col_weights_fmt]].copy()
                        weights_to_merge.rename(columns={
                            last_date_col_weights_fmt: 'Weight Last Date',
                            second_last_date_col_weights_fmt: 'Weight Second Last Date'
                        }, inplace=True)
                         # Ensure weight columns are numeric
                        weights_to_merge['Weight Last Date'] = pd.to_numeric(weights_to_merge['Weight Last Date'], errors='coerce')
                        weights_to_merge['Weight Second Last Date'] = pd.to_numeric(weights_to_merge['Weight Second Last Date'], errors='coerce')
                    # Merge weights with filtered duration data
                    print(f"Merging duration data with weights on '{weight_id_col}'")
                    filtered_df = pd.merge(filtered_df, weights_to_merge, on=weight_id_col, how='left')
                    # Fill missing weights with 0 and calculate contribution
                    filtered_df['Weight Last Date'].fillna(0, inplace=True)
                    filtered_df['Weight Second Last Date'].fillna(0, inplace=True)
                    contrib_last_col = 'Duration Contribution Last Date'
                    contrib_second_last_col = 'Duration Contribution Second Last Date'
                    contrib_change_col = 'Duration Contribution Change'
                    filtered_df[contrib_last_col] = filtered_df[last_date_col] * filtered_df['Weight Last Date']
                    filtered_df[contrib_second_last_col] = filtered_df[second_last_date_col] * filtered_df['Weight Second Last Date']
                    filtered_df[contrib_change_col] = filtered_df[contrib_last_col] - filtered_df[contrib_second_last_col]
                    contribution_calculated = True
                    new_contribution_cols = [contrib_second_last_col, contrib_last_col, contrib_change_col]
                    print("Duration contribution calculated successfully.")
            except Exception as weight_err:
                 print(f"Error processing weight file '{weights_filename}': {weight_err}. Skipping contribution calculation.")
                 traceback.print_exc()
        # --- End Weight Data Processing ---
        if filtered_df.empty:
            print(f"No securities found for fund '{fund_code}' in {duration_filename} after initial filtering.")
            # Render a template indicating no data found for this fund
            return render_template('fund_duration_details.html',
                                   fund_code=fund_code,
                                   securities_data=[],
                                   column_order=[],
                                   id_col_name=None,
                                   message=f"No securities found held by fund '{fund_code}' in {duration_filename}.")
        print(f"Found {len(filtered_df)} securities for fund '{fund_code}'. Calculating duration changes...")
        # 4. Calculate 1-day Duration Change (already done if contribution wasn't skipped)
        change_col_name = '1 Day Duration Change'
        if change_col_name not in filtered_df.columns:
            filtered_df[change_col_name] = filtered_df[last_date_col] - filtered_df[second_last_date_col]
        # 5. Sort by Duration Change (descending, NaN last)
        filtered_df.sort_values(by=change_col_name, ascending=False, na_position='last', inplace=True)
        print(f"Sorted securities by {change_col_name}.")
        # 6. Prepare data for template
        # Define base display columns
        existing_static_cols = [col for col in static_cols if col in filtered_df.columns]
        if 'Security Name' in filtered_df.columns and 'Security Name' not in existing_static_cols:
            existing_static_cols.insert(0, 'Security Name') # Add Security Name near the start
        # Define column order, putting ISIN (the new id_col_name) first
        display_cols = [id_col_name] + existing_static_cols + [second_last_date_col, last_date_col, change_col_name]
        # Add contribution columns if they were calculated
        if contribution_calculated:
            display_cols.extend(new_contribution_cols)
        final_col_order = [col for col in display_cols if col in filtered_df.columns] # Ensure only existing columns are kept
        # Round numeric columns before converting to dict
        numeric_cols_in_final = filtered_df[final_col_order].select_dtypes(include=np.number).columns
        filtered_df[numeric_cols_in_final] = filtered_df[numeric_cols_in_final].round(4) # Use more precision for contribution?
        securities_data_list = filtered_df[final_col_order].to_dict(orient='records')
        # Handle potential NaN/NaT values for template rendering
        for row in securities_data_list:
             for key, value in row.items():
                 if pd.isna(value):
                     row[key] = None
        print(f"Final column order for display: {final_col_order}")
        return render_template('fund_duration_details.html',
                               fund_code=fund_code,
                               securities_data=securities_data_list,
                               column_order=final_col_order,
                               id_col_name=id_col_name,
                               message=None)
    except FileNotFoundError:
         return f"Error: Data file '{duration_filename}' not found.", 404
    except Exception as e:
        print(f"Error processing duration details for fund {fund_code}: {e}")
        traceback.print_exc()
        return f"An error occurred processing duration details for fund {fund_code}: {e}", 500
# --- New Route for Fund Detail Page ---
@fund_bp.route('/<fund_code>')
def fund_detail(fund_code):
    """Renders a page displaying all available time-series charts for a specific fund,
       optionally including comparison data from sp_ts_* files."""
    current_app.logger.info(f"--- Requesting Detail Page for Fund: {fund_code} ---")
    all_chart_data = []
    available_metrics = []
    processed_files = 0
    skipped_files = 0
    error_messages = [] # Collect specific errors
    try:
        # Find all primary time-series files
        ts_files_pattern = os.path.join(data_folder, 'ts_*.csv')
        ts_files = glob.glob(ts_files_pattern)
        # Exclude sp_ts files initially, handle them later
        ts_files = [f for f in ts_files if not os.path.basename(f).startswith('sp_ts_')]
        current_app.logger.info(f"Found {len(ts_files)} primary ts_ files: {[os.path.basename(f) for f in ts_files]}")
        if not ts_files:
            current_app.logger.warning("No primary ts_*.csv files found in Data folder.")
            return render_template('fund_detail_page.html',
                                   fund_code=fund_code,
                                   chart_data_json='[]',
                                   available_metrics=[],
                                   message="No time-series data files (ts_*.csv) found.")
        # Process each file
        for file_path in ts_files:
            filename = os.path.basename(file_path)
            # Extract metric name
            match = re.match(r'ts_(.+?)(?:_processed)?\.csv', filename, re.IGNORECASE)
            if not match:
                current_app.logger.warning(f"Could not extract metric name from filename: {filename}. Skipping.")
                skipped_files += 1
                continue
            metric_name_raw = match.group(1) # Keep raw name for SP file lookup
            metric_name_display = metric_name_raw.replace('_', ' ').title()
            current_app.logger.info(f"\\nProcessing {filename} for metric: {metric_name_display}")
            # --- Prepare SP file path ---
            sp_filename = f"sp_{filename}"
            sp_file_path = os.path.join(data_folder, sp_filename)
            sp_df = None
            sp_fund_col_name = None
            sp_load_error = None
            # --- Load primary data ---
            df = None
            fund_cols = None
            benchmark_col = None
            primary_load_error = None
            try:
                load_result = load_and_process_data(filename, data_folder=data_folder)
                df = load_result[0]
                fund_cols = load_result[1]
                benchmark_col = load_result[2]
                if df is None or df.empty:
                    current_app.logger.warning(f"No data loaded or DataFrame empty for {filename}. Skipping.")
                    skipped_files += 1
                    continue
                if 'Code' not in df.index.names:
                     current_app.logger.error(f"Index level 'Code' not found in DataFrame loaded from {filename}. Index: {df.index.names}. Skipping.")
                     error_messages.append(f"Failed to process {filename}: Missing 'Code' index level.")
                     skipped_files += 1
                     continue
            except Exception as e:
                current_app.logger.error(f"Error loading primary file {filename}: {e}", exc_info=False)
                primary_load_error = f"Error loading {filename}: {e}"
                error_messages.append(primary_load_error)
                skipped_files += 1
                # Continue processing other files, but skip this metric
                continue # Skip to next ts_file
            # --- Load SP data if primary load was successful and SP file exists ---
            if primary_load_error is None and os.path.exists(sp_file_path):
                current_app.logger.info(f"Found corresponding SP file: {sp_filename}. Attempting to load.")
                try:
                    sp_load_result = load_and_process_data(sp_filename, data_folder=data_folder)
                    sp_df = sp_load_result[0]
                    sp_fund_cols = sp_load_result[1] # Assuming same structure for fund cols
                    # SP files typically don't have benchmarks in this context, ignore sp_load_result[2]
                    # --- Correction: Let's check for SP benchmark column too ---
                    sp_benchmark_col = sp_load_result[2] # Get potential SP benchmark col name
                    if sp_df is None or sp_df.empty:
                        current_app.logger.warning(f"No data loaded or DataFrame empty for SP file {sp_filename}.")
                        sp_df = None # Ensure sp_df is None if empty
                    elif 'Code' not in sp_df.index.names:
                         current_app.logger.error(f"Index level 'Code' not found in DataFrame loaded from SP file {sp_filename}. Index: {sp_df.index.names}.")
                         sp_df = None # Ensure sp_df is None if index is wrong
                         sp_load_error = f"SP file {sp_filename} missing 'Code' index."
                         error_messages.append(sp_load_error)
                    else:
                        # Find the fund column name in the SP data
                        sp_fund_col_name = next((col for col in sp_fund_cols if col in sp_df.columns), None)
                        if not sp_fund_col_name:
                             current_app.logger.warning(f"Could not find fund data column in SP file {sp_filename}.")
                             sp_df = None # Cannot use this SP data without fund column
                        # We keep sp_df if benchmark exists, even if fund doesn't
                except Exception as e:
                    current_app.logger.error(f"Error loading SP file {sp_filename}: {e}", exc_info=False)
                    sp_load_error = f"Error loading SP file {sp_filename}: {e}"
                    error_messages.append(sp_load_error)
                    sp_df = None # Ensure sp_df is None on error
            # --- Filter primary data for the fund ---
            fund_mask = df.index.get_level_values('Code') == fund_code
            fund_df = df[fund_mask]
            if fund_df.empty:
                current_app.logger.info(f"Fund code '{fund_code}' not found in primary data from {filename}. Skipping metric.")
                skipped_files += 1 # Processed file, but no data for this fund
                continue # Skip to next ts_file
            current_app.logger.info(f"Fund code '{fund_code}' found in primary data. Preparing chart data for '{metric_name_display}'...")
            available_metrics.append(metric_name_display) # Use display name
            # Drop the 'Code' level now we've filtered
            fund_df = fund_df.droplevel('Code')
            # --- Filter SP data for the fund (if loaded) ---
            sp_fund_df = None
            if sp_df is not None:
                sp_fund_mask = sp_df.index.get_level_values('Code') == fund_code
                sp_fund_df = sp_df[sp_fund_mask]
                if not sp_fund_df.empty:
                    sp_fund_df = sp_fund_df.droplevel('Code')
                    current_app.logger.info(f"Fund code '{fund_code}' found in SP data from {sp_filename}.")
                else:
                    current_app.logger.info(f"Fund code '{fund_code}' *not* found in SP data from {sp_filename}.")
                    sp_fund_df = None # Treat as if no SP data for this fund
            # --- Prepare chart data structure ---
            # Use the primary fund_df index as the master list of labels
            # Align SP data to this index later if needed
            chart_labels = fund_df.index.strftime('%Y-%m-%d').tolist()
            chart_data = {
                'metricName': metric_name_display, # Use display name
                'labels': chart_labels,
                'datasets': []
            }
            # Add primary fund dataset
            fund_col_name = next((col for col in fund_cols if col in fund_df.columns), None)
            if fund_col_name:
                # Reindex to ensure consistent length and alignment, fill missing with None
                fund_values = fund_df[fund_col_name].reindex(fund_df.index).where(pd.notna, None).tolist()
                chart_data['datasets'].append({
                    'label': f"{fund_code} {metric_name_display}",
                    'data': fund_values,
                    'borderColor': current_app.config['COLOR_PALETTE'][0 % len(current_app.config['COLOR_PALETTE'])],
                    'tension': 0.1,
                    'pointRadius': 1,
                    'borderWidth': 1.5,
                    'isSpData': False # Explicitly mark as not SP
                })
            else:
                current_app.logger.warning(f"Warning: Could not find primary fund data column ({fund_cols}) in {filename} for fund {fund_code}")
            # Add benchmark dataset (from primary data)
            if benchmark_col and benchmark_col in fund_df.columns:
                # Reindex to ensure consistent length and alignment
                bench_values = fund_df[benchmark_col].reindex(fund_df.index).where(pd.notna, None).tolist()
                chart_data['datasets'].append({
                    'label': f"Benchmark ({benchmark_col})",
                    'data': bench_values,
                    'borderColor': current_app.config['COLOR_PALETTE'][1 % len(current_app.config['COLOR_PALETTE'])],
                    'tension': 0.1,
                    'pointRadius': 1,
                    'borderDash': [5, 5],
                    'borderWidth': 1,
                    'isSpData': False # Explicitly mark as not SP
                })
            # --- Add SP fund dataset (if available) ---
            if sp_fund_df is not None:
                # Add SP Fund Data (if column exists)
                if sp_fund_col_name:
                    # Reindex SP data to the primary data's date index to ensure alignment for the chart
                    sp_fund_aligned = sp_fund_df[sp_fund_col_name].reindex(fund_df.index)
                    sp_values = sp_fund_aligned.where(pd.notna, None).tolist() # Replace NaN with None for JSON
                    chart_data['datasets'].append({
                        'label': f"{fund_code} {metric_name_display} (SP)",
                        'data': sp_values,
                        'borderColor': current_app.config['COLOR_PALETTE'][2 % len(current_app.config['COLOR_PALETTE'])], # Use a different color
                        'tension': 0.1,
                        'pointRadius': 1,
                        'borderDash': [2, 2], # Different dash style
                        'borderWidth': 1.5,
                        'isSpData': True # Mark this dataset as SP data
                        # 'hidden': True # Optionally start hidden
                    })
                    current_app.logger.info(f"Added SP Fund dataset for metric '{metric_name_display}'.")
                else:
                    current_app.logger.warning(f"SP Fund column ('{sp_fund_cols}') not found in filtered SP data for {sp_filename}, fund {fund_code}.")
                # --- Add SP benchmark dataset (if available) ---
                if sp_benchmark_col and sp_benchmark_col in sp_fund_df.columns:
                    # Reindex SP benchmark data to the primary data's date index
                    sp_bench_aligned = sp_fund_df[sp_benchmark_col].reindex(fund_df.index)
                    sp_bench_values = sp_bench_aligned.where(pd.notna, None).tolist()
                    chart_data['datasets'].append({
                        'label': f"Benchmark ({sp_benchmark_col}) (SP)", # Label appropriately
                        'data': sp_bench_values,
                        'borderColor': current_app.config['COLOR_PALETTE'][3 % len(current_app.config['COLOR_PALETTE'])], # Use another color
                        'tension': 0.1,
                        'pointRadius': 1,
                        'borderDash': [2, 2], # Use dash similar to SP fund
                        'borderWidth': 1,
                        'isSpData': True # Mark this dataset as SP data
                    })
                    current_app.logger.info(f"Added SP Benchmark dataset ('{sp_benchmark_col}') for metric '{metric_name_display}'.")
                elif sp_benchmark_col:
                    current_app.logger.warning(f"SP Benchmark column ('{sp_benchmark_col}') specified but not found in filtered SP data for {sp_filename}, fund {fund_code}.")
            # Only add chart if we have at least one non-empty dataset
            if any(d['data'] for d in chart_data['datasets']):
                all_chart_data.append(chart_data)
                processed_files += 1
            else:
                 current_app.logger.warning(f"No valid datasets generated for metric '{metric_name_display}' from {filename} (and potentially {sp_filename}). Skipping chart.")
                 skipped_files += 1 # Count as skipped if no dataset generated
            # Explicitly remove large dataframes from memory
            del df, fund_df, sp_df, sp_fund_df, load_result
            if 'sp_load_result' in locals(): del sp_load_result
            import gc
            gc.collect()
        # --- After processing all files ---
        current_app.logger.info(f"Finished processing files for fund {fund_code}. Generated charts for: {available_metrics}. Total Processed: {processed_files}, Skipped/No Data/Errors: {skipped_files}")
        if not all_chart_data:
             # Combine specific errors with the generic message if available
             final_message = f"No metrics found with data for fund '{fund_code}'."
             if error_messages:
                 final_message += " Errors encountered: " + "; ".join(error_messages)
             elif skipped_files > 0:
                 final_message += f" ({skipped_files} files skipped or had no data for this fund). Check logs for details."
             current_app.logger.warning(final_message) # Log the final message
             return render_template('fund_detail_page.html',
                                   fund_code=fund_code,
                                   chart_data_json='[]',
                                   available_metrics=[],
                                   message=final_message)
        # Convert chart data to JSON for the template
        chart_data_json = jsonify(all_chart_data).get_data(as_text=True)
        return render_template('fund_detail_page.html',
                               fund_code=fund_code,
                               chart_data_json=chart_data_json,
                               available_metrics=available_metrics,
                               message=None) # No message if data was found
    except Exception as e:
        current_app.logger.error(f"Unexpected error in fund_detail for {fund_code}: {e}", exc_info=True)
        traceback.print_exc()
        # Render the page with an error message
        return render_template('fund_detail_page.html',
                               fund_code=fund_code,
                               chart_data_json='[]',
                               available_metrics=[],
                               message=f"An unexpected error occurred: {e}")
</file>

<file path="views/main_views.py">
# This file defines the routes related to the main, top-level views of the application.
# It primarily handles the dashboard or index page.
"""
Blueprint for main application routes, like the index page.
"""
from flask import Blueprint, render_template, current_app
import os
import pandas as pd
import traceback
# Import necessary functions/constants from other modules
# Removed: from config import DATA_FOLDER
from data_loader import load_and_process_data
from metric_calculator import calculate_latest_metrics
# Define the blueprint for main routes
main_bp = Blueprint('main', __name__)
@main_bp.route('/')
def index():
    """Renders the main dashboard page (`index.html`).
    This view performs the following steps:
    1. Scans the configured data directory for time-series metric files (prefixed with `ts_`).
    2. For each `ts_` file found:
        a. Loads and processes the data using `data_loader.load_and_process_data`,
           providing the configured data directory path.
        b. Calculates metrics (including Z-scores) using `metric_calculator.calculate_latest_metrics`.
        c. Extracts the 'Change Z-Score' columns for both the benchmark and any specific fund columns.
    3. Aggregates all extracted 'Change Z-score' columns from all files into a single pandas DataFrame (`summary_df`).
    4. Creates unique column names for the summary table by combining the original column name and the metric file name
       (e.g., 'Benchmark - Yield', 'FUND_A - Duration').
    5. Passes the list of available metric display names (filenames without `ts_`) and the aggregated Z-score
       DataFrame (`summary_df`) along with its corresponding column headers (`summary_metrics`) to the `index.html` template.
    This allows the dashboard to display a consolidated view of the most recent significant changes across all metrics.
    """
    # Retrieve the absolute data folder path from the app context
    data_folder = current_app.config['DATA_FOLDER']
    if not data_folder:
        current_app.logger.error("DATA_FOLDER is not configured in the application.")
        return "Internal Server Error: Data folder not configured", 500
    current_app.logger.info(f"Scanning data folder for dashboard: {data_folder}")
    # Find only files starting with ts_ and ending with .csv in the configured data folder
    try:
        files = [f for f in os.listdir(data_folder) if f.startswith('ts_') and f.endswith('.csv')]
    except FileNotFoundError:
        current_app.logger.error(f"Configured DATA_FOLDER does not exist: {data_folder}")
        files = []
    except Exception as e:
        current_app.logger.error(f"Error listing files in data folder {data_folder}: {e}")
        files = []
    # Create two lists: one for filenames (with ts_), one for display (without ts_)
    metric_filenames = sorted([os.path.splitext(f)[0] for f in files])
    metric_display_names = sorted([name[3:] for name in metric_filenames]) # Remove 'ts_' prefix
    all_z_scores_list = []
    # Store the unique combined column names for the summary table header
    processed_summary_columns = []
    print("Starting Change Z-score aggregation for dashboard (ts_ files only)...")
    # Iterate using the filenames with prefix
    for metric_filename in metric_filenames:
        filename = f"{metric_filename}.csv"
        # Get the corresponding display name for this file
        display_name = metric_filename[3:]
        try:
            print(f"Processing {filename}...")
            # Unpack all 6 values, but only use the primary ones for the dashboard summary
            # Pass the absolute data folder path to the loader
            df, fund_cols, benchmark_col, _sec_df, _sec_fund_cols, _sec_bench_col = load_and_process_data(
                primary_filename=filename,
                data_folder_path=data_folder # Pass the absolute path
            )
            # Check if data loading failed (df will be None)
            if df is None:
                 print(f"Warning: Failed to load data for {filename}. Skipping.")
                 continue # Skip this file if loading failed
            # Skip if no benchmark AND no fund columns identified
            if not benchmark_col and not fund_cols:
                 print(f"Warning: No benchmark or fund columns identified in {filename}. Skipping.")
                 continue
            # Calculate metrics using the current function
            latest_metrics = calculate_latest_metrics(df, fund_cols, benchmark_col)
            # --- Extract Change Z-score for ALL columns (benchmark + funds) --- 
            if not latest_metrics.empty:
                columns_to_check = []
                if benchmark_col:
                    columns_to_check.append(benchmark_col)
                if fund_cols:
                    columns_to_check.extend(fund_cols)
                if not columns_to_check:
                    print(f"Warning: No columns to check for Z-scores in {filename} despite loading data.")
                    continue
                print(f"Checking for Z-scores for columns: {columns_to_check} in metric {display_name}")
                found_z_for_metric = False
                for original_col_name in columns_to_check:
                    z_score_col_name = f'{original_col_name} Change Z-Score'
                    if z_score_col_name in latest_metrics.columns:
                        # Create a unique name for the summary table column
                        summary_col_name = f"{original_col_name} - {display_name}"
                        # Extract and rename
                        metric_z_scores = latest_metrics[[z_score_col_name]].rename(columns={z_score_col_name: summary_col_name})
                        all_z_scores_list.append(metric_z_scores)
                        # Add the unique column name to our list if not already present (preserves order of discovery)
                        if summary_col_name not in processed_summary_columns:
                             processed_summary_columns.append(summary_col_name)
                        found_z_for_metric = True
                        print(f"  -> Extracted: {summary_col_name}")
                    else:
                        print(f"  -> Z-score column '{z_score_col_name}' not found.")
                if not found_z_for_metric:
                    print(f"Warning: No Z-score columns found for any checked column in metric {display_name} (from {filename}).")
            else:
                 print(f"Warning: Could not calculate latest_metrics for {filename}. Skipping Z-score extraction.")
        except FileNotFoundError:
            print(f"Error: Data file '{filename}' not found.")
        except ValueError as ve:
            print(f"Value Error processing {metric_filename}: {ve}") # Log with filename
        except Exception as e:
            print(f"Error processing {metric_filename} during dashboard aggregation: {e}") # Log with filename
            traceback.print_exc()
    # Combine all Z-score Series/DataFrames into one
    summary_df = pd.DataFrame()
    if all_z_scores_list:
        summary_df = pd.concat(all_z_scores_list, axis=1)
        # Ensure the columns are in the order they were discovered
        if processed_summary_columns:
             # Handle potential missing columns if a file failed processing midway
             cols_available_in_summary = [col for col in processed_summary_columns if col in summary_df.columns]
             summary_df = summary_df[cols_available_in_summary]
             # Update the list of columns to only those actually present
             processed_summary_columns = cols_available_in_summary
        print("Successfully combined Change Z-scores.")
        print(f"Summary DF columns: {summary_df.columns.tolist()}")
    else:
        print("No Change Z-scores could be extracted for the summary.")
    return render_template('index.html',
                           metrics=metric_display_names, # Still used for top-level metric links
                           summary_data=summary_df,
                           summary_metrics=processed_summary_columns) # Pass the NEW list of combined column names
</file>

<file path="views/metric_views.py">
# This file defines the routes for displaying detailed views of specific time-series metrics.
# It handles requests where the user wants to see the data and charts for a single metric
# (like 'Yield' or 'Spread Duration') across all applicable funds.
# Updated to optionally load and display a secondary data source (prefixed with 'sp_').
"""
Blueprint for metric-specific routes (e.g., displaying individual metric charts).
"""
from flask import Blueprint, render_template, jsonify, current_app
import os
import pandas as pd
import numpy as np
import traceback
# Import necessary functions/constants from other modules
from config import COLOR_PALETTE
from data_loader import load_and_process_data, LoadResult # Import LoadResult type
from metric_calculator import calculate_latest_metrics
# Define the blueprint for metric routes, using '/metric' as the URL prefix
metric_bp = Blueprint('metric', __name__, url_prefix='/metric')
@metric_bp.route('/<string:metric_name>')
def metric_page(metric_name):
    """Renders the detailed page (`metric_page_js.html`) for a specific metric.
    Loads primary data (e.g., 'ts_Yield.csv') and optionally secondary data ('sp_ts_Yield.csv').
    Calculates metrics for both, prepares data for Chart.js, and passes it to the template.
    Includes a flag to indicate if secondary data is available.
    """
    primary_filename = f"ts_{metric_name}.csv"
    secondary_filename = f"sp_{primary_filename}"
    fund_code = 'N/A' # Default for logging fallback in case of early error
    latest_date_overall = pd.Timestamp.min # Initialize
    try:
        print(f"--- Processing metric: {metric_name} ---")
        print(f"Primary file: {primary_filename}, Secondary file: {secondary_filename}")
        # Load Data (Primary and Secondary)
        load_result: LoadResult = load_and_process_data(primary_filename, secondary_filename)
        primary_df, pri_fund_cols, pri_bench_col, secondary_df, sec_fund_cols, sec_bench_col = load_result
        # --- Validate Primary Data --- 
        if primary_df is None or primary_df.empty or pri_fund_cols is None:
            # Retrieve the configured absolute data folder path for error reporting
            data_folder_for_error = current_app.config['DATA_FOLDER']
            # Construct the full path using the absolute data_folder path
            primary_filepath = os.path.join(data_folder_for_error, primary_filename)
            if not os.path.exists(primary_filepath):
                 current_app.logger.error(f"Error: Primary data file not found: {primary_filepath}")
                 return f"Error: Data file for metric '{metric_name}' (expected: '{primary_filename}') not found.", 404
            else:
                 print(f"Error: Failed to process primary data file: {primary_filename}")
                 return f"Error: Could not process required data for metric '{metric_name}' (file: {primary_filename}). Check file format or logs.", 500
        # --- Determine Combined Metadata --- 
        all_dfs = [df for df in [primary_df, secondary_df] if df is not None and not df.empty]
        if not all_dfs:
             # Should be caught by primary check, but safeguard
            print(f"Error: No valid data loaded for {metric_name}")
            return f"Error: No data found for metric '{metric_name}'.", 404
        try:
            combined_index = pd.concat(all_dfs).index
            latest_date_overall = combined_index.get_level_values(0).max()
            latest_date_str = latest_date_overall.strftime('%Y-%m-%d')
        except Exception as idx_err:
            print(f"Error combining indices or getting latest date for {metric_name}: {idx_err}")
            # Fallback or re-raise? Let's try to proceed if possible, using primary date
            latest_date_overall = primary_df.index.get_level_values(0).max()
            latest_date_str = latest_date_overall.strftime('%Y-%m-%d')
            print(f"Warning: Using latest date from primary data only: {latest_date_str}")
        secondary_data_available = secondary_df is not None and not secondary_df.empty and sec_fund_cols is not None
        print(f"Secondary data available for {metric_name}: {secondary_data_available}")
        # --- Calculate Metrics --- 
        print(f"Calculating metrics for {metric_name}...")
        latest_metrics = calculate_latest_metrics(
            primary_df=primary_df,
            primary_fund_cols=pri_fund_cols,
            primary_benchmark_col=pri_bench_col,
            secondary_df=secondary_df if secondary_data_available else None,
            secondary_fund_cols=sec_fund_cols if secondary_data_available else None,
            secondary_benchmark_col=sec_bench_col if secondary_data_available else None,
            secondary_prefix="S&P "
        )
        # --- Handle Empty Metrics Result --- 
        if latest_metrics.empty:
            print(f"Warning: Metric calculation returned empty DataFrame for {metric_name}. Rendering page with no fund data.")
            missing_latest = pd.DataFrame()
            json_payload = {
                "metadata": {
                    "metric_name": metric_name,
                    "latest_date": latest_date_str,
                    "fund_col_names": pri_fund_cols or [],
                    "benchmark_col_name": pri_bench_col,
                    "secondary_fund_col_names": sec_fund_cols if secondary_data_available else [],
                    "secondary_benchmark_col_name": sec_bench_col if secondary_data_available else None,
                    "secondary_data_available": secondary_data_available
                },
                "funds": {}
            }
            return render_template('metric_page_js.html',
                           metric_name=metric_name,
                           charts_data_json=jsonify(json_payload).get_data(as_text=True),
                           latest_date=latest_date_overall.strftime('%d/%m/%Y'), 
                           missing_funds=missing_latest)
        # --- Identify Missing Funds (based on primary data) --- 
        print(f"Identifying potentially missing latest data for {metric_name}...")
        primary_cols_for_check = []
        if pri_bench_col:
            primary_cols_for_check.append(pri_bench_col)
        if pri_fund_cols:
            primary_cols_for_check.extend(pri_fund_cols)
        # Prefer checking Z-Score, fallback to Latest Value
        primary_z_score_cols = [f'{col} Change Z-Score' for col in primary_cols_for_check 
                                if f'{col} Change Z-Score' in latest_metrics.columns]
        primary_latest_val_cols = [f'{col} Latest Value' for col in primary_cols_for_check
                                   if f'{col} Latest Value' in latest_metrics.columns]
        check_cols_for_missing = primary_z_score_cols if primary_z_score_cols else primary_latest_val_cols
        if check_cols_for_missing:
            missing_latest = latest_metrics[latest_metrics[check_cols_for_missing].isna().any(axis=1)]
        else:
            print(f"Warning: No primary Z-Score or Latest Value columns found for {metric_name} to check for missing data.")
            missing_latest = pd.DataFrame(index=latest_metrics.index) # Assume none are missing if no check cols
        # --- Prepare Data Structure for JavaScript --- 
        print(f"Preparing chart and metric data for JavaScript for {metric_name}...")
        funds_data_for_js = {}
        fund_codes_in_metrics = latest_metrics.index
        primary_df_index = primary_df.index
        secondary_df_index = secondary_df.index if secondary_data_available and secondary_df is not None else None
        # Loop through funds present in the calculated metrics
        for fund_code in fund_codes_in_metrics:
            fund_latest_metrics_row = latest_metrics.loc[fund_code]
            is_missing_latest = fund_code in missing_latest.index
            fund_charts = [] # Initialize list to hold chart configs for this fund
            primary_labels = []
            primary_dt_index = None
            fund_hist_primary = None
            relative_primary_hist = None
            relative_secondary_hist = None # Initialize
            # --- Get Primary Historical Data ---
            if fund_code in primary_df_index.get_level_values(1):
                fund_hist_primary = primary_df.xs(fund_code, level=1).sort_index()
                if isinstance(fund_hist_primary.index, pd.DatetimeIndex):
                    primary_dt_index = fund_hist_primary.index # Store before filtering
                    # Filter out weekends (assuming data is daily/business daily)
                    fund_hist_primary = fund_hist_primary[primary_dt_index.dayofweek < 5]
                    primary_dt_index = fund_hist_primary.index # Update after filtering
                    primary_labels = primary_dt_index.strftime('%Y-%m-%d').tolist()
                else:
                    primary_labels = fund_hist_primary.index.astype(str).tolist()
                    print(f"Warning: Primary index for {fund_code} is not DatetimeIndex.")
            # --- Get Secondary Historical Data ---
            fund_hist_secondary = None
            if secondary_data_available and secondary_df_index is not None and fund_code in secondary_df_index.get_level_values(1):
                fund_hist_secondary_raw = secondary_df.xs(fund_code, level=1).sort_index()
                if isinstance(fund_hist_secondary_raw.index, pd.DatetimeIndex):
                    fund_hist_secondary_raw = fund_hist_secondary_raw[fund_hist_secondary_raw.index.dayofweek < 5]
                    # Reindex to primary date index if possible
                    if primary_dt_index is not None and not primary_dt_index.empty:
                         try:
                             if isinstance(fund_hist_secondary_raw.index, pd.DatetimeIndex):
                                 fund_hist_secondary = fund_hist_secondary_raw.reindex(primary_dt_index)
                                 print(f"Successfully reindexed secondary data for {fund_code}.")
                             else:
                                 print(f"Warning: Cannot reindex - Secondary index for {fund_code} is not DatetimeIndex after filtering.")
                         except Exception as reindex_err:
                             print(f"Warning: Reindexing secondary data for {fund_code} failed: {reindex_err}. Chart may be misaligned.")
                             fund_hist_secondary = fund_hist_secondary_raw # Use unaligned as fallback
                    else:
                         print(f"Warning: Cannot reindex secondary for {fund_code} - Primary DatetimeIndex unavailable.")
                         fund_hist_secondary = fund_hist_secondary_raw # Use unaligned as fallback
                else:
                    print(f"Warning: Secondary index for {fund_code} is not DatetimeIndex.")
                    fund_hist_secondary = fund_hist_secondary_raw # Use raw if not datetime
            # --- Prepare Relative Chart Data (if possible) ---
            relative_datasets = []
            relative_chart_config = None
            relative_metrics_for_js = {}
            # 1. Calculate Primary Relative Series
            pri_fund_col_used = None
            if fund_hist_primary is not None and pri_fund_cols:
                for f_col in pri_fund_cols:
                    if f_col in fund_hist_primary.columns:
                        pri_fund_col_used = f_col
                        break
            if pri_fund_col_used and pri_bench_col and pri_bench_col in fund_hist_primary.columns:
                port_col_hist = fund_hist_primary[pri_fund_col_used]
                bench_col_hist = fund_hist_primary[pri_bench_col]
                if not port_col_hist.dropna().empty and not bench_col_hist.dropna().empty:
                    relative_primary_hist = (port_col_hist - bench_col_hist).round(3).replace([np.inf, -np.inf], np.nan).where(pd.notnull, None)
                    relative_datasets.append({
                        'label': 'Relative (Port - Bench)',
                        'data': relative_primary_hist.tolist(),
                        'borderColor': '#1f77b4', # Specific color for primary relative
                        'backgroundColor': '#aec7e8',
                        'tension': 0.1,
                        'source': 'primary_relative',
                        'isSpData': False
                    })
                    # Extract primary relative metrics
                    for col in fund_latest_metrics_row.index:
                        if col.startswith('Relative '):
                             relative_metrics_for_js[col] = fund_latest_metrics_row[col] if pd.notna(fund_latest_metrics_row[col]) else None
            # 2. Calculate Secondary Relative Series (if applicable)
            sec_fund_col_used = None
            if fund_hist_secondary is not None and sec_fund_cols:
                 for f_col in sec_fund_cols:
                    if f_col in fund_hist_secondary.columns:
                        sec_fund_col_used = f_col
                        break
            if sec_fund_col_used and sec_bench_col and sec_bench_col in fund_hist_secondary.columns:
                port_col_hist_sec = fund_hist_secondary[sec_fund_col_used]
                bench_col_hist_sec = fund_hist_secondary[sec_bench_col]
                # Check if S&P Relative metrics exist, indicating calculation happened
                if 'S&P Relative Change Z-Score' in fund_latest_metrics_row.index and pd.notna(fund_latest_metrics_row['S&P Relative Change Z-Score']):
                    if not port_col_hist_sec.dropna().empty and not bench_col_hist_sec.dropna().empty:
                        relative_secondary_hist = (port_col_hist_sec - bench_col_hist_sec).round(3).replace([np.inf, -np.inf], np.nan).where(pd.notnull, None)
                        relative_datasets.append({
                            'label': 'S&P Relative (Port - Bench)',
                            'data': relative_secondary_hist.tolist(),
                            'borderColor': '#ff7f0e', # Specific color for secondary relative
                            'backgroundColor': '#ffbb78',
                            'borderDash': [2, 2],
                            'tension': 0.1,
                            'source': 'secondary_relative',
                            'isSpData': True,
                            'hidden': True # Initially hidden
                        })
                         # Extract secondary relative metrics
                        for col in fund_latest_metrics_row.index:
                            if col.startswith('S&P Relative '):
                                relative_metrics_for_js[col] = fund_latest_metrics_row[col] if pd.notna(fund_latest_metrics_row[col]) else None
            # 3. Create Relative Chart Config if primary relative data exists
            if relative_primary_hist is not None:
                relative_chart_config = {
                    'chart_type': 'relative',
                    'title': f'{fund_code} - Relative ({metric_name})',
                    'labels': primary_labels,
                    'datasets': relative_datasets,
                    'latest_metrics': relative_metrics_for_js
                }
                # We will add this later, after the main chart
                # fund_charts.append(relative_chart_config) # Add relative chart first
            # --- Prepare Main Chart Data ---
            main_datasets = []
            main_metrics_for_js = {}
            # 1. Primary Datasets (Portfolio/Benchmark)
            if fund_hist_primary is not None:
                if pri_bench_col and pri_bench_col in fund_hist_primary.columns:
                    bench_values = fund_hist_primary[pri_bench_col].round(3).replace([np.inf, -np.inf], np.nan).where(pd.notnull, None).tolist()
                    main_datasets.append({
                        'label': pri_bench_col,
                        'data': bench_values,
                        'borderColor': 'black', 'backgroundColor': 'grey',
                        'borderDash': [5, 5], 'tension': 0.1,
                        'source': 'primary', 'isSpData': False
                    })
                if pri_fund_cols:
                    for i, fund_col in enumerate(pri_fund_cols):
                        if fund_col in fund_hist_primary.columns:
                            fund_values = fund_hist_primary[fund_col].round(3).replace([np.inf, -np.inf], np.nan).where(pd.notnull, None).tolist()
                            color = COLOR_PALETTE[i % len(COLOR_PALETTE)]
                            main_datasets.append({
                                'label': fund_col,
                                'data': fund_values,
                                'borderColor': color, 'backgroundColor': color + '40',
                                'tension': 0.1,
                                'source': 'primary', 'isSpData': False
                            })
                # Extract primary non-relative metrics
                for col in fund_latest_metrics_row.index:
                    if not col.startswith('Relative ') and not col.startswith('S&P Relative '):
                        main_metrics_for_js[col] = fund_latest_metrics_row[col] if pd.notna(fund_latest_metrics_row[col]) else None
            # 2. Secondary Datasets (Portfolio/Benchmark)
            if fund_hist_secondary is not None:
                if sec_bench_col and sec_bench_col in fund_hist_secondary.columns:
                    bench_values_sec = fund_hist_secondary[sec_bench_col].round(3).replace([np.inf, -np.inf], np.nan).where(pd.notnull, None).tolist()
                    # Check if S&P benchmark metrics exist
                    if f'S&P {sec_bench_col} Change Z-Score' in fund_latest_metrics_row.index and pd.notna(fund_latest_metrics_row[f'S&P {sec_bench_col} Change Z-Score']):
                         main_datasets.append({
                            'label': f"S&P {sec_bench_col}",
                            'data': bench_values_sec,
                            'borderColor': '#FFA500', 'backgroundColor': '#FFDAB9',
                            'borderDash': [2, 2], 'tension': 0.1,
                            'source': 'secondary', 'isSpData': True, 'hidden': True
                        })
                if sec_fund_cols:
                    for i, fund_col in enumerate(sec_fund_cols):
                        if fund_col in fund_hist_secondary.columns:
                             # Check if S&P fund metrics exist
                             if f'S&P {fund_col} Change Z-Score' in fund_latest_metrics_row.index and pd.notna(fund_latest_metrics_row[f'S&P {fund_col} Change Z-Score']):
                                fund_values_sec = fund_hist_secondary[fund_col].round(3).replace([np.inf, -np.inf], np.nan).where(pd.notnull, None).tolist()
                                color_index = i
                                if pri_fund_cols:
                                    try: color_index = pri_fund_cols.index(fund_col)
                                    except ValueError: pass
                                base_color = COLOR_PALETTE[color_index % len(COLOR_PALETTE)]
                                main_datasets.append({
                                    'label': f"S&P {fund_col}",
                                    'data': fund_values_sec,
                                    'borderColor': base_color, 'backgroundColor': base_color + '20',
                                    'borderDash': [2, 2], 'tension': 0.1,
                                    'source': 'secondary', 'isSpData': True, 'hidden': True
                                })
                 # Extract secondary non-relative metrics
                for col in fund_latest_metrics_row.index:
                    if col.startswith('S&P ') and not col.startswith('S&P Relative '):
                         main_metrics_for_js[col] = fund_latest_metrics_row[col] if pd.notna(fund_latest_metrics_row[col]) else None
            # 3. Create Main Chart Config
            main_chart_config = None # Initialize
            if main_datasets: # Only create if there's actual data
                main_chart_config = {
                    'chart_type': 'main',
                    'title': f'{fund_code} - {metric_name}',
                    'labels': primary_labels,
                    'datasets': main_datasets,
                    'latest_metrics': main_metrics_for_js
                }
                # Add main chart FIRST
                fund_charts.append(main_chart_config)
            # Now add the relative chart config if it exists
            if relative_chart_config:
                fund_charts.append(relative_chart_config)
            # --- Store Fund Data ---
            funds_data_for_js[fund_code] = {
                'charts': fund_charts,
                'is_missing_latest': is_missing_latest
            }
        # --- Final JSON Payload ---
        json_payload = {
            "metadata": {
                "metric_name": metric_name,
                "latest_date": latest_date_str,
                 # Keep original column names for potential reference, though chart uses specific labels now
                "fund_col_names": pri_fund_cols or [],
                "benchmark_col_name": pri_bench_col,
                "secondary_fund_col_names": sec_fund_cols if secondary_data_available else [],
                "secondary_benchmark_col_name": sec_bench_col if secondary_data_available else None,
                "secondary_data_available": secondary_data_available
            },
            "funds": funds_data_for_js # Use the new structure
        }
        print(f"--- Completed processing metric: {metric_name} ---")
        return render_template('metric_page_js.html',
                               metric_name=metric_name,
                               charts_data_json=jsonify(json_payload).get_data(as_text=True),
                               latest_date=latest_date_overall.strftime('%d/%m/%Y'),
                               missing_funds=missing_latest,
                               error_message=None) # Explicitly set error to None on success
    except FileNotFoundError as e:
        print(f"Error: File not found during processing for {metric_name}. Details: {e}")
        traceback.print_exc()
        error_msg = f"Error: Required data file not found for metric '{metric_name}'. {e}"
        return render_template('metric_page_js.html', metric_name=metric_name, charts_data_json='{}', latest_date='N/A', missing_funds=pd.DataFrame(), error_message=error_msg), 404
    except Exception as e:
        print(f"Error processing metric page for {metric_name} (Fund: {fund_code}): {e}")
        traceback.print_exc() # Log the full traceback to console/log file
        error_msg = f"An error occurred while processing metric '{metric_name}'. Please check the server logs for details. Error: {e}"
        # Attempt to render template with error message
        return render_template('metric_page_js.html', metric_name=metric_name, charts_data_json='{}', latest_date='N/A', missing_funds=pd.DataFrame(), error_message=error_msg), 500
</file>

<file path="views/security_views.py">
"""
Blueprint for security-related routes (e.g., summary page and individual details).
"""
from flask import Blueprint, render_template, jsonify, send_from_directory, url_for, current_app
import os
import pandas as pd
import numpy as np
import traceback
from urllib.parse import unquote
from datetime import datetime
from flask import request # Import request
import math
# Import necessary functions/constants from other modules
from config import COLOR_PALETTE # Keep palette
from security_processing import load_and_process_security_data, calculate_security_latest_metrics
# Import the exclusion loading function
from views.exclusion_views import load_exclusions # Only import load_exclusions
# Define the blueprint
security_bp = Blueprint('security', __name__, url_prefix='/security')
PER_PAGE = 50 # Define how many items per page
def get_active_exclusions(data_folder_path: str):
    """Loads exclusions and returns a set of SecurityIDs that are currently active."""
    # Pass the data folder path to the load_exclusions function
    exclusions = load_exclusions(data_folder_path)
    active_exclusions = set()
    today = datetime.now().date()
    for ex in exclusions:
        try:
            add_date = ex['AddDate'].date() if pd.notna(ex['AddDate']) else None
            end_date = ex['EndDate'].date() if pd.notna(ex['EndDate']) else None
            security_id = str(ex['SecurityID']) # Ensure it's string for comparison
            if add_date and add_date <= today:
                if end_date is None or end_date >= today:
                    active_exclusions.add(security_id)
        except Exception as e:
            print(f"Error processing exclusion record {ex}: {e}") # Use logging in production
    print(f"Found {len(active_exclusions)} active exclusions: {active_exclusions}")
    return active_exclusions
@security_bp.route('/summary')
def securities_page():
    """Renders a page summarizing potential issues in security-level data, with server-side pagination, filtering, and sorting."""
    print("\n--- Starting Security Data Processing (Paginated) ---")
    # Retrieve the configured absolute data folder path
    data_folder = current_app.config['DATA_FOLDER']
    if not data_folder:
        current_app.logger.error("DATA_FOLDER is not configured in the application.")
        return "Internal Server Error: Data folder not configured", 500
    # --- Get Request Parameters ---
    page = request.args.get('page', 1, type=int)
    search_term = request.args.get('search_term', '', type=str).strip()
    sort_by = request.args.get('sort_by', None, type=str)
    # Default sort: Abs Change Z-Score Descending
    sort_order = request.args.get('sort_order', 'desc', type=str).lower() 
    # Ensure sort_order is either 'asc' or 'desc'
    if sort_order not in ['asc', 'desc']:
        sort_order = 'desc'
    # Collect active filters from request args (e.g., ?filter_Country=USA&filter_Sector=Tech)
    active_filters = {
        key.replace('filter_', ''): value 
        for key, value in request.args.items() 
        if key.startswith('filter_') and value # Ensure value is not empty
    }
    print(f"Request Params: Page={page}, Search='{search_term}', SortBy='{sort_by}', SortOrder='{sort_order}', Filters={active_filters}")
    # --- Load Base Data ---
    spread_filename = "sec_Spread.csv"
    # Construct absolute path
    data_filepath = os.path.join(data_folder, spread_filename)
    filter_options = {} # To store all possible options for filter dropdowns
    if not os.path.exists(data_filepath):
        print(f"Error: The required file '{spread_filename}' not found.")
        return render_template('securities_page.html', message=f"Error: Required data file '{spread_filename}' not found.", securities_data=[], pagination=None)
    try:
        print(f"Loading and processing file: {spread_filename}")
        # Pass the absolute data folder path
        df_long, static_cols = load_and_process_security_data(spread_filename, data_folder)
        if df_long is None or df_long.empty:
            print(f"Skipping {spread_filename} due to load/process errors or empty data.")
            return render_template('securities_page.html', message=f"Error loading or processing '{spread_filename}'.", securities_data=[], pagination=None)
        print("Calculating latest metrics...")
        combined_metrics_df = calculate_security_latest_metrics(df_long, static_cols)
        if combined_metrics_df.empty:
            print(f"No metrics calculated for {spread_filename}.")
            return render_template('securities_page.html', message=f"Could not calculate metrics from '{spread_filename}'.", securities_data=[], pagination=None)
        # Define ID column name
        id_col_name = 'ISIN' # <<< Use ISIN as the identifier
        # Check if the chosen ID column exists in the index or columns
        if id_col_name in combined_metrics_df.index.names:
            combined_metrics_df.index.name = id_col_name # Ensure index name is set if using index
            combined_metrics_df.reset_index(inplace=True)
        elif id_col_name in combined_metrics_df.columns:
            pass # ID is already a column
        else:
            # Fallback or error if ISIN isn't found
            old_id_col = combined_metrics_df.index.name or 'Security ID'
            print(f"Warning: ID column '{id_col_name}' not found. Falling back to '{old_id_col}'.")
            if old_id_col in combined_metrics_df.index.names:
                 combined_metrics_df.index.name = old_id_col
                 combined_metrics_df.reset_index(inplace=True)
                 id_col_name = old_id_col # Use the fallback name
            elif old_id_col in combined_metrics_df.columns:
                 id_col_name = old_id_col
            else:
                 print(f"Error: Cannot find a usable ID column ('{id_col_name}' or fallback '{old_id_col}') in {spread_filename}.")
                 return render_template('securities_page.html', message=f"Error: Cannot identify securities in {spread_filename}.", securities_data=[], pagination=None)
        # Store the original unfiltered dataframe's columns 
        original_columns = combined_metrics_df.columns.tolist()
        # combined_metrics_df.reset_index(inplace=True) # Reset index to make ID a regular column - ALREADY DONE OR ID IS A COLUMN
        # --- Collect Filter Options (from the full dataset BEFORE filtering) ---
        print("Collecting filter options...")
        # Ensure ID column is not treated as a filterable static column
        current_static_in_df = [col for col in static_cols if col in combined_metrics_df.columns and col != id_col_name]
        for col in current_static_in_df:
            unique_vals = combined_metrics_df[col].unique().tolist()
            unique_vals = [item.item() if isinstance(item, np.generic) else item for item in unique_vals]
            unique_vals = sorted([val for val in unique_vals if pd.notna(val) and val != '']) # Remove NaN/empty and sort
            if unique_vals: # Only add if there are valid options
                 filter_options[col] = unique_vals
        # Sort filter options dictionary by key for consistent display order
        final_filter_options = dict(sorted(filter_options.items()))
        # --- Apply Filtering Steps Sequentially ---
        print("Applying filters...")
        # 1. Search Term Filter (on ID column - now ISIN)
        if search_term:
            combined_metrics_df = combined_metrics_df[combined_metrics_df[id_col_name].astype(str).str.contains(search_term, case=False, na=False)]
            print(f"Applied search term '{search_term}'. Rows remaining: {len(combined_metrics_df)}")
        # 2. Active Exclusions Filter (should still work if exclusions use SecurityID/Name, adapt if needed)
        try:
            # Pass the absolute data folder path to get active exclusions
            active_exclusion_ids = get_active_exclusions(data_folder)
            # Assuming exclusions use Security Name/ID for now. If they use ISIN, this is correct.
            # If they use Security Name, we need to filter on that column instead.
            exclusion_col_to_check = id_col_name # Assumes exclusions use ISIN
            # If exclusions.csv uses Security Name, use this instead:
            # exclusion_col_to_check = 'Security Name' if 'Security Name' in combined_metrics_df.columns else id_col_name 
            if active_exclusion_ids:
                 combined_metrics_df = combined_metrics_df[~combined_metrics_df[exclusion_col_to_check].astype(str).isin(active_exclusion_ids)]
                 print(f"Applied {len(active_exclusion_ids)} exclusions based on '{exclusion_col_to_check}'. Rows remaining: {len(combined_metrics_df)}")
        except Exception as e:
            print(f"Warning: Error loading or applying exclusions: {e}")
        # 3. Dynamic Filters (from request args)
        if active_filters:
            for col, value in active_filters.items():
                if col in combined_metrics_df.columns:
                    # Ensure consistent type for comparison, handle NaNs
                    combined_metrics_df = combined_metrics_df[combined_metrics_df[col].astype(str) == str(value)]
                    print(f"Applied filter '{col}={value}'. Rows remaining: {len(combined_metrics_df)}")
                else:
                     print(f"Warning: Filter column '{col}' not found in DataFrame.")
        # --- Handle Empty DataFrame After Filtering ---
        if combined_metrics_df.empty:
            print("No data matches the specified filters.")
            message = "No securities found matching the current criteria."
            if search_term:
                message += f" Search term: '{search_term}'."
            if active_filters:
                 message += f" Active filters: {active_filters}."
            return render_template('securities_page.html',
                                   message=message,
                                   securities_data=[],
                                   filter_options=final_filter_options,
                                   column_order=[],
                                   id_col_name=id_col_name,
                                   search_term=search_term,
                                   active_filters=active_filters,
                                   pagination=None,
                                   current_sort_by=sort_by,
                                   current_sort_order=sort_order)
        # --- Apply Sorting ---
        print(f"Applying sort: By='{sort_by}', Order='{sort_order}'")
        # Default sort column if not provided or invalid
        effective_sort_by = sort_by
        is_default_sort = False
        if sort_by not in combined_metrics_df.columns:
             # Default to sorting by absolute Z-score if 'sort_by' is invalid or not provided
             if 'Change Z-Score' in combined_metrics_df.columns:
                 print(f"'{sort_by}' not valid or not provided. Defaulting sort to 'Abs Change Z-Score' {sort_order}")
                 # Calculate Abs Z-Score temporarily for sorting
                 combined_metrics_df['_abs_z_score_'] = combined_metrics_df['Change Z-Score'].fillna(0).abs()
                 effective_sort_by = '_abs_z_score_'
                 # Default Z-score sort is always descending unless explicitly requested otherwise for Z-score itself
                 if sort_by != 'Change Z-Score': 
                      sort_order = 'desc' 
                 is_default_sort = True
             else:
                  print("Warning: Cannot apply default sort, 'Change Z-Score' missing.")
                  effective_sort_by = id_col_name # Fallback sort
                  sort_order = 'asc'
        ascending_order = (sort_order == 'asc')
        try:
            # Use na_position='last' to handle NaNs consistently
            combined_metrics_df.sort_values(by=effective_sort_by, ascending=ascending_order, inplace=True, na_position='last', key=lambda col: col.astype(str).str.lower() if col.dtype == 'object' else col)
            print(f"Sorted by '{effective_sort_by}', {sort_order}.")
        except Exception as e:
             print(f"Error during sorting by {effective_sort_by}: {e}. Falling back to sorting by ID.")
             combined_metrics_df.sort_values(by=id_col_name, ascending=True, inplace=True, na_position='last')
             sort_by = id_col_name # Update sort_by to reflect fallback
             sort_order = 'asc'
        # Remove temporary sort column if added
        if is_default_sort and '_abs_z_score_' in combined_metrics_df.columns:
            combined_metrics_df.drop(columns=['_abs_z_score_'], inplace=True)
            # Set sort_by for template correctly if default was used
            sort_by = 'Change Z-Score' # Reflect the conceptual sort column
        # --- Pagination ---
        total_items = len(combined_metrics_df)
        # Ensure PER_PAGE is positive to avoid division by zero or negative pages
        safe_per_page = max(1, PER_PAGE)
        total_pages = math.ceil(total_items / safe_per_page)
        total_pages = max(1, total_pages) # Ensure at least 1 page, even if total_items is 0
        page = max(1, min(page, total_pages)) # Ensure page is within valid range [1, total_pages]
        start_index = (page - 1) * safe_per_page
        end_index = start_index + safe_per_page
        print(f"Pagination: Total items={total_items}, Total pages={total_pages}, Current page={page}, Per page={safe_per_page}")
        # Calculate page numbers to display in pagination controls (e.g., show 2 pages before and after current)
        page_window = 2 # Number of pages to show before/after current page
        start_page_display = max(1, page - page_window)
        end_page_display = min(total_pages, page + page_window)
        paginated_df = combined_metrics_df.iloc[start_index:end_index]
        # --- Prepare Data for Template ---
        securities_data_list = paginated_df.round(3).to_dict(orient='records')
        # Replace NaN with None for JSON compatibility / template rendering
        for row in securities_data_list:
            for key, value in row.items():
                if pd.isna(value):
                    row[key] = None
        # Define column order (ID first, then Static, then Metrics)
        # Ensure ISIN (id_col_name) is not in ordered_static_cols
        ordered_static_cols = sorted([col for col in static_cols if col in paginated_df.columns and col != id_col_name])
        metric_cols_ordered = ['Latest Value', 'Change', 'Change Z-Score', 'Mean', 'Max', 'Min']
        # Ensure only existing columns are included and ID col is first
        final_col_order = [id_col_name] + \
                          [col for col in ordered_static_cols if col in paginated_df.columns] + \
                          [col for col in metric_cols_ordered if col in paginated_df.columns]
        # Ensure all original columns are considered if they aren't static or metric
        # Make sure not to add id_col_name again
        other_cols = [col for col in paginated_df.columns if col not in final_col_order]
        final_col_order.extend(other_cols) # Add any remaining columns
        print(f"Final column order for display: {final_col_order}")
        # Create pagination context for the template
        pagination_context = {
            'page': page,
            'per_page': safe_per_page,
            'total_pages': total_pages,
            'total_items': total_items,
            'has_prev': page > 1,
            'has_next': page < total_pages,
            'prev_num': page - 1,
            'next_num': page + 1,
            'start_page_display': start_page_display, # Pass calculated start page
            'end_page_display': end_page_display,     # Pass calculated end page
            # Function to generate URLs for pagination links, preserving state
            'url_for_page': lambda p: url_for('security.securities_page', 
                                              page=p, 
                                              search_term=search_term, 
                                              sort_by=sort_by, 
                                              sort_order=sort_order, 
                                              **{f'filter_{k}': v for k, v in active_filters.items()}) 
        }
    except Exception as e:
        print(f"!!! Unexpected error during security page processing: {e}")
        traceback.print_exc()
        return render_template('securities_page.html', 
                               message=f"An unexpected error occurred: {e}", 
                               securities_data=[], 
                               pagination=None,
                               filter_options=final_filter_options if 'final_filter_options' in locals() else {},
                               active_filters=active_filters)
    # --- Render Template ---
    return render_template('securities_page.html',
                           securities_data=securities_data_list,
                           filter_options=final_filter_options,
                           column_order=final_col_order,
                           id_col_name=id_col_name,
                           search_term=search_term,
                           active_filters=active_filters, # Pass active filters for form state
                           pagination=pagination_context, # Pass pagination object
                           current_sort_by=sort_by,
                           current_sort_order=sort_order,
                           message=None) # Clear any previous error message if successful
# --- Helper Function to Replace NaN --- 
def replace_nan_with_none(obj):
    """Recursively replaces np.nan with None in a nested structure (dicts, lists)."""
    if isinstance(obj, dict):
        return {k: replace_nan_with_none(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [replace_nan_with_none(elem) for elem in obj]
    # Check specifically for pandas/numpy NaN values
    elif pd.isna(obj) and isinstance(obj, (float, np.floating)):
        return None
    else:
        return obj
@security_bp.route('/security/details/<metric_name>/<path:security_id>')
def security_details(metric_name, security_id):
    """Renders the details page for a specific security, showing historical charts."""
    # Decode the security_id from the URL path, which might contain encoded characters
    decoded_security_id = unquote(security_id)
    print(f"\n--- Requesting Security Details: Metric='{metric_name}', Decoded ID='{decoded_security_id}' ---")
    # Retrieve the configured absolute data folder path
    data_folder = current_app.config['DATA_FOLDER']
    if not data_folder:
        current_app.logger.error("DATA_FOLDER is not configured in the application.")
        return "Internal Server Error: Data folder not configured", 500
    # --- Define ID Column Name (consistent with summary page) --- 
    id_col = 'ISIN' 
    # --- Data Files to Load --- 
    # We need the base metric file, plus potentially Price and Duration
    base_metric_filename = f"sec_{metric_name}.csv"
    price_filename = "sec_Price.csv"
    duration_filename = "sec_Duration.csv"
    # Add new filenames
    spread_duration_filename = "sec_Spread duration.csv"
    spread_filename = "sec_Spread.csv"
    sp_spread_duration_filename = "sec_Spread durationSP.csv" 
    sp_duration_filename = "sec_DurationSP.csv" 
    sp_spread_filename = "sec_SpreadSP.csv"
    chart_data = {
        'labels': [],
        'primary_datasets': [], # For Base Metric + Price
        'duration_dataset': None,
        'sp_duration_dataset': None, # New
        'spread_duration_dataset': None, # New
        'sp_spread_duration_dataset': None, # New
        'spread_dataset': None, # New
        'sp_spread_dataset': None, # New
        'static_info': None,
        'latest_date': None,
        'metric_name': metric_name,
        'security_id': decoded_security_id
    }
    try:
        # --- Load Base Metric Data --- 
        print(f"Loading base metric file: {base_metric_filename}")
        # df_long now has columns: Date, ISIN, Security Name, other_static, Value
        df_long, static_cols = load_and_process_security_data(base_metric_filename, data_folder)
        if df_long is None or df_long.empty:
            print(f"Error: Failed to load or process base metric data '{base_metric_filename}'.")
            return render_template('security_details_page.html', 
                                   security_id=decoded_security_id, 
                                   metric_name=metric_name, 
                                   chart_data_json='{}', 
                                   static_info=None,
                                   latest_date='N/A',
                                   message=f"Error loading base data for {metric_name}.")
        # --- Filter by Security ID (ISIN) --- 
        print(f"Filtering data for ISIN='{decoded_security_id}'")
        filter_col = 'ISIN' 
        if filter_col not in df_long.columns:
            print(f"Error: Required filter column '{filter_col}' not found in the loaded data from {base_metric_filename}. Columns: {df_long.columns.tolist()}")
            return render_template('security_details_page.html', security_id=decoded_security_id, metric_name=metric_name, 
                                    chart_data_json='{}', static_info=None, latest_date='N/A',
                                    message=f"Error: Identifier column '{filter_col}' not found in data.")
        # Filter the base dataframe
        security_data_filtered = df_long[df_long[filter_col].astype(str) == decoded_security_id].copy()
        if security_data_filtered.empty:
            print(f"No data found for {filter_col}='{decoded_security_id}' in {base_metric_filename}.")
            return render_template('security_details_page.html', 
                                   security_id=decoded_security_id, 
                                   metric_name=metric_name, 
                                   chart_data_json='{}',
                                   static_info=None,
                                   latest_date='N/A',
                                   message=f"No data found for {filter_col}: {decoded_security_id}.")
        # --- Set Date Index for Plotting/Reindexing --- 
        print(f"Found {len(security_data_filtered)} data points for {decoded_security_id}. Setting Date index.")
        if 'Date' not in security_data_filtered.columns:
             print("Error: 'Date' column missing after filtering base data.")
             # Handle error...
        security_data = security_data_filtered.set_index('Date').sort_index()
        # --- Get Static Info (from filtered data before indexing) --- 
        static_info = {}
        first_row_series = security_data_filtered.iloc[0] 
        for col in static_cols:
            if col in security_data_filtered.columns: # Check against the columns
                static_info[col] = first_row_series[col]
        # Also add Security Name if it exists and isn't already in static_cols
        if 'Security Name' in security_data_filtered.columns and 'Security Name' not in static_cols:
             static_info['Security Name'] = first_row_series['Security Name']
        chart_data['static_info'] = static_info
        # --- Get Labels & Latest Date (from Date index) --- 
        chart_data['labels'] = security_data.index.strftime('%Y-%m-%d').tolist()
        chart_data['latest_date'] = chart_data['labels'][-1] if chart_data['labels'] else 'N/A'
        # --- Prepare Base Metric Dataset --- 
        if 'Value' in security_data.columns:
             metric_values = security_data['Value'].round(3).fillna(np.nan).tolist()
             chart_data['primary_datasets'].append({
                 'label': metric_name,
                 'data': metric_values,
                 'borderColor': COLOR_PALETTE[0 % len(COLOR_PALETTE)],
                 'backgroundColor': COLOR_PALETTE[0 % len(COLOR_PALETTE)] + '40',
                 'yAxisID': 'y', # Assign to the primary Y-axis
                 'tension': 0.1
             })
        else:
             print(f"Warning: 'Value' column not found in security_data for metric {metric_name}")
        # --- Load, Filter, Index, Reindex Price Data --- 
        try:
            print(f"Loading price file: {price_filename}")
            df_price_long, _ = load_and_process_security_data(price_filename, data_folder)
            if df_price_long is not None and not df_price_long.empty and filter_col in df_price_long.columns:
                print(f"Filtering price data for {filter_col}='{decoded_security_id}'")
                price_data_filtered = df_price_long[df_price_long[filter_col].astype(str) == decoded_security_id].copy()
                if not price_data_filtered.empty:
                    if 'Date' in price_data_filtered.columns:
                        # Set Date index for Price data
                        price_data = price_data_filtered.set_index('Date').sort_index()
                        # Reindex using the main security_data's Date index
                        price_values = price_data['Value'].reindex(security_data.index).round(3).fillna(np.nan).tolist()
                        chart_data['primary_datasets'].append({
                            'label': 'Price',
                            'data': price_values,
                            'borderColor': COLOR_PALETTE[1 % len(COLOR_PALETTE)],
                            'backgroundColor': COLOR_PALETTE[1 % len(COLOR_PALETTE)] + '40',
                            'yAxisID': 'y1', # Assign to the secondary Y-axis
                            'tension': 0.1,
                            'borderDash': [5, 5] # Optional: dashed line for price
                        })
                        print("Added Price data to primary chart.")
                    else:
                         print("Warning: Price data missing 'Date' column after filtering.")
                else:
                     print(f"No Price data found for {filter_col}='{decoded_security_id}'")
            else:
                 print(f"Failed to load price data or {filter_col} missing.")
        except FileNotFoundError:
             print(f"Price file {price_filename} not found.")
        except Exception as e:
            print(f"Error processing price data: {e}")
        # --- Load, Filter, Index, Reindex Duration Data --- 
        try:
            print(f"Loading duration file: {duration_filename}")
            df_duration_long, _ = load_and_process_security_data(duration_filename, data_folder)
            if df_duration_long is not None and not df_duration_long.empty and filter_col in df_duration_long.columns:
                print(f"Filtering duration data for {filter_col}='{decoded_security_id}'")
                duration_data_filtered = df_duration_long[df_duration_long[filter_col].astype(str) == decoded_security_id].copy()
                if not duration_data_filtered.empty:
                     if 'Date' in duration_data_filtered.columns:
                        # Set Date index for Duration data
                        duration_data = duration_data_filtered.set_index('Date').sort_index()
                        # Reindex using the main security_data's Date index
                        duration_values = duration_data['Value'].reindex(security_data.index).round(3).fillna(np.nan).tolist()
                        chart_data['duration_dataset'] = {
                           'label': 'Duration',
                           'data': duration_values,
                           'borderColor': COLOR_PALETTE[2 % len(COLOR_PALETTE)],
                           'backgroundColor': COLOR_PALETTE[2 % len(COLOR_PALETTE)] + '40',
                           'yAxisID': 'y', # Use standard Y-axis for this separate chart
                           'tension': 0.1
                        }
                        print("Prepared Duration data for separate chart.")
                     else:
                          print("Warning: Duration data missing 'Date' column after filtering.")
                else:
                    print(f"No Duration data found for {filter_col}='{decoded_security_id}'")
            else:
                 print(f"Failed to load duration data or {filter_col} missing.")
        except FileNotFoundError:
             print(f"Duration file {duration_filename} not found.")
        except Exception as e:
            print(f"Error processing duration data: {e}")
        # --- Load, Filter, Index, Reindex Spread Duration Data --- 
        try:
            print(f"Loading spread duration file: {spread_duration_filename}")
            df_sd_long, _ = load_and_process_security_data(spread_duration_filename, data_folder)
            if df_sd_long is not None and not df_sd_long.empty and filter_col in df_sd_long.columns:
                print(f"Filtering spread duration data for {filter_col}='{decoded_security_id}'")
                sd_data_filtered = df_sd_long[df_sd_long[filter_col].astype(str) == decoded_security_id].copy()
                if not sd_data_filtered.empty:
                     if 'Date' in sd_data_filtered.columns:
                        sd_data = sd_data_filtered.set_index('Date').sort_index()
                        # Reindex using the main security_data's Date index
                        sd_values = sd_data['Value'].reindex(security_data.index).round(3).fillna(np.nan).tolist()
                        chart_data['spread_duration_dataset'] = {
                           'label': 'Spread Duration',
                           'data': sd_values,
                           'borderColor': COLOR_PALETTE[3 % len(COLOR_PALETTE)],
                           'backgroundColor': COLOR_PALETTE[3 % len(COLOR_PALETTE)] + '40',
                           'yAxisID': 'y', 
                           'tension': 0.1
                        }
                        print("Prepared Spread Duration data.")
                     else:
                          print("Warning: Spread Duration data missing 'Date' column after filtering.")
                else:
                    print(f"No Spread Duration data found for {filter_col}='{decoded_security_id}'")
            else:
                 print(f"Failed to load spread duration data or {filter_col} missing.")
        except FileNotFoundError:
             print(f"Spread duration file {spread_duration_filename} not found.")
        except Exception as e:
            print(f"Error processing spread duration data: {e}")
        # --- Load, Filter, Index, Reindex Spread Data --- 
        try:
            print(f"Loading spread file: {spread_filename}")
            df_s_long, _ = load_and_process_security_data(spread_filename, data_folder)
            if df_s_long is not None and not df_s_long.empty and filter_col in df_s_long.columns:
                print(f"Filtering spread data for {filter_col}='{decoded_security_id}'")
                s_data_filtered = df_s_long[df_s_long[filter_col].astype(str) == decoded_security_id].copy()
                if not s_data_filtered.empty:
                     if 'Date' in s_data_filtered.columns:
                        s_data = s_data_filtered.set_index('Date').sort_index()
                        s_values = s_data['Value'].reindex(security_data.index).round(3).fillna(np.nan).tolist()
                        chart_data['spread_dataset'] = {
                           'label': 'Spread',
                           'data': s_values,
                           'borderColor': COLOR_PALETTE[4 % len(COLOR_PALETTE)],
                           'backgroundColor': COLOR_PALETTE[4 % len(COLOR_PALETTE)] + '40',
                           'yAxisID': 'y', 
                           'tension': 0.1
                        }
                        print("Prepared Spread data.")
                     else:
                          print("Warning: Spread data missing 'Date' column after filtering.")
                else:
                    print(f"No Spread data found for {filter_col}='{decoded_security_id}'")
            else:
                 print(f"Failed to load spread data or {filter_col} missing.")
        except FileNotFoundError:
             print(f"Spread file {spread_filename} not found.")
        except Exception as e:
            print(f"Error processing spread data: {e}")
        # --- Load, Filter, Index, Reindex SP Spread Duration Data --- 
        try:
            print(f"Loading SP spread duration file: {sp_spread_duration_filename}")
            df_spsd_long, _ = load_and_process_security_data(sp_spread_duration_filename, data_folder)
            if df_spsd_long is not None and not df_spsd_long.empty and filter_col in df_spsd_long.columns:
                print(f"Filtering SP spread duration data for {filter_col}='{decoded_security_id}'")
                spsd_data_filtered = df_spsd_long[df_spsd_long[filter_col].astype(str) == decoded_security_id].copy()
                if not spsd_data_filtered.empty:
                     if 'Date' in spsd_data_filtered.columns:
                        spsd_data = spsd_data_filtered.set_index('Date').sort_index()
                        spsd_values = spsd_data['Value'].reindex(security_data.index).round(3).fillna(np.nan).tolist()
                        chart_data['sp_spread_duration_dataset'] = {
                           'label': 'SP Spread Duration',
                           'data': spsd_values,
                           'borderColor': COLOR_PALETTE[5 % len(COLOR_PALETTE)],
                           'backgroundColor': COLOR_PALETTE[5 % len(COLOR_PALETTE)] + '40',
                           'yAxisID': 'y', 
                           'borderDash': [5, 5], # Dashed line for SP
                           'tension': 0.1
                        }
                        print("Prepared SP Spread Duration data.")
                     else:
                          print("Warning: SP Spread Duration data missing 'Date' column after filtering.")
                else:
                    print(f"No SP Spread Duration data found for {filter_col}='{decoded_security_id}'")
            else:
                 print(f"Failed to load SP spread duration data or {filter_col} missing.")
        except FileNotFoundError:
             print(f"SP Spread duration file {sp_spread_duration_filename} not found.")
        except Exception as e:
            print(f"Error processing SP spread duration data: {e}")
        # --- Load, Filter, Index, Reindex SP Duration Data --- 
        try:
            print(f"Loading SP duration file: {sp_duration_filename}")
            df_spd_long, _ = load_and_process_security_data(sp_duration_filename, data_folder)
            if df_spd_long is not None and not df_spd_long.empty and filter_col in df_spd_long.columns:
                print(f"Filtering SP duration data for {filter_col}='{decoded_security_id}'")
                spd_data_filtered = df_spd_long[df_spd_long[filter_col].astype(str) == decoded_security_id].copy()
                if not spd_data_filtered.empty:
                     if 'Date' in spd_data_filtered.columns:
                        spd_data = spd_data_filtered.set_index('Date').sort_index()
                        spd_values = spd_data['Value'].reindex(security_data.index).round(3).fillna(np.nan).tolist()
                        chart_data['sp_duration_dataset'] = {
                           'label': 'SP Duration',
                           'data': spd_values,
                           'borderColor': COLOR_PALETTE[2 % len(COLOR_PALETTE)], # Match base duration color index
                           'backgroundColor': COLOR_PALETTE[2 % len(COLOR_PALETTE)] + '20', # Lighter bg
                           'yAxisID': 'y', 
                           'borderDash': [5, 5], # Dashed line for SP
                           'tension': 0.1
                        }
                        print("Prepared SP Duration data.")
                     else:
                          print("Warning: SP Duration data missing 'Date' column after filtering.")
                else:
                    print(f"No SP Duration data found for {filter_col}='{decoded_security_id}'")
            else:
                 print(f"Failed to load SP duration data or {filter_col} missing.")
        except FileNotFoundError:
             print(f"SP Duration file {sp_duration_filename} not found.")
        except Exception as e:
            print(f"Error processing SP duration data: {e}")
        # --- Load, Filter, Index, Reindex SP Spread Data --- 
        try:
            print(f"Loading SP spread file: {sp_spread_filename}")
            df_sps_long, _ = load_and_process_security_data(sp_spread_filename, data_folder)
            if df_sps_long is not None and not df_sps_long.empty and filter_col in df_sps_long.columns:
                print(f"Filtering SP spread data for {filter_col}='{decoded_security_id}'")
                sps_data_filtered = df_sps_long[df_sps_long[filter_col].astype(str) == decoded_security_id].copy()
                if not sps_data_filtered.empty:
                     if 'Date' in sps_data_filtered.columns:
                        sps_data = sps_data_filtered.set_index('Date').sort_index()
                        sps_values = sps_data['Value'].reindex(security_data.index).round(3).fillna(np.nan).tolist()
                        chart_data['sp_spread_dataset'] = {
                           'label': 'SP Spread',
                           'data': sps_values,
                           'borderColor': COLOR_PALETTE[4 % len(COLOR_PALETTE)], # Match base spread color index
                           'backgroundColor': COLOR_PALETTE[4 % len(COLOR_PALETTE)] + '20', # Lighter bg
                           'yAxisID': 'y', 
                           'borderDash': [5, 5], # Dashed line for SP
                           'tension': 0.1
                        }
                        print("Prepared SP Spread data.")
                     else:
                          print("Warning: SP Spread data missing 'Date' column after filtering.")
                else:
                    print(f"No SP Spread data found for {filter_col}='{decoded_security_id}'")
            else:
                 print(f"Failed to load SP spread data or {filter_col} missing.")
        except FileNotFoundError:
             print(f"SP Spread file {sp_spread_filename} not found.")
        except Exception as e:
            print(f"Error processing SP spread data: {e}")
        # --- Render Template --- 
        print(f"Rendering security details page for {decoded_security_id}")
        # Replace NaN with None before JSON serialization
        chart_data_clean = replace_nan_with_none(chart_data)
        return render_template('security_details_page.html',
                               security_id=decoded_security_id,
                               metric_name=metric_name,
                               # Use the cleaned data for JSON
                               chart_data_json=jsonify(chart_data_clean).get_data(as_text=True),
                               static_info=chart_data['static_info'], # Static info likely doesn't have NaNs
                               latest_date=chart_data['latest_date'])
    except FileNotFoundError as e:
        print(f"Error: File not found during processing for {decoded_security_id}. Details: {e}")
        traceback.print_exc()
        return f"Error: Required data file not found for security details. {e}", 404
    except Exception as e:
        print(f"Error processing security details for {decoded_security_id}: {e}")
        traceback.print_exc()
        return f"An error occurred processing security details for {decoded_security_id}: {e}", 500
# Add a route for static asset discovery (like in metric_views)
@security_bp.route('/static/<path:filename>')
def static_files(filename):
    return send_from_directory(os.path.join(security_bp.root_path, '..', 'static'), filename)
</file>

<file path="views/spread_duration_comparison_views.py">
# views/spread_duration_comparison_views.py
# This module defines the Flask Blueprint for comparing two security spread duration datasets.
# It includes routes for a summary view listing securities with comparison metrics
# and a detail view showing overlayed time-series charts and statistics for a single security.
from flask import Blueprint, render_template, request, current_app, jsonify, url_for
import pandas as pd
import os
import logging
import math # Add math for pagination calculation
from urllib.parse import unquote # Import unquote
# Assuming security_processing and utils are in the parent directory or configured in PYTHONPATH
try:
    from security_processing import load_and_process_security_data # May need adjustments
    from utils import parse_fund_list # Example utility
    from config import COLOR_PALETTE # Still need colors
except ImportError:
    # Handle potential import errors if the structure is different
    logging.error("Could not import required modules from parent directory.")
    # Add fallback imports or path adjustments if necessary
    # Example: sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
    from ..security_processing import load_and_process_security_data
    from ..utils import parse_fund_list
    from ..config import COLOR_PALETTE
spread_duration_comparison_bp = Blueprint('spread_duration_comparison_bp', __name__,
                        template_folder='../templates',
                        static_folder='../static')
# Configure logging
log = logging.getLogger(__name__)
PER_PAGE_COMPARISON = 50 # Items per page for comparison summary
# --- Data Loading and Processing ---
def load_weights_and_held_status(data_folder_path: str, weights_filename='w_secs.csv'):
    """Loads weights data and determines the latest held status for each security ID.
    Args:
        data_folder_path (str): The absolute path to the data folder.
        weights_filename (str, optional): The name of the weights file. Defaults to 'w_secs.csv'.
    Returns:
        pd.Series: Series indexed by Security ID indicating held status (True/False).
                   Returns an empty Series on error.
    """
    if not data_folder_path:
        log.error("No data_folder_path provided to load_weights_and_held_status.")
        return pd.Series(dtype=bool)
    weights_filepath = os.path.join(data_folder_path, weights_filename)
    log.info(f"Loading weights data from: {weights_filepath}")
    # Pass the full path to the data loading function
    df_weights, _ = load_and_process_security_data(weights_filename, data_folder_path)
    if df_weights.empty:
        log.warning(f"Weights file '{weights_filepath}' is empty or failed to load.")
        return pd.Series(dtype=bool)
    # --- Check index and columns AFTER loading --- 
    if df_weights.index.nlevels != 2:
        log.error(f"Weights file '{weights_filepath}' did not have the expected 2 index levels (Date, ID) after processing.")
        return pd.Series(dtype=bool)
    # Get index names dynamically
    date_level_name, id_level_name = df_weights.index.names
    log.info(f"Weights file index levels identified: Date='{date_level_name}', ID='{id_level_name}'")
    # Reset index to access Date and ID as columns
    df_weights = df_weights.reset_index()
    # Check if required columns are present AFTER resetting index
    required_cols = [date_level_name, id_level_name, 'Value']
    missing_cols = [col for col in required_cols if col not in df_weights.columns]
    if missing_cols:
        log.error(f"Weights file '{weights_filepath}' is missing required columns after processing and index reset: {missing_cols}. Available columns: {df_weights.columns.tolist()}")
        return pd.Series(dtype=bool)
    # Find the latest date in the weights data using the dynamic date column name
    latest_date = df_weights[date_level_name].max()
    if pd.isna(latest_date):
        log.warning(f"Could not determine the latest date in '{weights_filepath}'.")
        return pd.Series(dtype=bool)
    log.info(f"Latest date in weights file '{weights_filepath}': {latest_date}")
    # Filter for the latest date using the dynamic date column name
    latest_weights = df_weights[df_weights[date_level_name] == latest_date]
    # Ensure 'Value' is numeric before comparison
    latest_weights['Value'] = pd.to_numeric(latest_weights['Value'], errors='coerce')
    # Set index using the dynamic ID column name and check weight > 0
    held_status = latest_weights.set_index(id_level_name)['Value'].fillna(0) > 0 # Fillna before comparison
    held_status.name = 'is_held' # Name the series for easier merging later
    log.info(f"Determined held status for {len(held_status)} IDs based on weights on {latest_date}.")
    return held_status
def load_spread_duration_comparison_data(data_folder_path: str, file1='sec_Spread duration.csv', file2='sec_Spread durationSP.csv'):
    """Loads, processes, merges data from two security spread duration files, and gets held status.
    Args:
        data_folder_path (str): The absolute path to the data folder.
        file1 (str, optional): Filename for the first dataset. Defaults to 'sec_Spread duration.csv'.
        file2 (str, optional): Filename for the second dataset. Defaults to 'sec_Spread durationSP.csv'.
    Returns:
        tuple: (merged_df, static_data, common_static_cols, id_col_name, held_status)
               Returns (pd.DataFrame(), pd.DataFrame(), [], None, pd.Series(dtype=bool)) on error.
    """
    log.info(f"Loading spread duration comparison data: {file1} and {file2} from {data_folder_path}")
    if not data_folder_path:
        log.error("No data_folder_path provided to load_spread_duration_comparison_data.")
        return pd.DataFrame(), pd.DataFrame(), [], None, pd.Series(dtype=bool)
    # Load held status first
    held_status = load_weights_and_held_status(data_folder_path)
    # Load main data files
    df1, static_cols1 = load_and_process_security_data(file1, data_folder_path)
    df2, static_cols2 = load_and_process_security_data(file2, data_folder_path)
    if df1.empty or df2.empty:
        log.warning(f"One or both spread duration dataframes are empty after loading. File1 empty: {df1.empty}, File2 empty: {df2.empty}")
        return pd.DataFrame(), pd.DataFrame(), [], None, held_status
    # --- Verify Index and Get Actual Names ---
    if df1.index.nlevels != 2 or df2.index.nlevels != 2:
        log.error("One or both spread duration dataframes do not have the expected 2 index levels after loading.")
        return pd.DataFrame(), pd.DataFrame(), [], None, held_status
    date_level_name, id_level_name = df1.index.names
    log.info(f"Spread duration data index levels identified: Date='{date_level_name}', ID='{id_level_name}'")
    # --- Reset Index ---
    df1 = df1.reset_index()
    df2 = df2.reset_index()
    log.debug(f"Spread duration df1 columns after reset: {df1.columns.tolist()}")
    log.debug(f"Spread duration df2 columns after reset: {df2.columns.tolist()}")
    # --- Check Required Columns (Post-Reset) ---
    required_cols_df1 = [id_level_name, date_level_name, 'Value']
    required_cols_df2 = [id_level_name, date_level_name, 'Value']
    missing_cols_df1 = [col for col in required_cols_df1 if col not in df1.columns]
    missing_cols_df2 = [col for col in required_cols_df2 if col not in df2.columns]
    if missing_cols_df1 or missing_cols_df2:
        log.error(f"Missing required columns after index reset in spread duration. Df1 missing: {missing_cols_df1}, Df2 missing: {missing_cols_df2}")
        return pd.DataFrame(), pd.DataFrame(), [], None, held_status
    # --- Prepare for Merge ---
    common_static_cols = list(set(static_cols1) & set(static_cols2))
    if id_level_name in common_static_cols:
        common_static_cols.remove(id_level_name)
    if 'Value' in common_static_cols:
        common_static_cols.remove('Value')
    try:
        df1_merge = df1[[id_level_name, date_level_name, 'Value'] + common_static_cols].rename(columns={'Value': 'Value_Orig'})
        df2_merge = df2[[id_level_name, date_level_name, 'Value']].rename(columns={'Value': 'Value_New'})
    except KeyError as e:
        log.error(f"KeyError during spread duration merge prep using names '{id_level_name}', '{date_level_name}': {e}")
        return pd.DataFrame(), pd.DataFrame(), [], None, held_status
    # --- Perform Merge and Calculate Changes ---
    merged_df = pd.merge(df1_merge, df2_merge, on=[id_level_name, date_level_name], how='outer')
    merged_df = merged_df.sort_values(by=[id_level_name, date_level_name])
    merged_df['Change_Orig'] = merged_df.groupby(id_level_name)['Value_Orig'].diff()
    merged_df['Change_New'] = merged_df.groupby(id_level_name)['Value_New'].diff()
    # --- Extract Static Data ---
    static_data = merged_df.groupby(id_level_name)[common_static_cols].last().reset_index()
    log.info(f"Successfully merged spread duration data. Shape: {merged_df.shape}")
    return merged_df, static_data, common_static_cols, id_level_name, held_status
def calculate_comparison_stats(merged_df, static_data, id_col):
    """Calculates comparison statistics for each security's spread duration.
    Args:
        merged_df (pd.DataFrame): The merged dataframe of original and new spread duration values.
        static_data (pd.DataFrame): DataFrame with static info per security.
        id_col (str): The name of the column containing the Security ID/Name.
    """
    if merged_df.empty:
        return pd.DataFrame()
    if id_col not in merged_df.columns:
        log.error(f"Specified id_col '{id_col}' not found in merged_df columns: {merged_df.columns.tolist()}")
        return pd.DataFrame() # Cannot group without the ID column
    log.info(f"Calculating spread duration comparison statistics using ID column: {id_col}...")
    stats_list = []
    # Use the passed id_col here
    for sec_id, group in merged_df.groupby(id_col):
        sec_stats = {id_col: sec_id} # Use actual id_col name
        # Filter out rows where both values are NaN for overall analysis period
        group_valid_overall = group.dropna(subset=['Value_Orig', 'Value_New'], how='all')
        overall_min_date = group_valid_overall['Date'].min()
        overall_max_date = group_valid_overall['Date'].max()
        # Filter out rows where EITHER value is NaN for correlation/diff calculations
        valid_comparison = group.dropna(subset=['Value_Orig', 'Value_New'])
        # 1. Correlation of Levels
        if len(valid_comparison) >= 2: # Need at least 2 points for correlation
            # Use the NaN-dropped dataframe for correlation
            level_corr = valid_comparison['Value_Orig'].corr(valid_comparison['Value_New'])
            sec_stats['Level_Correlation'] = level_corr if pd.notna(level_corr) else None
        else:
             sec_stats['Level_Correlation'] = None
        # 2. Max / Min (use original group to get true max/min including non-overlapping points)
        sec_stats['Max_Orig'] = group['Value_Orig'].max()
        sec_stats['Min_Orig'] = group['Value_Orig'].min()
        sec_stats['Max_New'] = group['Value_New'].max()
        sec_stats['Min_New'] = group['Value_New'].min()
        # 3. Date Range Comparison - Refined Logic
        # Find min/max dates within the MERGED data where each series is individually valid
        min_date_orig_idx = group['Value_Orig'].first_valid_index()
        max_date_orig_idx = group['Value_Orig'].last_valid_index()
        min_date_new_idx = group['Value_New'].first_valid_index()
        max_date_new_idx = group['Value_New'].last_valid_index()
        sec_stats['Start_Date_Orig'] = group.loc[min_date_orig_idx, 'Date'] if min_date_orig_idx is not None else None
        sec_stats['End_Date_Orig'] = group.loc[max_date_orig_idx, 'Date'] if max_date_orig_idx is not None else None
        sec_stats['Start_Date_New'] = group.loc[min_date_new_idx, 'Date'] if min_date_new_idx is not None else None
        sec_stats['End_Date_New'] = group.loc[max_date_new_idx, 'Date'] if max_date_new_idx is not None else None
        # Check if the start and end dates MATCH for the valid periods of EACH series
        same_start = pd.Timestamp(sec_stats['Start_Date_Orig']) == pd.Timestamp(sec_stats['Start_Date_New']) if sec_stats['Start_Date_Orig'] and sec_stats['Start_Date_New'] else False
        same_end = pd.Timestamp(sec_stats['End_Date_Orig']) == pd.Timestamp(sec_stats['End_Date_New']) if sec_stats['End_Date_Orig'] and sec_stats['End_Date_New'] else False
        sec_stats['Same_Date_Range'] = same_start and same_end
        # Add overall date range for info
        sec_stats['Overall_Start_Date'] = overall_min_date
        sec_stats['Overall_End_Date'] = overall_max_date
        # 4. Correlation of Daily Changes (Volatility Alignment)
        # Use the dataframe where BOTH values are non-NaN to calculate changes for correlation
        valid_comparison = valid_comparison.copy() # Avoid SettingWithCopyWarning
        valid_comparison['Change_Orig_Corr'] = valid_comparison['Value_Orig'].diff()
        valid_comparison['Change_New_Corr'] = valid_comparison['Value_New'].diff()
        # Drop NaNs created by the diff() itself (first row)
        valid_changes = valid_comparison.dropna(subset=['Change_Orig_Corr', 'Change_New_Corr'])
        if len(valid_changes) >= 2:
            change_corr = valid_changes['Change_Orig_Corr'].corr(valid_changes['Change_New_Corr'])
            sec_stats['Change_Correlation'] = change_corr if pd.notna(change_corr) else None
        else:
            sec_stats['Change_Correlation'] = None
            log.debug(f"Cannot calculate Spread Duration Change_Correlation for {sec_id}. Need >= 2 valid change pairs, found {len(valid_changes)}.")
        # 5. Difference Statistics (use the valid_comparison df where both values exist)
        valid_comparison['Abs_Diff'] = (valid_comparison['Value_Orig'] - valid_comparison['Value_New']).abs()
        sec_stats['Mean_Abs_Diff'] = valid_comparison['Abs_Diff'].mean() # Mean diff where both values exist
        sec_stats['Max_Abs_Diff'] = valid_comparison['Abs_Diff'].max() # Max diff where both values exist
        # Count NaNs - use original group
        sec_stats['NaN_Count_Orig'] = group['Value_Orig'].isna().sum()
        sec_stats['NaN_Count_New'] = group['Value_New'].isna().sum()
        sec_stats['Total_Points'] = len(group)
        stats_list.append(sec_stats)
    summary_df = pd.DataFrame(stats_list)
    # Merge static data back
    if not static_data.empty and id_col in static_data.columns and id_col in summary_df.columns:
        summary_df = pd.merge(summary_df, static_data, on=id_col, how='left')
    elif not static_data.empty:
         log.warning(f"Could not merge static data back for spread duration comparison. ID column '{id_col}' missing from static_data ({id_col in static_data.columns}) or summary_df ({id_col in summary_df.columns}).")
    log.info(f"Finished calculating spread duration stats. Summary shape: {summary_df.shape}")
    return summary_df
# --- Routes ---
@spread_duration_comparison_bp.route('/spread_duration_comparison/summary') # Updated route
def summary():
    """Displays the spread duration comparison summary page with server-side filtering, sorting, and pagination."""
    log.info("--- Starting Spread Duration Comparison Summary Request ---")
    # Retrieve the configured absolute data folder path
    data_folder = current_app.config['DATA_FOLDER']
    if not data_folder:
        current_app.logger.error("DATA_FOLDER is not configured in the application.")
        return "Internal Server Error: Data folder not configured", 500
    try:
        # --- Get Request Parameters ---
        page = request.args.get('page', 1, type=int)
        sort_by = request.args.get('sort_by', 'Change_Correlation') # Default sort
        sort_order = request.args.get('sort_order', 'desc').lower()
        if sort_order not in ['asc', 'desc']:
            sort_order = 'desc'
        ascending = sort_order == 'asc'
        # NEW: Get holding status filter
        show_sold = request.args.get('show_sold', 'false').lower() == 'true'
        # Get active filters (ensuring keys are correct)
        active_filters = {k.replace('filter_', ''): v
                          for k, v in request.args.items()
                          if k.startswith('filter_') and v}
        log.info(f"Request Params: Page={page}, SortBy={sort_by}, Order={sort_order}, Filters={active_filters}, ShowSold={show_sold}")
        # --- Load and Prepare Data ---
        merged_data, static_data, static_cols, actual_id_col, held_status = load_spread_duration_comparison_data(data_folder)
        if actual_id_col is None:
            log.error("Failed to get ID column name during spread duration data loading.")
            return "Error loading spread duration comparison data: Could not determine ID column.", 500
        summary_stats = calculate_comparison_stats(merged_data, static_data, id_col=actual_id_col)
        if summary_stats.empty:
             log.info("No spread duration summary statistics could be calculated.")
             return render_template('spread_duration_comparison_page.html', # Updated template
                                    table_data=[],
                                    columns_to_display=[],
                                    id_column_name=actual_id_col,
                                    filter_options={},
                                    active_filters={},
                                    current_sort_by=sort_by,
                                    current_sort_order=sort_order,
                                    pagination=None,
                                    show_sold=show_sold, # Pass filter status
                                    message="No spread duration comparison data available.")
        # --- Merge Held Status --- 
        if not held_status.empty and actual_id_col in summary_stats.columns:
            summary_stats = pd.merge(summary_stats, held_status, left_on=actual_id_col, right_index=True, how='left')
            summary_stats['is_held'] = summary_stats['is_held'].fillna(False)
            log.info(f"Merged held status. Stats shape: {summary_stats.shape}")
        else:
            log.warning("Could not merge held status for spread duration data.")
            summary_stats['is_held'] = False
        # --- Apply Holding Status Filter --- 
        original_count = len(summary_stats)
        if not show_sold:
            summary_stats = summary_stats[summary_stats['is_held'] == True]
            log.info(f"Applied 'Show Held Only' filter. Kept {len(summary_stats)} out of {original_count} securities.")
        else:
            log.info("Skipping 'Show Held Only' filter (show_sold is True).")
        if summary_stats.empty:
             log.info("No securities remaining after applying holding status filter.")
             return render_template('spread_duration_comparison_page.html', # Updated template
                                    table_data=[],
                                    columns_to_display=[actual_id_col] + static_cols, # Show basic cols
                                    id_column_name=actual_id_col,
                                    filter_options={},
                                    active_filters={},
                                    current_sort_by=sort_by,
                                    current_sort_order=sort_order,
                                    pagination=None,
                                    show_sold=show_sold, # Pass filter status
                                    message="No currently held securities found.")
        # --- Collect Filter Options (From Data *After* Holding Filter) --- 
        filter_options = {}
        potential_filter_cols = static_cols 
        for col in potential_filter_cols:
            if col in summary_stats.columns:
                unique_vals = summary_stats[col].dropna().unique().tolist()
                try:
                    sorted_vals = sorted(unique_vals, key=lambda x: (isinstance(x, (int, float)), x))
                except TypeError:
                    sorted_vals = sorted(unique_vals, key=str)
                filter_options[col] = sorted_vals
        final_filter_options = dict(sorted(filter_options.items())) # Sort filter dropdowns alphabetically
        log.info(f"Filter options generated: {list(final_filter_options.keys())}") # Use final_filter_options
        # --- Apply Static Column Filters --- 
        filtered_data = summary_stats.copy()
        if active_filters:
            log.info(f"Applying static column filters: {active_filters}")
            for col, value in active_filters.items():
                if col in filtered_data.columns and value:
                    try:
                        # Robust string comparison
                         filtered_data = filtered_data[filtered_data[col].astype(str).str.lower() == str(value).lower()]
                    except Exception as e:
                        log.warning(f"Could not apply filter for column '{col}' with value '{value}'. Error: {e}. Skipping filter.")
                else:
                    log.warning(f"Filter column '{col}' not found in data. Skipping filter.")
            log.info(f"Data shape after static filtering: {filtered_data.shape}")
        else:
            log.info("No active static column filters.")
        if filtered_data.empty:
             log.info("No data remaining after applying static column filters.")
             return render_template('spread_duration_comparison_page.html', # Updated template
                                    table_data=[],
                                    columns_to_display=[actual_id_col] + static_cols, # Show basic cols
                                    id_column_name=actual_id_col,
                                    filter_options=final_filter_options, # Show filter options
                                    active_filters=active_filters,
                                    current_sort_by=sort_by,
                                    current_sort_order=sort_order,
                                    pagination=None,
                                    show_sold=show_sold, # Pass filter status
                                    message="No data matches the current filters.")
        # --- Apply Sorting ---
        if sort_by in filtered_data.columns:
            log.info(f"Sorting by '{sort_by}' ({'Ascending' if ascending else 'Descending'})")
            na_position = 'last' 
            try:
                filtered_data = filtered_data.sort_values(by=sort_by, ascending=ascending, na_position=na_position)
            except Exception as e:
                log.error(f"Error during sorting by '{sort_by}': {e}. Falling back to default sort.")
                sort_by = 'Change_Correlation' 
                ascending = False
                filtered_data = filtered_data.sort_values(by=sort_by, ascending=ascending, na_position=na_position)
        else:
            log.warning(f"Sort column '{sort_by}' not found. Using default ID sort.")
            sort_by = actual_id_col 
            ascending = True
            filtered_data = filtered_data.sort_values(by=actual_id_col, ascending=ascending, na_position='last')
        # --- Pagination ---
        total_items = len(filtered_data)
        safe_per_page = max(1, PER_PAGE_COMPARISON)
        total_pages = math.ceil(total_items / safe_per_page)
        total_pages = max(1, total_pages)
        page = max(1, min(page, total_pages))
        start_index = (page - 1) * safe_per_page
        end_index = start_index + safe_per_page
        paginated_data = filtered_data.iloc[start_index:end_index]
        log.info(f"Pagination: Total items={total_items}, Total pages={total_pages}, Current page={page}, Displaying items {start_index}-{end_index-1}")
        page_window = 2
        start_page_display = max(1, page - page_window)
        end_page_display = min(total_pages, page + page_window)
        # --- Prepare for Template ---
        base_cols = [
            'Level_Correlation', 'Change_Correlation',
            'Mean_Abs_Diff', 'Max_Abs_Diff',
            'NaN_Count_Orig', 'NaN_Count_New', 'Total_Points',
            'Same_Date_Range',
            # Add/remove columns as needed
        ]
        columns_to_display = [actual_id_col] + \
                             [col for col in static_cols if col != actual_id_col and col in paginated_data.columns] + \
                             [col for col in base_cols if col in paginated_data.columns]
        table_data = paginated_data.to_dict(orient='records')
        # Format specific columns 
        for row in table_data:
            for col in ['Level_Correlation', 'Change_Correlation']:
                 if col in row and pd.notna(row[col]):
                    row[col] = f"{row[col]:.4f}" 
            # Add date formatting if needed for stats cols
        # Create pagination object
        pagination_context = {
            'page': page,
            'per_page': safe_per_page,
            'total_items': total_items,
            'total_pages': total_pages,
            'has_prev': page > 1,
            'has_next': page < total_pages,
            'prev_num': page - 1,
            'next_num': page + 1,
            'start_page_display': start_page_display,
            'end_page_display': end_page_display,
            # Function to generate URLs for pagination links, preserving state
             'url_for_page': lambda p: url_for('spread_duration_comparison_bp.summary', 
                                              page=p, 
                                              sort_by=sort_by, 
                                              sort_order=sort_order, 
                                              show_sold=str(show_sold).lower(), # Pass holding status
                                              **{f'filter_{k}': v for k, v in active_filters.items()})
        }
        log.info("--- Successfully Prepared Data for Spread Duration Comparison Template ---")
        return render_template('spread_duration_comparison_page.html', # Updated template
                               table_data=table_data,
                               columns_to_display=columns_to_display,
                               id_column_name=actual_id_col, # Pass the ID column name
                               filter_options=final_filter_options,
                               active_filters=active_filters,
                               current_sort_by=sort_by,
                               current_sort_order=sort_order,
                               pagination=pagination_context,
                               show_sold=show_sold, # Pass holding filter status
                               message=None) # No message if data is present
    except FileNotFoundError as e:
        log.error(f"Spread duration comparison file not found: {e}")
        return f"Error: Required spread duration comparison file not found ({e.filename}). Check the Data folder.", 404
    except Exception as e:
        log.exception("An unexpected error occurred in the spread duration comparison summary view.") # Log full traceback
        return render_template('spread_duration_comparison_page.html', 
                               message=f"An unexpected error occurred: {e}",
                               table_data=[], pagination=None, filter_options={}, 
                               active_filters={}, show_sold=show_sold, columns_to_display=[], 
                               id_column_name='Security') # Include show_sold in error template
@spread_duration_comparison_bp.route('/spread_duration_comparison/details/<path:security_id>')
def spread_duration_comparison_details(security_id):
    """Displays side-by-side historical spread duration charts for a specific security."""
    log.info(f"--- Starting Spread Duration Comparison Detail Request for Security ID: {security_id} ---")
    # Retrieve the configured absolute data folder path
    data_folder = current_app.config['DATA_FOLDER']
    if not data_folder:
        current_app.logger.error("DATA_FOLDER is not configured in the application.")
        return "Internal Server Error: Data folder not configured", 500
    # Decode the security_id from URL encoding
    decoded_security_id = unquote(security_id)
    log.info(f"Decoded Security ID: {decoded_security_id}")
    try:
        # Pass the absolute data folder path
        merged_data, static_data, common_static_cols, id_col_name, _ = load_spread_duration_comparison_data(data_folder)
        if id_col_name is None:
             log.error(f"Failed to get ID column name for details view (Security: {decoded_security_id}).") # Use decoded ID
             return "Error loading spread duration comparison data: Could not determine ID column.", 500
        if merged_data.empty:
            log.warning(f"Merged spread duration data is empty for details view (Security: {decoded_security_id}).") # Use decoded ID
            return f"No merged spread duration data found for Security ID: {decoded_security_id}", 404 # Use decoded ID
        # --- Debugging: Log ID column and sample IDs from DataFrame --- START
        log.info(f"Identified ID column name: '{id_col_name}'")
        if id_col_name in merged_data.columns:
             sample_ids = merged_data[id_col_name].unique()[:5] # Get first 5 unique IDs
             log.info(f"Sample IDs from DataFrame column '{id_col_name}': {sample_ids}")
             log.info(f"Data type of column '{id_col_name}': {merged_data[id_col_name].dtype}")
        else:
            log.warning(f"ID column '{id_col_name}' not found in merged_data columns for sampling.")
        # --- Debugging: Log ID column and sample IDs from DataFrame --- END
        # Filter data for the specific security using the DECODED ID and correct ID column name
        security_data = merged_data[merged_data[id_col_name] == decoded_security_id].copy() # Use decoded_security_id
        if security_data.empty:
            log.warning(f"No spread duration data found after filtering for the specific Security ID: {decoded_security_id}") # Use decoded ID
            # Consider checking if the ID exists in the original files?
            return f"Spread Duration data not found for Security ID: {decoded_security_id}", 404 # Use decoded ID
        # Get static info for this security (handle potential multiple rows if ID isn't unique, take first)
        static_info = security_data[[id_col_name] + common_static_cols].iloc[0].to_dict() if not security_data.empty else {}
        # Sort by date for charting
        security_data = security_data.sort_values(by='Date')
        # Prepare data for Chart.js
        # Ensure 'Date' is in the correct string format for JSON/JS
        security_data['Date_Str'] = security_data['Date'].dt.strftime('%Y-%m-%d')
        chart_data = {
            'labels': security_data['Date_Str'].tolist(),
            'datasets': [
                {
                    'label': 'Original Spread Duration', # Updated Label
                    'data': security_data['Value_Orig'].where(pd.notna(security_data['Value_Orig']), None).tolist(), # Replace NaN with None for JSON
                    'borderColor': COLOR_PALETTE[0 % len(COLOR_PALETTE)],
                    'fill': False,
                    'tension': 0.1
                },
                {
                    'label': 'New Spread Duration', # Updated Label
                    'data': security_data['Value_New'].where(pd.notna(security_data['Value_New']), None).tolist(), # Replace NaN with None for JSON
                    'borderColor': COLOR_PALETTE[1 % len(COLOR_PALETTE)],
                    'fill': False,
                    'tension': 0.1
                }
            ]
        }
        # Calculate overall statistics for this security
        stats_summary = calculate_comparison_stats(security_data, pd.DataFrame([static_info]), id_col=id_col_name) # Pass single security data
        stats_dict = stats_summary.iloc[0].to_dict() if not stats_summary.empty else {}
         # Format dates and numbers in stats_dict before passing
        for key, value in stats_dict.items():
            if isinstance(value, pd.Timestamp):
                stats_dict[key] = value.strftime('%Y-%m-%d')
            elif isinstance(value, (int, float)):
                 if 'Correlation' in key and pd.notna(value):
                     stats_dict[key] = f"{value:.4f}"
                 elif 'Diff' in key and pd.notna(value):
                      stats_dict[key] = f"{value:.2f}" # Adjust formatting as needed
        log.info(f"Successfully prepared data for spread duration details template (Security: {decoded_security_id})") # Use decoded ID
        return render_template('spread_duration_comparison_details_page.html', # Updated template
                               security_id=decoded_security_id, # Pass decoded ID to template
                               static_info=static_info, # Pass static info
                               chart_data=chart_data,
                               stats_summary=stats_dict) # Pass calculated stats
    except FileNotFoundError as e:
        log.error(f"Spread duration comparison file not found for details view: {e} (Security: {decoded_security_id})") # Use decoded ID
        return f"Error: Required spread duration comparison file not found ({e.filename}). Check the Data folder.", 404
    except KeyError as e:
         log.error(f"KeyError accessing data for security '{decoded_security_id}': {e}. ID column used: '{id_col_name}'") # Use decoded ID
         return f"Error accessing data for security '{decoded_security_id}'. It might be missing required columns or have unexpected formatting.", 500 # Use decoded ID
    except Exception as e:
        log.exception(f"An unexpected error occurred in the spread duration comparison details view for security '{decoded_security_id}'.") # Use decoded ID
        return f"An internal error occurred while processing details for security '{decoded_security_id}': {e}", 500 # Use decoded ID
</file>

<file path="views/weight_views.py">
# views/weight_views.py
# Purpose: Handles routes related to weight checks (e.g., ensuring weights are 100%).
import os
import pandas as pd
import traceback
import logging
from flask import Blueprint, render_template, current_app
# Define the blueprint
weight_bp = Blueprint('weight', __name__, url_prefix='/weights')
def _parse_percentage(value):
    """Attempts to parse a string like '99.5%' into a float 99.5."""
    if pd.isna(value) or value == '':
        return None
    try:
        # Remove '%' and convert to float
        return float(str(value).replace('%', '').strip())
    except (ValueError, TypeError):
        logging.warning(f"Could not parse percentage value: {value}")
        return None # Indicate parsing failure
def _is_date_like_column(col_name):
    """Checks if a column name matches YYYY-MM-DD or YYYY-MM-DDTHH:MM:SS format."""
    try:
        # Attempt to parse using pandas with flexible inference first
        # errors='raise' will fail if it's not recognizable as a date/datetime
        pd.to_datetime(col_name, errors='raise')
        # Optionally, add more specific format checks if needed, but `to_datetime` is quite good
        # e.g., check if it matches common regex patterns if `to_datetime` is too broad
        return True
    except (ValueError, TypeError):
        return False
def load_and_process_weight_data(data_folder_path: str, filename: str):
    """Loads a wide weight file, processes percentages, checks against 100%.
    Args:
        data_folder_path (str): The absolute path to the data folder.
        filename (str): The name of the weight file (e.g., 'w_Funds.csv').
    Returns:
        tuple: (dict | None, list[str])
               - Dictionary of processed data {fund_code: {date: {data}}} or None on error.
               - List of sorted date headers (YYYY-MM-DD) found in the file.
    """
    if not data_folder_path:
        logging.error(f"No data_folder_path provided for file {filename}")
        return None, []
    filepath = os.path.join(data_folder_path, filename)
    if not os.path.exists(filepath):
        logging.error(f"Weight file not found: {filepath}")
        return None, []
    try:
        df = pd.read_csv(filepath, encoding='utf-8')
        df.columns = df.columns.str.strip() # Ensure no leading/trailing spaces
        # Identify ID column (assuming 'Fund Code')
        id_col = 'Fund Code'
        if id_col not in df.columns:
            logging.error(f"Required column '{id_col}' not found in {filename}")
            return None, []
        # Identify date columns based on ISO format
        date_cols = [col for col in df.columns if _is_date_like_column(col)]
        if not date_cols:
            logging.warning(f"No date-like columns found in {filename}")
            return None, []
        # Sort date columns chronologically
        # Convert to datetime objects for reliable sorting
        datetime_objs = [pd.to_datetime(d) for d in date_cols]
        # Sort based on datetime objects
        datetime_objs.sort()
        # Convert back to display format (YYYY-MM-DD) after sorting
        date_headers_display = [dt.strftime('%Y-%m-%d') for dt in datetime_objs]
        # Get the original column names corresponding to the sorted display headers
        # We need this to access the correct columns in the DataFrame later
        # Create a map from display format back to original format
        display_to_original_map = {pd.to_datetime(d).strftime('%Y-%m-%d'): d for d in date_cols}
        original_date_cols_sorted = [display_to_original_map[d] for d in date_headers_display]
        processed_data = {}
        # Set index for easier access
        df.set_index(id_col, inplace=True)
        for fund_code in df.index:
            processed_data[fund_code] = {}
            # Iterate using the sorted original column names and the sorted display headers
            for original_date_col, display_date_header in zip(original_date_cols_sorted, date_headers_display):
                original_value_str = str(df.loc[fund_code, original_date_col])
                parsed_value_float = _parse_percentage(original_value_str)
                is_100 = False
                if parsed_value_float is not None:
                    # Check if the value is within the tolerance range [99.99, 100.01]
                    is_100 = abs(parsed_value_float - 100.0) <= 0.01
                processed_data[fund_code][display_date_header] = {
                    'value_str': original_value_str if not pd.isna(original_value_str) else 'N/A',
                    'is_100': is_100,
                    'parsed_value': parsed_value_float # Keep for potential future use/debugging
                }
        return processed_data, date_headers_display
    except Exception as e:
        logging.error(f"Error processing weight file {filename}: {e}")
        traceback.print_exc()
        return None, []
@weight_bp.route('/check')
def weight_check():
    """Displays the weight check page."""
    # Retrieve the configured absolute data folder path
    data_folder = current_app.config['DATA_FOLDER']
    if not data_folder:
        current_app.logger.error("DATA_FOLDER is not configured in the application.")
        return "Internal Server Error: Data folder not configured", 500
    fund_filename = 'w_Funds.csv'
    bench_filename = 'w_Bench.csv'
    # Pass the absolute data folder path to the helper function
    fund_data, fund_date_headers = load_and_process_weight_data(data_folder, fund_filename)
    bench_data, bench_date_headers = load_and_process_weight_data(data_folder, bench_filename)
    # Use the longer list of dates as the canonical header list, assuming they might differ slightly
    all_date_headers = sorted(list(set(fund_date_headers + bench_date_headers)))
    return render_template('weight_check_page.html',
                           fund_data=fund_data,
                           bench_data=bench_data,
                           date_headers=all_date_headers, # Pass combined, sorted list
                           fund_filename=fund_filename,
                           bench_filename=bench_filename)
</file>

<file path="weight_processing.py">
# weight_processing.py
# This script provides functionality to process weight files (e.g., w_Funds.csv).
# It reads a weight file, identifies the relevant columns, and saves the processed data
# to a specified output path. (Original header replacement logic is removed as per the
# simplification in process_data.py's call).
import pandas as pd
import logging
import os
import io
# Get the logger instance. Assumes Flask app has configured logging.
logger = logging.getLogger(__name__)
def process_weight_file(input_path: str, output_path: str):
    """
    Reads a weight CSV file, performs necessary processing (if any), and saves
    it to the specified output path.
    Currently, this function primarily copies the file, assuming pre-processing
    (like header replacement) might happen elsewhere or is not needed for weights.
    Add specific weight processing logic here if required in the future.
    Args:
        input_path (str): Absolute path to the input weight CSV file (e.g., w_Funds.csv).
        output_path (str): Absolute path where the processed weight file should be saved.
    """
    if not os.path.exists(input_path):
        logger.error(f"Weight file not found: {input_path}. Skipping processing.")
        return
    logger.info(f"Processing weight file: {input_path} -> {output_path}")
    try:
        # Read the input CSV - add robustness
        df = pd.read_csv(input_path, on_bad_lines='skip', encoding='utf-8', encoding_errors='replace')
        # Log DataFrame info at DEBUG level
        buf = io.StringIO()
        df.info(verbose=True, buf=buf)
        logger.debug(f"DataFrame info after read for {input_path}:\n{buf.getvalue()}")
        if df.empty:
            logger.warning(f"Weight file {input_path} is empty or contains only invalid lines. Saving empty file to {output_path}.")
            # Save an empty file or a file with just headers, depending on desired behavior
            df.to_csv(output_path, index=False, encoding='utf-8')
            return
        # --- Placeholder for future weight-specific processing --- 
        # Example: Rename columns, calculate new metrics, filter rows, etc.
        # df['NewWeight'] = df['SomeWeight'] * 100 
        # logger.info(f"Applied custom processing to weight data from {input_path}.")
        # --- End Placeholder --- 
        # Save the processed DataFrame to the output path
        df.to_csv(output_path, index=False, encoding='utf-8')
        logger.info(f"Successfully processed and saved weight file to: {output_path}")
    except FileNotFoundError:
        # This case is handled by the initial check, but included for completeness
        logger.error(f"Error: Input weight file not found during processing - {input_path}")
    except pd.errors.EmptyDataError:
         logger.warning(f"Weight file is empty - {input_path}. Skipping save.")
    except pd.errors.ParserError as pe:
        logger.error(f"Error parsing CSV weight file {input_path}: {pe}. Check file format and integrity.", exc_info=True)
    except Exception as e:
        logger.error(f"An unexpected error occurred processing weight file {input_path} to {output_path}: {e}", exc_info=True)
# Example usage note:
# This script is typically called by process_data.py, which provides
# the absolute input and output paths derived from the configured data directory.
# Standalone execution would require manual path specification.
#
# Example (for understanding, not direct execution without setup):
# if __name__ == "__main__":
#    # Requires manual setup of paths if run standalone
#    test_input_dir = '/path/to/your/data' # Replace with actual path
#    test_input_file = os.path.join(test_input_dir, 'w_Funds.csv')
#    test_output_file = os.path.join(test_input_dir, 'w_Funds_Processed.csv')
#
#    if os.path.exists(test_input_file):
#        print(f"Testing weight processing: {test_input_file} -> {test_output_file}")
#        process_weight_file(test_input_file, test_output_file)
#    else:
#        print(f"Test input file not found: {test_input_file}")
</file>

</files>
